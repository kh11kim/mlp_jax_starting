{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jax\n",
    "from jax import random\n",
    "import jax.numpy as jnp\n",
    "from network import *\n",
    "from train import *\n",
    "from dataset import *\n",
    "from loss import *\n",
    "from utils import *\n",
    "from flax.training.train_state import TrainState\n",
    "import torch.utils.data as data\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = Hyperparam()\n",
    "hyperparams.layers = [2, 10, 10, 1]\n",
    "hyperparams.lr = 0.001\n",
    "hyperparams.batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "df = pd.read_csv(\"circle.csv\")\n",
    "dataset = NumpyDataset(df[[\"x\", \"y\"]].to_numpy(), df[\"d\"].to_numpy())\n",
    "train_dataset, val_dataset = train_test_split(dataset, train_size=0.9, shuffle=True)\n",
    "train_loader = data.DataLoader(\n",
    "    train_dataset, batch_size=hyperparams.batch_size, shuffle=True, collate_fn=numpy_collate)\n",
    "val_loader = data.DataLoader(\n",
    "    val_dataset, batch_size=hyperparams.batch_size, collate_fn=numpy_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(hyperparams.layers)\n",
    "\n",
    "key1, key2 = random.split(random.PRNGKey(0))\n",
    "x = random.normal(key1, (2,)) # Dummy input data\n",
    "params = model.init(key2, x) # Initialization call\n",
    "tx = optax.adam(learning_rate=hyperparams.lr)\n",
    "state = TrainState.create(apply_fn=model.apply, params=params, tx=tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: EPOCH 1/1000 | BATCH 0/71 | LOSS: 0.3886238932609558\n",
      "TRAIN: EPOCH 1/1000 | BATCH 1/71 | LOSS: 0.39779481291770935\n",
      "TRAIN: EPOCH 1/1000 | BATCH 2/71 | LOSS: 0.4026128053665161\n",
      "TRAIN: EPOCH 1/1000 | BATCH 3/71 | LOSS: 0.39013029634952545\n",
      "TRAIN: EPOCH 1/1000 | BATCH 4/71 | LOSS: 0.3844456195831299\n",
      "TRAIN: EPOCH 1/1000 | BATCH 5/71 | LOSS: 0.3762319087982178\n",
      "TRAIN: EPOCH 1/1000 | BATCH 6/71 | LOSS: 0.37614625692367554\n",
      "TRAIN: EPOCH 1/1000 | BATCH 7/71 | LOSS: 0.36857252195477486\n",
      "TRAIN: EPOCH 1/1000 | BATCH 8/71 | LOSS: 0.3647873236073388\n",
      "TRAIN: EPOCH 1/1000 | BATCH 9/71 | LOSS: 0.3602416843175888\n",
      "TRAIN: EPOCH 1/1000 | BATCH 10/71 | LOSS: 0.36205787279389123\n",
      "TRAIN: EPOCH 1/1000 | BATCH 11/71 | LOSS: 0.35807065417369205\n",
      "TRAIN: EPOCH 1/1000 | BATCH 12/71 | LOSS: 0.3524192273616791\n",
      "TRAIN: EPOCH 1/1000 | BATCH 13/71 | LOSS: 0.3483713056359972\n",
      "TRAIN: EPOCH 1/1000 | BATCH 14/71 | LOSS: 0.34549347559611004\n",
      "TRAIN: EPOCH 1/1000 | BATCH 15/71 | LOSS: 0.3419052269309759\n",
      "TRAIN: EPOCH 1/1000 | BATCH 16/71 | LOSS: 0.3403390347957611\n",
      "TRAIN: EPOCH 1/1000 | BATCH 17/71 | LOSS: 0.3376985771788491\n",
      "TRAIN: EPOCH 1/1000 | BATCH 18/71 | LOSS: 0.333180507546977\n",
      "TRAIN: EPOCH 1/1000 | BATCH 19/71 | LOSS: 0.33204384595155717\n",
      "TRAIN: EPOCH 1/1000 | BATCH 20/71 | LOSS: 0.3279536919934409\n",
      "TRAIN: EPOCH 1/1000 | BATCH 21/71 | LOSS: 0.3253982351584868\n",
      "TRAIN: EPOCH 1/1000 | BATCH 22/71 | LOSS: 0.322465295376985\n",
      "TRAIN: EPOCH 1/1000 | BATCH 23/71 | LOSS: 0.3198344086607297\n",
      "TRAIN: EPOCH 1/1000 | BATCH 24/71 | LOSS: 0.31726919651031493\n",
      "TRAIN: EPOCH 1/1000 | BATCH 25/71 | LOSS: 0.3143470407678531\n",
      "TRAIN: EPOCH 1/1000 | BATCH 26/71 | LOSS: 0.3118042587130158\n",
      "TRAIN: EPOCH 1/1000 | BATCH 27/71 | LOSS: 0.30910820034997805\n",
      "TRAIN: EPOCH 1/1000 | BATCH 28/71 | LOSS: 0.30540889758488227\n",
      "TRAIN: EPOCH 1/1000 | BATCH 29/71 | LOSS: 0.30037442545096077\n",
      "TRAIN: EPOCH 1/1000 | BATCH 30/71 | LOSS: 0.29623174859631446\n",
      "TRAIN: EPOCH 1/1000 | BATCH 31/71 | LOSS: 0.292181508615613\n",
      "TRAIN: EPOCH 1/1000 | BATCH 32/71 | LOSS: 0.2895043103983908\n",
      "TRAIN: EPOCH 1/1000 | BATCH 33/71 | LOSS: 0.2874649079406963\n",
      "TRAIN: EPOCH 1/1000 | BATCH 34/71 | LOSS: 0.28473008360181534\n",
      "TRAIN: EPOCH 1/1000 | BATCH 35/71 | LOSS: 0.2814961386223634\n",
      "TRAIN: EPOCH 1/1000 | BATCH 36/71 | LOSS: 0.27731232425651037\n",
      "TRAIN: EPOCH 1/1000 | BATCH 37/71 | LOSS: 0.27377436231625707\n",
      "TRAIN: EPOCH 1/1000 | BATCH 38/71 | LOSS: 0.27027311959327793\n",
      "TRAIN: EPOCH 1/1000 | BATCH 39/71 | LOSS: 0.2670604806393385\n",
      "TRAIN: EPOCH 1/1000 | BATCH 40/71 | LOSS: 0.2638787238336191\n",
      "TRAIN: EPOCH 1/1000 | BATCH 41/71 | LOSS: 0.2611649621810232\n",
      "TRAIN: EPOCH 1/1000 | BATCH 42/71 | LOSS: 0.2590634316899056\n",
      "TRAIN: EPOCH 1/1000 | BATCH 43/71 | LOSS: 0.25665428828109393\n",
      "TRAIN: EPOCH 1/1000 | BATCH 44/71 | LOSS: 0.25362846818235185\n",
      "TRAIN: EPOCH 1/1000 | BATCH 45/71 | LOSS: 0.2513713240623474\n",
      "TRAIN: EPOCH 1/1000 | BATCH 46/71 | LOSS: 0.2489766281970004\n",
      "TRAIN: EPOCH 1/1000 | BATCH 47/71 | LOSS: 0.2473717831696073\n",
      "TRAIN: EPOCH 1/1000 | BATCH 48/71 | LOSS: 0.2450090893069092\n",
      "TRAIN: EPOCH 1/1000 | BATCH 49/71 | LOSS: 0.24270010858774185\n",
      "TRAIN: EPOCH 1/1000 | BATCH 50/71 | LOSS: 0.2409143404049032\n",
      "TRAIN: EPOCH 1/1000 | BATCH 51/71 | LOSS: 0.23943642182992056\n",
      "TRAIN: EPOCH 1/1000 | BATCH 52/71 | LOSS: 0.23761507137766424\n",
      "TRAIN: EPOCH 1/1000 | BATCH 53/71 | LOSS: 0.23558971175441035\n",
      "TRAIN: EPOCH 1/1000 | BATCH 54/71 | LOSS: 0.23328782496127215\n",
      "TRAIN: EPOCH 1/1000 | BATCH 55/71 | LOSS: 0.23118304314890079\n",
      "TRAIN: EPOCH 1/1000 | BATCH 56/71 | LOSS: 0.22937905749208048\n",
      "TRAIN: EPOCH 1/1000 | BATCH 57/71 | LOSS: 0.227748843744911\n",
      "TRAIN: EPOCH 1/1000 | BATCH 58/71 | LOSS: 0.2259561673311864\n",
      "TRAIN: EPOCH 1/1000 | BATCH 59/71 | LOSS: 0.22438208796083928\n",
      "TRAIN: EPOCH 1/1000 | BATCH 60/71 | LOSS: 0.22276573188480783\n",
      "TRAIN: EPOCH 1/1000 | BATCH 61/71 | LOSS: 0.2203902372669789\n",
      "TRAIN: EPOCH 1/1000 | BATCH 62/71 | LOSS: 0.21879639488364022\n",
      "TRAIN: EPOCH 1/1000 | BATCH 63/71 | LOSS: 0.21709470869973302\n",
      "TRAIN: EPOCH 1/1000 | BATCH 64/71 | LOSS: 0.21519229641327492\n",
      "TRAIN: EPOCH 1/1000 | BATCH 65/71 | LOSS: 0.21333266320553693\n",
      "TRAIN: EPOCH 1/1000 | BATCH 66/71 | LOSS: 0.21199348224187964\n",
      "TRAIN: EPOCH 1/1000 | BATCH 67/71 | LOSS: 0.21084483285598896\n",
      "TRAIN: EPOCH 1/1000 | BATCH 68/71 | LOSS: 0.20911105117504147\n",
      "TRAIN: EPOCH 1/1000 | BATCH 69/71 | LOSS: 0.20786082265632494\n",
      "TRAIN: EPOCH 1/1000 | BATCH 70/71 | LOSS: 0.20642240313996732\n",
      "VAL: EPOCH 1/1000 | BATCH 0/8 | LOSS: 0.10111365467309952\n",
      "VAL: EPOCH 1/1000 | BATCH 1/8 | LOSS: 0.10427109524607658\n",
      "VAL: EPOCH 1/1000 | BATCH 2/8 | LOSS: 0.10197434326012929\n",
      "VAL: EPOCH 1/1000 | BATCH 3/8 | LOSS: 0.10046692192554474\n",
      "VAL: EPOCH 1/1000 | BATCH 4/8 | LOSS: 0.10238297879695893\n",
      "VAL: EPOCH 1/1000 | BATCH 5/8 | LOSS: 0.10257651408513387\n",
      "VAL: EPOCH 1/1000 | BATCH 6/8 | LOSS: 0.10383196707282748\n",
      "VAL: EPOCH 1/1000 | BATCH 7/8 | LOSS: 0.1018604263663292\n",
      "TRAIN: EPOCH 2/1000 | BATCH 0/71 | LOSS: 0.10331778228282928\n",
      "TRAIN: EPOCH 2/1000 | BATCH 1/71 | LOSS: 0.09947744011878967\n",
      "TRAIN: EPOCH 2/1000 | BATCH 2/71 | LOSS: 0.10142419238885243\n",
      "TRAIN: EPOCH 2/1000 | BATCH 3/71 | LOSS: 0.1007632240653038\n",
      "TRAIN: EPOCH 2/1000 | BATCH 4/71 | LOSS: 0.10163578391075134\n",
      "TRAIN: EPOCH 2/1000 | BATCH 5/71 | LOSS: 0.10483093435565631\n",
      "TRAIN: EPOCH 2/1000 | BATCH 6/71 | LOSS: 0.10309269172804696\n",
      "TRAIN: EPOCH 2/1000 | BATCH 7/71 | LOSS: 0.1017952673137188\n",
      "TRAIN: EPOCH 2/1000 | BATCH 8/71 | LOSS: 0.10037330372465982\n",
      "TRAIN: EPOCH 2/1000 | BATCH 9/71 | LOSS: 0.09867234975099563\n",
      "TRAIN: EPOCH 2/1000 | BATCH 10/71 | LOSS: 0.09921696104786613\n",
      "TRAIN: EPOCH 2/1000 | BATCH 11/71 | LOSS: 0.09899633688231309\n",
      "TRAIN: EPOCH 2/1000 | BATCH 12/71 | LOSS: 0.09672928372254738\n",
      "TRAIN: EPOCH 2/1000 | BATCH 13/71 | LOSS: 0.09598065114447049\n",
      "TRAIN: EPOCH 2/1000 | BATCH 14/71 | LOSS: 0.09456095397472382\n",
      "TRAIN: EPOCH 2/1000 | BATCH 15/71 | LOSS: 0.09412948973476887\n",
      "TRAIN: EPOCH 2/1000 | BATCH 16/71 | LOSS: 0.09299669896855074\n",
      "TRAIN: EPOCH 2/1000 | BATCH 17/71 | LOSS: 0.09191437645090951\n",
      "TRAIN: EPOCH 2/1000 | BATCH 18/71 | LOSS: 0.09161423382006194\n",
      "TRAIN: EPOCH 2/1000 | BATCH 19/71 | LOSS: 0.09069152548909187\n",
      "TRAIN: EPOCH 2/1000 | BATCH 20/71 | LOSS: 0.09054032464822133\n",
      "TRAIN: EPOCH 2/1000 | BATCH 21/71 | LOSS: 0.0902553376826373\n",
      "TRAIN: EPOCH 2/1000 | BATCH 22/71 | LOSS: 0.08994442775197652\n",
      "TRAIN: EPOCH 2/1000 | BATCH 23/71 | LOSS: 0.08913513676573832\n",
      "TRAIN: EPOCH 2/1000 | BATCH 24/71 | LOSS: 0.0878499262034893\n",
      "TRAIN: EPOCH 2/1000 | BATCH 25/71 | LOSS: 0.08762618025334981\n",
      "TRAIN: EPOCH 2/1000 | BATCH 26/71 | LOSS: 0.0875565452432191\n",
      "TRAIN: EPOCH 2/1000 | BATCH 27/71 | LOSS: 0.08652312162199191\n",
      "TRAIN: EPOCH 2/1000 | BATCH 28/71 | LOSS: 0.08645262633418214\n",
      "TRAIN: EPOCH 2/1000 | BATCH 29/71 | LOSS: 0.0863053864488999\n",
      "TRAIN: EPOCH 2/1000 | BATCH 30/71 | LOSS: 0.0860573333357611\n",
      "TRAIN: EPOCH 2/1000 | BATCH 31/71 | LOSS: 0.08554859680589288\n",
      "TRAIN: EPOCH 2/1000 | BATCH 32/71 | LOSS: 0.08526503960743095\n",
      "TRAIN: EPOCH 2/1000 | BATCH 33/71 | LOSS: 0.08495576033259139\n",
      "TRAIN: EPOCH 2/1000 | BATCH 34/71 | LOSS: 0.08419483570115907\n",
      "TRAIN: EPOCH 2/1000 | BATCH 35/71 | LOSS: 0.08342709651009904\n",
      "TRAIN: EPOCH 2/1000 | BATCH 36/71 | LOSS: 0.08292061342178164\n",
      "TRAIN: EPOCH 2/1000 | BATCH 37/71 | LOSS: 0.08253516854816362\n",
      "TRAIN: EPOCH 2/1000 | BATCH 38/71 | LOSS: 0.08216019079853328\n",
      "TRAIN: EPOCH 2/1000 | BATCH 39/71 | LOSS: 0.08169984864071012\n",
      "TRAIN: EPOCH 2/1000 | BATCH 40/71 | LOSS: 0.08124618277680583\n",
      "TRAIN: EPOCH 2/1000 | BATCH 41/71 | LOSS: 0.08094105913880326\n",
      "TRAIN: EPOCH 2/1000 | BATCH 42/71 | LOSS: 0.08091213926672935\n",
      "TRAIN: EPOCH 2/1000 | BATCH 43/71 | LOSS: 0.0803530343215574\n",
      "TRAIN: EPOCH 2/1000 | BATCH 44/71 | LOSS: 0.07999332166380352\n",
      "TRAIN: EPOCH 2/1000 | BATCH 45/71 | LOSS: 0.08007141364657361\n",
      "TRAIN: EPOCH 2/1000 | BATCH 46/71 | LOSS: 0.08011238435481458\n",
      "TRAIN: EPOCH 2/1000 | BATCH 47/71 | LOSS: 0.07977898077418406\n",
      "TRAIN: EPOCH 2/1000 | BATCH 48/71 | LOSS: 0.07951465918093312\n",
      "TRAIN: EPOCH 2/1000 | BATCH 49/71 | LOSS: 0.0792241470515728\n",
      "TRAIN: EPOCH 2/1000 | BATCH 50/71 | LOSS: 0.07893752014520121\n",
      "TRAIN: EPOCH 2/1000 | BATCH 51/71 | LOSS: 0.0788211990147829\n",
      "TRAIN: EPOCH 2/1000 | BATCH 52/71 | LOSS: 0.07847531055504421\n",
      "TRAIN: EPOCH 2/1000 | BATCH 53/71 | LOSS: 0.0780534409676437\n",
      "TRAIN: EPOCH 2/1000 | BATCH 54/71 | LOSS: 0.07775559906255115\n",
      "TRAIN: EPOCH 2/1000 | BATCH 55/71 | LOSS: 0.07751875151214856\n",
      "TRAIN: EPOCH 2/1000 | BATCH 56/71 | LOSS: 0.07749291123789653\n",
      "TRAIN: EPOCH 2/1000 | BATCH 57/71 | LOSS: 0.0771398988134902\n",
      "TRAIN: EPOCH 2/1000 | BATCH 58/71 | LOSS: 0.0768432691693306\n",
      "TRAIN: EPOCH 2/1000 | BATCH 59/71 | LOSS: 0.07677177637815476\n",
      "TRAIN: EPOCH 2/1000 | BATCH 60/71 | LOSS: 0.07666971277995188\n",
      "TRAIN: EPOCH 2/1000 | BATCH 61/71 | LOSS: 0.07640051685513989\n",
      "TRAIN: EPOCH 2/1000 | BATCH 62/71 | LOSS: 0.07621008122251147\n",
      "TRAIN: EPOCH 2/1000 | BATCH 63/71 | LOSS: 0.07576605083886534\n",
      "TRAIN: EPOCH 2/1000 | BATCH 64/71 | LOSS: 0.07558534764326535\n",
      "TRAIN: EPOCH 2/1000 | BATCH 65/71 | LOSS: 0.07528713994631261\n",
      "TRAIN: EPOCH 2/1000 | BATCH 66/71 | LOSS: 0.07505431710116899\n",
      "TRAIN: EPOCH 2/1000 | BATCH 67/71 | LOSS: 0.07491449841900784\n",
      "TRAIN: EPOCH 2/1000 | BATCH 68/71 | LOSS: 0.07473906318562618\n",
      "TRAIN: EPOCH 2/1000 | BATCH 69/71 | LOSS: 0.07448359365974154\n",
      "TRAIN: EPOCH 2/1000 | BATCH 70/71 | LOSS: 0.07439115517575975\n",
      "VAL: EPOCH 2/1000 | BATCH 0/8 | LOSS: 0.06402745842933655\n",
      "VAL: EPOCH 2/1000 | BATCH 1/8 | LOSS: 0.06405936181545258\n",
      "VAL: EPOCH 2/1000 | BATCH 2/8 | LOSS: 0.06114927679300308\n",
      "VAL: EPOCH 2/1000 | BATCH 3/8 | LOSS: 0.05859684199094772\n",
      "VAL: EPOCH 2/1000 | BATCH 4/8 | LOSS: 0.059311111271381375\n",
      "VAL: EPOCH 2/1000 | BATCH 5/8 | LOSS: 0.06043665980299314\n",
      "VAL: EPOCH 2/1000 | BATCH 6/8 | LOSS: 0.0610442789537566\n",
      "VAL: EPOCH 2/1000 | BATCH 7/8 | LOSS: 0.06013077171519399\n",
      "TRAIN: EPOCH 3/1000 | BATCH 0/71 | LOSS: 0.05510029196739197\n",
      "TRAIN: EPOCH 3/1000 | BATCH 1/71 | LOSS: 0.05536382086575031\n",
      "TRAIN: EPOCH 3/1000 | BATCH 2/71 | LOSS: 0.054466300954421364\n",
      "TRAIN: EPOCH 3/1000 | BATCH 3/71 | LOSS: 0.05310618970543146\n",
      "TRAIN: EPOCH 3/1000 | BATCH 4/71 | LOSS: 0.054458143562078475\n",
      "TRAIN: EPOCH 3/1000 | BATCH 5/71 | LOSS: 0.0550399466107289\n",
      "TRAIN: EPOCH 3/1000 | BATCH 6/71 | LOSS: 0.05477495544723102\n",
      "TRAIN: EPOCH 3/1000 | BATCH 7/71 | LOSS: 0.055726566817611456\n",
      "TRAIN: EPOCH 3/1000 | BATCH 8/71 | LOSS: 0.0561177602244748\n",
      "TRAIN: EPOCH 3/1000 | BATCH 9/71 | LOSS: 0.05570128373801708\n",
      "TRAIN: EPOCH 3/1000 | BATCH 10/71 | LOSS: 0.05567986145615578\n",
      "TRAIN: EPOCH 3/1000 | BATCH 11/71 | LOSS: 0.056256553468604885\n",
      "TRAIN: EPOCH 3/1000 | BATCH 12/71 | LOSS: 0.055820393161131784\n",
      "TRAIN: EPOCH 3/1000 | BATCH 13/71 | LOSS: 0.05540426314941475\n",
      "TRAIN: EPOCH 3/1000 | BATCH 14/71 | LOSS: 0.05613037322958311\n",
      "TRAIN: EPOCH 3/1000 | BATCH 15/71 | LOSS: 0.05632519186474383\n",
      "TRAIN: EPOCH 3/1000 | BATCH 16/71 | LOSS: 0.056466642328921485\n",
      "TRAIN: EPOCH 3/1000 | BATCH 17/71 | LOSS: 0.05626943603985839\n",
      "TRAIN: EPOCH 3/1000 | BATCH 18/71 | LOSS: 0.05618223861644143\n",
      "TRAIN: EPOCH 3/1000 | BATCH 19/71 | LOSS: 0.05607644207775593\n",
      "TRAIN: EPOCH 3/1000 | BATCH 20/71 | LOSS: 0.055885815904254\n",
      "TRAIN: EPOCH 3/1000 | BATCH 21/71 | LOSS: 0.055417410521344704\n",
      "TRAIN: EPOCH 3/1000 | BATCH 22/71 | LOSS: 0.0554136802320895\n",
      "TRAIN: EPOCH 3/1000 | BATCH 23/71 | LOSS: 0.05518006905913353\n",
      "TRAIN: EPOCH 3/1000 | BATCH 24/71 | LOSS: 0.05507923156023026\n",
      "TRAIN: EPOCH 3/1000 | BATCH 25/71 | LOSS: 0.055074821011378214\n",
      "TRAIN: EPOCH 3/1000 | BATCH 26/71 | LOSS: 0.05497080198040715\n",
      "TRAIN: EPOCH 3/1000 | BATCH 27/71 | LOSS: 0.05477058355297361\n",
      "TRAIN: EPOCH 3/1000 | BATCH 28/71 | LOSS: 0.054417089013190104\n",
      "TRAIN: EPOCH 3/1000 | BATCH 29/71 | LOSS: 0.0540675060202678\n",
      "TRAIN: EPOCH 3/1000 | BATCH 30/71 | LOSS: 0.05411169990416496\n",
      "TRAIN: EPOCH 3/1000 | BATCH 31/71 | LOSS: 0.05434907611925155\n",
      "TRAIN: EPOCH 3/1000 | BATCH 32/71 | LOSS: 0.054315640393531685\n",
      "TRAIN: EPOCH 3/1000 | BATCH 33/71 | LOSS: 0.054220078403458875\n",
      "TRAIN: EPOCH 3/1000 | BATCH 34/71 | LOSS: 0.05420741545302527\n",
      "TRAIN: EPOCH 3/1000 | BATCH 35/71 | LOSS: 0.053876943265398346\n",
      "TRAIN: EPOCH 3/1000 | BATCH 36/71 | LOSS: 0.05369070937504639\n",
      "TRAIN: EPOCH 3/1000 | BATCH 37/71 | LOSS: 0.053732571633238545\n",
      "TRAIN: EPOCH 3/1000 | BATCH 38/71 | LOSS: 0.05366209273537\n",
      "TRAIN: EPOCH 3/1000 | BATCH 39/71 | LOSS: 0.05352782532572746\n",
      "TRAIN: EPOCH 3/1000 | BATCH 40/71 | LOSS: 0.05351276623039711\n",
      "TRAIN: EPOCH 3/1000 | BATCH 41/71 | LOSS: 0.053239734222491585\n",
      "TRAIN: EPOCH 3/1000 | BATCH 42/71 | LOSS: 0.05304730640247811\n",
      "TRAIN: EPOCH 3/1000 | BATCH 43/71 | LOSS: 0.05280008014630188\n",
      "TRAIN: EPOCH 3/1000 | BATCH 44/71 | LOSS: 0.05265853918260998\n",
      "TRAIN: EPOCH 3/1000 | BATCH 45/71 | LOSS: 0.052520066580694656\n",
      "TRAIN: EPOCH 3/1000 | BATCH 46/71 | LOSS: 0.05255445338627125\n",
      "TRAIN: EPOCH 3/1000 | BATCH 47/71 | LOSS: 0.05243071153139075\n",
      "TRAIN: EPOCH 3/1000 | BATCH 48/71 | LOSS: 0.05233307585728412\n",
      "TRAIN: EPOCH 3/1000 | BATCH 49/71 | LOSS: 0.05203337408602238\n",
      "TRAIN: EPOCH 3/1000 | BATCH 50/71 | LOSS: 0.05207328695584746\n",
      "TRAIN: EPOCH 3/1000 | BATCH 51/71 | LOSS: 0.0519066287491184\n",
      "TRAIN: EPOCH 3/1000 | BATCH 52/71 | LOSS: 0.0518239012585496\n",
      "TRAIN: EPOCH 3/1000 | BATCH 53/71 | LOSS: 0.05170403655480455\n",
      "TRAIN: EPOCH 3/1000 | BATCH 54/71 | LOSS: 0.0516996981068091\n",
      "TRAIN: EPOCH 3/1000 | BATCH 55/71 | LOSS: 0.05160894304780023\n",
      "TRAIN: EPOCH 3/1000 | BATCH 56/71 | LOSS: 0.05141100359329006\n",
      "TRAIN: EPOCH 3/1000 | BATCH 57/71 | LOSS: 0.051212446250278376\n",
      "TRAIN: EPOCH 3/1000 | BATCH 58/71 | LOSS: 0.05112717521645255\n",
      "TRAIN: EPOCH 3/1000 | BATCH 59/71 | LOSS: 0.05097148126612107\n",
      "TRAIN: EPOCH 3/1000 | BATCH 60/71 | LOSS: 0.05089956259385484\n",
      "TRAIN: EPOCH 3/1000 | BATCH 61/71 | LOSS: 0.05086718848155391\n",
      "TRAIN: EPOCH 3/1000 | BATCH 62/71 | LOSS: 0.05065474351720205\n",
      "TRAIN: EPOCH 3/1000 | BATCH 63/71 | LOSS: 0.05065674218349159\n",
      "TRAIN: EPOCH 3/1000 | BATCH 64/71 | LOSS: 0.050703953493099946\n",
      "TRAIN: EPOCH 3/1000 | BATCH 65/71 | LOSS: 0.05059829089000369\n",
      "TRAIN: EPOCH 3/1000 | BATCH 66/71 | LOSS: 0.050450136023226066\n",
      "TRAIN: EPOCH 3/1000 | BATCH 67/71 | LOSS: 0.05032912285669761\n",
      "TRAIN: EPOCH 3/1000 | BATCH 68/71 | LOSS: 0.05031041968343915\n",
      "TRAIN: EPOCH 3/1000 | BATCH 69/71 | LOSS: 0.05027742976588862\n",
      "TRAIN: EPOCH 3/1000 | BATCH 70/71 | LOSS: 0.05012862017037163\n",
      "VAL: EPOCH 3/1000 | BATCH 0/8 | LOSS: 0.04781387373805046\n",
      "VAL: EPOCH 3/1000 | BATCH 1/8 | LOSS: 0.048069803044199944\n",
      "VAL: EPOCH 3/1000 | BATCH 2/8 | LOSS: 0.04620473459362984\n",
      "VAL: EPOCH 3/1000 | BATCH 3/8 | LOSS: 0.04424497950822115\n",
      "VAL: EPOCH 3/1000 | BATCH 4/8 | LOSS: 0.044544767588377\n",
      "VAL: EPOCH 3/1000 | BATCH 5/8 | LOSS: 0.04561921954154968\n",
      "VAL: EPOCH 3/1000 | BATCH 6/8 | LOSS: 0.04619210373078074\n",
      "VAL: EPOCH 3/1000 | BATCH 7/8 | LOSS: 0.04553096927702427\n",
      "TRAIN: EPOCH 4/1000 | BATCH 0/71 | LOSS: 0.04181915894150734\n",
      "TRAIN: EPOCH 4/1000 | BATCH 1/71 | LOSS: 0.04133968986570835\n",
      "TRAIN: EPOCH 4/1000 | BATCH 2/71 | LOSS: 0.04236185302337011\n",
      "TRAIN: EPOCH 4/1000 | BATCH 3/71 | LOSS: 0.04491021204739809\n",
      "TRAIN: EPOCH 4/1000 | BATCH 4/71 | LOSS: 0.04320505857467651\n",
      "TRAIN: EPOCH 4/1000 | BATCH 5/71 | LOSS: 0.04433737571040789\n",
      "TRAIN: EPOCH 4/1000 | BATCH 6/71 | LOSS: 0.044583335518836975\n",
      "TRAIN: EPOCH 4/1000 | BATCH 7/71 | LOSS: 0.04464191570878029\n",
      "TRAIN: EPOCH 4/1000 | BATCH 8/71 | LOSS: 0.04402283579111099\n",
      "TRAIN: EPOCH 4/1000 | BATCH 9/71 | LOSS: 0.04451370686292648\n",
      "TRAIN: EPOCH 4/1000 | BATCH 10/71 | LOSS: 0.04355925253846429\n",
      "TRAIN: EPOCH 4/1000 | BATCH 11/71 | LOSS: 0.04342401213943958\n",
      "TRAIN: EPOCH 4/1000 | BATCH 12/71 | LOSS: 0.04298935458064079\n",
      "TRAIN: EPOCH 4/1000 | BATCH 13/71 | LOSS: 0.0430818112300975\n",
      "TRAIN: EPOCH 4/1000 | BATCH 14/71 | LOSS: 0.04324504286050797\n",
      "TRAIN: EPOCH 4/1000 | BATCH 15/71 | LOSS: 0.04340976756066084\n",
      "TRAIN: EPOCH 4/1000 | BATCH 16/71 | LOSS: 0.042834848603781533\n",
      "TRAIN: EPOCH 4/1000 | BATCH 17/71 | LOSS: 0.042818153070078954\n",
      "TRAIN: EPOCH 4/1000 | BATCH 18/71 | LOSS: 0.04273201642852081\n",
      "TRAIN: EPOCH 4/1000 | BATCH 19/71 | LOSS: 0.04231133069843054\n",
      "TRAIN: EPOCH 4/1000 | BATCH 20/71 | LOSS: 0.042090286988587605\n",
      "TRAIN: EPOCH 4/1000 | BATCH 21/71 | LOSS: 0.04187598451972008\n",
      "TRAIN: EPOCH 4/1000 | BATCH 22/71 | LOSS: 0.04159555438420047\n",
      "TRAIN: EPOCH 4/1000 | BATCH 23/71 | LOSS: 0.041433585031578936\n",
      "TRAIN: EPOCH 4/1000 | BATCH 24/71 | LOSS: 0.041679147183895114\n",
      "TRAIN: EPOCH 4/1000 | BATCH 25/71 | LOSS: 0.04135197057173802\n",
      "TRAIN: EPOCH 4/1000 | BATCH 26/71 | LOSS: 0.041476807522552984\n",
      "TRAIN: EPOCH 4/1000 | BATCH 27/71 | LOSS: 0.041325384085731845\n",
      "TRAIN: EPOCH 4/1000 | BATCH 28/71 | LOSS: 0.041210513305047464\n",
      "TRAIN: EPOCH 4/1000 | BATCH 29/71 | LOSS: 0.04099661732713381\n",
      "TRAIN: EPOCH 4/1000 | BATCH 30/71 | LOSS: 0.04114595368023841\n",
      "TRAIN: EPOCH 4/1000 | BATCH 31/71 | LOSS: 0.041206316789612174\n",
      "TRAIN: EPOCH 4/1000 | BATCH 32/71 | LOSS: 0.041117708226948074\n",
      "TRAIN: EPOCH 4/1000 | BATCH 33/71 | LOSS: 0.041074332616784996\n",
      "TRAIN: EPOCH 4/1000 | BATCH 34/71 | LOSS: 0.04104374476841518\n",
      "TRAIN: EPOCH 4/1000 | BATCH 35/71 | LOSS: 0.04090098001890712\n",
      "TRAIN: EPOCH 4/1000 | BATCH 36/71 | LOSS: 0.040807407248664544\n",
      "TRAIN: EPOCH 4/1000 | BATCH 37/71 | LOSS: 0.04075907974650985\n",
      "TRAIN: EPOCH 4/1000 | BATCH 38/71 | LOSS: 0.040600184542246356\n",
      "TRAIN: EPOCH 4/1000 | BATCH 39/71 | LOSS: 0.040574552956968546\n",
      "TRAIN: EPOCH 4/1000 | BATCH 40/71 | LOSS: 0.040375058698218044\n",
      "TRAIN: EPOCH 4/1000 | BATCH 41/71 | LOSS: 0.040360186426412495\n",
      "TRAIN: EPOCH 4/1000 | BATCH 42/71 | LOSS: 0.04024443290261335\n",
      "TRAIN: EPOCH 4/1000 | BATCH 43/71 | LOSS: 0.0400048749182712\n",
      "TRAIN: EPOCH 4/1000 | BATCH 44/71 | LOSS: 0.039778988601432905\n",
      "TRAIN: EPOCH 4/1000 | BATCH 45/71 | LOSS: 0.039620952397260975\n",
      "TRAIN: EPOCH 4/1000 | BATCH 46/71 | LOSS: 0.03943700157105923\n",
      "TRAIN: EPOCH 4/1000 | BATCH 47/71 | LOSS: 0.03937655279878527\n",
      "TRAIN: EPOCH 4/1000 | BATCH 48/71 | LOSS: 0.03928250602769608\n",
      "TRAIN: EPOCH 4/1000 | BATCH 49/71 | LOSS: 0.03926915969699621\n",
      "TRAIN: EPOCH 4/1000 | BATCH 50/71 | LOSS: 0.03920943841484247\n",
      "TRAIN: EPOCH 4/1000 | BATCH 51/71 | LOSS: 0.039036253503022283\n",
      "TRAIN: EPOCH 4/1000 | BATCH 52/71 | LOSS: 0.038906284695807494\n",
      "TRAIN: EPOCH 4/1000 | BATCH 53/71 | LOSS: 0.03883917985001096\n",
      "TRAIN: EPOCH 4/1000 | BATCH 54/71 | LOSS: 0.03877615999769081\n",
      "TRAIN: EPOCH 4/1000 | BATCH 55/71 | LOSS: 0.03873697951036904\n",
      "TRAIN: EPOCH 4/1000 | BATCH 56/71 | LOSS: 0.03858114880297268\n",
      "TRAIN: EPOCH 4/1000 | BATCH 57/71 | LOSS: 0.03844828874771965\n",
      "TRAIN: EPOCH 4/1000 | BATCH 58/71 | LOSS: 0.0384249156498808\n",
      "TRAIN: EPOCH 4/1000 | BATCH 59/71 | LOSS: 0.03828657226016124\n",
      "TRAIN: EPOCH 4/1000 | BATCH 60/71 | LOSS: 0.03815902629103817\n",
      "TRAIN: EPOCH 4/1000 | BATCH 61/71 | LOSS: 0.03802626688153513\n",
      "TRAIN: EPOCH 4/1000 | BATCH 62/71 | LOSS: 0.037956984151923466\n",
      "TRAIN: EPOCH 4/1000 | BATCH 63/71 | LOSS: 0.03786604868946597\n",
      "TRAIN: EPOCH 4/1000 | BATCH 64/71 | LOSS: 0.03793714877504569\n",
      "TRAIN: EPOCH 4/1000 | BATCH 65/71 | LOSS: 0.03778041532319604\n",
      "TRAIN: EPOCH 4/1000 | BATCH 66/71 | LOSS: 0.03772222695510779\n",
      "TRAIN: EPOCH 4/1000 | BATCH 67/71 | LOSS: 0.03761215458678849\n",
      "TRAIN: EPOCH 4/1000 | BATCH 68/71 | LOSS: 0.03745576936373676\n",
      "TRAIN: EPOCH 4/1000 | BATCH 69/71 | LOSS: 0.03735024705529213\n",
      "TRAIN: EPOCH 4/1000 | BATCH 70/71 | LOSS: 0.03726743324331834\n",
      "VAL: EPOCH 4/1000 | BATCH 0/8 | LOSS: 0.0356232151389122\n",
      "VAL: EPOCH 4/1000 | BATCH 1/8 | LOSS: 0.035590365529060364\n",
      "VAL: EPOCH 4/1000 | BATCH 2/8 | LOSS: 0.0343732014298439\n",
      "VAL: EPOCH 4/1000 | BATCH 3/8 | LOSS: 0.032933654729276896\n",
      "VAL: EPOCH 4/1000 | BATCH 4/8 | LOSS: 0.03318140469491482\n",
      "VAL: EPOCH 4/1000 | BATCH 5/8 | LOSS: 0.03397807820389668\n",
      "VAL: EPOCH 4/1000 | BATCH 6/8 | LOSS: 0.034397252702287266\n",
      "VAL: EPOCH 4/1000 | BATCH 7/8 | LOSS: 0.03382100583985448\n",
      "TRAIN: EPOCH 5/1000 | BATCH 0/71 | LOSS: 0.032225511968135834\n",
      "TRAIN: EPOCH 5/1000 | BATCH 1/71 | LOSS: 0.03219224140048027\n",
      "TRAIN: EPOCH 5/1000 | BATCH 2/71 | LOSS: 0.03236614167690277\n",
      "TRAIN: EPOCH 5/1000 | BATCH 3/71 | LOSS: 0.03221889305859804\n",
      "TRAIN: EPOCH 5/1000 | BATCH 4/71 | LOSS: 0.032650019973516464\n",
      "TRAIN: EPOCH 5/1000 | BATCH 5/71 | LOSS: 0.03188537620007992\n",
      "TRAIN: EPOCH 5/1000 | BATCH 6/71 | LOSS: 0.032141946256160736\n",
      "TRAIN: EPOCH 5/1000 | BATCH 7/71 | LOSS: 0.03122302796691656\n",
      "TRAIN: EPOCH 5/1000 | BATCH 8/71 | LOSS: 0.03115449556046062\n",
      "TRAIN: EPOCH 5/1000 | BATCH 9/71 | LOSS: 0.03147359937429428\n",
      "TRAIN: EPOCH 5/1000 | BATCH 10/71 | LOSS: 0.03233879465948452\n",
      "TRAIN: EPOCH 5/1000 | BATCH 11/71 | LOSS: 0.03235767719646295\n",
      "TRAIN: EPOCH 5/1000 | BATCH 12/71 | LOSS: 0.032465029221314654\n",
      "TRAIN: EPOCH 5/1000 | BATCH 13/71 | LOSS: 0.032170280015894344\n",
      "TRAIN: EPOCH 5/1000 | BATCH 14/71 | LOSS: 0.031756870945294696\n",
      "TRAIN: EPOCH 5/1000 | BATCH 15/71 | LOSS: 0.03194161760620773\n",
      "TRAIN: EPOCH 5/1000 | BATCH 16/71 | LOSS: 0.03182668677147697\n",
      "TRAIN: EPOCH 5/1000 | BATCH 17/71 | LOSS: 0.03154619721074899\n",
      "TRAIN: EPOCH 5/1000 | BATCH 18/71 | LOSS: 0.031538469814940503\n",
      "TRAIN: EPOCH 5/1000 | BATCH 19/71 | LOSS: 0.031168714445084334\n",
      "TRAIN: EPOCH 5/1000 | BATCH 20/71 | LOSS: 0.03100849590486004\n",
      "TRAIN: EPOCH 5/1000 | BATCH 21/71 | LOSS: 0.031043732284822247\n",
      "TRAIN: EPOCH 5/1000 | BATCH 22/71 | LOSS: 0.030879065475386124\n",
      "TRAIN: EPOCH 5/1000 | BATCH 23/71 | LOSS: 0.03093834244646132\n",
      "TRAIN: EPOCH 5/1000 | BATCH 24/71 | LOSS: 0.030554040595889093\n",
      "TRAIN: EPOCH 5/1000 | BATCH 25/71 | LOSS: 0.030613086472910184\n",
      "TRAIN: EPOCH 5/1000 | BATCH 26/71 | LOSS: 0.030666140425536368\n",
      "TRAIN: EPOCH 5/1000 | BATCH 27/71 | LOSS: 0.030619873199611902\n",
      "TRAIN: EPOCH 5/1000 | BATCH 28/71 | LOSS: 0.03055518987620699\n",
      "TRAIN: EPOCH 5/1000 | BATCH 29/71 | LOSS: 0.030346193350851536\n",
      "TRAIN: EPOCH 5/1000 | BATCH 30/71 | LOSS: 0.030280112439105587\n",
      "TRAIN: EPOCH 5/1000 | BATCH 31/71 | LOSS: 0.030242551933042705\n",
      "TRAIN: EPOCH 5/1000 | BATCH 32/71 | LOSS: 0.030169502465110836\n",
      "TRAIN: EPOCH 5/1000 | BATCH 33/71 | LOSS: 0.0301950152086861\n",
      "TRAIN: EPOCH 5/1000 | BATCH 34/71 | LOSS: 0.030078710775290216\n",
      "TRAIN: EPOCH 5/1000 | BATCH 35/71 | LOSS: 0.029983589787864022\n",
      "TRAIN: EPOCH 5/1000 | BATCH 36/71 | LOSS: 0.029900877308603878\n",
      "TRAIN: EPOCH 5/1000 | BATCH 37/71 | LOSS: 0.02977445491246487\n",
      "TRAIN: EPOCH 5/1000 | BATCH 38/71 | LOSS: 0.029657101449676048\n",
      "TRAIN: EPOCH 5/1000 | BATCH 39/71 | LOSS: 0.0294987790286541\n",
      "TRAIN: EPOCH 5/1000 | BATCH 40/71 | LOSS: 0.02935738707097565\n",
      "TRAIN: EPOCH 5/1000 | BATCH 41/71 | LOSS: 0.029189529695681164\n",
      "TRAIN: EPOCH 5/1000 | BATCH 42/71 | LOSS: 0.029033569489107576\n",
      "TRAIN: EPOCH 5/1000 | BATCH 43/71 | LOSS: 0.028876575895331123\n",
      "TRAIN: EPOCH 5/1000 | BATCH 44/71 | LOSS: 0.028683883572618165\n",
      "TRAIN: EPOCH 5/1000 | BATCH 45/71 | LOSS: 0.0286028652816363\n",
      "TRAIN: EPOCH 5/1000 | BATCH 46/71 | LOSS: 0.02856069010622958\n",
      "TRAIN: EPOCH 5/1000 | BATCH 47/71 | LOSS: 0.02854360973772903\n",
      "TRAIN: EPOCH 5/1000 | BATCH 48/71 | LOSS: 0.028615507772382424\n",
      "TRAIN: EPOCH 5/1000 | BATCH 49/71 | LOSS: 0.028470389656722547\n",
      "TRAIN: EPOCH 5/1000 | BATCH 50/71 | LOSS: 0.02849840745329857\n",
      "TRAIN: EPOCH 5/1000 | BATCH 51/71 | LOSS: 0.028490976430475712\n",
      "TRAIN: EPOCH 5/1000 | BATCH 52/71 | LOSS: 0.028395028679438356\n",
      "TRAIN: EPOCH 5/1000 | BATCH 53/71 | LOSS: 0.028306972269934637\n",
      "TRAIN: EPOCH 5/1000 | BATCH 54/71 | LOSS: 0.028187805583531206\n",
      "TRAIN: EPOCH 5/1000 | BATCH 55/71 | LOSS: 0.02807932024422501\n",
      "TRAIN: EPOCH 5/1000 | BATCH 56/71 | LOSS: 0.02799456733229913\n",
      "TRAIN: EPOCH 5/1000 | BATCH 57/71 | LOSS: 0.027920913208147574\n",
      "TRAIN: EPOCH 5/1000 | BATCH 58/71 | LOSS: 0.02783511515896199\n",
      "TRAIN: EPOCH 5/1000 | BATCH 59/71 | LOSS: 0.02768943573658665\n",
      "TRAIN: EPOCH 5/1000 | BATCH 60/71 | LOSS: 0.02754566186397779\n",
      "TRAIN: EPOCH 5/1000 | BATCH 61/71 | LOSS: 0.02737375411895975\n",
      "TRAIN: EPOCH 5/1000 | BATCH 62/71 | LOSS: 0.02733446228953581\n",
      "TRAIN: EPOCH 5/1000 | BATCH 63/71 | LOSS: 0.027352675941074267\n",
      "TRAIN: EPOCH 5/1000 | BATCH 64/71 | LOSS: 0.027257918021999873\n",
      "TRAIN: EPOCH 5/1000 | BATCH 65/71 | LOSS: 0.027237815867093475\n",
      "TRAIN: EPOCH 5/1000 | BATCH 66/71 | LOSS: 0.02709301953106674\n",
      "TRAIN: EPOCH 5/1000 | BATCH 67/71 | LOSS: 0.02699088817462325\n",
      "TRAIN: EPOCH 5/1000 | BATCH 68/71 | LOSS: 0.026914594452018322\n",
      "TRAIN: EPOCH 5/1000 | BATCH 69/71 | LOSS: 0.026806753022330147\n",
      "TRAIN: EPOCH 5/1000 | BATCH 70/71 | LOSS: 0.02676348941741695\n",
      "VAL: EPOCH 5/1000 | BATCH 0/8 | LOSS: 0.02505411021411419\n",
      "VAL: EPOCH 5/1000 | BATCH 1/8 | LOSS: 0.024243389256298542\n",
      "VAL: EPOCH 5/1000 | BATCH 2/8 | LOSS: 0.023484588290254276\n",
      "VAL: EPOCH 5/1000 | BATCH 3/8 | LOSS: 0.022505427710711956\n",
      "VAL: EPOCH 5/1000 | BATCH 4/8 | LOSS: 0.022622943669557572\n",
      "VAL: EPOCH 5/1000 | BATCH 5/8 | LOSS: 0.0229979632422328\n",
      "VAL: EPOCH 5/1000 | BATCH 6/8 | LOSS: 0.02333138058228152\n",
      "VAL: EPOCH 5/1000 | BATCH 7/8 | LOSS: 0.02284810086712241\n",
      "TRAIN: EPOCH 6/1000 | BATCH 0/71 | LOSS: 0.022491730749607086\n",
      "TRAIN: EPOCH 6/1000 | BATCH 1/71 | LOSS: 0.01927107572555542\n",
      "TRAIN: EPOCH 6/1000 | BATCH 2/71 | LOSS: 0.019666946182648342\n",
      "TRAIN: EPOCH 6/1000 | BATCH 3/71 | LOSS: 0.019982857163995504\n",
      "TRAIN: EPOCH 6/1000 | BATCH 4/71 | LOSS: 0.019609520211815834\n",
      "TRAIN: EPOCH 6/1000 | BATCH 5/71 | LOSS: 0.018800705205649137\n",
      "TRAIN: EPOCH 6/1000 | BATCH 6/71 | LOSS: 0.019166412497205392\n",
      "TRAIN: EPOCH 6/1000 | BATCH 7/71 | LOSS: 0.019386144704185426\n",
      "TRAIN: EPOCH 6/1000 | BATCH 8/71 | LOSS: 0.019597075776093535\n",
      "TRAIN: EPOCH 6/1000 | BATCH 9/71 | LOSS: 0.0196796833537519\n",
      "TRAIN: EPOCH 6/1000 | BATCH 10/71 | LOSS: 0.01980028309944001\n",
      "TRAIN: EPOCH 6/1000 | BATCH 11/71 | LOSS: 0.019834874275450904\n",
      "TRAIN: EPOCH 6/1000 | BATCH 12/71 | LOSS: 0.01972860606530538\n",
      "TRAIN: EPOCH 6/1000 | BATCH 13/71 | LOSS: 0.019480529307786907\n",
      "TRAIN: EPOCH 6/1000 | BATCH 14/71 | LOSS: 0.019402143793801466\n",
      "TRAIN: EPOCH 6/1000 | BATCH 15/71 | LOSS: 0.01930297230137512\n",
      "TRAIN: EPOCH 6/1000 | BATCH 16/71 | LOSS: 0.01917100911412169\n",
      "TRAIN: EPOCH 6/1000 | BATCH 17/71 | LOSS: 0.019236076054059796\n",
      "TRAIN: EPOCH 6/1000 | BATCH 18/71 | LOSS: 0.019204498562765748\n",
      "TRAIN: EPOCH 6/1000 | BATCH 19/71 | LOSS: 0.019010940613225103\n",
      "TRAIN: EPOCH 6/1000 | BATCH 20/71 | LOSS: 0.018892826113317693\n",
      "TRAIN: EPOCH 6/1000 | BATCH 21/71 | LOSS: 0.01914557090706446\n",
      "TRAIN: EPOCH 6/1000 | BATCH 22/71 | LOSS: 0.01896160480606815\n",
      "TRAIN: EPOCH 6/1000 | BATCH 23/71 | LOSS: 0.018727699876762927\n",
      "TRAIN: EPOCH 6/1000 | BATCH 24/71 | LOSS: 0.0188396455720067\n",
      "TRAIN: EPOCH 6/1000 | BATCH 25/71 | LOSS: 0.01860678944593439\n",
      "TRAIN: EPOCH 6/1000 | BATCH 26/71 | LOSS: 0.018541526497790107\n",
      "TRAIN: EPOCH 6/1000 | BATCH 27/71 | LOSS: 0.01856831251643598\n",
      "TRAIN: EPOCH 6/1000 | BATCH 28/71 | LOSS: 0.018481220311388886\n",
      "TRAIN: EPOCH 6/1000 | BATCH 29/71 | LOSS: 0.018453457610060772\n",
      "TRAIN: EPOCH 6/1000 | BATCH 30/71 | LOSS: 0.018472789666585384\n",
      "TRAIN: EPOCH 6/1000 | BATCH 31/71 | LOSS: 0.018391309800790623\n",
      "TRAIN: EPOCH 6/1000 | BATCH 32/71 | LOSS: 0.018379102065933472\n",
      "TRAIN: EPOCH 6/1000 | BATCH 33/71 | LOSS: 0.01825752659865162\n",
      "TRAIN: EPOCH 6/1000 | BATCH 34/71 | LOSS: 0.01834229877484696\n",
      "TRAIN: EPOCH 6/1000 | BATCH 35/71 | LOSS: 0.018261933233588934\n",
      "TRAIN: EPOCH 6/1000 | BATCH 36/71 | LOSS: 0.018108824217641675\n",
      "TRAIN: EPOCH 6/1000 | BATCH 37/71 | LOSS: 0.01804435468818012\n",
      "TRAIN: EPOCH 6/1000 | BATCH 38/71 | LOSS: 0.01809359571108451\n",
      "TRAIN: EPOCH 6/1000 | BATCH 39/71 | LOSS: 0.018047712603583933\n",
      "TRAIN: EPOCH 6/1000 | BATCH 40/71 | LOSS: 0.017942437129776653\n",
      "TRAIN: EPOCH 6/1000 | BATCH 41/71 | LOSS: 0.017907758997309776\n",
      "TRAIN: EPOCH 6/1000 | BATCH 42/71 | LOSS: 0.01782658081068549\n",
      "TRAIN: EPOCH 6/1000 | BATCH 43/71 | LOSS: 0.017704488921233198\n",
      "TRAIN: EPOCH 6/1000 | BATCH 44/71 | LOSS: 0.01762058569325341\n",
      "TRAIN: EPOCH 6/1000 | BATCH 45/71 | LOSS: 0.01748607718669202\n",
      "TRAIN: EPOCH 6/1000 | BATCH 46/71 | LOSS: 0.01740509608483061\n",
      "TRAIN: EPOCH 6/1000 | BATCH 47/71 | LOSS: 0.017367841229618836\n",
      "TRAIN: EPOCH 6/1000 | BATCH 48/71 | LOSS: 0.017243212355034693\n",
      "TRAIN: EPOCH 6/1000 | BATCH 49/71 | LOSS: 0.01713309984654188\n",
      "TRAIN: EPOCH 6/1000 | BATCH 50/71 | LOSS: 0.017169879749417305\n",
      "TRAIN: EPOCH 6/1000 | BATCH 51/71 | LOSS: 0.01712490860014581\n",
      "TRAIN: EPOCH 6/1000 | BATCH 52/71 | LOSS: 0.017006115864892053\n",
      "TRAIN: EPOCH 6/1000 | BATCH 53/71 | LOSS: 0.01691901635516573\n",
      "TRAIN: EPOCH 6/1000 | BATCH 54/71 | LOSS: 0.016840788874436508\n",
      "TRAIN: EPOCH 6/1000 | BATCH 55/71 | LOSS: 0.016755831161780015\n",
      "TRAIN: EPOCH 6/1000 | BATCH 56/71 | LOSS: 0.01667648789129759\n",
      "TRAIN: EPOCH 6/1000 | BATCH 57/71 | LOSS: 0.016669795719970917\n",
      "TRAIN: EPOCH 6/1000 | BATCH 58/71 | LOSS: 0.016578362875823247\n",
      "TRAIN: EPOCH 6/1000 | BATCH 59/71 | LOSS: 0.01655359173504015\n",
      "TRAIN: EPOCH 6/1000 | BATCH 60/71 | LOSS: 0.01649620130536009\n",
      "TRAIN: EPOCH 6/1000 | BATCH 61/71 | LOSS: 0.01643612563249565\n",
      "TRAIN: EPOCH 6/1000 | BATCH 62/71 | LOSS: 0.01639058600578989\n",
      "TRAIN: EPOCH 6/1000 | BATCH 63/71 | LOSS: 0.01633413329545874\n",
      "TRAIN: EPOCH 6/1000 | BATCH 64/71 | LOSS: 0.016250686333156548\n",
      "TRAIN: EPOCH 6/1000 | BATCH 65/71 | LOSS: 0.016221844520645613\n",
      "TRAIN: EPOCH 6/1000 | BATCH 66/71 | LOSS: 0.016129100639651072\n",
      "TRAIN: EPOCH 6/1000 | BATCH 67/71 | LOSS: 0.016064687275930363\n",
      "TRAIN: EPOCH 6/1000 | BATCH 68/71 | LOSS: 0.01595914343614941\n",
      "TRAIN: EPOCH 6/1000 | BATCH 69/71 | LOSS: 0.015900706446596555\n",
      "TRAIN: EPOCH 6/1000 | BATCH 70/71 | LOSS: 0.015785375249270404\n",
      "VAL: EPOCH 6/1000 | BATCH 0/8 | LOSS: 0.013203699141740799\n",
      "VAL: EPOCH 6/1000 | BATCH 1/8 | LOSS: 0.011943131685256958\n",
      "VAL: EPOCH 6/1000 | BATCH 2/8 | LOSS: 0.011589502294858297\n",
      "VAL: EPOCH 6/1000 | BATCH 3/8 | LOSS: 0.011045415187254548\n",
      "VAL: EPOCH 6/1000 | BATCH 4/8 | LOSS: 0.01116031762212515\n",
      "VAL: EPOCH 6/1000 | BATCH 5/8 | LOSS: 0.011275742823878923\n",
      "VAL: EPOCH 6/1000 | BATCH 6/8 | LOSS: 0.011404926223414285\n",
      "VAL: EPOCH 6/1000 | BATCH 7/8 | LOSS: 0.011093185166828334\n",
      "TRAIN: EPOCH 7/1000 | BATCH 0/71 | LOSS: 0.010494669899344444\n",
      "TRAIN: EPOCH 7/1000 | BATCH 1/71 | LOSS: 0.010832072235643864\n",
      "TRAIN: EPOCH 7/1000 | BATCH 2/71 | LOSS: 0.010563692077994347\n",
      "TRAIN: EPOCH 7/1000 | BATCH 3/71 | LOSS: 0.010227235266938806\n",
      "TRAIN: EPOCH 7/1000 | BATCH 4/71 | LOSS: 0.01039117258042097\n",
      "TRAIN: EPOCH 7/1000 | BATCH 5/71 | LOSS: 0.010099301890780529\n",
      "TRAIN: EPOCH 7/1000 | BATCH 6/71 | LOSS: 0.010135410486587457\n",
      "TRAIN: EPOCH 7/1000 | BATCH 7/71 | LOSS: 0.010075166588649154\n",
      "TRAIN: EPOCH 7/1000 | BATCH 8/71 | LOSS: 0.010237718414929178\n",
      "TRAIN: EPOCH 7/1000 | BATCH 9/71 | LOSS: 0.010192575678229333\n",
      "TRAIN: EPOCH 7/1000 | BATCH 10/71 | LOSS: 0.010110063817013393\n",
      "TRAIN: EPOCH 7/1000 | BATCH 11/71 | LOSS: 0.00997178473820289\n",
      "TRAIN: EPOCH 7/1000 | BATCH 12/71 | LOSS: 0.009863461511066327\n",
      "TRAIN: EPOCH 7/1000 | BATCH 13/71 | LOSS: 0.009920337836125068\n",
      "TRAIN: EPOCH 7/1000 | BATCH 14/71 | LOSS: 0.009772955346852541\n",
      "TRAIN: EPOCH 7/1000 | BATCH 15/71 | LOSS: 0.009860823600320145\n",
      "TRAIN: EPOCH 7/1000 | BATCH 16/71 | LOSS: 0.009780626607072703\n",
      "TRAIN: EPOCH 7/1000 | BATCH 17/71 | LOSS: 0.009711213384030594\n",
      "TRAIN: EPOCH 7/1000 | BATCH 18/71 | LOSS: 0.00954454350530317\n",
      "TRAIN: EPOCH 7/1000 | BATCH 19/71 | LOSS: 0.009384320606477559\n",
      "TRAIN: EPOCH 7/1000 | BATCH 20/71 | LOSS: 0.009319778420918044\n",
      "TRAIN: EPOCH 7/1000 | BATCH 21/71 | LOSS: 0.00925516034476459\n",
      "TRAIN: EPOCH 7/1000 | BATCH 22/71 | LOSS: 0.00917641695021935\n",
      "TRAIN: EPOCH 7/1000 | BATCH 23/71 | LOSS: 0.009139885175197074\n",
      "TRAIN: EPOCH 7/1000 | BATCH 24/71 | LOSS: 0.008981014210730791\n",
      "TRAIN: EPOCH 7/1000 | BATCH 25/71 | LOSS: 0.008897138920684274\n",
      "TRAIN: EPOCH 7/1000 | BATCH 26/71 | LOSS: 0.008824527384368357\n",
      "TRAIN: EPOCH 7/1000 | BATCH 27/71 | LOSS: 0.008774753011363958\n",
      "TRAIN: EPOCH 7/1000 | BATCH 28/71 | LOSS: 0.008682691090708149\n",
      "TRAIN: EPOCH 7/1000 | BATCH 29/71 | LOSS: 0.008619660061473649\n",
      "TRAIN: EPOCH 7/1000 | BATCH 30/71 | LOSS: 0.008531678543095627\n",
      "TRAIN: EPOCH 7/1000 | BATCH 31/71 | LOSS: 0.008447517393506132\n",
      "TRAIN: EPOCH 7/1000 | BATCH 32/71 | LOSS: 0.008411786631878578\n",
      "TRAIN: EPOCH 7/1000 | BATCH 33/71 | LOSS: 0.008370731505291426\n",
      "TRAIN: EPOCH 7/1000 | BATCH 34/71 | LOSS: 0.008285527769476175\n",
      "TRAIN: EPOCH 7/1000 | BATCH 35/71 | LOSS: 0.008190973144438531\n",
      "TRAIN: EPOCH 7/1000 | BATCH 36/71 | LOSS: 0.008172606948662448\n",
      "TRAIN: EPOCH 7/1000 | BATCH 37/71 | LOSS: 0.008086205141520813\n",
      "TRAIN: EPOCH 7/1000 | BATCH 38/71 | LOSS: 0.0080400818767838\n",
      "TRAIN: EPOCH 7/1000 | BATCH 39/71 | LOSS: 0.00802391319302842\n",
      "TRAIN: EPOCH 7/1000 | BATCH 40/71 | LOSS: 0.007962990177386418\n",
      "TRAIN: EPOCH 7/1000 | BATCH 41/71 | LOSS: 0.007931899390227738\n",
      "TRAIN: EPOCH 7/1000 | BATCH 42/71 | LOSS: 0.007898161718405263\n",
      "TRAIN: EPOCH 7/1000 | BATCH 43/71 | LOSS: 0.007838720085353336\n",
      "TRAIN: EPOCH 7/1000 | BATCH 44/71 | LOSS: 0.007800345236642493\n",
      "TRAIN: EPOCH 7/1000 | BATCH 45/71 | LOSS: 0.007740881972257857\n",
      "TRAIN: EPOCH 7/1000 | BATCH 46/71 | LOSS: 0.007685320758993955\n",
      "TRAIN: EPOCH 7/1000 | BATCH 47/71 | LOSS: 0.007665190544988339\n",
      "TRAIN: EPOCH 7/1000 | BATCH 48/71 | LOSS: 0.007586835432151447\n",
      "TRAIN: EPOCH 7/1000 | BATCH 49/71 | LOSS: 0.007562720258720219\n",
      "TRAIN: EPOCH 7/1000 | BATCH 50/71 | LOSS: 0.0075395921400437755\n",
      "TRAIN: EPOCH 7/1000 | BATCH 51/71 | LOSS: 0.007479328568129299\n",
      "TRAIN: EPOCH 7/1000 | BATCH 52/71 | LOSS: 0.0074167896615657605\n",
      "TRAIN: EPOCH 7/1000 | BATCH 53/71 | LOSS: 0.007376027392671892\n",
      "TRAIN: EPOCH 7/1000 | BATCH 54/71 | LOSS: 0.007364370263266293\n",
      "TRAIN: EPOCH 7/1000 | BATCH 55/71 | LOSS: 0.007338503404753283\n",
      "TRAIN: EPOCH 7/1000 | BATCH 56/71 | LOSS: 0.007304583974345995\n",
      "TRAIN: EPOCH 7/1000 | BATCH 57/71 | LOSS: 0.007259912487793842\n",
      "TRAIN: EPOCH 7/1000 | BATCH 58/71 | LOSS: 0.007221228162900118\n",
      "TRAIN: EPOCH 7/1000 | BATCH 59/71 | LOSS: 0.0071798298934785025\n",
      "TRAIN: EPOCH 7/1000 | BATCH 60/71 | LOSS: 0.0071399899039295365\n",
      "TRAIN: EPOCH 7/1000 | BATCH 61/71 | LOSS: 0.007105538174660216\n",
      "TRAIN: EPOCH 7/1000 | BATCH 62/71 | LOSS: 0.007063964318986687\n",
      "TRAIN: EPOCH 7/1000 | BATCH 63/71 | LOSS: 0.0070269305833789986\n",
      "TRAIN: EPOCH 7/1000 | BATCH 64/71 | LOSS: 0.006977457932841319\n",
      "TRAIN: EPOCH 7/1000 | BATCH 65/71 | LOSS: 0.00693420850615384\n",
      "TRAIN: EPOCH 7/1000 | BATCH 66/71 | LOSS: 0.006903922873368459\n",
      "TRAIN: EPOCH 7/1000 | BATCH 67/71 | LOSS: 0.006868234253488481\n",
      "TRAIN: EPOCH 7/1000 | BATCH 68/71 | LOSS: 0.006837554599927819\n",
      "TRAIN: EPOCH 7/1000 | BATCH 69/71 | LOSS: 0.00680548580629485\n",
      "TRAIN: EPOCH 7/1000 | BATCH 70/71 | LOSS: 0.006765033689025842\n",
      "VAL: EPOCH 7/1000 | BATCH 0/8 | LOSS: 0.005895177833735943\n",
      "VAL: EPOCH 7/1000 | BATCH 1/8 | LOSS: 0.004781301831826568\n",
      "VAL: EPOCH 7/1000 | BATCH 2/8 | LOSS: 0.004681956799079974\n",
      "VAL: EPOCH 7/1000 | BATCH 3/8 | LOSS: 0.0043501416221261024\n",
      "VAL: EPOCH 7/1000 | BATCH 4/8 | LOSS: 0.004477052297443151\n",
      "VAL: EPOCH 7/1000 | BATCH 5/8 | LOSS: 0.004431681319450338\n",
      "VAL: EPOCH 7/1000 | BATCH 6/8 | LOSS: 0.004442721272685698\n",
      "VAL: EPOCH 7/1000 | BATCH 7/8 | LOSS: 0.0042944655288010836\n",
      "TRAIN: EPOCH 8/1000 | BATCH 0/71 | LOSS: 0.0037801882717758417\n",
      "TRAIN: EPOCH 8/1000 | BATCH 1/71 | LOSS: 0.00379454402718693\n",
      "TRAIN: EPOCH 8/1000 | BATCH 2/71 | LOSS: 0.0041703391664971905\n",
      "TRAIN: EPOCH 8/1000 | BATCH 3/71 | LOSS: 0.004041066102217883\n",
      "TRAIN: EPOCH 8/1000 | BATCH 4/71 | LOSS: 0.003806203929707408\n",
      "TRAIN: EPOCH 8/1000 | BATCH 5/71 | LOSS: 0.0036250880997007093\n",
      "TRAIN: EPOCH 8/1000 | BATCH 6/71 | LOSS: 0.003641324383871896\n",
      "TRAIN: EPOCH 8/1000 | BATCH 7/71 | LOSS: 0.003723507688846439\n",
      "TRAIN: EPOCH 8/1000 | BATCH 8/71 | LOSS: 0.003649009805586603\n",
      "TRAIN: EPOCH 8/1000 | BATCH 9/71 | LOSS: 0.0035963252186775206\n",
      "TRAIN: EPOCH 8/1000 | BATCH 10/71 | LOSS: 0.003556944006545977\n",
      "TRAIN: EPOCH 8/1000 | BATCH 11/71 | LOSS: 0.0035208757035434246\n",
      "TRAIN: EPOCH 8/1000 | BATCH 12/71 | LOSS: 0.003588812998854197\n",
      "TRAIN: EPOCH 8/1000 | BATCH 13/71 | LOSS: 0.0036244149918534924\n",
      "TRAIN: EPOCH 8/1000 | BATCH 14/71 | LOSS: 0.003631372470408678\n",
      "TRAIN: EPOCH 8/1000 | BATCH 15/71 | LOSS: 0.003576885603251867\n",
      "TRAIN: EPOCH 8/1000 | BATCH 16/71 | LOSS: 0.0035946940975811552\n",
      "TRAIN: EPOCH 8/1000 | BATCH 17/71 | LOSS: 0.003549692673712141\n",
      "TRAIN: EPOCH 8/1000 | BATCH 18/71 | LOSS: 0.003516471025681025\n",
      "TRAIN: EPOCH 8/1000 | BATCH 19/71 | LOSS: 0.0035235229996033015\n",
      "TRAIN: EPOCH 8/1000 | BATCH 20/71 | LOSS: 0.0035501844582280944\n",
      "TRAIN: EPOCH 8/1000 | BATCH 21/71 | LOSS: 0.003537147782150317\n",
      "TRAIN: EPOCH 8/1000 | BATCH 22/71 | LOSS: 0.0035231568668361592\n",
      "TRAIN: EPOCH 8/1000 | BATCH 23/71 | LOSS: 0.0035321950272191316\n",
      "TRAIN: EPOCH 8/1000 | BATCH 24/71 | LOSS: 0.003501131506636739\n",
      "TRAIN: EPOCH 8/1000 | BATCH 25/71 | LOSS: 0.0034843408054887103\n",
      "TRAIN: EPOCH 8/1000 | BATCH 26/71 | LOSS: 0.003457926334468303\n",
      "TRAIN: EPOCH 8/1000 | BATCH 27/71 | LOSS: 0.003445187001489103\n",
      "TRAIN: EPOCH 8/1000 | BATCH 28/71 | LOSS: 0.003426476895552257\n",
      "TRAIN: EPOCH 8/1000 | BATCH 29/71 | LOSS: 0.00341351847940435\n",
      "TRAIN: EPOCH 8/1000 | BATCH 30/71 | LOSS: 0.003395877568231475\n",
      "TRAIN: EPOCH 8/1000 | BATCH 31/71 | LOSS: 0.003403345384867862\n",
      "TRAIN: EPOCH 8/1000 | BATCH 32/71 | LOSS: 0.0033686355278460364\n",
      "TRAIN: EPOCH 8/1000 | BATCH 33/71 | LOSS: 0.003343782230170772\n",
      "TRAIN: EPOCH 8/1000 | BATCH 34/71 | LOSS: 0.003306308528408408\n",
      "TRAIN: EPOCH 8/1000 | BATCH 35/71 | LOSS: 0.0033277524441170194\n",
      "TRAIN: EPOCH 8/1000 | BATCH 36/71 | LOSS: 0.0033254517129043468\n",
      "TRAIN: EPOCH 8/1000 | BATCH 37/71 | LOSS: 0.00329892734972466\n",
      "TRAIN: EPOCH 8/1000 | BATCH 38/71 | LOSS: 0.003280502475368289\n",
      "TRAIN: EPOCH 8/1000 | BATCH 39/71 | LOSS: 0.0032646497420500964\n",
      "TRAIN: EPOCH 8/1000 | BATCH 40/71 | LOSS: 0.0032661183703145604\n",
      "TRAIN: EPOCH 8/1000 | BATCH 41/71 | LOSS: 0.0032424898097468983\n",
      "TRAIN: EPOCH 8/1000 | BATCH 42/71 | LOSS: 0.003236467646824759\n",
      "TRAIN: EPOCH 8/1000 | BATCH 43/71 | LOSS: 0.003224595412823626\n",
      "TRAIN: EPOCH 8/1000 | BATCH 44/71 | LOSS: 0.003204320241800613\n",
      "TRAIN: EPOCH 8/1000 | BATCH 45/71 | LOSS: 0.003203542852450324\n",
      "TRAIN: EPOCH 8/1000 | BATCH 46/71 | LOSS: 0.0031882727498862336\n",
      "TRAIN: EPOCH 8/1000 | BATCH 47/71 | LOSS: 0.003171184153567689\n",
      "TRAIN: EPOCH 8/1000 | BATCH 48/71 | LOSS: 0.0031473687327257835\n",
      "TRAIN: EPOCH 8/1000 | BATCH 49/71 | LOSS: 0.0031209084205329417\n",
      "TRAIN: EPOCH 8/1000 | BATCH 50/71 | LOSS: 0.0030864330565593405\n",
      "TRAIN: EPOCH 8/1000 | BATCH 51/71 | LOSS: 0.0030643925106582735\n",
      "TRAIN: EPOCH 8/1000 | BATCH 52/71 | LOSS: 0.0030441408063162046\n",
      "TRAIN: EPOCH 8/1000 | BATCH 53/71 | LOSS: 0.003016757744329947\n",
      "TRAIN: EPOCH 8/1000 | BATCH 54/71 | LOSS: 0.002996668177233501\n",
      "TRAIN: EPOCH 8/1000 | BATCH 55/71 | LOSS: 0.0029771286353934556\n",
      "TRAIN: EPOCH 8/1000 | BATCH 56/71 | LOSS: 0.0029548647090498555\n",
      "TRAIN: EPOCH 8/1000 | BATCH 57/71 | LOSS: 0.0029310466910327047\n",
      "TRAIN: EPOCH 8/1000 | BATCH 58/71 | LOSS: 0.002915060709988288\n",
      "TRAIN: EPOCH 8/1000 | BATCH 59/71 | LOSS: 0.0028924319330447664\n",
      "TRAIN: EPOCH 8/1000 | BATCH 60/71 | LOSS: 0.0028826169703980204\n",
      "TRAIN: EPOCH 8/1000 | BATCH 61/71 | LOSS: 0.002871594198363563\n",
      "TRAIN: EPOCH 8/1000 | BATCH 62/71 | LOSS: 0.002853571147184878\n",
      "TRAIN: EPOCH 8/1000 | BATCH 63/71 | LOSS: 0.0028418367601261707\n",
      "TRAIN: EPOCH 8/1000 | BATCH 64/71 | LOSS: 0.0028247640593550526\n",
      "TRAIN: EPOCH 8/1000 | BATCH 65/71 | LOSS: 0.002810645509849895\n",
      "TRAIN: EPOCH 8/1000 | BATCH 66/71 | LOSS: 0.002798971417707516\n",
      "TRAIN: EPOCH 8/1000 | BATCH 67/71 | LOSS: 0.0027841813348726753\n",
      "TRAIN: EPOCH 8/1000 | BATCH 68/71 | LOSS: 0.002769438271491748\n",
      "TRAIN: EPOCH 8/1000 | BATCH 69/71 | LOSS: 0.0027615686296485363\n",
      "TRAIN: EPOCH 8/1000 | BATCH 70/71 | LOSS: 0.0027413717618631856\n",
      "VAL: EPOCH 8/1000 | BATCH 0/8 | LOSS: 0.002763566095381975\n",
      "VAL: EPOCH 8/1000 | BATCH 1/8 | LOSS: 0.002060376340523362\n",
      "VAL: EPOCH 8/1000 | BATCH 2/8 | LOSS: 0.0020244597302128873\n",
      "VAL: EPOCH 8/1000 | BATCH 3/8 | LOSS: 0.001834840135416016\n",
      "VAL: EPOCH 8/1000 | BATCH 4/8 | LOSS: 0.0019091363763436675\n",
      "VAL: EPOCH 8/1000 | BATCH 5/8 | LOSS: 0.0018797193382245798\n",
      "VAL: EPOCH 8/1000 | BATCH 6/8 | LOSS: 0.0018812257663479873\n",
      "VAL: EPOCH 8/1000 | BATCH 7/8 | LOSS: 0.001802482336643152\n",
      "TRAIN: EPOCH 9/1000 | BATCH 0/71 | LOSS: 0.0017077237134799361\n",
      "TRAIN: EPOCH 9/1000 | BATCH 1/71 | LOSS: 0.0016302081057801843\n",
      "TRAIN: EPOCH 9/1000 | BATCH 2/71 | LOSS: 0.001957503380253911\n",
      "TRAIN: EPOCH 9/1000 | BATCH 3/71 | LOSS: 0.00183881024713628\n",
      "TRAIN: EPOCH 9/1000 | BATCH 4/71 | LOSS: 0.0017448514234274625\n",
      "TRAIN: EPOCH 9/1000 | BATCH 5/71 | LOSS: 0.0017094635598671932\n",
      "TRAIN: EPOCH 9/1000 | BATCH 6/71 | LOSS: 0.0016594009274350746\n",
      "TRAIN: EPOCH 9/1000 | BATCH 7/71 | LOSS: 0.0016217322263401002\n",
      "TRAIN: EPOCH 9/1000 | BATCH 8/71 | LOSS: 0.0016539539727899763\n",
      "TRAIN: EPOCH 9/1000 | BATCH 9/71 | LOSS: 0.0016729471157304943\n",
      "TRAIN: EPOCH 9/1000 | BATCH 10/71 | LOSS: 0.0016806918852539225\n",
      "TRAIN: EPOCH 9/1000 | BATCH 11/71 | LOSS: 0.0016214073257287964\n",
      "TRAIN: EPOCH 9/1000 | BATCH 12/71 | LOSS: 0.001611343992408365\n",
      "TRAIN: EPOCH 9/1000 | BATCH 13/71 | LOSS: 0.0015867659032145248\n",
      "TRAIN: EPOCH 9/1000 | BATCH 14/71 | LOSS: 0.0015800858731381596\n",
      "TRAIN: EPOCH 9/1000 | BATCH 15/71 | LOSS: 0.0015577143749396782\n",
      "TRAIN: EPOCH 9/1000 | BATCH 16/71 | LOSS: 0.0015513763783554383\n",
      "TRAIN: EPOCH 9/1000 | BATCH 17/71 | LOSS: 0.0015481451377531306\n",
      "TRAIN: EPOCH 9/1000 | BATCH 18/71 | LOSS: 0.0015353662801269245\n",
      "TRAIN: EPOCH 9/1000 | BATCH 19/71 | LOSS: 0.0015241739834891631\n",
      "TRAIN: EPOCH 9/1000 | BATCH 20/71 | LOSS: 0.0015097387783628488\n",
      "TRAIN: EPOCH 9/1000 | BATCH 21/71 | LOSS: 0.00149656995464201\n",
      "TRAIN: EPOCH 9/1000 | BATCH 22/71 | LOSS: 0.0014769440218198883\n",
      "TRAIN: EPOCH 9/1000 | BATCH 23/71 | LOSS: 0.001487690351495985\n",
      "TRAIN: EPOCH 9/1000 | BATCH 24/71 | LOSS: 0.0014813151094131172\n",
      "TRAIN: EPOCH 9/1000 | BATCH 25/71 | LOSS: 0.0014793131066163858\n",
      "TRAIN: EPOCH 9/1000 | BATCH 26/71 | LOSS: 0.0014854482379397032\n",
      "TRAIN: EPOCH 9/1000 | BATCH 27/71 | LOSS: 0.0014744590714274506\n",
      "TRAIN: EPOCH 9/1000 | BATCH 28/71 | LOSS: 0.0014508343040782574\n",
      "TRAIN: EPOCH 9/1000 | BATCH 29/71 | LOSS: 0.0014440001900463055\n",
      "TRAIN: EPOCH 9/1000 | BATCH 30/71 | LOSS: 0.001432156128313152\n",
      "TRAIN: EPOCH 9/1000 | BATCH 31/71 | LOSS: 0.0014216016043064883\n",
      "TRAIN: EPOCH 9/1000 | BATCH 32/71 | LOSS: 0.001417787475723096\n",
      "TRAIN: EPOCH 9/1000 | BATCH 33/71 | LOSS: 0.0014014022873358473\n",
      "TRAIN: EPOCH 9/1000 | BATCH 34/71 | LOSS: 0.001398429841667946\n",
      "TRAIN: EPOCH 9/1000 | BATCH 35/71 | LOSS: 0.0013954307893679168\n",
      "TRAIN: EPOCH 9/1000 | BATCH 36/71 | LOSS: 0.0013825218476405418\n",
      "TRAIN: EPOCH 9/1000 | BATCH 37/71 | LOSS: 0.0013742379776790347\n",
      "TRAIN: EPOCH 9/1000 | BATCH 38/71 | LOSS: 0.0013584657691610165\n",
      "TRAIN: EPOCH 9/1000 | BATCH 39/71 | LOSS: 0.0013499116554157808\n",
      "TRAIN: EPOCH 9/1000 | BATCH 40/71 | LOSS: 0.0013447960231044308\n",
      "TRAIN: EPOCH 9/1000 | BATCH 41/71 | LOSS: 0.0013377849661212946\n",
      "TRAIN: EPOCH 9/1000 | BATCH 42/71 | LOSS: 0.0013274286445880005\n",
      "TRAIN: EPOCH 9/1000 | BATCH 43/71 | LOSS: 0.0013298877139194783\n",
      "TRAIN: EPOCH 9/1000 | BATCH 44/71 | LOSS: 0.0013252734206616879\n",
      "TRAIN: EPOCH 9/1000 | BATCH 45/71 | LOSS: 0.0013177826615941265\n",
      "TRAIN: EPOCH 9/1000 | BATCH 46/71 | LOSS: 0.0013042273733051533\n",
      "TRAIN: EPOCH 9/1000 | BATCH 47/71 | LOSS: 0.0012932338795508258\n",
      "TRAIN: EPOCH 9/1000 | BATCH 48/71 | LOSS: 0.0012806604630598913\n",
      "TRAIN: EPOCH 9/1000 | BATCH 49/71 | LOSS: 0.0012722169735934585\n",
      "TRAIN: EPOCH 9/1000 | BATCH 50/71 | LOSS: 0.0012636499938226359\n",
      "TRAIN: EPOCH 9/1000 | BATCH 51/71 | LOSS: 0.0012713791836106863\n",
      "TRAIN: EPOCH 9/1000 | BATCH 52/71 | LOSS: 0.0012655636034290885\n",
      "TRAIN: EPOCH 9/1000 | BATCH 53/71 | LOSS: 0.0012527429337044144\n",
      "TRAIN: EPOCH 9/1000 | BATCH 54/71 | LOSS: 0.0012463187894106588\n",
      "TRAIN: EPOCH 9/1000 | BATCH 55/71 | LOSS: 0.0012456338580315268\n",
      "TRAIN: EPOCH 9/1000 | BATCH 56/71 | LOSS: 0.0012326885674459238\n",
      "TRAIN: EPOCH 9/1000 | BATCH 57/71 | LOSS: 0.0012269610767895035\n",
      "TRAIN: EPOCH 9/1000 | BATCH 58/71 | LOSS: 0.0012186727529959912\n",
      "TRAIN: EPOCH 9/1000 | BATCH 59/71 | LOSS: 0.0012113021570257843\n",
      "TRAIN: EPOCH 9/1000 | BATCH 60/71 | LOSS: 0.00120843829182511\n",
      "TRAIN: EPOCH 9/1000 | BATCH 61/71 | LOSS: 0.0012005167682805368\n",
      "TRAIN: EPOCH 9/1000 | BATCH 62/71 | LOSS: 0.0011937492471631793\n",
      "TRAIN: EPOCH 9/1000 | BATCH 63/71 | LOSS: 0.0011901836842298508\n",
      "TRAIN: EPOCH 9/1000 | BATCH 64/71 | LOSS: 0.0011854405685041386\n",
      "TRAIN: EPOCH 9/1000 | BATCH 65/71 | LOSS: 0.001184450369857421\n",
      "TRAIN: EPOCH 9/1000 | BATCH 66/71 | LOSS: 0.0011811863219212573\n",
      "TRAIN: EPOCH 9/1000 | BATCH 67/71 | LOSS: 0.0011748475115061464\n",
      "TRAIN: EPOCH 9/1000 | BATCH 68/71 | LOSS: 0.0011669037580166173\n",
      "TRAIN: EPOCH 9/1000 | BATCH 69/71 | LOSS: 0.0011615571682341396\n",
      "TRAIN: EPOCH 9/1000 | BATCH 70/71 | LOSS: 0.0011546670510040097\n",
      "VAL: EPOCH 9/1000 | BATCH 0/8 | LOSS: 0.001523478189483285\n",
      "VAL: EPOCH 9/1000 | BATCH 1/8 | LOSS: 0.0010674343793652952\n",
      "VAL: EPOCH 9/1000 | BATCH 2/8 | LOSS: 0.0010398901843776305\n",
      "VAL: EPOCH 9/1000 | BATCH 3/8 | LOSS: 0.0008935290243243799\n",
      "VAL: EPOCH 9/1000 | BATCH 4/8 | LOSS: 0.0009288802859373391\n",
      "VAL: EPOCH 9/1000 | BATCH 5/8 | LOSS: 0.0009274837211705744\n",
      "VAL: EPOCH 9/1000 | BATCH 6/8 | LOSS: 0.0009245991740109665\n",
      "VAL: EPOCH 9/1000 | BATCH 7/8 | LOSS: 0.0008785022946540266\n",
      "TRAIN: EPOCH 10/1000 | BATCH 0/71 | LOSS: 0.0012303163530305028\n",
      "TRAIN: EPOCH 10/1000 | BATCH 1/71 | LOSS: 0.0013299733400344849\n",
      "TRAIN: EPOCH 10/1000 | BATCH 2/71 | LOSS: 0.0011570842082922657\n",
      "TRAIN: EPOCH 10/1000 | BATCH 3/71 | LOSS: 0.001014977809973061\n",
      "TRAIN: EPOCH 10/1000 | BATCH 4/71 | LOSS: 0.0009177867439575493\n",
      "TRAIN: EPOCH 10/1000 | BATCH 5/71 | LOSS: 0.0008972363139037043\n",
      "TRAIN: EPOCH 10/1000 | BATCH 6/71 | LOSS: 0.0008587110870783883\n",
      "TRAIN: EPOCH 10/1000 | BATCH 7/71 | LOSS: 0.000843723515572492\n",
      "TRAIN: EPOCH 10/1000 | BATCH 8/71 | LOSS: 0.0008682359928368694\n",
      "TRAIN: EPOCH 10/1000 | BATCH 9/71 | LOSS: 0.0008945724403019994\n",
      "TRAIN: EPOCH 10/1000 | BATCH 10/71 | LOSS: 0.000865050141741945\n",
      "TRAIN: EPOCH 10/1000 | BATCH 11/71 | LOSS: 0.0008617413986939937\n",
      "TRAIN: EPOCH 10/1000 | BATCH 12/71 | LOSS: 0.0008536414780582373\n",
      "TRAIN: EPOCH 10/1000 | BATCH 13/71 | LOSS: 0.0008490349781433386\n",
      "TRAIN: EPOCH 10/1000 | BATCH 14/71 | LOSS: 0.0008540515089407563\n",
      "TRAIN: EPOCH 10/1000 | BATCH 15/71 | LOSS: 0.00082918445514224\n",
      "TRAIN: EPOCH 10/1000 | BATCH 16/71 | LOSS: 0.0008261696322519771\n",
      "TRAIN: EPOCH 10/1000 | BATCH 17/71 | LOSS: 0.0008172080221508319\n",
      "TRAIN: EPOCH 10/1000 | BATCH 18/71 | LOSS: 0.0008221810245518817\n",
      "TRAIN: EPOCH 10/1000 | BATCH 19/71 | LOSS: 0.0008092507850960828\n",
      "TRAIN: EPOCH 10/1000 | BATCH 20/71 | LOSS: 0.0008233886203795139\n",
      "TRAIN: EPOCH 10/1000 | BATCH 21/71 | LOSS: 0.0008192773291904649\n",
      "TRAIN: EPOCH 10/1000 | BATCH 22/71 | LOSS: 0.0008318626369936796\n",
      "TRAIN: EPOCH 10/1000 | BATCH 23/71 | LOSS: 0.0008223565322017142\n",
      "TRAIN: EPOCH 10/1000 | BATCH 24/71 | LOSS: 0.0008094742812681943\n",
      "TRAIN: EPOCH 10/1000 | BATCH 25/71 | LOSS: 0.0008063829927078376\n",
      "TRAIN: EPOCH 10/1000 | BATCH 26/71 | LOSS: 0.0008131704752385202\n",
      "TRAIN: EPOCH 10/1000 | BATCH 27/71 | LOSS: 0.0008049006417942499\n",
      "TRAIN: EPOCH 10/1000 | BATCH 28/71 | LOSS: 0.0007925395329948515\n",
      "TRAIN: EPOCH 10/1000 | BATCH 29/71 | LOSS: 0.0007903900550445542\n",
      "TRAIN: EPOCH 10/1000 | BATCH 30/71 | LOSS: 0.000781436474250269\n",
      "TRAIN: EPOCH 10/1000 | BATCH 31/71 | LOSS: 0.0007863939326853142\n",
      "TRAIN: EPOCH 10/1000 | BATCH 32/71 | LOSS: 0.0007812218929316397\n",
      "TRAIN: EPOCH 10/1000 | BATCH 33/71 | LOSS: 0.0007943842236486756\n",
      "TRAIN: EPOCH 10/1000 | BATCH 34/71 | LOSS: 0.0007877048196470631\n",
      "TRAIN: EPOCH 10/1000 | BATCH 35/71 | LOSS: 0.0007771181796366969\n",
      "TRAIN: EPOCH 10/1000 | BATCH 36/71 | LOSS: 0.0007774692557349399\n",
      "TRAIN: EPOCH 10/1000 | BATCH 37/71 | LOSS: 0.0007696591094013696\n",
      "TRAIN: EPOCH 10/1000 | BATCH 38/71 | LOSS: 0.0007730496596568861\n",
      "TRAIN: EPOCH 10/1000 | BATCH 39/71 | LOSS: 0.0007677818139200099\n",
      "TRAIN: EPOCH 10/1000 | BATCH 40/71 | LOSS: 0.0007650603571475097\n",
      "TRAIN: EPOCH 10/1000 | BATCH 41/71 | LOSS: 0.0007712910398064802\n",
      "TRAIN: EPOCH 10/1000 | BATCH 42/71 | LOSS: 0.0007680863978579467\n",
      "TRAIN: EPOCH 10/1000 | BATCH 43/71 | LOSS: 0.000769672175837596\n",
      "TRAIN: EPOCH 10/1000 | BATCH 44/71 | LOSS: 0.0007691175534596874\n",
      "TRAIN: EPOCH 10/1000 | BATCH 45/71 | LOSS: 0.0007633133158695115\n",
      "TRAIN: EPOCH 10/1000 | BATCH 46/71 | LOSS: 0.0007578169139339886\n",
      "TRAIN: EPOCH 10/1000 | BATCH 47/71 | LOSS: 0.000755674162064679\n",
      "TRAIN: EPOCH 10/1000 | BATCH 48/71 | LOSS: 0.0007561735050961832\n",
      "TRAIN: EPOCH 10/1000 | BATCH 49/71 | LOSS: 0.0007505311450222507\n",
      "TRAIN: EPOCH 10/1000 | BATCH 50/71 | LOSS: 0.0007524870344045042\n",
      "TRAIN: EPOCH 10/1000 | BATCH 51/71 | LOSS: 0.0007541976447100751\n",
      "TRAIN: EPOCH 10/1000 | BATCH 52/71 | LOSS: 0.0007608750596739141\n",
      "TRAIN: EPOCH 10/1000 | BATCH 53/71 | LOSS: 0.0007603711775219482\n",
      "TRAIN: EPOCH 10/1000 | BATCH 54/71 | LOSS: 0.0007585022296883505\n",
      "TRAIN: EPOCH 10/1000 | BATCH 55/71 | LOSS: 0.0007531308017080716\n",
      "TRAIN: EPOCH 10/1000 | BATCH 56/71 | LOSS: 0.0007481523243130365\n",
      "TRAIN: EPOCH 10/1000 | BATCH 57/71 | LOSS: 0.0007413268011015166\n",
      "TRAIN: EPOCH 10/1000 | BATCH 58/71 | LOSS: 0.0007388889029921995\n",
      "TRAIN: EPOCH 10/1000 | BATCH 59/71 | LOSS: 0.0007379235952006032\n",
      "TRAIN: EPOCH 10/1000 | BATCH 60/71 | LOSS: 0.000742385642923659\n",
      "TRAIN: EPOCH 10/1000 | BATCH 61/71 | LOSS: 0.000739048242629055\n",
      "TRAIN: EPOCH 10/1000 | BATCH 62/71 | LOSS: 0.0007344673799779562\n",
      "TRAIN: EPOCH 10/1000 | BATCH 63/71 | LOSS: 0.0007324265925490181\n",
      "TRAIN: EPOCH 10/1000 | BATCH 64/71 | LOSS: 0.0007317048058701822\n",
      "TRAIN: EPOCH 10/1000 | BATCH 65/71 | LOSS: 0.0007337418977509845\n",
      "TRAIN: EPOCH 10/1000 | BATCH 66/71 | LOSS: 0.0007321150094242905\n",
      "TRAIN: EPOCH 10/1000 | BATCH 67/71 | LOSS: 0.000726652805443497\n",
      "TRAIN: EPOCH 10/1000 | BATCH 68/71 | LOSS: 0.0007238073482859771\n",
      "TRAIN: EPOCH 10/1000 | BATCH 69/71 | LOSS: 0.0007197420116946367\n",
      "TRAIN: EPOCH 10/1000 | BATCH 70/71 | LOSS: 0.0007136990578556565\n",
      "VAL: EPOCH 10/1000 | BATCH 0/8 | LOSS: 0.001165759633295238\n",
      "VAL: EPOCH 10/1000 | BATCH 1/8 | LOSS: 0.0007930070860311389\n",
      "VAL: EPOCH 10/1000 | BATCH 2/8 | LOSS: 0.0007726159334803621\n",
      "VAL: EPOCH 10/1000 | BATCH 3/8 | LOSS: 0.0006547998054884374\n",
      "VAL: EPOCH 10/1000 | BATCH 4/8 | LOSS: 0.0006794190034270286\n",
      "VAL: EPOCH 10/1000 | BATCH 5/8 | LOSS: 0.0006808021765512725\n",
      "VAL: EPOCH 10/1000 | BATCH 6/8 | LOSS: 0.0006759842924241509\n",
      "VAL: EPOCH 10/1000 | BATCH 7/8 | LOSS: 0.0006438739583245479\n",
      "TRAIN: EPOCH 11/1000 | BATCH 0/71 | LOSS: 0.000547330011613667\n",
      "TRAIN: EPOCH 11/1000 | BATCH 1/71 | LOSS: 0.0005280276236589998\n",
      "TRAIN: EPOCH 11/1000 | BATCH 2/71 | LOSS: 0.0004651043758106728\n",
      "TRAIN: EPOCH 11/1000 | BATCH 3/71 | LOSS: 0.0004508785641519353\n",
      "TRAIN: EPOCH 11/1000 | BATCH 4/71 | LOSS: 0.00043644390534609557\n",
      "TRAIN: EPOCH 11/1000 | BATCH 5/71 | LOSS: 0.00042886492641021806\n",
      "TRAIN: EPOCH 11/1000 | BATCH 6/71 | LOSS: 0.00046187230119747776\n",
      "TRAIN: EPOCH 11/1000 | BATCH 7/71 | LOSS: 0.0005219359154580161\n",
      "TRAIN: EPOCH 11/1000 | BATCH 8/71 | LOSS: 0.0005743214731208152\n",
      "TRAIN: EPOCH 11/1000 | BATCH 9/71 | LOSS: 0.0005733039462938905\n",
      "TRAIN: EPOCH 11/1000 | BATCH 10/71 | LOSS: 0.0005745103328742764\n",
      "TRAIN: EPOCH 11/1000 | BATCH 11/71 | LOSS: 0.0005820529816749817\n",
      "TRAIN: EPOCH 11/1000 | BATCH 12/71 | LOSS: 0.0005654867608637477\n",
      "TRAIN: EPOCH 11/1000 | BATCH 13/71 | LOSS: 0.0005510707352576512\n",
      "TRAIN: EPOCH 11/1000 | BATCH 14/71 | LOSS: 0.0005500125504719714\n",
      "TRAIN: EPOCH 11/1000 | BATCH 15/71 | LOSS: 0.0005434859995148145\n",
      "TRAIN: EPOCH 11/1000 | BATCH 16/71 | LOSS: 0.0005426046075573301\n",
      "TRAIN: EPOCH 11/1000 | BATCH 17/71 | LOSS: 0.0005346184415328833\n",
      "TRAIN: EPOCH 11/1000 | BATCH 18/71 | LOSS: 0.0005492737369709893\n",
      "TRAIN: EPOCH 11/1000 | BATCH 19/71 | LOSS: 0.0005392625636886806\n",
      "TRAIN: EPOCH 11/1000 | BATCH 20/71 | LOSS: 0.0005328463268510642\n",
      "TRAIN: EPOCH 11/1000 | BATCH 21/71 | LOSS: 0.0005404149150391194\n",
      "TRAIN: EPOCH 11/1000 | BATCH 22/71 | LOSS: 0.0005489713740130157\n",
      "TRAIN: EPOCH 11/1000 | BATCH 23/71 | LOSS: 0.000550576216483023\n",
      "TRAIN: EPOCH 11/1000 | BATCH 24/71 | LOSS: 0.0005640536244027317\n",
      "TRAIN: EPOCH 11/1000 | BATCH 25/71 | LOSS: 0.000563049879229556\n",
      "TRAIN: EPOCH 11/1000 | BATCH 26/71 | LOSS: 0.0005591983499471098\n",
      "TRAIN: EPOCH 11/1000 | BATCH 27/71 | LOSS: 0.000568632638273162\n",
      "TRAIN: EPOCH 11/1000 | BATCH 28/71 | LOSS: 0.0005769469837481477\n",
      "TRAIN: EPOCH 11/1000 | BATCH 29/71 | LOSS: 0.000573510822141543\n",
      "TRAIN: EPOCH 11/1000 | BATCH 30/71 | LOSS: 0.0005726205098683074\n",
      "TRAIN: EPOCH 11/1000 | BATCH 31/71 | LOSS: 0.0005732093268306926\n",
      "TRAIN: EPOCH 11/1000 | BATCH 32/71 | LOSS: 0.0005689255538692868\n",
      "TRAIN: EPOCH 11/1000 | BATCH 33/71 | LOSS: 0.0005702970999717602\n",
      "TRAIN: EPOCH 11/1000 | BATCH 34/71 | LOSS: 0.0005726842667042677\n",
      "TRAIN: EPOCH 11/1000 | BATCH 35/71 | LOSS: 0.0005736632794853196\n",
      "TRAIN: EPOCH 11/1000 | BATCH 36/71 | LOSS: 0.0005699683812988067\n",
      "TRAIN: EPOCH 11/1000 | BATCH 37/71 | LOSS: 0.0005718578659549454\n",
      "TRAIN: EPOCH 11/1000 | BATCH 38/71 | LOSS: 0.0005644151742438761\n",
      "TRAIN: EPOCH 11/1000 | BATCH 39/71 | LOSS: 0.000564037846197607\n",
      "TRAIN: EPOCH 11/1000 | BATCH 40/71 | LOSS: 0.0005607992210482224\n",
      "TRAIN: EPOCH 11/1000 | BATCH 41/71 | LOSS: 0.0005657398037978315\n",
      "TRAIN: EPOCH 11/1000 | BATCH 42/71 | LOSS: 0.0005771312490376363\n",
      "TRAIN: EPOCH 11/1000 | BATCH 43/71 | LOSS: 0.0005814184134413318\n",
      "TRAIN: EPOCH 11/1000 | BATCH 44/71 | LOSS: 0.0005836140849472334\n",
      "TRAIN: EPOCH 11/1000 | BATCH 45/71 | LOSS: 0.0005831205211427954\n",
      "TRAIN: EPOCH 11/1000 | BATCH 46/71 | LOSS: 0.0005871026208277474\n",
      "TRAIN: EPOCH 11/1000 | BATCH 47/71 | LOSS: 0.0005848646093606172\n",
      "TRAIN: EPOCH 11/1000 | BATCH 48/71 | LOSS: 0.0005832742543282861\n",
      "TRAIN: EPOCH 11/1000 | BATCH 49/71 | LOSS: 0.0005874392611440271\n",
      "TRAIN: EPOCH 11/1000 | BATCH 50/71 | LOSS: 0.0005897718935967514\n",
      "TRAIN: EPOCH 11/1000 | BATCH 51/71 | LOSS: 0.0005873829806939914\n",
      "TRAIN: EPOCH 11/1000 | BATCH 52/71 | LOSS: 0.0005824276458094213\n",
      "TRAIN: EPOCH 11/1000 | BATCH 53/71 | LOSS: 0.0005866875543250461\n",
      "TRAIN: EPOCH 11/1000 | BATCH 54/71 | LOSS: 0.0005844742636492645\n",
      "TRAIN: EPOCH 11/1000 | BATCH 55/71 | LOSS: 0.0005881485896160095\n",
      "TRAIN: EPOCH 11/1000 | BATCH 56/71 | LOSS: 0.0005905424788994551\n",
      "TRAIN: EPOCH 11/1000 | BATCH 57/71 | LOSS: 0.0005866337947924782\n",
      "TRAIN: EPOCH 11/1000 | BATCH 58/71 | LOSS: 0.0005827275325812525\n",
      "TRAIN: EPOCH 11/1000 | BATCH 59/71 | LOSS: 0.0005789882901202267\n",
      "TRAIN: EPOCH 11/1000 | BATCH 60/71 | LOSS: 0.0005803053261574786\n",
      "TRAIN: EPOCH 11/1000 | BATCH 61/71 | LOSS: 0.0005793155991517367\n",
      "TRAIN: EPOCH 11/1000 | BATCH 62/71 | LOSS: 0.0005767186447638013\n",
      "TRAIN: EPOCH 11/1000 | BATCH 63/71 | LOSS: 0.0005752816136919137\n",
      "TRAIN: EPOCH 11/1000 | BATCH 64/71 | LOSS: 0.0005745630590424228\n",
      "TRAIN: EPOCH 11/1000 | BATCH 65/71 | LOSS: 0.0005715144705556503\n",
      "TRAIN: EPOCH 11/1000 | BATCH 66/71 | LOSS: 0.0005716704411619801\n",
      "TRAIN: EPOCH 11/1000 | BATCH 67/71 | LOSS: 0.0005708225050030331\n",
      "TRAIN: EPOCH 11/1000 | BATCH 68/71 | LOSS: 0.0005706075407679368\n",
      "TRAIN: EPOCH 11/1000 | BATCH 69/71 | LOSS: 0.0005677019129507243\n",
      "TRAIN: EPOCH 11/1000 | BATCH 70/71 | LOSS: 0.0005656962982267404\n",
      "VAL: EPOCH 11/1000 | BATCH 0/8 | LOSS: 0.0009713388280943036\n",
      "VAL: EPOCH 11/1000 | BATCH 1/8 | LOSS: 0.0006441115110646933\n",
      "VAL: EPOCH 11/1000 | BATCH 2/8 | LOSS: 0.0006321374288139244\n",
      "VAL: EPOCH 11/1000 | BATCH 3/8 | LOSS: 0.0005412645841715857\n",
      "VAL: EPOCH 11/1000 | BATCH 4/8 | LOSS: 0.0005593530484475195\n",
      "VAL: EPOCH 11/1000 | BATCH 5/8 | LOSS: 0.0005608835684445997\n",
      "VAL: EPOCH 11/1000 | BATCH 6/8 | LOSS: 0.0005569348993178989\n",
      "VAL: EPOCH 11/1000 | BATCH 7/8 | LOSS: 0.000531614656210877\n",
      "TRAIN: EPOCH 12/1000 | BATCH 0/71 | LOSS: 0.00036003923742100596\n",
      "TRAIN: EPOCH 12/1000 | BATCH 1/71 | LOSS: 0.0004896401951555163\n",
      "TRAIN: EPOCH 12/1000 | BATCH 2/71 | LOSS: 0.00045974168460816145\n",
      "TRAIN: EPOCH 12/1000 | BATCH 3/71 | LOSS: 0.00042840711830649525\n",
      "TRAIN: EPOCH 12/1000 | BATCH 4/71 | LOSS: 0.0005504412460140884\n",
      "TRAIN: EPOCH 12/1000 | BATCH 5/71 | LOSS: 0.000532162880214552\n",
      "TRAIN: EPOCH 12/1000 | BATCH 6/71 | LOSS: 0.0005146842824095594\n",
      "TRAIN: EPOCH 12/1000 | BATCH 7/71 | LOSS: 0.0005046611149737146\n",
      "TRAIN: EPOCH 12/1000 | BATCH 8/71 | LOSS: 0.0005466747518059694\n",
      "TRAIN: EPOCH 12/1000 | BATCH 9/71 | LOSS: 0.0005221597122726962\n",
      "TRAIN: EPOCH 12/1000 | BATCH 10/71 | LOSS: 0.0005189379206223583\n",
      "TRAIN: EPOCH 12/1000 | BATCH 11/71 | LOSS: 0.0005183731142703133\n",
      "TRAIN: EPOCH 12/1000 | BATCH 12/71 | LOSS: 0.0005145983819742329\n",
      "TRAIN: EPOCH 12/1000 | BATCH 13/71 | LOSS: 0.0005095907420452152\n",
      "TRAIN: EPOCH 12/1000 | BATCH 14/71 | LOSS: 0.0005356395849958062\n",
      "TRAIN: EPOCH 12/1000 | BATCH 15/71 | LOSS: 0.0005275911626085872\n",
      "TRAIN: EPOCH 12/1000 | BATCH 16/71 | LOSS: 0.000529530449592344\n",
      "TRAIN: EPOCH 12/1000 | BATCH 17/71 | LOSS: 0.0005323791362267608\n",
      "TRAIN: EPOCH 12/1000 | BATCH 18/71 | LOSS: 0.0005545197529595738\n",
      "TRAIN: EPOCH 12/1000 | BATCH 19/71 | LOSS: 0.0005382411909522489\n",
      "TRAIN: EPOCH 12/1000 | BATCH 20/71 | LOSS: 0.0005417148266652865\n",
      "TRAIN: EPOCH 12/1000 | BATCH 21/71 | LOSS: 0.0005399979391685603\n",
      "TRAIN: EPOCH 12/1000 | BATCH 22/71 | LOSS: 0.0005370910474321927\n",
      "TRAIN: EPOCH 12/1000 | BATCH 23/71 | LOSS: 0.00053004498113296\n",
      "TRAIN: EPOCH 12/1000 | BATCH 24/71 | LOSS: 0.000522722052410245\n",
      "TRAIN: EPOCH 12/1000 | BATCH 25/71 | LOSS: 0.0005241911500119246\n",
      "TRAIN: EPOCH 12/1000 | BATCH 26/71 | LOSS: 0.0005174481549248514\n",
      "TRAIN: EPOCH 12/1000 | BATCH 27/71 | LOSS: 0.0005199613608835664\n",
      "TRAIN: EPOCH 12/1000 | BATCH 28/71 | LOSS: 0.0005123518459530996\n",
      "TRAIN: EPOCH 12/1000 | BATCH 29/71 | LOSS: 0.0005153235571924597\n",
      "TRAIN: EPOCH 12/1000 | BATCH 30/71 | LOSS: 0.0005134632961163598\n",
      "TRAIN: EPOCH 12/1000 | BATCH 31/71 | LOSS: 0.0005103188359498745\n",
      "TRAIN: EPOCH 12/1000 | BATCH 32/71 | LOSS: 0.0005079589830005937\n",
      "TRAIN: EPOCH 12/1000 | BATCH 33/71 | LOSS: 0.0005125826111693374\n",
      "TRAIN: EPOCH 12/1000 | BATCH 34/71 | LOSS: 0.0005165126656980387\n",
      "TRAIN: EPOCH 12/1000 | BATCH 35/71 | LOSS: 0.0005122233059309009\n",
      "TRAIN: EPOCH 12/1000 | BATCH 36/71 | LOSS: 0.0005123016622697784\n",
      "TRAIN: EPOCH 12/1000 | BATCH 37/71 | LOSS: 0.000509937798549225\n",
      "TRAIN: EPOCH 12/1000 | BATCH 38/71 | LOSS: 0.0005037901536501849\n",
      "TRAIN: EPOCH 12/1000 | BATCH 39/71 | LOSS: 0.000500804614421213\n",
      "TRAIN: EPOCH 12/1000 | BATCH 40/71 | LOSS: 0.0005036992646992298\n",
      "TRAIN: EPOCH 12/1000 | BATCH 41/71 | LOSS: 0.000510420639849534\n",
      "TRAIN: EPOCH 12/1000 | BATCH 42/71 | LOSS: 0.00050971555715795\n",
      "TRAIN: EPOCH 12/1000 | BATCH 43/71 | LOSS: 0.0005097974942393855\n",
      "TRAIN: EPOCH 12/1000 | BATCH 44/71 | LOSS: 0.0005048442981205881\n",
      "TRAIN: EPOCH 12/1000 | BATCH 45/71 | LOSS: 0.0005044460172623234\n",
      "TRAIN: EPOCH 12/1000 | BATCH 46/71 | LOSS: 0.000507039001875339\n",
      "TRAIN: EPOCH 12/1000 | BATCH 47/71 | LOSS: 0.0005036984069496006\n",
      "TRAIN: EPOCH 12/1000 | BATCH 48/71 | LOSS: 0.0005009952434861311\n",
      "TRAIN: EPOCH 12/1000 | BATCH 49/71 | LOSS: 0.0005008224915945903\n",
      "TRAIN: EPOCH 12/1000 | BATCH 50/71 | LOSS: 0.0005016742432809563\n",
      "TRAIN: EPOCH 12/1000 | BATCH 51/71 | LOSS: 0.0005065958265242024\n",
      "TRAIN: EPOCH 12/1000 | BATCH 52/71 | LOSS: 0.0005082634122787431\n",
      "TRAIN: EPOCH 12/1000 | BATCH 53/71 | LOSS: 0.0005042093888968575\n",
      "TRAIN: EPOCH 12/1000 | BATCH 54/71 | LOSS: 0.0005017886631487107\n",
      "TRAIN: EPOCH 12/1000 | BATCH 55/71 | LOSS: 0.000498733307072793\n",
      "TRAIN: EPOCH 12/1000 | BATCH 56/71 | LOSS: 0.0004968772506496558\n",
      "TRAIN: EPOCH 12/1000 | BATCH 57/71 | LOSS: 0.0004981388423214506\n",
      "TRAIN: EPOCH 12/1000 | BATCH 58/71 | LOSS: 0.0004941614574587004\n",
      "TRAIN: EPOCH 12/1000 | BATCH 59/71 | LOSS: 0.0004928900234517641\n",
      "TRAIN: EPOCH 12/1000 | BATCH 60/71 | LOSS: 0.0004897719214777233\n",
      "TRAIN: EPOCH 12/1000 | BATCH 61/71 | LOSS: 0.0004875996120033726\n",
      "TRAIN: EPOCH 12/1000 | BATCH 62/71 | LOSS: 0.0004875187185548601\n",
      "TRAIN: EPOCH 12/1000 | BATCH 63/71 | LOSS: 0.0004840403976231755\n",
      "TRAIN: EPOCH 12/1000 | BATCH 64/71 | LOSS: 0.0004854825473068139\n",
      "TRAIN: EPOCH 12/1000 | BATCH 65/71 | LOSS: 0.00048632720515315395\n",
      "TRAIN: EPOCH 12/1000 | BATCH 66/71 | LOSS: 0.00048456221659070074\n",
      "TRAIN: EPOCH 12/1000 | BATCH 67/71 | LOSS: 0.00048219137020436497\n",
      "TRAIN: EPOCH 12/1000 | BATCH 68/71 | LOSS: 0.00048036529029638547\n",
      "TRAIN: EPOCH 12/1000 | BATCH 69/71 | LOSS: 0.00047941566985433125\n",
      "TRAIN: EPOCH 12/1000 | BATCH 70/71 | LOSS: 0.00047566265303930136\n",
      "VAL: EPOCH 12/1000 | BATCH 0/8 | LOSS: 0.000831303303129971\n",
      "VAL: EPOCH 12/1000 | BATCH 1/8 | LOSS: 0.0005485395377036184\n",
      "VAL: EPOCH 12/1000 | BATCH 2/8 | LOSS: 0.0005444762452195088\n",
      "VAL: EPOCH 12/1000 | BATCH 3/8 | LOSS: 0.0004658985744754318\n",
      "VAL: EPOCH 12/1000 | BATCH 4/8 | LOSS: 0.00048096895043272527\n",
      "VAL: EPOCH 12/1000 | BATCH 5/8 | LOSS: 0.00048250872108231607\n",
      "VAL: EPOCH 12/1000 | BATCH 6/8 | LOSS: 0.00047789393283892423\n",
      "VAL: EPOCH 12/1000 | BATCH 7/8 | LOSS: 0.0004569190259644529\n",
      "TRAIN: EPOCH 13/1000 | BATCH 0/71 | LOSS: 0.0003983433125540614\n",
      "TRAIN: EPOCH 13/1000 | BATCH 1/71 | LOSS: 0.0003755757788894698\n",
      "TRAIN: EPOCH 13/1000 | BATCH 2/71 | LOSS: 0.00048538888222537935\n",
      "TRAIN: EPOCH 13/1000 | BATCH 3/71 | LOSS: 0.0004339813385740854\n",
      "TRAIN: EPOCH 13/1000 | BATCH 4/71 | LOSS: 0.00045951250358484687\n",
      "TRAIN: EPOCH 13/1000 | BATCH 5/71 | LOSS: 0.0004451978262901927\n",
      "TRAIN: EPOCH 13/1000 | BATCH 6/71 | LOSS: 0.0004145811212116054\n",
      "TRAIN: EPOCH 13/1000 | BATCH 7/71 | LOSS: 0.0004411656263982877\n",
      "TRAIN: EPOCH 13/1000 | BATCH 8/71 | LOSS: 0.00044035905092540715\n",
      "TRAIN: EPOCH 13/1000 | BATCH 9/71 | LOSS: 0.0004327075992478058\n",
      "TRAIN: EPOCH 13/1000 | BATCH 10/71 | LOSS: 0.0004573298528240147\n",
      "TRAIN: EPOCH 13/1000 | BATCH 11/71 | LOSS: 0.0004472737903900755\n",
      "TRAIN: EPOCH 13/1000 | BATCH 12/71 | LOSS: 0.00043123812289335404\n",
      "TRAIN: EPOCH 13/1000 | BATCH 13/71 | LOSS: 0.0004359123787643122\n",
      "TRAIN: EPOCH 13/1000 | BATCH 14/71 | LOSS: 0.00044665918685495856\n",
      "TRAIN: EPOCH 13/1000 | BATCH 15/71 | LOSS: 0.0004615560101228766\n",
      "TRAIN: EPOCH 13/1000 | BATCH 16/71 | LOSS: 0.00046800743952831803\n",
      "TRAIN: EPOCH 13/1000 | BATCH 17/71 | LOSS: 0.0004791439019350542\n",
      "TRAIN: EPOCH 13/1000 | BATCH 18/71 | LOSS: 0.0004652732007487334\n",
      "TRAIN: EPOCH 13/1000 | BATCH 19/71 | LOSS: 0.00046330616678460503\n",
      "TRAIN: EPOCH 13/1000 | BATCH 20/71 | LOSS: 0.000454398295481778\n",
      "TRAIN: EPOCH 13/1000 | BATCH 21/71 | LOSS: 0.00045477443795376035\n",
      "TRAIN: EPOCH 13/1000 | BATCH 22/71 | LOSS: 0.00045055676562934303\n",
      "TRAIN: EPOCH 13/1000 | BATCH 23/71 | LOSS: 0.0004433840967976721\n",
      "TRAIN: EPOCH 13/1000 | BATCH 24/71 | LOSS: 0.0004400034848367795\n",
      "TRAIN: EPOCH 13/1000 | BATCH 25/71 | LOSS: 0.00043502151283274335\n",
      "TRAIN: EPOCH 13/1000 | BATCH 26/71 | LOSS: 0.00043049921374336853\n",
      "TRAIN: EPOCH 13/1000 | BATCH 27/71 | LOSS: 0.0004273317458033229\n",
      "TRAIN: EPOCH 13/1000 | BATCH 28/71 | LOSS: 0.0004240286346838071\n",
      "TRAIN: EPOCH 13/1000 | BATCH 29/71 | LOSS: 0.0004194524338042053\n",
      "TRAIN: EPOCH 13/1000 | BATCH 30/71 | LOSS: 0.0004201721068614373\n",
      "TRAIN: EPOCH 13/1000 | BATCH 31/71 | LOSS: 0.00043451187912069145\n",
      "TRAIN: EPOCH 13/1000 | BATCH 32/71 | LOSS: 0.000434661939807208\n",
      "TRAIN: EPOCH 13/1000 | BATCH 33/71 | LOSS: 0.00043130480823011194\n",
      "TRAIN: EPOCH 13/1000 | BATCH 34/71 | LOSS: 0.00042591409915725565\n",
      "TRAIN: EPOCH 13/1000 | BATCH 35/71 | LOSS: 0.0004297912168517036\n",
      "TRAIN: EPOCH 13/1000 | BATCH 36/71 | LOSS: 0.0004276747644620922\n",
      "TRAIN: EPOCH 13/1000 | BATCH 37/71 | LOSS: 0.0004284934774133083\n",
      "TRAIN: EPOCH 13/1000 | BATCH 38/71 | LOSS: 0.0004340533032052171\n",
      "TRAIN: EPOCH 13/1000 | BATCH 39/71 | LOSS: 0.0004321043070376618\n",
      "TRAIN: EPOCH 13/1000 | BATCH 40/71 | LOSS: 0.0004337549492824686\n",
      "TRAIN: EPOCH 13/1000 | BATCH 41/71 | LOSS: 0.00042984718934998715\n",
      "TRAIN: EPOCH 13/1000 | BATCH 42/71 | LOSS: 0.00043046542106272087\n",
      "TRAIN: EPOCH 13/1000 | BATCH 43/71 | LOSS: 0.00042689971947269936\n",
      "TRAIN: EPOCH 13/1000 | BATCH 44/71 | LOSS: 0.0004253743342511977\n",
      "TRAIN: EPOCH 13/1000 | BATCH 45/71 | LOSS: 0.0004267949992461818\n",
      "TRAIN: EPOCH 13/1000 | BATCH 46/71 | LOSS: 0.000424689175796378\n",
      "TRAIN: EPOCH 13/1000 | BATCH 47/71 | LOSS: 0.0004210061351841432\n",
      "TRAIN: EPOCH 13/1000 | BATCH 48/71 | LOSS: 0.0004176878128604659\n",
      "TRAIN: EPOCH 13/1000 | BATCH 49/71 | LOSS: 0.00041567242267774416\n",
      "TRAIN: EPOCH 13/1000 | BATCH 50/71 | LOSS: 0.0004183435895338691\n",
      "TRAIN: EPOCH 13/1000 | BATCH 51/71 | LOSS: 0.00042695222257932\n",
      "TRAIN: EPOCH 13/1000 | BATCH 52/71 | LOSS: 0.00042370702156507113\n",
      "TRAIN: EPOCH 13/1000 | BATCH 53/71 | LOSS: 0.00042357711747711247\n",
      "TRAIN: EPOCH 13/1000 | BATCH 54/71 | LOSS: 0.00042142444740007206\n",
      "TRAIN: EPOCH 13/1000 | BATCH 55/71 | LOSS: 0.00042180087023422984\n",
      "TRAIN: EPOCH 13/1000 | BATCH 56/71 | LOSS: 0.0004180931236509136\n",
      "TRAIN: EPOCH 13/1000 | BATCH 57/71 | LOSS: 0.0004201973555107794\n",
      "TRAIN: EPOCH 13/1000 | BATCH 58/71 | LOSS: 0.0004198624201462273\n",
      "TRAIN: EPOCH 13/1000 | BATCH 59/71 | LOSS: 0.00041708621987102866\n",
      "TRAIN: EPOCH 13/1000 | BATCH 60/71 | LOSS: 0.000418402771532276\n",
      "TRAIN: EPOCH 13/1000 | BATCH 61/71 | LOSS: 0.0004186904639467567\n",
      "TRAIN: EPOCH 13/1000 | BATCH 62/71 | LOSS: 0.00042083215163392384\n",
      "TRAIN: EPOCH 13/1000 | BATCH 63/71 | LOSS: 0.0004203522537409299\n",
      "TRAIN: EPOCH 13/1000 | BATCH 64/71 | LOSS: 0.0004208790239107867\n",
      "TRAIN: EPOCH 13/1000 | BATCH 65/71 | LOSS: 0.0004196354784653522\n",
      "TRAIN: EPOCH 13/1000 | BATCH 66/71 | LOSS: 0.0004195318283224756\n",
      "TRAIN: EPOCH 13/1000 | BATCH 67/71 | LOSS: 0.0004177226272825946\n",
      "TRAIN: EPOCH 13/1000 | BATCH 68/71 | LOSS: 0.0004159308621150347\n",
      "TRAIN: EPOCH 13/1000 | BATCH 69/71 | LOSS: 0.0004141823698384022\n",
      "TRAIN: EPOCH 13/1000 | BATCH 70/71 | LOSS: 0.00041196141150143836\n",
      "VAL: EPOCH 13/1000 | BATCH 0/8 | LOSS: 0.000734279106836766\n",
      "VAL: EPOCH 13/1000 | BATCH 1/8 | LOSS: 0.00048308755503967404\n",
      "VAL: EPOCH 13/1000 | BATCH 2/8 | LOSS: 0.0004783452410871784\n",
      "VAL: EPOCH 13/1000 | BATCH 3/8 | LOSS: 0.00040987144893733785\n",
      "VAL: EPOCH 13/1000 | BATCH 4/8 | LOSS: 0.00042011182522401215\n",
      "VAL: EPOCH 13/1000 | BATCH 5/8 | LOSS: 0.00042368885381923366\n",
      "VAL: EPOCH 13/1000 | BATCH 6/8 | LOSS: 0.0004193795321043581\n",
      "VAL: EPOCH 13/1000 | BATCH 7/8 | LOSS: 0.0004013337347714696\n",
      "TRAIN: EPOCH 14/1000 | BATCH 0/71 | LOSS: 0.00028500784537754953\n",
      "TRAIN: EPOCH 14/1000 | BATCH 1/71 | LOSS: 0.0003945961216231808\n",
      "TRAIN: EPOCH 14/1000 | BATCH 2/71 | LOSS: 0.0006632974739962568\n",
      "TRAIN: EPOCH 14/1000 | BATCH 3/71 | LOSS: 0.0005897032970096916\n",
      "TRAIN: EPOCH 14/1000 | BATCH 4/71 | LOSS: 0.0005472876480780541\n",
      "TRAIN: EPOCH 14/1000 | BATCH 5/71 | LOSS: 0.000496282341676609\n",
      "TRAIN: EPOCH 14/1000 | BATCH 6/71 | LOSS: 0.00047703539894428104\n",
      "TRAIN: EPOCH 14/1000 | BATCH 7/71 | LOSS: 0.0004597665374603821\n",
      "TRAIN: EPOCH 14/1000 | BATCH 8/71 | LOSS: 0.00044249920594868146\n",
      "TRAIN: EPOCH 14/1000 | BATCH 9/71 | LOSS: 0.0004332588068791665\n",
      "TRAIN: EPOCH 14/1000 | BATCH 10/71 | LOSS: 0.0004112135978754271\n",
      "TRAIN: EPOCH 14/1000 | BATCH 11/71 | LOSS: 0.0004204086144454777\n",
      "TRAIN: EPOCH 14/1000 | BATCH 12/71 | LOSS: 0.00040843817315852415\n",
      "TRAIN: EPOCH 14/1000 | BATCH 13/71 | LOSS: 0.0003949414676753804\n",
      "TRAIN: EPOCH 14/1000 | BATCH 14/71 | LOSS: 0.000403678360938405\n",
      "TRAIN: EPOCH 14/1000 | BATCH 15/71 | LOSS: 0.0004148289717704756\n",
      "TRAIN: EPOCH 14/1000 | BATCH 16/71 | LOSS: 0.00041693890200215667\n",
      "TRAIN: EPOCH 14/1000 | BATCH 17/71 | LOSS: 0.0004123818734013993\n",
      "TRAIN: EPOCH 14/1000 | BATCH 18/71 | LOSS: 0.00041628372855484486\n",
      "TRAIN: EPOCH 14/1000 | BATCH 19/71 | LOSS: 0.0004038896353449672\n",
      "TRAIN: EPOCH 14/1000 | BATCH 20/71 | LOSS: 0.0003952108603525197\n",
      "TRAIN: EPOCH 14/1000 | BATCH 21/71 | LOSS: 0.0003931064314780418\n",
      "TRAIN: EPOCH 14/1000 | BATCH 22/71 | LOSS: 0.0003958810386551625\n",
      "TRAIN: EPOCH 14/1000 | BATCH 23/71 | LOSS: 0.0003908346576887804\n",
      "TRAIN: EPOCH 14/1000 | BATCH 24/71 | LOSS: 0.0003826666937675327\n",
      "TRAIN: EPOCH 14/1000 | BATCH 25/71 | LOSS: 0.00038061279449791\n",
      "TRAIN: EPOCH 14/1000 | BATCH 26/71 | LOSS: 0.000382286663537983\n",
      "TRAIN: EPOCH 14/1000 | BATCH 27/71 | LOSS: 0.00037630325667643253\n",
      "TRAIN: EPOCH 14/1000 | BATCH 28/71 | LOSS: 0.00037149020825946255\n",
      "TRAIN: EPOCH 14/1000 | BATCH 29/71 | LOSS: 0.00036928786964078125\n",
      "TRAIN: EPOCH 14/1000 | BATCH 30/71 | LOSS: 0.0003711320896400139\n",
      "TRAIN: EPOCH 14/1000 | BATCH 31/71 | LOSS: 0.00037245675093799946\n",
      "TRAIN: EPOCH 14/1000 | BATCH 32/71 | LOSS: 0.000369178375959481\n",
      "TRAIN: EPOCH 14/1000 | BATCH 33/71 | LOSS: 0.00037614493040189916\n",
      "TRAIN: EPOCH 14/1000 | BATCH 34/71 | LOSS: 0.0003744927214159231\n",
      "TRAIN: EPOCH 14/1000 | BATCH 35/71 | LOSS: 0.00037350494555236463\n",
      "TRAIN: EPOCH 14/1000 | BATCH 36/71 | LOSS: 0.00038486692348079806\n",
      "TRAIN: EPOCH 14/1000 | BATCH 37/71 | LOSS: 0.0003801515344285259\n",
      "TRAIN: EPOCH 14/1000 | BATCH 38/71 | LOSS: 0.00038074888736726\n",
      "TRAIN: EPOCH 14/1000 | BATCH 39/71 | LOSS: 0.000381134772032965\n",
      "TRAIN: EPOCH 14/1000 | BATCH 40/71 | LOSS: 0.0003782022735898996\n",
      "TRAIN: EPOCH 14/1000 | BATCH 41/71 | LOSS: 0.0003795724728011659\n",
      "TRAIN: EPOCH 14/1000 | BATCH 42/71 | LOSS: 0.00037552651727225546\n",
      "TRAIN: EPOCH 14/1000 | BATCH 43/71 | LOSS: 0.00037313371087128127\n",
      "TRAIN: EPOCH 14/1000 | BATCH 44/71 | LOSS: 0.0003707893516143991\n",
      "TRAIN: EPOCH 14/1000 | BATCH 45/71 | LOSS: 0.00037160921277498585\n",
      "TRAIN: EPOCH 14/1000 | BATCH 46/71 | LOSS: 0.0003682143879098263\n",
      "TRAIN: EPOCH 14/1000 | BATCH 47/71 | LOSS: 0.00036896770355573\n",
      "TRAIN: EPOCH 14/1000 | BATCH 48/71 | LOSS: 0.00036995984377501036\n",
      "TRAIN: EPOCH 14/1000 | BATCH 49/71 | LOSS: 0.00037029381637694315\n",
      "TRAIN: EPOCH 14/1000 | BATCH 50/71 | LOSS: 0.0003682092364589848\n",
      "TRAIN: EPOCH 14/1000 | BATCH 51/71 | LOSS: 0.000371607262562835\n",
      "TRAIN: EPOCH 14/1000 | BATCH 52/71 | LOSS: 0.0003737913999807546\n",
      "TRAIN: EPOCH 14/1000 | BATCH 53/71 | LOSS: 0.0003765196724324832\n",
      "TRAIN: EPOCH 14/1000 | BATCH 54/71 | LOSS: 0.0003761263947870413\n",
      "TRAIN: EPOCH 14/1000 | BATCH 55/71 | LOSS: 0.00037567107652388846\n",
      "TRAIN: EPOCH 14/1000 | BATCH 56/71 | LOSS: 0.00037453662458063804\n",
      "TRAIN: EPOCH 14/1000 | BATCH 57/71 | LOSS: 0.0003735290577831871\n",
      "TRAIN: EPOCH 14/1000 | BATCH 58/71 | LOSS: 0.00037140723429344026\n",
      "TRAIN: EPOCH 14/1000 | BATCH 59/71 | LOSS: 0.0003693339480378199\n",
      "TRAIN: EPOCH 14/1000 | BATCH 60/71 | LOSS: 0.0003699006230787939\n",
      "TRAIN: EPOCH 14/1000 | BATCH 61/71 | LOSS: 0.00036785577216388415\n",
      "TRAIN: EPOCH 14/1000 | BATCH 62/71 | LOSS: 0.0003690178354089666\n",
      "TRAIN: EPOCH 14/1000 | BATCH 63/71 | LOSS: 0.00036765989329978765\n",
      "TRAIN: EPOCH 14/1000 | BATCH 64/71 | LOSS: 0.00036999937751366256\n",
      "TRAIN: EPOCH 14/1000 | BATCH 65/71 | LOSS: 0.0003676449736248645\n",
      "TRAIN: EPOCH 14/1000 | BATCH 66/71 | LOSS: 0.000366118728311788\n",
      "TRAIN: EPOCH 14/1000 | BATCH 67/71 | LOSS: 0.00036596509230816187\n",
      "TRAIN: EPOCH 14/1000 | BATCH 68/71 | LOSS: 0.00036416318587304187\n",
      "TRAIN: EPOCH 14/1000 | BATCH 69/71 | LOSS: 0.00036365663670169724\n",
      "TRAIN: EPOCH 14/1000 | BATCH 70/71 | LOSS: 0.00036166214427589376\n",
      "VAL: EPOCH 14/1000 | BATCH 0/8 | LOSS: 0.0006472642417065799\n",
      "VAL: EPOCH 14/1000 | BATCH 1/8 | LOSS: 0.00042688939720392227\n",
      "VAL: EPOCH 14/1000 | BATCH 2/8 | LOSS: 0.0004236051657547553\n",
      "VAL: EPOCH 14/1000 | BATCH 3/8 | LOSS: 0.0003630494938988704\n",
      "VAL: EPOCH 14/1000 | BATCH 4/8 | LOSS: 0.0003697472420753911\n",
      "VAL: EPOCH 14/1000 | BATCH 5/8 | LOSS: 0.0003755142994729492\n",
      "VAL: EPOCH 14/1000 | BATCH 6/8 | LOSS: 0.00037087481177877635\n",
      "VAL: EPOCH 14/1000 | BATCH 7/8 | LOSS: 0.000355002521246206\n",
      "TRAIN: EPOCH 15/1000 | BATCH 0/71 | LOSS: 0.00032158676185645163\n",
      "TRAIN: EPOCH 15/1000 | BATCH 1/71 | LOSS: 0.0003103053168160841\n",
      "TRAIN: EPOCH 15/1000 | BATCH 2/71 | LOSS: 0.00026883636019192636\n",
      "TRAIN: EPOCH 15/1000 | BATCH 3/71 | LOSS: 0.00029361984343267977\n",
      "TRAIN: EPOCH 15/1000 | BATCH 4/71 | LOSS: 0.0002838152227923274\n",
      "TRAIN: EPOCH 15/1000 | BATCH 5/71 | LOSS: 0.0003083036378181229\n",
      "TRAIN: EPOCH 15/1000 | BATCH 6/71 | LOSS: 0.00031803056065525325\n",
      "TRAIN: EPOCH 15/1000 | BATCH 7/71 | LOSS: 0.00031657311046728864\n",
      "TRAIN: EPOCH 15/1000 | BATCH 8/71 | LOSS: 0.00031354209123593237\n",
      "TRAIN: EPOCH 15/1000 | BATCH 9/71 | LOSS: 0.0003157182713039219\n",
      "TRAIN: EPOCH 15/1000 | BATCH 10/71 | LOSS: 0.0003363953437656164\n",
      "TRAIN: EPOCH 15/1000 | BATCH 11/71 | LOSS: 0.0003418242607343321\n",
      "TRAIN: EPOCH 15/1000 | BATCH 12/71 | LOSS: 0.00033314426904186036\n",
      "TRAIN: EPOCH 15/1000 | BATCH 13/71 | LOSS: 0.0003507015083284516\n",
      "TRAIN: EPOCH 15/1000 | BATCH 14/71 | LOSS: 0.0003492961385442565\n",
      "TRAIN: EPOCH 15/1000 | BATCH 15/71 | LOSS: 0.0003524656658555614\n",
      "TRAIN: EPOCH 15/1000 | BATCH 16/71 | LOSS: 0.0003498282595126725\n",
      "TRAIN: EPOCH 15/1000 | BATCH 17/71 | LOSS: 0.000349870021131614\n",
      "TRAIN: EPOCH 15/1000 | BATCH 18/71 | LOSS: 0.0003669014473225137\n",
      "TRAIN: EPOCH 15/1000 | BATCH 19/71 | LOSS: 0.00036808026052312923\n",
      "TRAIN: EPOCH 15/1000 | BATCH 20/71 | LOSS: 0.00036032032152260874\n",
      "TRAIN: EPOCH 15/1000 | BATCH 21/71 | LOSS: 0.0003648801059832542\n",
      "TRAIN: EPOCH 15/1000 | BATCH 22/71 | LOSS: 0.0003628851662379811\n",
      "TRAIN: EPOCH 15/1000 | BATCH 23/71 | LOSS: 0.00035886495061276946\n",
      "TRAIN: EPOCH 15/1000 | BATCH 24/71 | LOSS: 0.0003618857654510066\n",
      "TRAIN: EPOCH 15/1000 | BATCH 25/71 | LOSS: 0.0003562911291597769\n",
      "TRAIN: EPOCH 15/1000 | BATCH 26/71 | LOSS: 0.00036023124455284605\n",
      "TRAIN: EPOCH 15/1000 | BATCH 27/71 | LOSS: 0.0003661241410424866\n",
      "TRAIN: EPOCH 15/1000 | BATCH 28/71 | LOSS: 0.00036367021675687283\n",
      "TRAIN: EPOCH 15/1000 | BATCH 29/71 | LOSS: 0.0003646868450838762\n",
      "TRAIN: EPOCH 15/1000 | BATCH 30/71 | LOSS: 0.00036012618746157854\n",
      "TRAIN: EPOCH 15/1000 | BATCH 31/71 | LOSS: 0.000356493868821417\n",
      "TRAIN: EPOCH 15/1000 | BATCH 32/71 | LOSS: 0.00036063881160578495\n",
      "TRAIN: EPOCH 15/1000 | BATCH 33/71 | LOSS: 0.0003577168664538904\n",
      "TRAIN: EPOCH 15/1000 | BATCH 34/71 | LOSS: 0.00036064871487074667\n",
      "TRAIN: EPOCH 15/1000 | BATCH 35/71 | LOSS: 0.00035584418138670217\n",
      "TRAIN: EPOCH 15/1000 | BATCH 36/71 | LOSS: 0.0003594486790117682\n",
      "TRAIN: EPOCH 15/1000 | BATCH 37/71 | LOSS: 0.0003557211220092875\n",
      "TRAIN: EPOCH 15/1000 | BATCH 38/71 | LOSS: 0.00035265186288131354\n",
      "TRAIN: EPOCH 15/1000 | BATCH 39/71 | LOSS: 0.0003514520329190418\n",
      "TRAIN: EPOCH 15/1000 | BATCH 40/71 | LOSS: 0.0003475022368081959\n",
      "TRAIN: EPOCH 15/1000 | BATCH 41/71 | LOSS: 0.00035080527283883257\n",
      "TRAIN: EPOCH 15/1000 | BATCH 42/71 | LOSS: 0.0003487836866677457\n",
      "TRAIN: EPOCH 15/1000 | BATCH 43/71 | LOSS: 0.0003448927663240201\n",
      "TRAIN: EPOCH 15/1000 | BATCH 44/71 | LOSS: 0.0003434759490321287\n",
      "TRAIN: EPOCH 15/1000 | BATCH 45/71 | LOSS: 0.00033951602318152055\n",
      "TRAIN: EPOCH 15/1000 | BATCH 46/71 | LOSS: 0.00033786211954182053\n",
      "TRAIN: EPOCH 15/1000 | BATCH 47/71 | LOSS: 0.0003351422962320309\n",
      "TRAIN: EPOCH 15/1000 | BATCH 48/71 | LOSS: 0.00033630573812026376\n",
      "TRAIN: EPOCH 15/1000 | BATCH 49/71 | LOSS: 0.0003326694670249708\n",
      "TRAIN: EPOCH 15/1000 | BATCH 50/71 | LOSS: 0.0003309589733629871\n",
      "TRAIN: EPOCH 15/1000 | BATCH 51/71 | LOSS: 0.00033440372751031717\n",
      "TRAIN: EPOCH 15/1000 | BATCH 52/71 | LOSS: 0.00033213519178251826\n",
      "TRAIN: EPOCH 15/1000 | BATCH 53/71 | LOSS: 0.00033003758424591205\n",
      "TRAIN: EPOCH 15/1000 | BATCH 54/71 | LOSS: 0.0003309590935135599\n",
      "TRAIN: EPOCH 15/1000 | BATCH 55/71 | LOSS: 0.0003293932581332878\n",
      "TRAIN: EPOCH 15/1000 | BATCH 56/71 | LOSS: 0.0003276431757911811\n",
      "TRAIN: EPOCH 15/1000 | BATCH 57/71 | LOSS: 0.00032788677194260126\n",
      "TRAIN: EPOCH 15/1000 | BATCH 58/71 | LOSS: 0.00032583454656375226\n",
      "TRAIN: EPOCH 15/1000 | BATCH 59/71 | LOSS: 0.00032741088968274803\n",
      "TRAIN: EPOCH 15/1000 | BATCH 60/71 | LOSS: 0.00032851487645291585\n",
      "TRAIN: EPOCH 15/1000 | BATCH 61/71 | LOSS: 0.00032614916656917384\n",
      "TRAIN: EPOCH 15/1000 | BATCH 62/71 | LOSS: 0.0003239825226566089\n",
      "TRAIN: EPOCH 15/1000 | BATCH 63/71 | LOSS: 0.0003232920967093378\n",
      "TRAIN: EPOCH 15/1000 | BATCH 64/71 | LOSS: 0.0003224919639671078\n",
      "TRAIN: EPOCH 15/1000 | BATCH 65/71 | LOSS: 0.0003233619716025494\n",
      "TRAIN: EPOCH 15/1000 | BATCH 66/71 | LOSS: 0.00032126941977054884\n",
      "TRAIN: EPOCH 15/1000 | BATCH 67/71 | LOSS: 0.00032292860080761947\n",
      "TRAIN: EPOCH 15/1000 | BATCH 68/71 | LOSS: 0.00032362200663951427\n",
      "TRAIN: EPOCH 15/1000 | BATCH 69/71 | LOSS: 0.0003220890453251611\n",
      "TRAIN: EPOCH 15/1000 | BATCH 70/71 | LOSS: 0.00032058623262753213\n",
      "VAL: EPOCH 15/1000 | BATCH 0/8 | LOSS: 0.0005679127061739564\n",
      "VAL: EPOCH 15/1000 | BATCH 1/8 | LOSS: 0.00037510247784666717\n",
      "VAL: EPOCH 15/1000 | BATCH 2/8 | LOSS: 0.0003757868738224109\n",
      "VAL: EPOCH 15/1000 | BATCH 3/8 | LOSS: 0.0003245206826250069\n",
      "VAL: EPOCH 15/1000 | BATCH 4/8 | LOSS: 0.00032916722120717167\n",
      "VAL: EPOCH 15/1000 | BATCH 5/8 | LOSS: 0.000332987227011472\n",
      "VAL: EPOCH 15/1000 | BATCH 6/8 | LOSS: 0.0003292185429017991\n",
      "VAL: EPOCH 15/1000 | BATCH 7/8 | LOSS: 0.00031529362058790866\n",
      "TRAIN: EPOCH 16/1000 | BATCH 0/71 | LOSS: 0.0003085253410972655\n",
      "TRAIN: EPOCH 16/1000 | BATCH 1/71 | LOSS: 0.0002476901572663337\n",
      "TRAIN: EPOCH 16/1000 | BATCH 2/71 | LOSS: 0.00025539716201213497\n",
      "TRAIN: EPOCH 16/1000 | BATCH 3/71 | LOSS: 0.0002966485408251174\n",
      "TRAIN: EPOCH 16/1000 | BATCH 4/71 | LOSS: 0.00028369174979161473\n",
      "TRAIN: EPOCH 16/1000 | BATCH 5/71 | LOSS: 0.0002830226512742229\n",
      "TRAIN: EPOCH 16/1000 | BATCH 6/71 | LOSS: 0.0002743546189906608\n",
      "TRAIN: EPOCH 16/1000 | BATCH 7/71 | LOSS: 0.00027823741947941016\n",
      "TRAIN: EPOCH 16/1000 | BATCH 8/71 | LOSS: 0.00028928412130982097\n",
      "TRAIN: EPOCH 16/1000 | BATCH 9/71 | LOSS: 0.00028524596855277194\n",
      "TRAIN: EPOCH 16/1000 | BATCH 10/71 | LOSS: 0.0003041035127931867\n",
      "TRAIN: EPOCH 16/1000 | BATCH 11/71 | LOSS: 0.00029958616869407706\n",
      "TRAIN: EPOCH 16/1000 | BATCH 12/71 | LOSS: 0.0002931831773961536\n",
      "TRAIN: EPOCH 16/1000 | BATCH 13/71 | LOSS: 0.00029031192181199525\n",
      "TRAIN: EPOCH 16/1000 | BATCH 14/71 | LOSS: 0.00029543027728019903\n",
      "TRAIN: EPOCH 16/1000 | BATCH 15/71 | LOSS: 0.0002976807800223469\n",
      "TRAIN: EPOCH 16/1000 | BATCH 16/71 | LOSS: 0.000306576636777369\n",
      "TRAIN: EPOCH 16/1000 | BATCH 17/71 | LOSS: 0.00029856233757325553\n",
      "TRAIN: EPOCH 16/1000 | BATCH 18/71 | LOSS: 0.0003119896014380318\n",
      "TRAIN: EPOCH 16/1000 | BATCH 19/71 | LOSS: 0.0003112595215498004\n",
      "TRAIN: EPOCH 16/1000 | BATCH 20/71 | LOSS: 0.0003121630877666619\n",
      "TRAIN: EPOCH 16/1000 | BATCH 21/71 | LOSS: 0.0003090061198401434\n",
      "TRAIN: EPOCH 16/1000 | BATCH 22/71 | LOSS: 0.00030340171867029983\n",
      "TRAIN: EPOCH 16/1000 | BATCH 23/71 | LOSS: 0.0002982819551107241\n",
      "TRAIN: EPOCH 16/1000 | BATCH 24/71 | LOSS: 0.00029642170236911626\n",
      "TRAIN: EPOCH 16/1000 | BATCH 25/71 | LOSS: 0.00030011084555343795\n",
      "TRAIN: EPOCH 16/1000 | BATCH 26/71 | LOSS: 0.0002992590582127579\n",
      "TRAIN: EPOCH 16/1000 | BATCH 27/71 | LOSS: 0.0002969212691823486\n",
      "TRAIN: EPOCH 16/1000 | BATCH 28/71 | LOSS: 0.0003003850554172271\n",
      "TRAIN: EPOCH 16/1000 | BATCH 29/71 | LOSS: 0.0003057952150508451\n",
      "TRAIN: EPOCH 16/1000 | BATCH 30/71 | LOSS: 0.0003046434153915353\n",
      "TRAIN: EPOCH 16/1000 | BATCH 31/71 | LOSS: 0.0003018393113052298\n",
      "TRAIN: EPOCH 16/1000 | BATCH 32/71 | LOSS: 0.00030007954928530097\n",
      "TRAIN: EPOCH 16/1000 | BATCH 33/71 | LOSS: 0.00029841473844819977\n",
      "TRAIN: EPOCH 16/1000 | BATCH 34/71 | LOSS: 0.00029880572567760413\n",
      "TRAIN: EPOCH 16/1000 | BATCH 35/71 | LOSS: 0.0002944373308208823\n",
      "TRAIN: EPOCH 16/1000 | BATCH 36/71 | LOSS: 0.00029943815732966304\n",
      "TRAIN: EPOCH 16/1000 | BATCH 37/71 | LOSS: 0.000302899320606804\n",
      "TRAIN: EPOCH 16/1000 | BATCH 38/71 | LOSS: 0.00030074531110958796\n",
      "TRAIN: EPOCH 16/1000 | BATCH 39/71 | LOSS: 0.0002967843633086886\n",
      "TRAIN: EPOCH 16/1000 | BATCH 40/71 | LOSS: 0.00029493009504044383\n",
      "TRAIN: EPOCH 16/1000 | BATCH 41/71 | LOSS: 0.00029808703125343614\n",
      "TRAIN: EPOCH 16/1000 | BATCH 42/71 | LOSS: 0.00029746067729172145\n",
      "TRAIN: EPOCH 16/1000 | BATCH 43/71 | LOSS: 0.00030079388406275854\n",
      "TRAIN: EPOCH 16/1000 | BATCH 44/71 | LOSS: 0.00029846183606423437\n",
      "TRAIN: EPOCH 16/1000 | BATCH 45/71 | LOSS: 0.00029856568194784063\n",
      "TRAIN: EPOCH 16/1000 | BATCH 46/71 | LOSS: 0.0002958627047383801\n",
      "TRAIN: EPOCH 16/1000 | BATCH 47/71 | LOSS: 0.00029987944617460016\n",
      "TRAIN: EPOCH 16/1000 | BATCH 48/71 | LOSS: 0.0002989098714778618\n",
      "TRAIN: EPOCH 16/1000 | BATCH 49/71 | LOSS: 0.00029636040737386794\n",
      "TRAIN: EPOCH 16/1000 | BATCH 50/71 | LOSS: 0.0002941616859935297\n",
      "TRAIN: EPOCH 16/1000 | BATCH 51/71 | LOSS: 0.0002941266278727338\n",
      "TRAIN: EPOCH 16/1000 | BATCH 52/71 | LOSS: 0.00029185531361090053\n",
      "TRAIN: EPOCH 16/1000 | BATCH 53/71 | LOSS: 0.0002922923080472241\n",
      "TRAIN: EPOCH 16/1000 | BATCH 54/71 | LOSS: 0.00029050429875496773\n",
      "TRAIN: EPOCH 16/1000 | BATCH 55/71 | LOSS: 0.0002930669078133568\n",
      "TRAIN: EPOCH 16/1000 | BATCH 56/71 | LOSS: 0.0002926022828895631\n",
      "TRAIN: EPOCH 16/1000 | BATCH 57/71 | LOSS: 0.00029312270158572635\n",
      "TRAIN: EPOCH 16/1000 | BATCH 58/71 | LOSS: 0.0002924551917496532\n",
      "TRAIN: EPOCH 16/1000 | BATCH 59/71 | LOSS: 0.00029201227516750805\n",
      "TRAIN: EPOCH 16/1000 | BATCH 60/71 | LOSS: 0.000293392197803411\n",
      "TRAIN: EPOCH 16/1000 | BATCH 61/71 | LOSS: 0.00029184914268215277\n",
      "TRAIN: EPOCH 16/1000 | BATCH 62/71 | LOSS: 0.00029062636132862066\n",
      "TRAIN: EPOCH 16/1000 | BATCH 63/71 | LOSS: 0.00028931006681887084\n",
      "TRAIN: EPOCH 16/1000 | BATCH 64/71 | LOSS: 0.00028805827874188813\n",
      "TRAIN: EPOCH 16/1000 | BATCH 65/71 | LOSS: 0.0002880528384897237\n",
      "TRAIN: EPOCH 16/1000 | BATCH 66/71 | LOSS: 0.0002879395841189952\n",
      "TRAIN: EPOCH 16/1000 | BATCH 67/71 | LOSS: 0.00028779624356984105\n",
      "TRAIN: EPOCH 16/1000 | BATCH 68/71 | LOSS: 0.00028693995736303157\n",
      "TRAIN: EPOCH 16/1000 | BATCH 69/71 | LOSS: 0.0002858472108658004\n",
      "TRAIN: EPOCH 16/1000 | BATCH 70/71 | LOSS: 0.00028584258197183706\n",
      "VAL: EPOCH 16/1000 | BATCH 0/8 | LOSS: 0.0005169176147319376\n",
      "VAL: EPOCH 16/1000 | BATCH 1/8 | LOSS: 0.0003460156876826659\n",
      "VAL: EPOCH 16/1000 | BATCH 2/8 | LOSS: 0.0003430137779408445\n",
      "VAL: EPOCH 16/1000 | BATCH 3/8 | LOSS: 0.000294126191874966\n",
      "VAL: EPOCH 16/1000 | BATCH 4/8 | LOSS: 0.00029728084919042883\n",
      "VAL: EPOCH 16/1000 | BATCH 5/8 | LOSS: 0.0003032068828664099\n",
      "VAL: EPOCH 16/1000 | BATCH 6/8 | LOSS: 0.0002992529480252415\n",
      "VAL: EPOCH 16/1000 | BATCH 7/8 | LOSS: 0.0002868409992515808\n",
      "TRAIN: EPOCH 17/1000 | BATCH 0/71 | LOSS: 0.00014106769231148064\n",
      "TRAIN: EPOCH 17/1000 | BATCH 1/71 | LOSS: 0.00016637847147649154\n",
      "TRAIN: EPOCH 17/1000 | BATCH 2/71 | LOSS: 0.00023487380531150848\n",
      "TRAIN: EPOCH 17/1000 | BATCH 3/71 | LOSS: 0.00024066206606221385\n",
      "TRAIN: EPOCH 17/1000 | BATCH 4/71 | LOSS: 0.0002516518608899787\n",
      "TRAIN: EPOCH 17/1000 | BATCH 5/71 | LOSS: 0.0002357681684467631\n",
      "TRAIN: EPOCH 17/1000 | BATCH 6/71 | LOSS: 0.00022795770408785238\n",
      "TRAIN: EPOCH 17/1000 | BATCH 7/71 | LOSS: 0.00022186080423125532\n",
      "TRAIN: EPOCH 17/1000 | BATCH 8/71 | LOSS: 0.00022321522798544416\n",
      "TRAIN: EPOCH 17/1000 | BATCH 9/71 | LOSS: 0.00021735114423790947\n",
      "TRAIN: EPOCH 17/1000 | BATCH 10/71 | LOSS: 0.0002253442507935688\n",
      "TRAIN: EPOCH 17/1000 | BATCH 11/71 | LOSS: 0.00023580548804602586\n",
      "TRAIN: EPOCH 17/1000 | BATCH 12/71 | LOSS: 0.00024713178124959365\n",
      "TRAIN: EPOCH 17/1000 | BATCH 13/71 | LOSS: 0.0002528680187034687\n",
      "TRAIN: EPOCH 17/1000 | BATCH 14/71 | LOSS: 0.00025534780288580803\n",
      "TRAIN: EPOCH 17/1000 | BATCH 15/71 | LOSS: 0.00026256864566676086\n",
      "TRAIN: EPOCH 17/1000 | BATCH 16/71 | LOSS: 0.00027205481498693935\n",
      "TRAIN: EPOCH 17/1000 | BATCH 17/71 | LOSS: 0.0002643327437302408\n",
      "TRAIN: EPOCH 17/1000 | BATCH 18/71 | LOSS: 0.00026249463323773324\n",
      "TRAIN: EPOCH 17/1000 | BATCH 19/71 | LOSS: 0.00025924553410732186\n",
      "TRAIN: EPOCH 17/1000 | BATCH 20/71 | LOSS: 0.0002689376699327979\n",
      "TRAIN: EPOCH 17/1000 | BATCH 21/71 | LOSS: 0.0002724589807753959\n",
      "TRAIN: EPOCH 17/1000 | BATCH 22/71 | LOSS: 0.0002693815489613411\n",
      "TRAIN: EPOCH 17/1000 | BATCH 23/71 | LOSS: 0.00026817397216897615\n",
      "TRAIN: EPOCH 17/1000 | BATCH 24/71 | LOSS: 0.0002696222212398425\n",
      "TRAIN: EPOCH 17/1000 | BATCH 25/71 | LOSS: 0.00026496991127854784\n",
      "TRAIN: EPOCH 17/1000 | BATCH 26/71 | LOSS: 0.00027203036276019974\n",
      "TRAIN: EPOCH 17/1000 | BATCH 27/71 | LOSS: 0.00026841519398398565\n",
      "TRAIN: EPOCH 17/1000 | BATCH 28/71 | LOSS: 0.0002700500546529054\n",
      "TRAIN: EPOCH 17/1000 | BATCH 29/71 | LOSS: 0.0002686374379360738\n",
      "TRAIN: EPOCH 17/1000 | BATCH 30/71 | LOSS: 0.0002669177436825609\n",
      "TRAIN: EPOCH 17/1000 | BATCH 31/71 | LOSS: 0.00026361831805843394\n",
      "TRAIN: EPOCH 17/1000 | BATCH 32/71 | LOSS: 0.00026481970615515655\n",
      "TRAIN: EPOCH 17/1000 | BATCH 33/71 | LOSS: 0.000263039704912123\n",
      "TRAIN: EPOCH 17/1000 | BATCH 34/71 | LOSS: 0.000268306812670614\n",
      "TRAIN: EPOCH 17/1000 | BATCH 35/71 | LOSS: 0.00026975850535867113\n",
      "TRAIN: EPOCH 17/1000 | BATCH 36/71 | LOSS: 0.00026670580091131096\n",
      "TRAIN: EPOCH 17/1000 | BATCH 37/71 | LOSS: 0.0002674559770017176\n",
      "TRAIN: EPOCH 17/1000 | BATCH 38/71 | LOSS: 0.00026714735894272913\n",
      "TRAIN: EPOCH 17/1000 | BATCH 39/71 | LOSS: 0.0002661901253304677\n",
      "TRAIN: EPOCH 17/1000 | BATCH 40/71 | LOSS: 0.0002655843311846938\n",
      "TRAIN: EPOCH 17/1000 | BATCH 41/71 | LOSS: 0.000264201360724179\n",
      "TRAIN: EPOCH 17/1000 | BATCH 42/71 | LOSS: 0.0002662078494363096\n",
      "TRAIN: EPOCH 17/1000 | BATCH 43/71 | LOSS: 0.00026755654206764035\n",
      "TRAIN: EPOCH 17/1000 | BATCH 44/71 | LOSS: 0.0002659408249504243\n",
      "TRAIN: EPOCH 17/1000 | BATCH 45/71 | LOSS: 0.00026657608996965394\n",
      "TRAIN: EPOCH 17/1000 | BATCH 46/71 | LOSS: 0.0002669196726353046\n",
      "TRAIN: EPOCH 17/1000 | BATCH 47/71 | LOSS: 0.000270245940858634\n",
      "TRAIN: EPOCH 17/1000 | BATCH 48/71 | LOSS: 0.0002694503099500791\n",
      "TRAIN: EPOCH 17/1000 | BATCH 49/71 | LOSS: 0.00026987139688571915\n",
      "TRAIN: EPOCH 17/1000 | BATCH 50/71 | LOSS: 0.00026880838656021905\n",
      "TRAIN: EPOCH 17/1000 | BATCH 51/71 | LOSS: 0.0002713923019585379\n",
      "TRAIN: EPOCH 17/1000 | BATCH 52/71 | LOSS: 0.0002714354061095466\n",
      "TRAIN: EPOCH 17/1000 | BATCH 53/71 | LOSS: 0.00027017136960273127\n",
      "TRAIN: EPOCH 17/1000 | BATCH 54/71 | LOSS: 0.0002703101887494664\n",
      "TRAIN: EPOCH 17/1000 | BATCH 55/71 | LOSS: 0.0002693219570833857\n",
      "TRAIN: EPOCH 17/1000 | BATCH 56/71 | LOSS: 0.00027165859845761016\n",
      "TRAIN: EPOCH 17/1000 | BATCH 57/71 | LOSS: 0.00026956995033883843\n",
      "TRAIN: EPOCH 17/1000 | BATCH 58/71 | LOSS: 0.0002697144636994828\n",
      "TRAIN: EPOCH 17/1000 | BATCH 59/71 | LOSS: 0.0002677209030177134\n",
      "TRAIN: EPOCH 17/1000 | BATCH 60/71 | LOSS: 0.0002664843894785545\n",
      "TRAIN: EPOCH 17/1000 | BATCH 61/71 | LOSS: 0.0002643557090777904\n",
      "TRAIN: EPOCH 17/1000 | BATCH 62/71 | LOSS: 0.00026351624327371757\n",
      "TRAIN: EPOCH 17/1000 | BATCH 63/71 | LOSS: 0.0002628968136377807\n",
      "TRAIN: EPOCH 17/1000 | BATCH 64/71 | LOSS: 0.0002610980904696939\n",
      "TRAIN: EPOCH 17/1000 | BATCH 65/71 | LOSS: 0.00026045132571281016\n",
      "TRAIN: EPOCH 17/1000 | BATCH 66/71 | LOSS: 0.0002599605995511859\n",
      "TRAIN: EPOCH 17/1000 | BATCH 67/71 | LOSS: 0.00026002239417935284\n",
      "TRAIN: EPOCH 17/1000 | BATCH 68/71 | LOSS: 0.0002591293353664999\n",
      "TRAIN: EPOCH 17/1000 | BATCH 69/71 | LOSS: 0.00025750486342336187\n",
      "TRAIN: EPOCH 17/1000 | BATCH 70/71 | LOSS: 0.00025597527339516705\n",
      "VAL: EPOCH 17/1000 | BATCH 0/8 | LOSS: 0.000446904479758814\n",
      "VAL: EPOCH 17/1000 | BATCH 1/8 | LOSS: 0.0003010313812410459\n",
      "VAL: EPOCH 17/1000 | BATCH 2/8 | LOSS: 0.0003031073623181631\n",
      "VAL: EPOCH 17/1000 | BATCH 3/8 | LOSS: 0.00026353403518442065\n",
      "VAL: EPOCH 17/1000 | BATCH 4/8 | LOSS: 0.0002643197542056441\n",
      "VAL: EPOCH 17/1000 | BATCH 5/8 | LOSS: 0.0002680147784606864\n",
      "VAL: EPOCH 17/1000 | BATCH 6/8 | LOSS: 0.0002645512327684888\n",
      "VAL: EPOCH 17/1000 | BATCH 7/8 | LOSS: 0.00025369017748744227\n",
      "TRAIN: EPOCH 18/1000 | BATCH 0/71 | LOSS: 0.0002635335549712181\n",
      "TRAIN: EPOCH 18/1000 | BATCH 1/71 | LOSS: 0.00022294923837762326\n",
      "TRAIN: EPOCH 18/1000 | BATCH 2/71 | LOSS: 0.00019656181878720722\n",
      "TRAIN: EPOCH 18/1000 | BATCH 3/71 | LOSS: 0.00019228507881052792\n",
      "TRAIN: EPOCH 18/1000 | BATCH 4/71 | LOSS: 0.00018694354803301395\n",
      "TRAIN: EPOCH 18/1000 | BATCH 5/71 | LOSS: 0.00018511046073399484\n",
      "TRAIN: EPOCH 18/1000 | BATCH 6/71 | LOSS: 0.00019312704638910612\n",
      "TRAIN: EPOCH 18/1000 | BATCH 7/71 | LOSS: 0.00019616335885075387\n",
      "TRAIN: EPOCH 18/1000 | BATCH 8/71 | LOSS: 0.00019429957319516689\n",
      "TRAIN: EPOCH 18/1000 | BATCH 9/71 | LOSS: 0.0002004452035180293\n",
      "TRAIN: EPOCH 18/1000 | BATCH 10/71 | LOSS: 0.00021064907328268006\n",
      "TRAIN: EPOCH 18/1000 | BATCH 11/71 | LOSS: 0.0002283763836506599\n",
      "TRAIN: EPOCH 18/1000 | BATCH 12/71 | LOSS: 0.00022909974750991052\n",
      "TRAIN: EPOCH 18/1000 | BATCH 13/71 | LOSS: 0.00023847227566875517\n",
      "TRAIN: EPOCH 18/1000 | BATCH 14/71 | LOSS: 0.00023730236086218307\n",
      "TRAIN: EPOCH 18/1000 | BATCH 15/71 | LOSS: 0.00024411906542809447\n",
      "TRAIN: EPOCH 18/1000 | BATCH 16/71 | LOSS: 0.00023977565561996445\n",
      "TRAIN: EPOCH 18/1000 | BATCH 17/71 | LOSS: 0.00023854790116375726\n",
      "TRAIN: EPOCH 18/1000 | BATCH 18/71 | LOSS: 0.00023564180791206462\n",
      "TRAIN: EPOCH 18/1000 | BATCH 19/71 | LOSS: 0.00023856207189965062\n",
      "TRAIN: EPOCH 18/1000 | BATCH 20/71 | LOSS: 0.00023712646257731\n",
      "TRAIN: EPOCH 18/1000 | BATCH 21/71 | LOSS: 0.00024082840205995706\n",
      "TRAIN: EPOCH 18/1000 | BATCH 22/71 | LOSS: 0.00023798207099468488\n",
      "TRAIN: EPOCH 18/1000 | BATCH 23/71 | LOSS: 0.00023648540566985807\n",
      "TRAIN: EPOCH 18/1000 | BATCH 24/71 | LOSS: 0.00023673808551393448\n",
      "TRAIN: EPOCH 18/1000 | BATCH 25/71 | LOSS: 0.00023874087715879656\n",
      "TRAIN: EPOCH 18/1000 | BATCH 26/71 | LOSS: 0.00023565038820918375\n",
      "TRAIN: EPOCH 18/1000 | BATCH 27/71 | LOSS: 0.00023536851170190078\n",
      "TRAIN: EPOCH 18/1000 | BATCH 28/71 | LOSS: 0.00023300014887856128\n",
      "TRAIN: EPOCH 18/1000 | BATCH 29/71 | LOSS: 0.00023645953673015658\n",
      "TRAIN: EPOCH 18/1000 | BATCH 30/71 | LOSS: 0.00023396078529091733\n",
      "TRAIN: EPOCH 18/1000 | BATCH 31/71 | LOSS: 0.00023408272045344347\n",
      "TRAIN: EPOCH 18/1000 | BATCH 32/71 | LOSS: 0.00023383542165075514\n",
      "TRAIN: EPOCH 18/1000 | BATCH 33/71 | LOSS: 0.0002338407134897459\n",
      "TRAIN: EPOCH 18/1000 | BATCH 34/71 | LOSS: 0.00023320916973586594\n",
      "TRAIN: EPOCH 18/1000 | BATCH 35/71 | LOSS: 0.000231375790463062\n",
      "TRAIN: EPOCH 18/1000 | BATCH 36/71 | LOSS: 0.00023776914482319576\n",
      "TRAIN: EPOCH 18/1000 | BATCH 37/71 | LOSS: 0.00023738433772354926\n",
      "TRAIN: EPOCH 18/1000 | BATCH 38/71 | LOSS: 0.0002388990848349073\n",
      "TRAIN: EPOCH 18/1000 | BATCH 39/71 | LOSS: 0.00024006126914173366\n",
      "TRAIN: EPOCH 18/1000 | BATCH 40/71 | LOSS: 0.00023802041705050391\n",
      "TRAIN: EPOCH 18/1000 | BATCH 41/71 | LOSS: 0.0002380861288180486\n",
      "TRAIN: EPOCH 18/1000 | BATCH 42/71 | LOSS: 0.00023852424225011883\n",
      "TRAIN: EPOCH 18/1000 | BATCH 43/71 | LOSS: 0.00023608503166046418\n",
      "TRAIN: EPOCH 18/1000 | BATCH 44/71 | LOSS: 0.00023850909815842494\n",
      "TRAIN: EPOCH 18/1000 | BATCH 45/71 | LOSS: 0.00023746730173867115\n",
      "TRAIN: EPOCH 18/1000 | BATCH 46/71 | LOSS: 0.00023524672259883758\n",
      "TRAIN: EPOCH 18/1000 | BATCH 47/71 | LOSS: 0.00023504235559812514\n",
      "TRAIN: EPOCH 18/1000 | BATCH 48/71 | LOSS: 0.00023585547632490265\n",
      "TRAIN: EPOCH 18/1000 | BATCH 49/71 | LOSS: 0.0002339254305115901\n",
      "TRAIN: EPOCH 18/1000 | BATCH 50/71 | LOSS: 0.0002350191281556024\n",
      "TRAIN: EPOCH 18/1000 | BATCH 51/71 | LOSS: 0.0002369220995975551\n",
      "TRAIN: EPOCH 18/1000 | BATCH 52/71 | LOSS: 0.00023546608693168482\n",
      "TRAIN: EPOCH 18/1000 | BATCH 53/71 | LOSS: 0.00023562878211830847\n",
      "TRAIN: EPOCH 18/1000 | BATCH 54/71 | LOSS: 0.0002341845948566598\n",
      "TRAIN: EPOCH 18/1000 | BATCH 55/71 | LOSS: 0.00023481750057336676\n",
      "TRAIN: EPOCH 18/1000 | BATCH 56/71 | LOSS: 0.00023312865002816054\n",
      "TRAIN: EPOCH 18/1000 | BATCH 57/71 | LOSS: 0.00023174685865624584\n",
      "TRAIN: EPOCH 18/1000 | BATCH 58/71 | LOSS: 0.00023108576419487027\n",
      "TRAIN: EPOCH 18/1000 | BATCH 59/71 | LOSS: 0.00023475843651491837\n",
      "TRAIN: EPOCH 18/1000 | BATCH 60/71 | LOSS: 0.00023387047055665954\n",
      "TRAIN: EPOCH 18/1000 | BATCH 61/71 | LOSS: 0.0002362351022442923\n",
      "TRAIN: EPOCH 18/1000 | BATCH 62/71 | LOSS: 0.00023488535745335476\n",
      "TRAIN: EPOCH 18/1000 | BATCH 63/71 | LOSS: 0.00023360124509963498\n",
      "TRAIN: EPOCH 18/1000 | BATCH 64/71 | LOSS: 0.00023357118748558255\n",
      "TRAIN: EPOCH 18/1000 | BATCH 65/71 | LOSS: 0.0002319163335910575\n",
      "TRAIN: EPOCH 18/1000 | BATCH 66/71 | LOSS: 0.00023287993685495276\n",
      "TRAIN: EPOCH 18/1000 | BATCH 67/71 | LOSS: 0.00023207279167870772\n",
      "TRAIN: EPOCH 18/1000 | BATCH 68/71 | LOSS: 0.000231591224555682\n",
      "TRAIN: EPOCH 18/1000 | BATCH 69/71 | LOSS: 0.00023133895322514166\n",
      "TRAIN: EPOCH 18/1000 | BATCH 70/71 | LOSS: 0.0002298249414643194\n",
      "VAL: EPOCH 18/1000 | BATCH 0/8 | LOSS: 0.0004061743966303766\n",
      "VAL: EPOCH 18/1000 | BATCH 1/8 | LOSS: 0.0002743032018770464\n",
      "VAL: EPOCH 18/1000 | BATCH 2/8 | LOSS: 0.0002733954897848889\n",
      "VAL: EPOCH 18/1000 | BATCH 3/8 | LOSS: 0.0002364864012633916\n",
      "VAL: EPOCH 18/1000 | BATCH 4/8 | LOSS: 0.00023633476230315863\n",
      "VAL: EPOCH 18/1000 | BATCH 5/8 | LOSS: 0.0002419938197514663\n",
      "VAL: EPOCH 18/1000 | BATCH 6/8 | LOSS: 0.00023844602817137326\n",
      "VAL: EPOCH 18/1000 | BATCH 7/8 | LOSS: 0.0002286535109305987\n",
      "TRAIN: EPOCH 19/1000 | BATCH 0/71 | LOSS: 0.0002233714039903134\n",
      "TRAIN: EPOCH 19/1000 | BATCH 1/71 | LOSS: 0.00020939514797646552\n",
      "TRAIN: EPOCH 19/1000 | BATCH 2/71 | LOSS: 0.00019172182267842194\n",
      "TRAIN: EPOCH 19/1000 | BATCH 3/71 | LOSS: 0.00018718113278737292\n",
      "TRAIN: EPOCH 19/1000 | BATCH 4/71 | LOSS: 0.0001840968063334003\n",
      "TRAIN: EPOCH 19/1000 | BATCH 5/71 | LOSS: 0.00017597425176063552\n",
      "TRAIN: EPOCH 19/1000 | BATCH 6/71 | LOSS: 0.00020191337223098214\n",
      "TRAIN: EPOCH 19/1000 | BATCH 7/71 | LOSS: 0.00019637307013908867\n",
      "TRAIN: EPOCH 19/1000 | BATCH 8/71 | LOSS: 0.00019556391634978354\n",
      "TRAIN: EPOCH 19/1000 | BATCH 9/71 | LOSS: 0.00020482565450947733\n",
      "TRAIN: EPOCH 19/1000 | BATCH 10/71 | LOSS: 0.0002056867471599782\n",
      "TRAIN: EPOCH 19/1000 | BATCH 11/71 | LOSS: 0.00019934526062570512\n",
      "TRAIN: EPOCH 19/1000 | BATCH 12/71 | LOSS: 0.00019548464311250986\n",
      "TRAIN: EPOCH 19/1000 | BATCH 13/71 | LOSS: 0.00020843511135483692\n",
      "TRAIN: EPOCH 19/1000 | BATCH 14/71 | LOSS: 0.00020320182375144213\n",
      "TRAIN: EPOCH 19/1000 | BATCH 15/71 | LOSS: 0.0002053438538496266\n",
      "TRAIN: EPOCH 19/1000 | BATCH 16/71 | LOSS: 0.00020671686626669457\n",
      "TRAIN: EPOCH 19/1000 | BATCH 17/71 | LOSS: 0.00021311016381433647\n",
      "TRAIN: EPOCH 19/1000 | BATCH 18/71 | LOSS: 0.00020878357769872405\n",
      "TRAIN: EPOCH 19/1000 | BATCH 19/71 | LOSS: 0.0002048723639745731\n",
      "TRAIN: EPOCH 19/1000 | BATCH 20/71 | LOSS: 0.00020667686961436024\n",
      "TRAIN: EPOCH 19/1000 | BATCH 21/71 | LOSS: 0.0002070854104452089\n",
      "TRAIN: EPOCH 19/1000 | BATCH 22/71 | LOSS: 0.00020755894618797237\n",
      "TRAIN: EPOCH 19/1000 | BATCH 23/71 | LOSS: 0.00020503152882156428\n",
      "TRAIN: EPOCH 19/1000 | BATCH 24/71 | LOSS: 0.00020153294230112806\n",
      "TRAIN: EPOCH 19/1000 | BATCH 25/71 | LOSS: 0.0002086537393692057\n",
      "TRAIN: EPOCH 19/1000 | BATCH 26/71 | LOSS: 0.00020871229639009539\n",
      "TRAIN: EPOCH 19/1000 | BATCH 27/71 | LOSS: 0.00021141302594125072\n",
      "TRAIN: EPOCH 19/1000 | BATCH 28/71 | LOSS: 0.0002101555008517482\n",
      "TRAIN: EPOCH 19/1000 | BATCH 29/71 | LOSS: 0.00021046588396226676\n",
      "TRAIN: EPOCH 19/1000 | BATCH 30/71 | LOSS: 0.00021058144897500414\n",
      "TRAIN: EPOCH 19/1000 | BATCH 31/71 | LOSS: 0.00021218953156676434\n",
      "TRAIN: EPOCH 19/1000 | BATCH 32/71 | LOSS: 0.0002108559863038615\n",
      "TRAIN: EPOCH 19/1000 | BATCH 33/71 | LOSS: 0.00021326747874434872\n",
      "TRAIN: EPOCH 19/1000 | BATCH 34/71 | LOSS: 0.0002130689301079006\n",
      "TRAIN: EPOCH 19/1000 | BATCH 35/71 | LOSS: 0.00021533101183498124\n",
      "TRAIN: EPOCH 19/1000 | BATCH 36/71 | LOSS: 0.0002150268530287478\n",
      "TRAIN: EPOCH 19/1000 | BATCH 37/71 | LOSS: 0.00021294564622190878\n",
      "TRAIN: EPOCH 19/1000 | BATCH 38/71 | LOSS: 0.00021642599369023138\n",
      "TRAIN: EPOCH 19/1000 | BATCH 39/71 | LOSS: 0.000215619776827225\n",
      "TRAIN: EPOCH 19/1000 | BATCH 40/71 | LOSS: 0.00021868353697511073\n",
      "TRAIN: EPOCH 19/1000 | BATCH 41/71 | LOSS: 0.00021678812360429825\n",
      "TRAIN: EPOCH 19/1000 | BATCH 42/71 | LOSS: 0.0002174821313738628\n",
      "TRAIN: EPOCH 19/1000 | BATCH 43/71 | LOSS: 0.00021967767464626706\n",
      "TRAIN: EPOCH 19/1000 | BATCH 44/71 | LOSS: 0.00021909633861570102\n",
      "TRAIN: EPOCH 19/1000 | BATCH 45/71 | LOSS: 0.00021716836367293422\n",
      "TRAIN: EPOCH 19/1000 | BATCH 46/71 | LOSS: 0.00021879307190674616\n",
      "TRAIN: EPOCH 19/1000 | BATCH 47/71 | LOSS: 0.00021849204904356156\n",
      "TRAIN: EPOCH 19/1000 | BATCH 48/71 | LOSS: 0.0002163227089818529\n",
      "TRAIN: EPOCH 19/1000 | BATCH 49/71 | LOSS: 0.00021664969361154363\n",
      "TRAIN: EPOCH 19/1000 | BATCH 50/71 | LOSS: 0.00021767749892794252\n",
      "TRAIN: EPOCH 19/1000 | BATCH 51/71 | LOSS: 0.000216421955253911\n",
      "TRAIN: EPOCH 19/1000 | BATCH 52/71 | LOSS: 0.0002156881577629631\n",
      "TRAIN: EPOCH 19/1000 | BATCH 53/71 | LOSS: 0.00021434995249836465\n",
      "TRAIN: EPOCH 19/1000 | BATCH 54/71 | LOSS: 0.00021331372289833697\n",
      "TRAIN: EPOCH 19/1000 | BATCH 55/71 | LOSS: 0.00021284863188546815\n",
      "TRAIN: EPOCH 19/1000 | BATCH 56/71 | LOSS: 0.00021278736076374915\n",
      "TRAIN: EPOCH 19/1000 | BATCH 57/71 | LOSS: 0.0002118829494253893\n",
      "TRAIN: EPOCH 19/1000 | BATCH 58/71 | LOSS: 0.00021144414815846516\n",
      "TRAIN: EPOCH 19/1000 | BATCH 59/71 | LOSS: 0.00021455179230542854\n",
      "TRAIN: EPOCH 19/1000 | BATCH 60/71 | LOSS: 0.0002133456391824379\n",
      "TRAIN: EPOCH 19/1000 | BATCH 61/71 | LOSS: 0.00021191285040620113\n",
      "TRAIN: EPOCH 19/1000 | BATCH 62/71 | LOSS: 0.00021091718476132622\n",
      "TRAIN: EPOCH 19/1000 | BATCH 63/71 | LOSS: 0.00021042348726041382\n",
      "TRAIN: EPOCH 19/1000 | BATCH 64/71 | LOSS: 0.00021044388077615832\n",
      "TRAIN: EPOCH 19/1000 | BATCH 65/71 | LOSS: 0.00020968432429733431\n",
      "TRAIN: EPOCH 19/1000 | BATCH 66/71 | LOSS: 0.00020993178959678748\n",
      "TRAIN: EPOCH 19/1000 | BATCH 67/71 | LOSS: 0.00020866357911419233\n",
      "TRAIN: EPOCH 19/1000 | BATCH 68/71 | LOSS: 0.00020998609134171537\n",
      "TRAIN: EPOCH 19/1000 | BATCH 69/71 | LOSS: 0.00020863364656439185\n",
      "TRAIN: EPOCH 19/1000 | BATCH 70/71 | LOSS: 0.00020911803021846654\n",
      "VAL: EPOCH 19/1000 | BATCH 0/8 | LOSS: 0.0003556421143002808\n",
      "VAL: EPOCH 19/1000 | BATCH 1/8 | LOSS: 0.00024035849492065609\n",
      "VAL: EPOCH 19/1000 | BATCH 2/8 | LOSS: 0.0002441874724657585\n",
      "VAL: EPOCH 19/1000 | BATCH 3/8 | LOSS: 0.00021508854115381837\n",
      "VAL: EPOCH 19/1000 | BATCH 4/8 | LOSS: 0.00021442483412101865\n",
      "VAL: EPOCH 19/1000 | BATCH 5/8 | LOSS: 0.0002165914120269008\n",
      "VAL: EPOCH 19/1000 | BATCH 6/8 | LOSS: 0.0002134578618487077\n",
      "VAL: EPOCH 19/1000 | BATCH 7/8 | LOSS: 0.00020477654288697522\n",
      "TRAIN: EPOCH 20/1000 | BATCH 0/71 | LOSS: 0.0002006810682360083\n",
      "TRAIN: EPOCH 20/1000 | BATCH 1/71 | LOSS: 0.00018659084889804944\n",
      "TRAIN: EPOCH 20/1000 | BATCH 2/71 | LOSS: 0.0002029211949169015\n",
      "TRAIN: EPOCH 20/1000 | BATCH 3/71 | LOSS: 0.0001948218014149461\n",
      "TRAIN: EPOCH 20/1000 | BATCH 4/71 | LOSS: 0.00019666137814056128\n",
      "TRAIN: EPOCH 20/1000 | BATCH 5/71 | LOSS: 0.00019230437707543993\n",
      "TRAIN: EPOCH 20/1000 | BATCH 6/71 | LOSS: 0.0002063873003602826\n",
      "TRAIN: EPOCH 20/1000 | BATCH 7/71 | LOSS: 0.00020104049508518074\n",
      "TRAIN: EPOCH 20/1000 | BATCH 8/71 | LOSS: 0.00019578991891143637\n",
      "TRAIN: EPOCH 20/1000 | BATCH 9/71 | LOSS: 0.00019054206350119785\n",
      "TRAIN: EPOCH 20/1000 | BATCH 10/71 | LOSS: 0.00018459852700206366\n",
      "TRAIN: EPOCH 20/1000 | BATCH 11/71 | LOSS: 0.0001824478094931692\n",
      "TRAIN: EPOCH 20/1000 | BATCH 12/71 | LOSS: 0.00018488637131388084\n",
      "TRAIN: EPOCH 20/1000 | BATCH 13/71 | LOSS: 0.00018611219690813283\n",
      "TRAIN: EPOCH 20/1000 | BATCH 14/71 | LOSS: 0.00018272901070304215\n",
      "TRAIN: EPOCH 20/1000 | BATCH 15/71 | LOSS: 0.00018318527054361766\n",
      "TRAIN: EPOCH 20/1000 | BATCH 16/71 | LOSS: 0.00018310514603987993\n",
      "TRAIN: EPOCH 20/1000 | BATCH 17/71 | LOSS: 0.00019173853878682066\n",
      "TRAIN: EPOCH 20/1000 | BATCH 18/71 | LOSS: 0.00019292016804683954\n",
      "TRAIN: EPOCH 20/1000 | BATCH 19/71 | LOSS: 0.00019155762493028305\n",
      "TRAIN: EPOCH 20/1000 | BATCH 20/71 | LOSS: 0.0001903505922417112\n",
      "TRAIN: EPOCH 20/1000 | BATCH 21/71 | LOSS: 0.00019016707069981336\n",
      "TRAIN: EPOCH 20/1000 | BATCH 22/71 | LOSS: 0.00018768407070361403\n",
      "TRAIN: EPOCH 20/1000 | BATCH 23/71 | LOSS: 0.0001872510850565353\n",
      "TRAIN: EPOCH 20/1000 | BATCH 24/71 | LOSS: 0.0001912684238050133\n",
      "TRAIN: EPOCH 20/1000 | BATCH 25/71 | LOSS: 0.000188907449438165\n",
      "TRAIN: EPOCH 20/1000 | BATCH 26/71 | LOSS: 0.00019073557686405602\n",
      "TRAIN: EPOCH 20/1000 | BATCH 27/71 | LOSS: 0.0001893478131803152\n",
      "TRAIN: EPOCH 20/1000 | BATCH 28/71 | LOSS: 0.0001873580471949716\n",
      "TRAIN: EPOCH 20/1000 | BATCH 29/71 | LOSS: 0.0001854080648627132\n",
      "TRAIN: EPOCH 20/1000 | BATCH 30/71 | LOSS: 0.00018642592476680875\n",
      "TRAIN: EPOCH 20/1000 | BATCH 31/71 | LOSS: 0.0001852050891102408\n",
      "TRAIN: EPOCH 20/1000 | BATCH 32/71 | LOSS: 0.00018335553944449532\n",
      "TRAIN: EPOCH 20/1000 | BATCH 33/71 | LOSS: 0.000181748164108927\n",
      "TRAIN: EPOCH 20/1000 | BATCH 34/71 | LOSS: 0.000185202803446113\n",
      "TRAIN: EPOCH 20/1000 | BATCH 35/71 | LOSS: 0.0001897698671705762\n",
      "TRAIN: EPOCH 20/1000 | BATCH 36/71 | LOSS: 0.0001901248308697816\n",
      "TRAIN: EPOCH 20/1000 | BATCH 37/71 | LOSS: 0.00018853996229345764\n",
      "TRAIN: EPOCH 20/1000 | BATCH 38/71 | LOSS: 0.00018620120644276866\n",
      "TRAIN: EPOCH 20/1000 | BATCH 39/71 | LOSS: 0.00018965719846164575\n",
      "TRAIN: EPOCH 20/1000 | BATCH 40/71 | LOSS: 0.00018869531174082446\n",
      "TRAIN: EPOCH 20/1000 | BATCH 41/71 | LOSS: 0.0001879811005935716\n",
      "TRAIN: EPOCH 20/1000 | BATCH 42/71 | LOSS: 0.0001883468359538224\n",
      "TRAIN: EPOCH 20/1000 | BATCH 43/71 | LOSS: 0.00018729613277008122\n",
      "TRAIN: EPOCH 20/1000 | BATCH 44/71 | LOSS: 0.00018633245215621882\n",
      "TRAIN: EPOCH 20/1000 | BATCH 45/71 | LOSS: 0.00018759432192461605\n",
      "TRAIN: EPOCH 20/1000 | BATCH 46/71 | LOSS: 0.0001864696571422602\n",
      "TRAIN: EPOCH 20/1000 | BATCH 47/71 | LOSS: 0.00018536530948646637\n",
      "TRAIN: EPOCH 20/1000 | BATCH 48/71 | LOSS: 0.00018835971437808013\n",
      "TRAIN: EPOCH 20/1000 | BATCH 49/71 | LOSS: 0.0001887476311821956\n",
      "TRAIN: EPOCH 20/1000 | BATCH 50/71 | LOSS: 0.00018703846909178822\n",
      "TRAIN: EPOCH 20/1000 | BATCH 51/71 | LOSS: 0.00018941213280553572\n",
      "TRAIN: EPOCH 20/1000 | BATCH 52/71 | LOSS: 0.00018990897345093062\n",
      "TRAIN: EPOCH 20/1000 | BATCH 53/71 | LOSS: 0.0001916660888430973\n",
      "TRAIN: EPOCH 20/1000 | BATCH 54/71 | LOSS: 0.000190793846426955\n",
      "TRAIN: EPOCH 20/1000 | BATCH 55/71 | LOSS: 0.00018973975004753032\n",
      "TRAIN: EPOCH 20/1000 | BATCH 56/71 | LOSS: 0.00019083172261608733\n",
      "TRAIN: EPOCH 20/1000 | BATCH 57/71 | LOSS: 0.00019377566335104598\n",
      "TRAIN: EPOCH 20/1000 | BATCH 58/71 | LOSS: 0.00019224249213732684\n",
      "TRAIN: EPOCH 20/1000 | BATCH 59/71 | LOSS: 0.00019143810895911885\n",
      "TRAIN: EPOCH 20/1000 | BATCH 60/71 | LOSS: 0.00019411297171991165\n",
      "TRAIN: EPOCH 20/1000 | BATCH 61/71 | LOSS: 0.00019357288117809673\n",
      "TRAIN: EPOCH 20/1000 | BATCH 62/71 | LOSS: 0.00019213798816689097\n",
      "TRAIN: EPOCH 20/1000 | BATCH 63/71 | LOSS: 0.00019158960333243158\n",
      "TRAIN: EPOCH 20/1000 | BATCH 64/71 | LOSS: 0.0001914073638125466\n",
      "TRAIN: EPOCH 20/1000 | BATCH 65/71 | LOSS: 0.0001902738784503361\n",
      "TRAIN: EPOCH 20/1000 | BATCH 66/71 | LOSS: 0.00018919087314818388\n",
      "TRAIN: EPOCH 20/1000 | BATCH 67/71 | LOSS: 0.00018787752763755098\n",
      "TRAIN: EPOCH 20/1000 | BATCH 68/71 | LOSS: 0.00018868953370458811\n",
      "TRAIN: EPOCH 20/1000 | BATCH 69/71 | LOSS: 0.00018988734050903337\n",
      "TRAIN: EPOCH 20/1000 | BATCH 70/71 | LOSS: 0.0001889380711847326\n",
      "VAL: EPOCH 20/1000 | BATCH 0/8 | LOSS: 0.00031637493520975113\n",
      "VAL: EPOCH 20/1000 | BATCH 1/8 | LOSS: 0.00021577855659415945\n",
      "VAL: EPOCH 20/1000 | BATCH 2/8 | LOSS: 0.0002196508770187696\n",
      "VAL: EPOCH 20/1000 | BATCH 3/8 | LOSS: 0.00019283454821561463\n",
      "VAL: EPOCH 20/1000 | BATCH 4/8 | LOSS: 0.00019182705145794898\n",
      "VAL: EPOCH 20/1000 | BATCH 5/8 | LOSS: 0.0001948021066103441\n",
      "VAL: EPOCH 20/1000 | BATCH 6/8 | LOSS: 0.00019148016040812114\n",
      "VAL: EPOCH 20/1000 | BATCH 7/8 | LOSS: 0.0001841681278165197\n",
      "TRAIN: EPOCH 21/1000 | BATCH 0/71 | LOSS: 0.00018241755606140941\n",
      "TRAIN: EPOCH 21/1000 | BATCH 1/71 | LOSS: 0.00020343611686257645\n",
      "TRAIN: EPOCH 21/1000 | BATCH 2/71 | LOSS: 0.0001810884423321113\n",
      "TRAIN: EPOCH 21/1000 | BATCH 3/71 | LOSS: 0.00020235826741554774\n",
      "TRAIN: EPOCH 21/1000 | BATCH 4/71 | LOSS: 0.0001868949446361512\n",
      "TRAIN: EPOCH 21/1000 | BATCH 5/71 | LOSS: 0.00019720960214423636\n",
      "TRAIN: EPOCH 21/1000 | BATCH 6/71 | LOSS: 0.00019879190118185112\n",
      "TRAIN: EPOCH 21/1000 | BATCH 7/71 | LOSS: 0.00021460002608364448\n",
      "TRAIN: EPOCH 21/1000 | BATCH 8/71 | LOSS: 0.00021558142099012103\n",
      "TRAIN: EPOCH 21/1000 | BATCH 9/71 | LOSS: 0.00021341181127354503\n",
      "TRAIN: EPOCH 21/1000 | BATCH 10/71 | LOSS: 0.000214604668806053\n",
      "TRAIN: EPOCH 21/1000 | BATCH 11/71 | LOSS: 0.0002120214315558163\n",
      "TRAIN: EPOCH 21/1000 | BATCH 12/71 | LOSS: 0.00020607718033716083\n",
      "TRAIN: EPOCH 21/1000 | BATCH 13/71 | LOSS: 0.00020747626279314448\n",
      "TRAIN: EPOCH 21/1000 | BATCH 14/71 | LOSS: 0.0002050847397185862\n",
      "TRAIN: EPOCH 21/1000 | BATCH 15/71 | LOSS: 0.00019973359303548932\n",
      "TRAIN: EPOCH 21/1000 | BATCH 16/71 | LOSS: 0.00019613896695184796\n",
      "TRAIN: EPOCH 21/1000 | BATCH 17/71 | LOSS: 0.00019381655632363012\n",
      "TRAIN: EPOCH 21/1000 | BATCH 18/71 | LOSS: 0.0001975726167744908\n",
      "TRAIN: EPOCH 21/1000 | BATCH 19/71 | LOSS: 0.00019746018951991574\n",
      "TRAIN: EPOCH 21/1000 | BATCH 20/71 | LOSS: 0.00019481179783941202\n",
      "TRAIN: EPOCH 21/1000 | BATCH 21/71 | LOSS: 0.0001927125995280221\n",
      "TRAIN: EPOCH 21/1000 | BATCH 22/71 | LOSS: 0.00019056542457648268\n",
      "TRAIN: EPOCH 21/1000 | BATCH 23/71 | LOSS: 0.00018934637167452215\n",
      "TRAIN: EPOCH 21/1000 | BATCH 24/71 | LOSS: 0.00018805576022714376\n",
      "TRAIN: EPOCH 21/1000 | BATCH 25/71 | LOSS: 0.00018549340921498908\n",
      "TRAIN: EPOCH 21/1000 | BATCH 26/71 | LOSS: 0.00018473166710464284\n",
      "TRAIN: EPOCH 21/1000 | BATCH 27/71 | LOSS: 0.0001830783598312077\n",
      "TRAIN: EPOCH 21/1000 | BATCH 28/71 | LOSS: 0.00018318225795218465\n",
      "TRAIN: EPOCH 21/1000 | BATCH 29/71 | LOSS: 0.00018160832575328338\n",
      "TRAIN: EPOCH 21/1000 | BATCH 30/71 | LOSS: 0.0001814673467437857\n",
      "TRAIN: EPOCH 21/1000 | BATCH 31/71 | LOSS: 0.00017991163554142986\n",
      "TRAIN: EPOCH 21/1000 | BATCH 32/71 | LOSS: 0.00017887013977584005\n",
      "TRAIN: EPOCH 21/1000 | BATCH 33/71 | LOSS: 0.00017999272270518464\n",
      "TRAIN: EPOCH 21/1000 | BATCH 34/71 | LOSS: 0.00017787924921971614\n",
      "TRAIN: EPOCH 21/1000 | BATCH 35/71 | LOSS: 0.00018052220325949343\n",
      "TRAIN: EPOCH 21/1000 | BATCH 36/71 | LOSS: 0.00017902140550447526\n",
      "TRAIN: EPOCH 21/1000 | BATCH 37/71 | LOSS: 0.0001789542341277028\n",
      "TRAIN: EPOCH 21/1000 | BATCH 38/71 | LOSS: 0.00017690967275158098\n",
      "TRAIN: EPOCH 21/1000 | BATCH 39/71 | LOSS: 0.00017535095521452603\n",
      "TRAIN: EPOCH 21/1000 | BATCH 40/71 | LOSS: 0.00017400581170322102\n",
      "TRAIN: EPOCH 21/1000 | BATCH 41/71 | LOSS: 0.00017260027154753472\n",
      "TRAIN: EPOCH 21/1000 | BATCH 42/71 | LOSS: 0.00017140211241704217\n",
      "TRAIN: EPOCH 21/1000 | BATCH 43/71 | LOSS: 0.00016981593101015525\n",
      "TRAIN: EPOCH 21/1000 | BATCH 44/71 | LOSS: 0.0001737146213094497\n",
      "TRAIN: EPOCH 21/1000 | BATCH 45/71 | LOSS: 0.00017276978289136542\n",
      "TRAIN: EPOCH 21/1000 | BATCH 46/71 | LOSS: 0.00017327022447097214\n",
      "TRAIN: EPOCH 21/1000 | BATCH 47/71 | LOSS: 0.0001728711905949846\n",
      "TRAIN: EPOCH 21/1000 | BATCH 48/71 | LOSS: 0.00017178806162092416\n",
      "TRAIN: EPOCH 21/1000 | BATCH 49/71 | LOSS: 0.00017332857809378765\n",
      "TRAIN: EPOCH 21/1000 | BATCH 50/71 | LOSS: 0.00017377678587389013\n",
      "TRAIN: EPOCH 21/1000 | BATCH 51/71 | LOSS: 0.0001735740346563174\n",
      "TRAIN: EPOCH 21/1000 | BATCH 52/71 | LOSS: 0.00017340394834667127\n",
      "TRAIN: EPOCH 21/1000 | BATCH 53/71 | LOSS: 0.00017412382846857696\n",
      "TRAIN: EPOCH 21/1000 | BATCH 54/71 | LOSS: 0.00017486721666169945\n",
      "TRAIN: EPOCH 21/1000 | BATCH 55/71 | LOSS: 0.00017534204419332257\n",
      "TRAIN: EPOCH 21/1000 | BATCH 56/71 | LOSS: 0.0001745200170900493\n",
      "TRAIN: EPOCH 21/1000 | BATCH 57/71 | LOSS: 0.000173504144186154\n",
      "TRAIN: EPOCH 21/1000 | BATCH 58/71 | LOSS: 0.00017300611598638154\n",
      "TRAIN: EPOCH 21/1000 | BATCH 59/71 | LOSS: 0.00017220611868348594\n",
      "TRAIN: EPOCH 21/1000 | BATCH 60/71 | LOSS: 0.00017099108305553617\n",
      "TRAIN: EPOCH 21/1000 | BATCH 61/71 | LOSS: 0.00017037576570498545\n",
      "TRAIN: EPOCH 21/1000 | BATCH 62/71 | LOSS: 0.0001695453205944172\n",
      "TRAIN: EPOCH 21/1000 | BATCH 63/71 | LOSS: 0.0001697226584838063\n",
      "TRAIN: EPOCH 21/1000 | BATCH 64/71 | LOSS: 0.0001689355756612853\n",
      "TRAIN: EPOCH 21/1000 | BATCH 65/71 | LOSS: 0.00016821249807959026\n",
      "TRAIN: EPOCH 21/1000 | BATCH 66/71 | LOSS: 0.00016777559727388644\n",
      "TRAIN: EPOCH 21/1000 | BATCH 67/71 | LOSS: 0.00016739960461991895\n",
      "TRAIN: EPOCH 21/1000 | BATCH 68/71 | LOSS: 0.00016877473630618465\n",
      "TRAIN: EPOCH 21/1000 | BATCH 69/71 | LOSS: 0.00017261602874246558\n",
      "TRAIN: EPOCH 21/1000 | BATCH 70/71 | LOSS: 0.0001715259914676672\n",
      "VAL: EPOCH 21/1000 | BATCH 0/8 | LOSS: 0.0002868573646992445\n",
      "VAL: EPOCH 21/1000 | BATCH 1/8 | LOSS: 0.00019667202286655083\n",
      "VAL: EPOCH 21/1000 | BATCH 2/8 | LOSS: 0.00019941851981760314\n",
      "VAL: EPOCH 21/1000 | BATCH 3/8 | LOSS: 0.0001747263413562905\n",
      "VAL: EPOCH 21/1000 | BATCH 4/8 | LOSS: 0.0001730300486087799\n",
      "VAL: EPOCH 21/1000 | BATCH 5/8 | LOSS: 0.0001758633297868073\n",
      "VAL: EPOCH 21/1000 | BATCH 6/8 | LOSS: 0.00017316349632372812\n",
      "VAL: EPOCH 21/1000 | BATCH 7/8 | LOSS: 0.00016657540982123464\n",
      "TRAIN: EPOCH 22/1000 | BATCH 0/71 | LOSS: 0.0001315067202085629\n",
      "TRAIN: EPOCH 22/1000 | BATCH 1/71 | LOSS: 0.0001465619498048909\n",
      "TRAIN: EPOCH 22/1000 | BATCH 2/71 | LOSS: 0.00014461347503432384\n",
      "TRAIN: EPOCH 22/1000 | BATCH 3/71 | LOSS: 0.0001391852056258358\n",
      "TRAIN: EPOCH 22/1000 | BATCH 4/71 | LOSS: 0.00015125962381716817\n",
      "TRAIN: EPOCH 22/1000 | BATCH 5/71 | LOSS: 0.00015168152458500117\n",
      "TRAIN: EPOCH 22/1000 | BATCH 6/71 | LOSS: 0.00014869856698039387\n",
      "TRAIN: EPOCH 22/1000 | BATCH 7/71 | LOSS: 0.0001447682770958636\n",
      "TRAIN: EPOCH 22/1000 | BATCH 8/71 | LOSS: 0.00014120171464229416\n",
      "TRAIN: EPOCH 22/1000 | BATCH 9/71 | LOSS: 0.00015417556787724606\n",
      "TRAIN: EPOCH 22/1000 | BATCH 10/71 | LOSS: 0.00015573092902989381\n",
      "TRAIN: EPOCH 22/1000 | BATCH 11/71 | LOSS: 0.00015375373974772325\n",
      "TRAIN: EPOCH 22/1000 | BATCH 12/71 | LOSS: 0.0001522968763870617\n",
      "TRAIN: EPOCH 22/1000 | BATCH 13/71 | LOSS: 0.00014857475980534218\n",
      "TRAIN: EPOCH 22/1000 | BATCH 14/71 | LOSS: 0.0001527943759962606\n",
      "TRAIN: EPOCH 22/1000 | BATCH 15/71 | LOSS: 0.00014946779174351832\n",
      "TRAIN: EPOCH 22/1000 | BATCH 16/71 | LOSS: 0.00014693696935222868\n",
      "TRAIN: EPOCH 22/1000 | BATCH 17/71 | LOSS: 0.00015605097804735933\n",
      "TRAIN: EPOCH 22/1000 | BATCH 18/71 | LOSS: 0.00016095146297869322\n",
      "TRAIN: EPOCH 22/1000 | BATCH 19/71 | LOSS: 0.0001616889116121456\n",
      "TRAIN: EPOCH 22/1000 | BATCH 20/71 | LOSS: 0.00016868173510634473\n",
      "TRAIN: EPOCH 22/1000 | BATCH 21/71 | LOSS: 0.00016859606363471937\n",
      "TRAIN: EPOCH 22/1000 | BATCH 22/71 | LOSS: 0.00017537498269635051\n",
      "TRAIN: EPOCH 22/1000 | BATCH 23/71 | LOSS: 0.00017284963572213505\n",
      "TRAIN: EPOCH 22/1000 | BATCH 24/71 | LOSS: 0.00017290390533162281\n",
      "TRAIN: EPOCH 22/1000 | BATCH 25/71 | LOSS: 0.00017279480464192323\n",
      "TRAIN: EPOCH 22/1000 | BATCH 26/71 | LOSS: 0.00017222330751354565\n",
      "TRAIN: EPOCH 22/1000 | BATCH 27/71 | LOSS: 0.00016953388928543842\n",
      "TRAIN: EPOCH 22/1000 | BATCH 28/71 | LOSS: 0.00016824710549152424\n",
      "TRAIN: EPOCH 22/1000 | BATCH 29/71 | LOSS: 0.000166387332380206\n",
      "TRAIN: EPOCH 22/1000 | BATCH 30/71 | LOSS: 0.00016495952454973913\n",
      "TRAIN: EPOCH 22/1000 | BATCH 31/71 | LOSS: 0.00016472627225994074\n",
      "TRAIN: EPOCH 22/1000 | BATCH 32/71 | LOSS: 0.0001625931964195898\n",
      "TRAIN: EPOCH 22/1000 | BATCH 33/71 | LOSS: 0.00016086116225136828\n",
      "TRAIN: EPOCH 22/1000 | BATCH 34/71 | LOSS: 0.0001600197551722106\n",
      "TRAIN: EPOCH 22/1000 | BATCH 35/71 | LOSS: 0.00015928069337355232\n",
      "TRAIN: EPOCH 22/1000 | BATCH 36/71 | LOSS: 0.00016192625913095685\n",
      "TRAIN: EPOCH 22/1000 | BATCH 37/71 | LOSS: 0.00016440022572121387\n",
      "TRAIN: EPOCH 22/1000 | BATCH 38/71 | LOSS: 0.00016382855504231813\n",
      "TRAIN: EPOCH 22/1000 | BATCH 39/71 | LOSS: 0.00016413406028732426\n",
      "TRAIN: EPOCH 22/1000 | BATCH 40/71 | LOSS: 0.0001643153953454003\n",
      "TRAIN: EPOCH 22/1000 | BATCH 41/71 | LOSS: 0.00016419001396917294\n",
      "TRAIN: EPOCH 22/1000 | BATCH 42/71 | LOSS: 0.00016224095063164916\n",
      "TRAIN: EPOCH 22/1000 | BATCH 43/71 | LOSS: 0.0001621765226115134\n",
      "TRAIN: EPOCH 22/1000 | BATCH 44/71 | LOSS: 0.0001620553964231577\n",
      "TRAIN: EPOCH 22/1000 | BATCH 45/71 | LOSS: 0.00016206325222889933\n",
      "TRAIN: EPOCH 22/1000 | BATCH 46/71 | LOSS: 0.00016087668212848974\n",
      "TRAIN: EPOCH 22/1000 | BATCH 47/71 | LOSS: 0.00016073083391650775\n",
      "TRAIN: EPOCH 22/1000 | BATCH 48/71 | LOSS: 0.0001623328599653549\n",
      "TRAIN: EPOCH 22/1000 | BATCH 49/71 | LOSS: 0.000162781441322295\n",
      "TRAIN: EPOCH 22/1000 | BATCH 50/71 | LOSS: 0.00016339395612400645\n",
      "TRAIN: EPOCH 22/1000 | BATCH 51/71 | LOSS: 0.00016400703638142798\n",
      "TRAIN: EPOCH 22/1000 | BATCH 52/71 | LOSS: 0.00016413703530627076\n",
      "TRAIN: EPOCH 22/1000 | BATCH 53/71 | LOSS: 0.00016590368654255549\n",
      "TRAIN: EPOCH 22/1000 | BATCH 54/71 | LOSS: 0.00016507141395281493\n",
      "TRAIN: EPOCH 22/1000 | BATCH 55/71 | LOSS: 0.000165287996294085\n",
      "TRAIN: EPOCH 22/1000 | BATCH 56/71 | LOSS: 0.00016444669962399066\n",
      "TRAIN: EPOCH 22/1000 | BATCH 57/71 | LOSS: 0.00016315008229114404\n",
      "TRAIN: EPOCH 22/1000 | BATCH 58/71 | LOSS: 0.00016193450105443628\n",
      "TRAIN: EPOCH 22/1000 | BATCH 59/71 | LOSS: 0.00016149183247762266\n",
      "TRAIN: EPOCH 22/1000 | BATCH 60/71 | LOSS: 0.00016200198249444823\n",
      "TRAIN: EPOCH 22/1000 | BATCH 61/71 | LOSS: 0.00016092633884381353\n",
      "TRAIN: EPOCH 22/1000 | BATCH 62/71 | LOSS: 0.00016047405243004923\n",
      "TRAIN: EPOCH 22/1000 | BATCH 63/71 | LOSS: 0.00016036461772728217\n",
      "TRAIN: EPOCH 22/1000 | BATCH 64/71 | LOSS: 0.0001596903774323157\n",
      "TRAIN: EPOCH 22/1000 | BATCH 65/71 | LOSS: 0.00015986210388346632\n",
      "TRAIN: EPOCH 22/1000 | BATCH 66/71 | LOSS: 0.00015929665943388758\n",
      "TRAIN: EPOCH 22/1000 | BATCH 67/71 | LOSS: 0.00015915374826684403\n",
      "TRAIN: EPOCH 22/1000 | BATCH 68/71 | LOSS: 0.00015859930251768884\n",
      "TRAIN: EPOCH 22/1000 | BATCH 69/71 | LOSS: 0.00015816582963452676\n",
      "TRAIN: EPOCH 22/1000 | BATCH 70/71 | LOSS: 0.00015654553535056603\n",
      "VAL: EPOCH 22/1000 | BATCH 0/8 | LOSS: 0.00025863200426101685\n",
      "VAL: EPOCH 22/1000 | BATCH 1/8 | LOSS: 0.00018235592506243847\n",
      "VAL: EPOCH 22/1000 | BATCH 2/8 | LOSS: 0.0001848916508606635\n",
      "VAL: EPOCH 22/1000 | BATCH 3/8 | LOSS: 0.00016297893489536364\n",
      "VAL: EPOCH 22/1000 | BATCH 4/8 | LOSS: 0.00016076405154308304\n",
      "VAL: EPOCH 22/1000 | BATCH 5/8 | LOSS: 0.00016238470925600268\n",
      "VAL: EPOCH 22/1000 | BATCH 6/8 | LOSS: 0.00016017304005799815\n",
      "VAL: EPOCH 22/1000 | BATCH 7/8 | LOSS: 0.0001548690006529796\n",
      "TRAIN: EPOCH 23/1000 | BATCH 0/71 | LOSS: 8.842138049658388e-05\n",
      "TRAIN: EPOCH 23/1000 | BATCH 1/71 | LOSS: 0.00010696314711822197\n",
      "TRAIN: EPOCH 23/1000 | BATCH 2/71 | LOSS: 0.00010656394685308139\n",
      "TRAIN: EPOCH 23/1000 | BATCH 3/71 | LOSS: 0.00013716142348130234\n",
      "TRAIN: EPOCH 23/1000 | BATCH 4/71 | LOSS: 0.00013471795537043363\n",
      "TRAIN: EPOCH 23/1000 | BATCH 5/71 | LOSS: 0.0001457278291733625\n",
      "TRAIN: EPOCH 23/1000 | BATCH 6/71 | LOSS: 0.000155602816708519\n",
      "TRAIN: EPOCH 23/1000 | BATCH 7/71 | LOSS: 0.00015325227832363453\n",
      "TRAIN: EPOCH 23/1000 | BATCH 8/71 | LOSS: 0.00015617498138453811\n",
      "TRAIN: EPOCH 23/1000 | BATCH 9/71 | LOSS: 0.00017110633052652702\n",
      "TRAIN: EPOCH 23/1000 | BATCH 10/71 | LOSS: 0.00017940689973660153\n",
      "TRAIN: EPOCH 23/1000 | BATCH 11/71 | LOSS: 0.00018059072681353427\n",
      "TRAIN: EPOCH 23/1000 | BATCH 12/71 | LOSS: 0.0001771430599127108\n",
      "TRAIN: EPOCH 23/1000 | BATCH 13/71 | LOSS: 0.00017776372364356315\n",
      "TRAIN: EPOCH 23/1000 | BATCH 14/71 | LOSS: 0.0001731828606959122\n",
      "TRAIN: EPOCH 23/1000 | BATCH 15/71 | LOSS: 0.00016842052718857303\n",
      "TRAIN: EPOCH 23/1000 | BATCH 16/71 | LOSS: 0.0001663997450925629\n",
      "TRAIN: EPOCH 23/1000 | BATCH 17/71 | LOSS: 0.0001620963254633049\n",
      "TRAIN: EPOCH 23/1000 | BATCH 18/71 | LOSS: 0.00016551259843828646\n",
      "TRAIN: EPOCH 23/1000 | BATCH 19/71 | LOSS: 0.00016264139303530102\n",
      "TRAIN: EPOCH 23/1000 | BATCH 20/71 | LOSS: 0.00016193652776391466\n",
      "TRAIN: EPOCH 23/1000 | BATCH 21/71 | LOSS: 0.00015952377064174718\n",
      "TRAIN: EPOCH 23/1000 | BATCH 22/71 | LOSS: 0.00015971396541040715\n",
      "TRAIN: EPOCH 23/1000 | BATCH 23/71 | LOSS: 0.0001622737017896725\n",
      "TRAIN: EPOCH 23/1000 | BATCH 24/71 | LOSS: 0.00016008369100745768\n",
      "TRAIN: EPOCH 23/1000 | BATCH 25/71 | LOSS: 0.00016196435787535919\n",
      "TRAIN: EPOCH 23/1000 | BATCH 26/71 | LOSS: 0.00015979464687579484\n",
      "TRAIN: EPOCH 23/1000 | BATCH 27/71 | LOSS: 0.00015821800090114784\n",
      "TRAIN: EPOCH 23/1000 | BATCH 28/71 | LOSS: 0.0001580949412590567\n",
      "TRAIN: EPOCH 23/1000 | BATCH 29/71 | LOSS: 0.00015743758388756153\n",
      "TRAIN: EPOCH 23/1000 | BATCH 30/71 | LOSS: 0.0001564302114157697\n",
      "TRAIN: EPOCH 23/1000 | BATCH 31/71 | LOSS: 0.00015726117567282927\n",
      "TRAIN: EPOCH 23/1000 | BATCH 32/71 | LOSS: 0.00015784721670735797\n",
      "TRAIN: EPOCH 23/1000 | BATCH 33/71 | LOSS: 0.00015925808903001083\n",
      "TRAIN: EPOCH 23/1000 | BATCH 34/71 | LOSS: 0.00015766691254352086\n",
      "TRAIN: EPOCH 23/1000 | BATCH 35/71 | LOSS: 0.00015729896626402883\n",
      "TRAIN: EPOCH 23/1000 | BATCH 36/71 | LOSS: 0.00015676409738434076\n",
      "TRAIN: EPOCH 23/1000 | BATCH 37/71 | LOSS: 0.00015622822741677642\n",
      "TRAIN: EPOCH 23/1000 | BATCH 38/71 | LOSS: 0.00015497645299780206\n",
      "TRAIN: EPOCH 23/1000 | BATCH 39/71 | LOSS: 0.0001548479356642929\n",
      "TRAIN: EPOCH 23/1000 | BATCH 40/71 | LOSS: 0.00015603140952695932\n",
      "TRAIN: EPOCH 23/1000 | BATCH 41/71 | LOSS: 0.00015531063348004994\n",
      "TRAIN: EPOCH 23/1000 | BATCH 42/71 | LOSS: 0.00015387894771180962\n",
      "TRAIN: EPOCH 23/1000 | BATCH 43/71 | LOSS: 0.00015378555822694167\n",
      "TRAIN: EPOCH 23/1000 | BATCH 44/71 | LOSS: 0.0001518597970263929\n",
      "TRAIN: EPOCH 23/1000 | BATCH 45/71 | LOSS: 0.000150955755088944\n",
      "TRAIN: EPOCH 23/1000 | BATCH 46/71 | LOSS: 0.000153601660945532\n",
      "TRAIN: EPOCH 23/1000 | BATCH 47/71 | LOSS: 0.0001521151427065585\n",
      "TRAIN: EPOCH 23/1000 | BATCH 48/71 | LOSS: 0.00015078409594526437\n",
      "TRAIN: EPOCH 23/1000 | BATCH 49/71 | LOSS: 0.00015057396623888053\n",
      "TRAIN: EPOCH 23/1000 | BATCH 50/71 | LOSS: 0.000150003053182873\n",
      "TRAIN: EPOCH 23/1000 | BATCH 51/71 | LOSS: 0.0001490658809276423\n",
      "TRAIN: EPOCH 23/1000 | BATCH 52/71 | LOSS: 0.00014881802959486245\n",
      "TRAIN: EPOCH 23/1000 | BATCH 53/71 | LOSS: 0.00015036650775410062\n",
      "TRAIN: EPOCH 23/1000 | BATCH 54/71 | LOSS: 0.00015026738335357302\n",
      "TRAIN: EPOCH 23/1000 | BATCH 55/71 | LOSS: 0.00014926867866701547\n",
      "TRAIN: EPOCH 23/1000 | BATCH 56/71 | LOSS: 0.0001483437318759235\n",
      "TRAIN: EPOCH 23/1000 | BATCH 57/71 | LOSS: 0.00014813516839961362\n",
      "TRAIN: EPOCH 23/1000 | BATCH 58/71 | LOSS: 0.0001471966904118436\n",
      "TRAIN: EPOCH 23/1000 | BATCH 59/71 | LOSS: 0.0001464679934239636\n",
      "TRAIN: EPOCH 23/1000 | BATCH 60/71 | LOSS: 0.0001465537691236947\n",
      "TRAIN: EPOCH 23/1000 | BATCH 61/71 | LOSS: 0.00014654417276277297\n",
      "TRAIN: EPOCH 23/1000 | BATCH 62/71 | LOSS: 0.00014628605484696372\n",
      "TRAIN: EPOCH 23/1000 | BATCH 63/71 | LOSS: 0.0001457665075577097\n",
      "TRAIN: EPOCH 23/1000 | BATCH 64/71 | LOSS: 0.00014543846437635902\n",
      "TRAIN: EPOCH 23/1000 | BATCH 65/71 | LOSS: 0.00014490033418317608\n",
      "TRAIN: EPOCH 23/1000 | BATCH 66/71 | LOSS: 0.00014417944378271671\n",
      "TRAIN: EPOCH 23/1000 | BATCH 67/71 | LOSS: 0.00014382022869968377\n",
      "TRAIN: EPOCH 23/1000 | BATCH 68/71 | LOSS: 0.00014323718431220132\n",
      "TRAIN: EPOCH 23/1000 | BATCH 69/71 | LOSS: 0.00014446158280147106\n",
      "TRAIN: EPOCH 23/1000 | BATCH 70/71 | LOSS: 0.00014314460324231718\n",
      "VAL: EPOCH 23/1000 | BATCH 0/8 | LOSS: 0.00022960160276852548\n",
      "VAL: EPOCH 23/1000 | BATCH 1/8 | LOSS: 0.00016155713092302904\n",
      "VAL: EPOCH 23/1000 | BATCH 2/8 | LOSS: 0.0001652812691948687\n",
      "VAL: EPOCH 23/1000 | BATCH 3/8 | LOSS: 0.0001463809203414712\n",
      "VAL: EPOCH 23/1000 | BATCH 4/8 | LOSS: 0.00014401877415366472\n",
      "VAL: EPOCH 23/1000 | BATCH 5/8 | LOSS: 0.00014604959384693453\n",
      "VAL: EPOCH 23/1000 | BATCH 6/8 | LOSS: 0.00014376474843759622\n",
      "VAL: EPOCH 23/1000 | BATCH 7/8 | LOSS: 0.00013882258281228133\n",
      "TRAIN: EPOCH 24/1000 | BATCH 0/71 | LOSS: 9.773598867468536e-05\n",
      "TRAIN: EPOCH 24/1000 | BATCH 1/71 | LOSS: 0.0001339047448709607\n",
      "TRAIN: EPOCH 24/1000 | BATCH 2/71 | LOSS: 0.00011840824784788613\n",
      "TRAIN: EPOCH 24/1000 | BATCH 3/71 | LOSS: 0.00011198729953321163\n",
      "TRAIN: EPOCH 24/1000 | BATCH 4/71 | LOSS: 0.00010888785618590191\n",
      "TRAIN: EPOCH 24/1000 | BATCH 5/71 | LOSS: 0.00010669221834784064\n",
      "TRAIN: EPOCH 24/1000 | BATCH 6/71 | LOSS: 0.00013438877171470916\n",
      "TRAIN: EPOCH 24/1000 | BATCH 7/71 | LOSS: 0.0001328453645328409\n",
      "TRAIN: EPOCH 24/1000 | BATCH 8/71 | LOSS: 0.00013245512268945781\n",
      "TRAIN: EPOCH 24/1000 | BATCH 9/71 | LOSS: 0.0001377552827761974\n",
      "TRAIN: EPOCH 24/1000 | BATCH 10/71 | LOSS: 0.00013608203880721703\n",
      "TRAIN: EPOCH 24/1000 | BATCH 11/71 | LOSS: 0.00015120521068941647\n",
      "TRAIN: EPOCH 24/1000 | BATCH 12/71 | LOSS: 0.00015794161243740327\n",
      "TRAIN: EPOCH 24/1000 | BATCH 13/71 | LOSS: 0.0001541046861218222\n",
      "TRAIN: EPOCH 24/1000 | BATCH 14/71 | LOSS: 0.00015920500979215528\n",
      "TRAIN: EPOCH 24/1000 | BATCH 15/71 | LOSS: 0.0001549796784274804\n",
      "TRAIN: EPOCH 24/1000 | BATCH 16/71 | LOSS: 0.00015189008006606907\n",
      "TRAIN: EPOCH 24/1000 | BATCH 17/71 | LOSS: 0.00014967299406028664\n",
      "TRAIN: EPOCH 24/1000 | BATCH 18/71 | LOSS: 0.0001481714203500336\n",
      "TRAIN: EPOCH 24/1000 | BATCH 19/71 | LOSS: 0.00014785003659198992\n",
      "TRAIN: EPOCH 24/1000 | BATCH 20/71 | LOSS: 0.00014871037967081758\n",
      "TRAIN: EPOCH 24/1000 | BATCH 21/71 | LOSS: 0.0001483650233818811\n",
      "TRAIN: EPOCH 24/1000 | BATCH 22/71 | LOSS: 0.00014650847784334633\n",
      "TRAIN: EPOCH 24/1000 | BATCH 23/71 | LOSS: 0.00014415337833876643\n",
      "TRAIN: EPOCH 24/1000 | BATCH 24/71 | LOSS: 0.00014265031495597214\n",
      "TRAIN: EPOCH 24/1000 | BATCH 25/71 | LOSS: 0.0001417524072166998\n",
      "TRAIN: EPOCH 24/1000 | BATCH 26/71 | LOSS: 0.00013999761426088366\n",
      "TRAIN: EPOCH 24/1000 | BATCH 27/71 | LOSS: 0.00013819277566134197\n",
      "TRAIN: EPOCH 24/1000 | BATCH 28/71 | LOSS: 0.00013792380547062653\n",
      "TRAIN: EPOCH 24/1000 | BATCH 29/71 | LOSS: 0.00013815435255916478\n",
      "TRAIN: EPOCH 24/1000 | BATCH 30/71 | LOSS: 0.00013663025874261473\n",
      "TRAIN: EPOCH 24/1000 | BATCH 31/71 | LOSS: 0.00013917718865741335\n",
      "TRAIN: EPOCH 24/1000 | BATCH 32/71 | LOSS: 0.00013755229420811784\n",
      "TRAIN: EPOCH 24/1000 | BATCH 33/71 | LOSS: 0.00013865415081347558\n",
      "TRAIN: EPOCH 24/1000 | BATCH 34/71 | LOSS: 0.0001392089422941873\n",
      "TRAIN: EPOCH 24/1000 | BATCH 35/71 | LOSS: 0.00013822089345517775\n",
      "TRAIN: EPOCH 24/1000 | BATCH 36/71 | LOSS: 0.00013905690984510993\n",
      "TRAIN: EPOCH 24/1000 | BATCH 37/71 | LOSS: 0.00013788850399003806\n",
      "TRAIN: EPOCH 24/1000 | BATCH 38/71 | LOSS: 0.00013769947317763208\n",
      "TRAIN: EPOCH 24/1000 | BATCH 39/71 | LOSS: 0.00013715763743675778\n",
      "TRAIN: EPOCH 24/1000 | BATCH 40/71 | LOSS: 0.00013649722127522136\n",
      "TRAIN: EPOCH 24/1000 | BATCH 41/71 | LOSS: 0.00013619857348939624\n",
      "TRAIN: EPOCH 24/1000 | BATCH 42/71 | LOSS: 0.00013590641072707598\n",
      "TRAIN: EPOCH 24/1000 | BATCH 43/71 | LOSS: 0.00013714934687992684\n",
      "TRAIN: EPOCH 24/1000 | BATCH 44/71 | LOSS: 0.00013934819904130158\n",
      "TRAIN: EPOCH 24/1000 | BATCH 45/71 | LOSS: 0.0001389518625016891\n",
      "TRAIN: EPOCH 24/1000 | BATCH 46/71 | LOSS: 0.00013786533499443707\n",
      "TRAIN: EPOCH 24/1000 | BATCH 47/71 | LOSS: 0.00013886604453243004\n",
      "TRAIN: EPOCH 24/1000 | BATCH 48/71 | LOSS: 0.00013783340501522987\n",
      "TRAIN: EPOCH 24/1000 | BATCH 49/71 | LOSS: 0.0001369697059271857\n",
      "TRAIN: EPOCH 24/1000 | BATCH 50/71 | LOSS: 0.00013716242449613762\n",
      "TRAIN: EPOCH 24/1000 | BATCH 51/71 | LOSS: 0.0001361981414027888\n",
      "TRAIN: EPOCH 24/1000 | BATCH 52/71 | LOSS: 0.00013494540133869346\n",
      "TRAIN: EPOCH 24/1000 | BATCH 53/71 | LOSS: 0.00013509194279861362\n",
      "TRAIN: EPOCH 24/1000 | BATCH 54/71 | LOSS: 0.00013470568374032154\n",
      "TRAIN: EPOCH 24/1000 | BATCH 55/71 | LOSS: 0.00013425139604805736\n",
      "TRAIN: EPOCH 24/1000 | BATCH 56/71 | LOSS: 0.0001351155215320831\n",
      "TRAIN: EPOCH 24/1000 | BATCH 57/71 | LOSS: 0.00013477726767225\n",
      "TRAIN: EPOCH 24/1000 | BATCH 58/71 | LOSS: 0.00013401322014988952\n",
      "TRAIN: EPOCH 24/1000 | BATCH 59/71 | LOSS: 0.00013403352143844435\n",
      "TRAIN: EPOCH 24/1000 | BATCH 60/71 | LOSS: 0.00013328635145253579\n",
      "TRAIN: EPOCH 24/1000 | BATCH 61/71 | LOSS: 0.0001330608553666201\n",
      "TRAIN: EPOCH 24/1000 | BATCH 62/71 | LOSS: 0.00013277889783715917\n",
      "TRAIN: EPOCH 24/1000 | BATCH 63/71 | LOSS: 0.0001326429901382653\n",
      "TRAIN: EPOCH 24/1000 | BATCH 64/71 | LOSS: 0.00013351955231673157\n",
      "TRAIN: EPOCH 24/1000 | BATCH 65/71 | LOSS: 0.00013365974155021831\n",
      "TRAIN: EPOCH 24/1000 | BATCH 66/71 | LOSS: 0.00013384685995849544\n",
      "TRAIN: EPOCH 24/1000 | BATCH 67/71 | LOSS: 0.00013357938020336032\n",
      "TRAIN: EPOCH 24/1000 | BATCH 68/71 | LOSS: 0.0001330407036036469\n",
      "TRAIN: EPOCH 24/1000 | BATCH 69/71 | LOSS: 0.00013266578711669094\n",
      "TRAIN: EPOCH 24/1000 | BATCH 70/71 | LOSS: 0.00013172248955143774\n",
      "VAL: EPOCH 24/1000 | BATCH 0/8 | LOSS: 0.0002069931651931256\n",
      "VAL: EPOCH 24/1000 | BATCH 1/8 | LOSS: 0.000147156861203257\n",
      "VAL: EPOCH 24/1000 | BATCH 2/8 | LOSS: 0.0001502372712517778\n",
      "VAL: EPOCH 24/1000 | BATCH 3/8 | LOSS: 0.00013379509618971497\n",
      "VAL: EPOCH 24/1000 | BATCH 4/8 | LOSS: 0.00013153865875210614\n",
      "VAL: EPOCH 24/1000 | BATCH 5/8 | LOSS: 0.00013299580556728566\n",
      "VAL: EPOCH 24/1000 | BATCH 6/8 | LOSS: 0.00013122934823124005\n",
      "VAL: EPOCH 24/1000 | BATCH 7/8 | LOSS: 0.00012648062875086907\n",
      "TRAIN: EPOCH 25/1000 | BATCH 0/71 | LOSS: 0.00015454544336535037\n",
      "TRAIN: EPOCH 25/1000 | BATCH 1/71 | LOSS: 0.00018230893329018727\n",
      "TRAIN: EPOCH 25/1000 | BATCH 2/71 | LOSS: 0.00017962678975891322\n",
      "TRAIN: EPOCH 25/1000 | BATCH 3/71 | LOSS: 0.00017506440781289712\n",
      "TRAIN: EPOCH 25/1000 | BATCH 4/71 | LOSS: 0.00016650449542794378\n",
      "TRAIN: EPOCH 25/1000 | BATCH 5/71 | LOSS: 0.00015435295548134795\n",
      "TRAIN: EPOCH 25/1000 | BATCH 6/71 | LOSS: 0.0001536094891240022\n",
      "TRAIN: EPOCH 25/1000 | BATCH 7/71 | LOSS: 0.00014801179531787056\n",
      "TRAIN: EPOCH 25/1000 | BATCH 8/71 | LOSS: 0.00014385162083069899\n",
      "TRAIN: EPOCH 25/1000 | BATCH 9/71 | LOSS: 0.0001428786898031831\n",
      "TRAIN: EPOCH 25/1000 | BATCH 10/71 | LOSS: 0.00014116067375818437\n",
      "TRAIN: EPOCH 25/1000 | BATCH 11/71 | LOSS: 0.00014219534690103805\n",
      "TRAIN: EPOCH 25/1000 | BATCH 12/71 | LOSS: 0.00014294484012330382\n",
      "TRAIN: EPOCH 25/1000 | BATCH 13/71 | LOSS: 0.00014379627204367092\n",
      "TRAIN: EPOCH 25/1000 | BATCH 14/71 | LOSS: 0.00014183387538651003\n",
      "TRAIN: EPOCH 25/1000 | BATCH 15/71 | LOSS: 0.00013811380586048472\n",
      "TRAIN: EPOCH 25/1000 | BATCH 16/71 | LOSS: 0.00013623037118711234\n",
      "TRAIN: EPOCH 25/1000 | BATCH 17/71 | LOSS: 0.00013548112555225898\n",
      "TRAIN: EPOCH 25/1000 | BATCH 18/71 | LOSS: 0.00013462984306419172\n",
      "TRAIN: EPOCH 25/1000 | BATCH 19/71 | LOSS: 0.00013219871507317294\n",
      "TRAIN: EPOCH 25/1000 | BATCH 20/71 | LOSS: 0.00013080887826314816\n",
      "TRAIN: EPOCH 25/1000 | BATCH 21/71 | LOSS: 0.0001322077335895632\n",
      "TRAIN: EPOCH 25/1000 | BATCH 22/71 | LOSS: 0.0001332771726479025\n",
      "TRAIN: EPOCH 25/1000 | BATCH 23/71 | LOSS: 0.00013354000960437892\n",
      "TRAIN: EPOCH 25/1000 | BATCH 24/71 | LOSS: 0.00013296414515934885\n",
      "TRAIN: EPOCH 25/1000 | BATCH 25/71 | LOSS: 0.00013457133956343078\n",
      "TRAIN: EPOCH 25/1000 | BATCH 26/71 | LOSS: 0.00013222299323253402\n",
      "TRAIN: EPOCH 25/1000 | BATCH 27/71 | LOSS: 0.00013258808472268617\n",
      "TRAIN: EPOCH 25/1000 | BATCH 28/71 | LOSS: 0.0001320035293686268\n",
      "TRAIN: EPOCH 25/1000 | BATCH 29/71 | LOSS: 0.00013037294775131159\n",
      "TRAIN: EPOCH 25/1000 | BATCH 30/71 | LOSS: 0.00013017061381458093\n",
      "TRAIN: EPOCH 25/1000 | BATCH 31/71 | LOSS: 0.00012835441089009691\n",
      "TRAIN: EPOCH 25/1000 | BATCH 32/71 | LOSS: 0.0001273468204255385\n",
      "TRAIN: EPOCH 25/1000 | BATCH 33/71 | LOSS: 0.00012607525701134684\n",
      "TRAIN: EPOCH 25/1000 | BATCH 34/71 | LOSS: 0.00012444861162553674\n",
      "TRAIN: EPOCH 25/1000 | BATCH 35/71 | LOSS: 0.00012637611447037975\n",
      "TRAIN: EPOCH 25/1000 | BATCH 36/71 | LOSS: 0.00012558912901629418\n",
      "TRAIN: EPOCH 25/1000 | BATCH 37/71 | LOSS: 0.00012518119547166862\n",
      "TRAIN: EPOCH 25/1000 | BATCH 38/71 | LOSS: 0.00012433598544120262\n",
      "TRAIN: EPOCH 25/1000 | BATCH 39/71 | LOSS: 0.00012469512821553508\n",
      "TRAIN: EPOCH 25/1000 | BATCH 40/71 | LOSS: 0.00012393521798757565\n",
      "TRAIN: EPOCH 25/1000 | BATCH 41/71 | LOSS: 0.000122804232974193\n",
      "TRAIN: EPOCH 25/1000 | BATCH 42/71 | LOSS: 0.00012337099060248367\n",
      "TRAIN: EPOCH 25/1000 | BATCH 43/71 | LOSS: 0.00012387550784544808\n",
      "TRAIN: EPOCH 25/1000 | BATCH 44/71 | LOSS: 0.00012360638631637105\n",
      "TRAIN: EPOCH 25/1000 | BATCH 45/71 | LOSS: 0.00012456823400419165\n",
      "TRAIN: EPOCH 25/1000 | BATCH 46/71 | LOSS: 0.00012415650835077456\n",
      "TRAIN: EPOCH 25/1000 | BATCH 47/71 | LOSS: 0.0001234602379251252\n",
      "TRAIN: EPOCH 25/1000 | BATCH 48/71 | LOSS: 0.00012294875890876605\n",
      "TRAIN: EPOCH 25/1000 | BATCH 49/71 | LOSS: 0.00012266579069546423\n",
      "TRAIN: EPOCH 25/1000 | BATCH 50/71 | LOSS: 0.00012259202833543074\n",
      "TRAIN: EPOCH 25/1000 | BATCH 51/71 | LOSS: 0.00012205986268572115\n",
      "TRAIN: EPOCH 25/1000 | BATCH 52/71 | LOSS: 0.000121078817618633\n",
      "TRAIN: EPOCH 25/1000 | BATCH 53/71 | LOSS: 0.00012071802713627996\n",
      "TRAIN: EPOCH 25/1000 | BATCH 54/71 | LOSS: 0.00012049798136682843\n",
      "TRAIN: EPOCH 25/1000 | BATCH 55/71 | LOSS: 0.000121921351689837\n",
      "TRAIN: EPOCH 25/1000 | BATCH 56/71 | LOSS: 0.00012413930214774026\n",
      "TRAIN: EPOCH 25/1000 | BATCH 57/71 | LOSS: 0.00012483829539570283\n",
      "TRAIN: EPOCH 25/1000 | BATCH 58/71 | LOSS: 0.00012419500841534209\n",
      "TRAIN: EPOCH 25/1000 | BATCH 59/71 | LOSS: 0.0001240728996587374\n",
      "TRAIN: EPOCH 25/1000 | BATCH 60/71 | LOSS: 0.00012341903624712628\n",
      "TRAIN: EPOCH 25/1000 | BATCH 61/71 | LOSS: 0.00012362220548957044\n",
      "TRAIN: EPOCH 25/1000 | BATCH 62/71 | LOSS: 0.00012401570284199562\n",
      "TRAIN: EPOCH 25/1000 | BATCH 63/71 | LOSS: 0.00012384836520595854\n",
      "TRAIN: EPOCH 25/1000 | BATCH 64/71 | LOSS: 0.00012394741788739337\n",
      "TRAIN: EPOCH 25/1000 | BATCH 65/71 | LOSS: 0.00012326539526287834\n",
      "TRAIN: EPOCH 25/1000 | BATCH 66/71 | LOSS: 0.00012376991853399425\n",
      "TRAIN: EPOCH 25/1000 | BATCH 67/71 | LOSS: 0.00012344428518297398\n",
      "TRAIN: EPOCH 25/1000 | BATCH 68/71 | LOSS: 0.0001230859995922546\n",
      "TRAIN: EPOCH 25/1000 | BATCH 69/71 | LOSS: 0.00012322026543967824\n",
      "TRAIN: EPOCH 25/1000 | BATCH 70/71 | LOSS: 0.00012252318449843097\n",
      "VAL: EPOCH 25/1000 | BATCH 0/8 | LOSS: 0.00018505443586036563\n",
      "VAL: EPOCH 25/1000 | BATCH 1/8 | LOSS: 0.00013461559501593\n",
      "VAL: EPOCH 25/1000 | BATCH 2/8 | LOSS: 0.00014027516347899413\n",
      "VAL: EPOCH 25/1000 | BATCH 3/8 | LOSS: 0.00012729948684864212\n",
      "VAL: EPOCH 25/1000 | BATCH 4/8 | LOSS: 0.00012476530246203766\n",
      "VAL: EPOCH 25/1000 | BATCH 5/8 | LOSS: 0.00012515909838839434\n",
      "VAL: EPOCH 25/1000 | BATCH 6/8 | LOSS: 0.00012354493706620166\n",
      "VAL: EPOCH 25/1000 | BATCH 7/8 | LOSS: 0.00012012401748506818\n",
      "TRAIN: EPOCH 26/1000 | BATCH 0/71 | LOSS: 8.701875776750967e-05\n",
      "TRAIN: EPOCH 26/1000 | BATCH 1/71 | LOSS: 0.00015312536197598092\n",
      "TRAIN: EPOCH 26/1000 | BATCH 2/71 | LOSS: 0.0001241357652664495\n",
      "TRAIN: EPOCH 26/1000 | BATCH 3/71 | LOSS: 0.00012163079372840002\n",
      "TRAIN: EPOCH 26/1000 | BATCH 4/71 | LOSS: 0.00012189887929707766\n",
      "TRAIN: EPOCH 26/1000 | BATCH 5/71 | LOSS: 0.00012209004004641125\n",
      "TRAIN: EPOCH 26/1000 | BATCH 6/71 | LOSS: 0.0001289488697823669\n",
      "TRAIN: EPOCH 26/1000 | BATCH 7/71 | LOSS: 0.00012364624126348644\n",
      "TRAIN: EPOCH 26/1000 | BATCH 8/71 | LOSS: 0.00012032229117014342\n",
      "TRAIN: EPOCH 26/1000 | BATCH 9/71 | LOSS: 0.00011768763652071357\n",
      "TRAIN: EPOCH 26/1000 | BATCH 10/71 | LOSS: 0.00011644118454899977\n",
      "TRAIN: EPOCH 26/1000 | BATCH 11/71 | LOSS: 0.00012321052296708027\n",
      "TRAIN: EPOCH 26/1000 | BATCH 12/71 | LOSS: 0.0001248023517501469\n",
      "TRAIN: EPOCH 26/1000 | BATCH 13/71 | LOSS: 0.00012159770890970581\n",
      "TRAIN: EPOCH 26/1000 | BATCH 14/71 | LOSS: 0.00012486507378829022\n",
      "TRAIN: EPOCH 26/1000 | BATCH 15/71 | LOSS: 0.00012215369315526914\n",
      "TRAIN: EPOCH 26/1000 | BATCH 16/71 | LOSS: 0.0001212906516978846\n",
      "TRAIN: EPOCH 26/1000 | BATCH 17/71 | LOSS: 0.00011815271621647601\n",
      "TRAIN: EPOCH 26/1000 | BATCH 18/71 | LOSS: 0.00011573069787118584\n",
      "TRAIN: EPOCH 26/1000 | BATCH 19/71 | LOSS: 0.000123511935089482\n",
      "TRAIN: EPOCH 26/1000 | BATCH 20/71 | LOSS: 0.0001223627075974253\n",
      "TRAIN: EPOCH 26/1000 | BATCH 21/71 | LOSS: 0.00012094278975961392\n",
      "TRAIN: EPOCH 26/1000 | BATCH 22/71 | LOSS: 0.00011862928753860457\n",
      "TRAIN: EPOCH 26/1000 | BATCH 23/71 | LOSS: 0.00011894280093353397\n",
      "TRAIN: EPOCH 26/1000 | BATCH 24/71 | LOSS: 0.00011831479700049386\n",
      "TRAIN: EPOCH 26/1000 | BATCH 25/71 | LOSS: 0.00011630819986189286\n",
      "TRAIN: EPOCH 26/1000 | BATCH 26/71 | LOSS: 0.00011609926527685106\n",
      "TRAIN: EPOCH 26/1000 | BATCH 27/71 | LOSS: 0.00011547311987669673\n",
      "TRAIN: EPOCH 26/1000 | BATCH 28/71 | LOSS: 0.0001147153849035084\n",
      "TRAIN: EPOCH 26/1000 | BATCH 29/71 | LOSS: 0.00011733840728993528\n",
      "TRAIN: EPOCH 26/1000 | BATCH 30/71 | LOSS: 0.0001164132071050605\n",
      "TRAIN: EPOCH 26/1000 | BATCH 31/71 | LOSS: 0.00011674335110001266\n",
      "TRAIN: EPOCH 26/1000 | BATCH 32/71 | LOSS: 0.00011610420761192499\n",
      "TRAIN: EPOCH 26/1000 | BATCH 33/71 | LOSS: 0.00011749641356907566\n",
      "TRAIN: EPOCH 26/1000 | BATCH 34/71 | LOSS: 0.00011630365874485246\n",
      "TRAIN: EPOCH 26/1000 | BATCH 35/71 | LOSS: 0.00011570232769978854\n",
      "TRAIN: EPOCH 26/1000 | BATCH 36/71 | LOSS: 0.00011527600872796029\n",
      "TRAIN: EPOCH 26/1000 | BATCH 37/71 | LOSS: 0.00011540317178801878\n",
      "TRAIN: EPOCH 26/1000 | BATCH 38/71 | LOSS: 0.00011554999106850188\n",
      "TRAIN: EPOCH 26/1000 | BATCH 39/71 | LOSS: 0.0001148037978055072\n",
      "TRAIN: EPOCH 26/1000 | BATCH 40/71 | LOSS: 0.00011539392662376574\n",
      "TRAIN: EPOCH 26/1000 | BATCH 41/71 | LOSS: 0.00011480875123414167\n",
      "TRAIN: EPOCH 26/1000 | BATCH 42/71 | LOSS: 0.00011546821516212942\n",
      "TRAIN: EPOCH 26/1000 | BATCH 43/71 | LOSS: 0.00011462438289361836\n",
      "TRAIN: EPOCH 26/1000 | BATCH 44/71 | LOSS: 0.00011506538689395205\n",
      "TRAIN: EPOCH 26/1000 | BATCH 45/71 | LOSS: 0.0001150029888579055\n",
      "TRAIN: EPOCH 26/1000 | BATCH 46/71 | LOSS: 0.00011473412125772657\n",
      "TRAIN: EPOCH 26/1000 | BATCH 47/71 | LOSS: 0.00011440628698740814\n",
      "TRAIN: EPOCH 26/1000 | BATCH 48/71 | LOSS: 0.00011453190871885008\n",
      "TRAIN: EPOCH 26/1000 | BATCH 49/71 | LOSS: 0.00011425699281971901\n",
      "TRAIN: EPOCH 26/1000 | BATCH 50/71 | LOSS: 0.00011372208588427919\n",
      "TRAIN: EPOCH 26/1000 | BATCH 51/71 | LOSS: 0.00011358617148988952\n",
      "TRAIN: EPOCH 26/1000 | BATCH 52/71 | LOSS: 0.00011298651719149554\n",
      "TRAIN: EPOCH 26/1000 | BATCH 53/71 | LOSS: 0.00011345660432535051\n",
      "TRAIN: EPOCH 26/1000 | BATCH 54/71 | LOSS: 0.00011478406482968819\n",
      "TRAIN: EPOCH 26/1000 | BATCH 55/71 | LOSS: 0.00011498415395700639\n",
      "TRAIN: EPOCH 26/1000 | BATCH 56/71 | LOSS: 0.00011598680800505048\n",
      "TRAIN: EPOCH 26/1000 | BATCH 57/71 | LOSS: 0.00011530808365807451\n",
      "TRAIN: EPOCH 26/1000 | BATCH 58/71 | LOSS: 0.00011526208353165727\n",
      "TRAIN: EPOCH 26/1000 | BATCH 59/71 | LOSS: 0.00011490284026270577\n",
      "TRAIN: EPOCH 26/1000 | BATCH 60/71 | LOSS: 0.0001140203355328699\n",
      "TRAIN: EPOCH 26/1000 | BATCH 61/71 | LOSS: 0.00011391636261607581\n",
      "TRAIN: EPOCH 26/1000 | BATCH 62/71 | LOSS: 0.00011378448673187652\n",
      "TRAIN: EPOCH 26/1000 | BATCH 63/71 | LOSS: 0.0001146262014799504\n",
      "TRAIN: EPOCH 26/1000 | BATCH 64/71 | LOSS: 0.00011410450007623204\n",
      "TRAIN: EPOCH 26/1000 | BATCH 65/71 | LOSS: 0.00011450249902819368\n",
      "TRAIN: EPOCH 26/1000 | BATCH 66/71 | LOSS: 0.00011405566904464486\n",
      "TRAIN: EPOCH 26/1000 | BATCH 67/71 | LOSS: 0.00011408747901869527\n",
      "TRAIN: EPOCH 26/1000 | BATCH 68/71 | LOSS: 0.00011375903008940995\n",
      "TRAIN: EPOCH 26/1000 | BATCH 69/71 | LOSS: 0.00011373930278101138\n",
      "TRAIN: EPOCH 26/1000 | BATCH 70/71 | LOSS: 0.00011338129269474015\n",
      "VAL: EPOCH 26/1000 | BATCH 0/8 | LOSS: 0.00016694467922206968\n",
      "VAL: EPOCH 26/1000 | BATCH 1/8 | LOSS: 0.00012084874833817594\n",
      "VAL: EPOCH 26/1000 | BATCH 2/8 | LOSS: 0.00012495370416824395\n",
      "VAL: EPOCH 26/1000 | BATCH 3/8 | LOSS: 0.00011171530604769941\n",
      "VAL: EPOCH 26/1000 | BATCH 4/8 | LOSS: 0.00010949774732580409\n",
      "VAL: EPOCH 26/1000 | BATCH 5/8 | LOSS: 0.00011125295835275513\n",
      "VAL: EPOCH 26/1000 | BATCH 6/8 | LOSS: 0.0001095452971640043\n",
      "VAL: EPOCH 26/1000 | BATCH 7/8 | LOSS: 0.00010613028734951513\n",
      "TRAIN: EPOCH 27/1000 | BATCH 0/71 | LOSS: 8.761620847508311e-05\n",
      "TRAIN: EPOCH 27/1000 | BATCH 1/71 | LOSS: 0.0001308556238655001\n",
      "TRAIN: EPOCH 27/1000 | BATCH 2/71 | LOSS: 0.00010894603231766571\n",
      "TRAIN: EPOCH 27/1000 | BATCH 3/71 | LOSS: 0.00010485681195859797\n",
      "TRAIN: EPOCH 27/1000 | BATCH 4/71 | LOSS: 0.00010263392323395237\n",
      "TRAIN: EPOCH 27/1000 | BATCH 5/71 | LOSS: 9.95997046023452e-05\n",
      "TRAIN: EPOCH 27/1000 | BATCH 6/71 | LOSS: 0.00010430476790393836\n",
      "TRAIN: EPOCH 27/1000 | BATCH 7/71 | LOSS: 0.00010454845960339298\n",
      "TRAIN: EPOCH 27/1000 | BATCH 8/71 | LOSS: 0.0001207443185396389\n",
      "TRAIN: EPOCH 27/1000 | BATCH 9/71 | LOSS: 0.00012808540996047668\n",
      "TRAIN: EPOCH 27/1000 | BATCH 10/71 | LOSS: 0.00012423134467098862\n",
      "TRAIN: EPOCH 27/1000 | BATCH 11/71 | LOSS: 0.0001252614614107491\n",
      "TRAIN: EPOCH 27/1000 | BATCH 12/71 | LOSS: 0.00012168574791688185\n",
      "TRAIN: EPOCH 27/1000 | BATCH 13/71 | LOSS: 0.00011843690403371252\n",
      "TRAIN: EPOCH 27/1000 | BATCH 14/71 | LOSS: 0.00011851624100624273\n",
      "TRAIN: EPOCH 27/1000 | BATCH 15/71 | LOSS: 0.00011684560104185948\n",
      "TRAIN: EPOCH 27/1000 | BATCH 16/71 | LOSS: 0.00011727738498901839\n",
      "TRAIN: EPOCH 27/1000 | BATCH 17/71 | LOSS: 0.00011467922013252974\n",
      "TRAIN: EPOCH 27/1000 | BATCH 18/71 | LOSS: 0.00011230592874773337\n",
      "TRAIN: EPOCH 27/1000 | BATCH 19/71 | LOSS: 0.00011226324313611258\n",
      "TRAIN: EPOCH 27/1000 | BATCH 20/71 | LOSS: 0.00011388420638728089\n",
      "TRAIN: EPOCH 27/1000 | BATCH 21/71 | LOSS: 0.00011539558479191989\n",
      "TRAIN: EPOCH 27/1000 | BATCH 22/71 | LOSS: 0.00011599369178799427\n",
      "TRAIN: EPOCH 27/1000 | BATCH 23/71 | LOSS: 0.00011470149790208477\n",
      "TRAIN: EPOCH 27/1000 | BATCH 24/71 | LOSS: 0.00011382042255718261\n",
      "TRAIN: EPOCH 27/1000 | BATCH 25/71 | LOSS: 0.00011255162686351329\n",
      "TRAIN: EPOCH 27/1000 | BATCH 26/71 | LOSS: 0.00011089030050681215\n",
      "TRAIN: EPOCH 27/1000 | BATCH 27/71 | LOSS: 0.00011263281625620689\n",
      "TRAIN: EPOCH 27/1000 | BATCH 28/71 | LOSS: 0.00011336547197123733\n",
      "TRAIN: EPOCH 27/1000 | BATCH 29/71 | LOSS: 0.00011204362769300739\n",
      "TRAIN: EPOCH 27/1000 | BATCH 30/71 | LOSS: 0.00011202876879495659\n",
      "TRAIN: EPOCH 27/1000 | BATCH 31/71 | LOSS: 0.00011082207788604137\n",
      "TRAIN: EPOCH 27/1000 | BATCH 32/71 | LOSS: 0.00011073105247994658\n",
      "TRAIN: EPOCH 27/1000 | BATCH 33/71 | LOSS: 0.00011014128005412845\n",
      "TRAIN: EPOCH 27/1000 | BATCH 34/71 | LOSS: 0.00010986387288929629\n",
      "TRAIN: EPOCH 27/1000 | BATCH 35/71 | LOSS: 0.00010882012631757082\n",
      "TRAIN: EPOCH 27/1000 | BATCH 36/71 | LOSS: 0.00010951380229655754\n",
      "TRAIN: EPOCH 27/1000 | BATCH 37/71 | LOSS: 0.00010980061358024709\n",
      "TRAIN: EPOCH 27/1000 | BATCH 38/71 | LOSS: 0.00010919054329860955\n",
      "TRAIN: EPOCH 27/1000 | BATCH 39/71 | LOSS: 0.00010876585238293047\n",
      "TRAIN: EPOCH 27/1000 | BATCH 40/71 | LOSS: 0.00010854749274496898\n",
      "TRAIN: EPOCH 27/1000 | BATCH 41/71 | LOSS: 0.00010927255927873332\n",
      "TRAIN: EPOCH 27/1000 | BATCH 42/71 | LOSS: 0.00010851494965230136\n",
      "TRAIN: EPOCH 27/1000 | BATCH 43/71 | LOSS: 0.00010805137108863246\n",
      "TRAIN: EPOCH 27/1000 | BATCH 44/71 | LOSS: 0.00010801545415435814\n",
      "TRAIN: EPOCH 27/1000 | BATCH 45/71 | LOSS: 0.00010793000641886307\n",
      "TRAIN: EPOCH 27/1000 | BATCH 46/71 | LOSS: 0.0001074534003682276\n",
      "TRAIN: EPOCH 27/1000 | BATCH 47/71 | LOSS: 0.00010666138405213133\n",
      "TRAIN: EPOCH 27/1000 | BATCH 48/71 | LOSS: 0.0001070550809215222\n",
      "TRAIN: EPOCH 27/1000 | BATCH 49/71 | LOSS: 0.00010646611015545204\n",
      "TRAIN: EPOCH 27/1000 | BATCH 50/71 | LOSS: 0.00010677171524673445\n",
      "TRAIN: EPOCH 27/1000 | BATCH 51/71 | LOSS: 0.00010602513835832584\n",
      "TRAIN: EPOCH 27/1000 | BATCH 52/71 | LOSS: 0.00010562736833718482\n",
      "TRAIN: EPOCH 27/1000 | BATCH 53/71 | LOSS: 0.00010551661654500533\n",
      "TRAIN: EPOCH 27/1000 | BATCH 54/71 | LOSS: 0.00010480904589216648\n",
      "TRAIN: EPOCH 27/1000 | BATCH 55/71 | LOSS: 0.00010427201518073812\n",
      "TRAIN: EPOCH 27/1000 | BATCH 56/71 | LOSS: 0.0001035955227276294\n",
      "TRAIN: EPOCH 27/1000 | BATCH 57/71 | LOSS: 0.00010334192484151572\n",
      "TRAIN: EPOCH 27/1000 | BATCH 58/71 | LOSS: 0.0001045810470606153\n",
      "TRAIN: EPOCH 27/1000 | BATCH 59/71 | LOSS: 0.00010400979190308134\n",
      "TRAIN: EPOCH 27/1000 | BATCH 60/71 | LOSS: 0.0001039264870847606\n",
      "TRAIN: EPOCH 27/1000 | BATCH 61/71 | LOSS: 0.00010356001286943924\n",
      "TRAIN: EPOCH 27/1000 | BATCH 62/71 | LOSS: 0.00010521655949848955\n",
      "TRAIN: EPOCH 27/1000 | BATCH 63/71 | LOSS: 0.00010576442889487225\n",
      "TRAIN: EPOCH 27/1000 | BATCH 64/71 | LOSS: 0.00010552051524148108\n",
      "TRAIN: EPOCH 27/1000 | BATCH 65/71 | LOSS: 0.00010502396771423237\n",
      "TRAIN: EPOCH 27/1000 | BATCH 66/71 | LOSS: 0.00010474947773775002\n",
      "TRAIN: EPOCH 27/1000 | BATCH 67/71 | LOSS: 0.00010509206549613737\n",
      "TRAIN: EPOCH 27/1000 | BATCH 68/71 | LOSS: 0.00010459238905425899\n",
      "TRAIN: EPOCH 27/1000 | BATCH 69/71 | LOSS: 0.00010502309326381822\n",
      "TRAIN: EPOCH 27/1000 | BATCH 70/71 | LOSS: 0.00010507254159114075\n",
      "VAL: EPOCH 27/1000 | BATCH 0/8 | LOSS: 0.00015279038052540272\n",
      "VAL: EPOCH 27/1000 | BATCH 1/8 | LOSS: 0.00011170932702952996\n",
      "VAL: EPOCH 27/1000 | BATCH 2/8 | LOSS: 0.00011509894829941913\n",
      "VAL: EPOCH 27/1000 | BATCH 3/8 | LOSS: 0.00010207234117842745\n",
      "VAL: EPOCH 27/1000 | BATCH 4/8 | LOSS: 0.00010005057411035522\n",
      "VAL: EPOCH 27/1000 | BATCH 5/8 | LOSS: 0.00010267204561387189\n",
      "VAL: EPOCH 27/1000 | BATCH 6/8 | LOSS: 0.00010074133128260396\n",
      "VAL: EPOCH 27/1000 | BATCH 7/8 | LOSS: 9.801590931601822e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 0/71 | LOSS: 7.191725308075547e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 1/71 | LOSS: 7.998794535524212e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 2/71 | LOSS: 8.346003596670926e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 3/71 | LOSS: 0.00010166285937884822\n",
      "TRAIN: EPOCH 28/1000 | BATCH 4/71 | LOSS: 9.685199765954166e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 5/71 | LOSS: 9.567733165264751e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 6/71 | LOSS: 9.409561191984852e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 7/71 | LOSS: 9.17284860406653e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 8/71 | LOSS: 9.014890707072077e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 9/71 | LOSS: 8.966821260401048e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 10/71 | LOSS: 9.033742786744948e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 11/71 | LOSS: 9.385824584266327e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 12/71 | LOSS: 9.233919599165137e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 13/71 | LOSS: 9.005232797270375e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 14/71 | LOSS: 8.971250063041225e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 15/71 | LOSS: 9.881801497613196e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 16/71 | LOSS: 9.740083837909076e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 17/71 | LOSS: 0.00010662306077493768\n",
      "TRAIN: EPOCH 28/1000 | BATCH 18/71 | LOSS: 0.00010565530254481066\n",
      "TRAIN: EPOCH 28/1000 | BATCH 19/71 | LOSS: 0.00010393373268016148\n",
      "TRAIN: EPOCH 28/1000 | BATCH 20/71 | LOSS: 0.0001024910372853767\n",
      "TRAIN: EPOCH 28/1000 | BATCH 21/71 | LOSS: 0.00010180812155753797\n",
      "TRAIN: EPOCH 28/1000 | BATCH 22/71 | LOSS: 0.00010187177584790017\n",
      "TRAIN: EPOCH 28/1000 | BATCH 23/71 | LOSS: 0.00010025297312192076\n",
      "TRAIN: EPOCH 28/1000 | BATCH 24/71 | LOSS: 9.93626078707166e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 25/71 | LOSS: 9.869607450897232e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 26/71 | LOSS: 9.877712134901365e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 27/71 | LOSS: 9.764840199620397e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 28/71 | LOSS: 9.63935060005089e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 29/71 | LOSS: 9.644719393691048e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 30/71 | LOSS: 9.599651891602985e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 31/71 | LOSS: 9.573382521921303e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 32/71 | LOSS: 9.513603149433479e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 33/71 | LOSS: 9.482583931565066e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 34/71 | LOSS: 9.64660950038316e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 35/71 | LOSS: 9.656027032178827e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 36/71 | LOSS: 9.734648660555281e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 37/71 | LOSS: 9.69280487257301e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 38/71 | LOSS: 9.827923256521806e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 39/71 | LOSS: 9.869098721537739e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 40/71 | LOSS: 9.840519221431417e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 41/71 | LOSS: 9.882806071324186e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 42/71 | LOSS: 9.817719191507718e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 43/71 | LOSS: 9.748200946804982e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 44/71 | LOSS: 9.773012392947244e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 45/71 | LOSS: 9.742402408332767e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 46/71 | LOSS: 9.667550206204202e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 47/71 | LOSS: 9.627225199437817e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 48/71 | LOSS: 9.718087424015702e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 49/71 | LOSS: 9.788620445760898e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 50/71 | LOSS: 9.755789177884878e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 51/71 | LOSS: 9.750343479726535e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 52/71 | LOSS: 9.684177307205555e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 53/71 | LOSS: 9.709202667754316e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 54/71 | LOSS: 9.770698313580149e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 55/71 | LOSS: 9.79415769636814e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 56/71 | LOSS: 9.802214379050655e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 57/71 | LOSS: 9.736005154865842e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 58/71 | LOSS: 0.00010003214974550263\n",
      "TRAIN: EPOCH 28/1000 | BATCH 59/71 | LOSS: 0.00010020066201832378\n",
      "TRAIN: EPOCH 28/1000 | BATCH 60/71 | LOSS: 0.00010004472618466472\n",
      "TRAIN: EPOCH 28/1000 | BATCH 61/71 | LOSS: 9.951027864336802e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 62/71 | LOSS: 9.897351778890302e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 63/71 | LOSS: 9.869498404668775e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 64/71 | LOSS: 9.847686725641743e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 65/71 | LOSS: 9.809300629995326e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 66/71 | LOSS: 9.818546035902143e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 67/71 | LOSS: 9.826768735579476e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 68/71 | LOSS: 9.81027628965777e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 69/71 | LOSS: 9.802464976798677e-05\n",
      "TRAIN: EPOCH 28/1000 | BATCH 70/71 | LOSS: 9.820085467933021e-05\n",
      "VAL: EPOCH 28/1000 | BATCH 0/8 | LOSS: 0.00013879378093406558\n",
      "VAL: EPOCH 28/1000 | BATCH 1/8 | LOSS: 0.00010188257874688134\n",
      "VAL: EPOCH 28/1000 | BATCH 2/8 | LOSS: 0.0001072159187363771\n",
      "VAL: EPOCH 28/1000 | BATCH 3/8 | LOSS: 9.653588494984433e-05\n",
      "VAL: EPOCH 28/1000 | BATCH 4/8 | LOSS: 9.421591530553997e-05\n",
      "VAL: EPOCH 28/1000 | BATCH 5/8 | LOSS: 9.550330772375067e-05\n",
      "VAL: EPOCH 28/1000 | BATCH 6/8 | LOSS: 9.370712879379945e-05\n",
      "VAL: EPOCH 28/1000 | BATCH 7/8 | LOSS: 9.160109857475618e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 0/71 | LOSS: 9.09024165594019e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 1/71 | LOSS: 7.895477756392211e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 2/71 | LOSS: 0.00010771172916671883\n",
      "TRAIN: EPOCH 29/1000 | BATCH 3/71 | LOSS: 0.00010185938299400732\n",
      "TRAIN: EPOCH 29/1000 | BATCH 4/71 | LOSS: 9.491748933214695e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 5/71 | LOSS: 9.541424272659545e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 6/71 | LOSS: 9.155955506555204e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 7/71 | LOSS: 9.148189747065771e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 8/71 | LOSS: 9.69235035073426e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 9/71 | LOSS: 9.191648496198468e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 10/71 | LOSS: 8.910124928330664e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 11/71 | LOSS: 8.79193812579615e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 12/71 | LOSS: 8.606109118912942e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 13/71 | LOSS: 8.983322498222281e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 14/71 | LOSS: 8.822146628517658e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 15/71 | LOSS: 9.05218284970033e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 16/71 | LOSS: 9.13449064896935e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 17/71 | LOSS: 9.203781438474025e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 18/71 | LOSS: 9.064304820065827e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 19/71 | LOSS: 9.078573930310085e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 20/71 | LOSS: 9.047344723339414e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 21/71 | LOSS: 8.92699999068017e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 22/71 | LOSS: 8.82075255503878e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 23/71 | LOSS: 8.777785084627492e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 24/71 | LOSS: 8.76431752112694e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 25/71 | LOSS: 8.880091543407895e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 26/71 | LOSS: 8.797293655031051e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 27/71 | LOSS: 8.787316125692866e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 28/71 | LOSS: 8.883358331613949e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 29/71 | LOSS: 8.858299988787622e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 30/71 | LOSS: 9.013016402135573e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 31/71 | LOSS: 9.08724077817169e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 32/71 | LOSS: 9.049214669644381e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 33/71 | LOSS: 9.025437954251709e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 34/71 | LOSS: 8.97749963249745e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 35/71 | LOSS: 8.965171218733303e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 36/71 | LOSS: 9.069161535228125e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 37/71 | LOSS: 9.177677144945942e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 38/71 | LOSS: 9.232709010412249e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 39/71 | LOSS: 9.162579008261673e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 40/71 | LOSS: 9.113114161087519e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 41/71 | LOSS: 9.028852547912503e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 42/71 | LOSS: 8.984356404547472e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 43/71 | LOSS: 8.949389203652655e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 44/71 | LOSS: 9.013263673599189e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 45/71 | LOSS: 8.935575517724551e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 46/71 | LOSS: 9.067958416516319e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 47/71 | LOSS: 9.109520730514002e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 48/71 | LOSS: 9.074440785465116e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 49/71 | LOSS: 9.0254917886341e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 50/71 | LOSS: 9.043767689339195e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 51/71 | LOSS: 9.171162379901104e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 52/71 | LOSS: 9.169360721057703e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 53/71 | LOSS: 9.167559228781438e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 54/71 | LOSS: 9.148051070620898e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 55/71 | LOSS: 9.131126379153491e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 56/71 | LOSS: 9.124879132988944e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 57/71 | LOSS: 9.314832227416979e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 58/71 | LOSS: 9.30559926003344e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 59/71 | LOSS: 9.376675613263312e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 60/71 | LOSS: 9.442950416638776e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 61/71 | LOSS: 9.40539107579107e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 62/71 | LOSS: 9.354527534765472e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 63/71 | LOSS: 9.314942087712552e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 64/71 | LOSS: 9.308010015117291e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 65/71 | LOSS: 9.30179382769645e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 66/71 | LOSS: 9.277012251724085e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 67/71 | LOSS: 9.24117150990402e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 68/71 | LOSS: 9.209349894039063e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 69/71 | LOSS: 9.1726773929882e-05\n",
      "TRAIN: EPOCH 29/1000 | BATCH 70/71 | LOSS: 9.137687673398726e-05\n",
      "VAL: EPOCH 29/1000 | BATCH 0/8 | LOSS: 0.000123208126751706\n",
      "VAL: EPOCH 29/1000 | BATCH 1/8 | LOSS: 9.267387940781191e-05\n",
      "VAL: EPOCH 29/1000 | BATCH 2/8 | LOSS: 9.790431067813188e-05\n",
      "VAL: EPOCH 29/1000 | BATCH 3/8 | LOSS: 9.061865057446994e-05\n",
      "VAL: EPOCH 29/1000 | BATCH 4/8 | LOSS: 8.822881645755842e-05\n",
      "VAL: EPOCH 29/1000 | BATCH 5/8 | LOSS: 8.843925267380352e-05\n",
      "VAL: EPOCH 29/1000 | BATCH 6/8 | LOSS: 8.749241429281287e-05\n",
      "VAL: EPOCH 29/1000 | BATCH 7/8 | LOSS: 8.540903399989475e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 0/71 | LOSS: 7.368699152721092e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 1/71 | LOSS: 8.938831524574198e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 2/71 | LOSS: 0.00010987080410510923\n",
      "TRAIN: EPOCH 30/1000 | BATCH 3/71 | LOSS: 9.722480172058567e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 4/71 | LOSS: 9.595407755114138e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 5/71 | LOSS: 9.600703197065741e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 6/71 | LOSS: 9.257515401779009e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 7/71 | LOSS: 9.097684232983738e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 8/71 | LOSS: 9.182032145973708e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 9/71 | LOSS: 8.943628417910077e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 10/71 | LOSS: 8.727167203853077e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 11/71 | LOSS: 8.420621209855501e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 12/71 | LOSS: 8.410454924719837e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 13/71 | LOSS: 8.290834583541644e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 14/71 | LOSS: 8.244423855406543e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 15/71 | LOSS: 8.726449141249759e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 16/71 | LOSS: 8.62714326050242e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 17/71 | LOSS: 8.41873410940429e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 18/71 | LOSS: 8.632745524856115e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 19/71 | LOSS: 8.870452911651228e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 20/71 | LOSS: 8.965486423611375e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 21/71 | LOSS: 8.870461566733535e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 22/71 | LOSS: 8.960647834971061e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 23/71 | LOSS: 8.985741260403302e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 24/71 | LOSS: 8.865278316079639e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 25/71 | LOSS: 8.889339635341178e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 26/71 | LOSS: 8.853503782625517e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 27/71 | LOSS: 8.860745258841365e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 28/71 | LOSS: 8.807921315972886e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 29/71 | LOSS: 8.82360314183946e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 30/71 | LOSS: 9.0325928707194e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 31/71 | LOSS: 8.966937991772284e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 32/71 | LOSS: 8.932723446350516e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 33/71 | LOSS: 8.886451574011623e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 34/71 | LOSS: 8.922461368326497e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 35/71 | LOSS: 8.858337029475176e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 36/71 | LOSS: 8.788527407592187e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 37/71 | LOSS: 8.747976962216566e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 38/71 | LOSS: 8.732041831722316e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 39/71 | LOSS: 8.844863577905926e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 40/71 | LOSS: 8.930704262172918e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 41/71 | LOSS: 8.92035143243404e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 42/71 | LOSS: 8.86797926457718e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 43/71 | LOSS: 8.810941546306077e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 44/71 | LOSS: 8.92156116606202e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 45/71 | LOSS: 8.925614091150119e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 46/71 | LOSS: 8.910040405487265e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 47/71 | LOSS: 9.015976130892038e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 48/71 | LOSS: 9.027349280623472e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 49/71 | LOSS: 9.028954875248019e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 50/71 | LOSS: 9.001283993426364e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 51/71 | LOSS: 9.104347617931825e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 52/71 | LOSS: 9.088481086984998e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 53/71 | LOSS: 9.022340822816154e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 54/71 | LOSS: 8.995463742228986e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 55/71 | LOSS: 8.917492992363154e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 56/71 | LOSS: 8.847101624971337e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 57/71 | LOSS: 8.823769788030968e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 58/71 | LOSS: 8.790273735076741e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 59/71 | LOSS: 8.744974517564212e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 60/71 | LOSS: 8.731437402111326e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 61/71 | LOSS: 8.716089236728411e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 62/71 | LOSS: 8.689471637591764e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 63/71 | LOSS: 8.670619507711308e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 64/71 | LOSS: 8.659375391569203e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 65/71 | LOSS: 8.631123433367588e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 66/71 | LOSS: 8.61228136680221e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 67/71 | LOSS: 8.606539262580234e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 68/71 | LOSS: 8.613745102184771e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 69/71 | LOSS: 8.594056681821322e-05\n",
      "TRAIN: EPOCH 30/1000 | BATCH 70/71 | LOSS: 8.542898413766189e-05\n",
      "VAL: EPOCH 30/1000 | BATCH 0/8 | LOSS: 0.00011542059655766934\n",
      "VAL: EPOCH 30/1000 | BATCH 1/8 | LOSS: 8.72954296937678e-05\n",
      "VAL: EPOCH 30/1000 | BATCH 2/8 | LOSS: 9.085638885153458e-05\n",
      "VAL: EPOCH 30/1000 | BATCH 3/8 | LOSS: 8.295807856484316e-05\n",
      "VAL: EPOCH 30/1000 | BATCH 4/8 | LOSS: 8.052329940255731e-05\n",
      "VAL: EPOCH 30/1000 | BATCH 5/8 | LOSS: 8.118209249611634e-05\n",
      "VAL: EPOCH 30/1000 | BATCH 6/8 | LOSS: 8.045525880463953e-05\n",
      "VAL: EPOCH 30/1000 | BATCH 7/8 | LOSS: 7.845449727028608e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 0/71 | LOSS: 8.507194434059784e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 1/71 | LOSS: 0.00011718374662450515\n",
      "TRAIN: EPOCH 31/1000 | BATCH 2/71 | LOSS: 9.77506788331084e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 3/71 | LOSS: 9.04812532098731e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 4/71 | LOSS: 8.275177679024636e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 5/71 | LOSS: 8.361992634794053e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 6/71 | LOSS: 8.062214224732347e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 7/71 | LOSS: 7.920340158307226e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 8/71 | LOSS: 7.723015328843353e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 9/71 | LOSS: 7.542523526353762e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 10/71 | LOSS: 7.641677141443573e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 11/71 | LOSS: 8.204031473724172e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 12/71 | LOSS: 8.147475119375695e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 13/71 | LOSS: 7.993469755872087e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 14/71 | LOSS: 7.840829906247866e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 15/71 | LOSS: 7.731222285656258e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 16/71 | LOSS: 7.592971772229408e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 17/71 | LOSS: 7.591347902133648e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 18/71 | LOSS: 7.711049386814825e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 19/71 | LOSS: 7.684895826969296e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 20/71 | LOSS: 7.775151607347652e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 21/71 | LOSS: 7.67736591197635e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 22/71 | LOSS: 7.656205359481923e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 23/71 | LOSS: 7.501805688055659e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 24/71 | LOSS: 7.712779959547334e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 25/71 | LOSS: 7.749438415115807e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 26/71 | LOSS: 7.723817853494202e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 27/71 | LOSS: 7.730219853588746e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 28/71 | LOSS: 7.652632296506861e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 29/71 | LOSS: 7.59193702833727e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 30/71 | LOSS: 7.5668569244895e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 31/71 | LOSS: 7.589919164274761e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 32/71 | LOSS: 7.636610142512698e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 33/71 | LOSS: 7.593941828578382e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 34/71 | LOSS: 7.566167706889766e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 35/71 | LOSS: 7.529068413229349e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 36/71 | LOSS: 7.499054245764037e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 37/71 | LOSS: 7.443537929487463e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 38/71 | LOSS: 7.584255311088875e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 39/71 | LOSS: 7.564435654785484e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 40/71 | LOSS: 7.797975168370346e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 41/71 | LOSS: 8.250724758476107e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 42/71 | LOSS: 8.202966798075236e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 43/71 | LOSS: 8.162948705086654e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 44/71 | LOSS: 8.205395958308752e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 45/71 | LOSS: 8.151068773852033e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 46/71 | LOSS: 8.12944955659406e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 47/71 | LOSS: 8.110425104253712e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 48/71 | LOSS: 8.109767040135624e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 49/71 | LOSS: 8.277763270598371e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 50/71 | LOSS: 8.274748010900985e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 51/71 | LOSS: 8.253516989498166e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 52/71 | LOSS: 8.287584349537215e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 53/71 | LOSS: 8.23695405684972e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 54/71 | LOSS: 8.209143872425722e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 55/71 | LOSS: 8.16483423997332e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 56/71 | LOSS: 8.164066885160936e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 57/71 | LOSS: 8.125105855940311e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 58/71 | LOSS: 8.087025319474702e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 59/71 | LOSS: 8.078068094619084e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 60/71 | LOSS: 8.036994183728029e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 61/71 | LOSS: 7.981727761826704e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 62/71 | LOSS: 7.94077758554421e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 63/71 | LOSS: 7.946398676494937e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 64/71 | LOSS: 7.991598837319404e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 65/71 | LOSS: 8.007022226978954e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 66/71 | LOSS: 8.001979428505066e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 67/71 | LOSS: 7.99271772527444e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 68/71 | LOSS: 8.012992818122797e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 69/71 | LOSS: 7.990843540548148e-05\n",
      "TRAIN: EPOCH 31/1000 | BATCH 70/71 | LOSS: 7.983086983022332e-05\n",
      "VAL: EPOCH 31/1000 | BATCH 0/8 | LOSS: 0.0001031920182867907\n",
      "VAL: EPOCH 31/1000 | BATCH 1/8 | LOSS: 7.977799759828486e-05\n",
      "VAL: EPOCH 31/1000 | BATCH 2/8 | LOSS: 8.456323606272538e-05\n",
      "VAL: EPOCH 31/1000 | BATCH 3/8 | LOSS: 7.925191493995953e-05\n",
      "VAL: EPOCH 31/1000 | BATCH 4/8 | LOSS: 7.736219995422289e-05\n",
      "VAL: EPOCH 31/1000 | BATCH 5/8 | LOSS: 7.71184701685949e-05\n",
      "VAL: EPOCH 31/1000 | BATCH 6/8 | LOSS: 7.650305529490911e-05\n",
      "VAL: EPOCH 31/1000 | BATCH 7/8 | LOSS: 7.501895106543088e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 0/71 | LOSS: 5.677491208189167e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 1/71 | LOSS: 5.5299375162576325e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 2/71 | LOSS: 5.5143637534153335e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 3/71 | LOSS: 5.533462626772234e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 4/71 | LOSS: 6.118310047895648e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 5/71 | LOSS: 6.030382246535737e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 6/71 | LOSS: 6.315020592800076e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 7/71 | LOSS: 6.148638522063266e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 8/71 | LOSS: 6.166039747768082e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 9/71 | LOSS: 6.11233106610598e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 10/71 | LOSS: 6.169418702484109e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 11/71 | LOSS: 6.109229646729848e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 12/71 | LOSS: 6.109193754337657e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 13/71 | LOSS: 6.303497684712056e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 14/71 | LOSS: 6.313555907884923e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 15/71 | LOSS: 6.80950677178771e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 16/71 | LOSS: 6.831788542673595e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 17/71 | LOSS: 6.803983564572668e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 18/71 | LOSS: 6.643244801364887e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 19/71 | LOSS: 6.579208766197553e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 20/71 | LOSS: 6.642470838241501e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 21/71 | LOSS: 6.860340148374566e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 22/71 | LOSS: 6.839312640216161e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 23/71 | LOSS: 6.825236975297837e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 24/71 | LOSS: 6.946964203962125e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 25/71 | LOSS: 7.161288200362693e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 26/71 | LOSS: 7.161588628487489e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 27/71 | LOSS: 7.149928270726897e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 28/71 | LOSS: 7.097950483198631e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 29/71 | LOSS: 7.087647900334559e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 30/71 | LOSS: 7.042328739302203e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 31/71 | LOSS: 7.022052466254536e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 32/71 | LOSS: 7.050863573429965e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 33/71 | LOSS: 7.183824613200747e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 34/71 | LOSS: 7.168831995971101e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 35/71 | LOSS: 7.283727538581136e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 36/71 | LOSS: 7.235904126286482e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 37/71 | LOSS: 7.217984861390984e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 38/71 | LOSS: 7.342876471319379e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 39/71 | LOSS: 7.31775518033828e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 40/71 | LOSS: 7.272414160713485e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 41/71 | LOSS: 7.333272627701739e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 42/71 | LOSS: 7.406291089949285e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 43/71 | LOSS: 7.53210554400787e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 44/71 | LOSS: 7.468880236653301e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 45/71 | LOSS: 7.401920178697607e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 46/71 | LOSS: 7.443557300807987e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 47/71 | LOSS: 7.54877532926912e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 48/71 | LOSS: 7.600528743398395e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 49/71 | LOSS: 7.613982903421856e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 50/71 | LOSS: 7.570822501792481e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 51/71 | LOSS: 7.625603491480713e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 52/71 | LOSS: 7.608179390136117e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 53/71 | LOSS: 7.576554755225381e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 54/71 | LOSS: 7.544489999418147e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 55/71 | LOSS: 7.513196325297551e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 56/71 | LOSS: 7.508997416089027e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 57/71 | LOSS: 7.46659001190986e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 58/71 | LOSS: 7.425724865150464e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 59/71 | LOSS: 7.526093322667294e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 60/71 | LOSS: 7.641396783941166e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 61/71 | LOSS: 7.60413194873235e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 62/71 | LOSS: 7.609321030377898e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 63/71 | LOSS: 7.613465538724995e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 64/71 | LOSS: 7.615475960147496e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 65/71 | LOSS: 7.599734729438117e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 66/71 | LOSS: 7.57444819345125e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 67/71 | LOSS: 7.549071800467038e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 68/71 | LOSS: 7.568716830354643e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 69/71 | LOSS: 7.54138711012534e-05\n",
      "TRAIN: EPOCH 32/1000 | BATCH 70/71 | LOSS: 7.55210580983901e-05\n",
      "VAL: EPOCH 32/1000 | BATCH 0/8 | LOSS: 0.00010006608499679714\n",
      "VAL: EPOCH 32/1000 | BATCH 1/8 | LOSS: 7.695778185734525e-05\n",
      "VAL: EPOCH 32/1000 | BATCH 2/8 | LOSS: 7.852989074308425e-05\n",
      "VAL: EPOCH 32/1000 | BATCH 3/8 | LOSS: 7.145329254854005e-05\n",
      "VAL: EPOCH 32/1000 | BATCH 4/8 | LOSS: 6.912882890901529e-05\n",
      "VAL: EPOCH 32/1000 | BATCH 5/8 | LOSS: 7.033954352664296e-05\n",
      "VAL: EPOCH 32/1000 | BATCH 6/8 | LOSS: 6.951539035071619e-05\n",
      "VAL: EPOCH 32/1000 | BATCH 7/8 | LOSS: 6.807849831602653e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 0/71 | LOSS: 5.517716272152029e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 1/71 | LOSS: 6.90660144755384e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 2/71 | LOSS: 6.961656254134141e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 3/71 | LOSS: 7.017892949079396e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 4/71 | LOSS: 7.479753912775777e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 5/71 | LOSS: 7.56286020380988e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 6/71 | LOSS: 7.709696728852578e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 7/71 | LOSS: 7.744391996311606e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 8/71 | LOSS: 7.672564202544486e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 9/71 | LOSS: 7.538321333413477e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 10/71 | LOSS: 7.525566583023068e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 11/71 | LOSS: 7.409769114019582e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 12/71 | LOSS: 7.416453697190333e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 13/71 | LOSS: 7.595179002756984e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 14/71 | LOSS: 7.95387592612921e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 15/71 | LOSS: 7.869011346883781e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 16/71 | LOSS: 7.687458919998094e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 17/71 | LOSS: 8.353740789364868e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 18/71 | LOSS: 8.583653853795687e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 19/71 | LOSS: 8.38546426166431e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 20/71 | LOSS: 8.288037792053295e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 21/71 | LOSS: 8.139055270558774e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 22/71 | LOSS: 8.038929302964117e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 23/71 | LOSS: 7.919913787191035e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 24/71 | LOSS: 7.775029516778886e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 25/71 | LOSS: 7.702711123923879e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 26/71 | LOSS: 7.679265197819202e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 27/71 | LOSS: 7.65514132581302e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 28/71 | LOSS: 7.555704974822283e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 29/71 | LOSS: 7.560708678890175e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 30/71 | LOSS: 7.493971603370512e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 31/71 | LOSS: 7.449733664088853e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 32/71 | LOSS: 7.41875856792004e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 33/71 | LOSS: 7.349992312597083e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 34/71 | LOSS: 7.376394132734277e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 35/71 | LOSS: 7.288511440290474e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 36/71 | LOSS: 7.227244193042704e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 37/71 | LOSS: 7.176101462391671e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 38/71 | LOSS: 7.103501103791551e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 39/71 | LOSS: 7.219571480163723e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 40/71 | LOSS: 7.232114068933808e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 41/71 | LOSS: 7.277469306981879e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 42/71 | LOSS: 7.236094495246382e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 43/71 | LOSS: 7.295201290121027e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 44/71 | LOSS: 7.257739351997669e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 45/71 | LOSS: 7.210433977888897e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 46/71 | LOSS: 7.270078810719535e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 47/71 | LOSS: 7.233241300734032e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 48/71 | LOSS: 7.176276230151119e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 49/71 | LOSS: 7.128202945750672e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 50/71 | LOSS: 7.120462332215324e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 51/71 | LOSS: 7.123521467852137e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 52/71 | LOSS: 7.145867411449183e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 53/71 | LOSS: 7.152674509227466e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 54/71 | LOSS: 7.150668342629532e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 55/71 | LOSS: 7.106658157291739e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 56/71 | LOSS: 7.087677384557187e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 57/71 | LOSS: 7.068314366263402e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 58/71 | LOSS: 7.085908010455985e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 59/71 | LOSS: 7.061825854179916e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 60/71 | LOSS: 7.150494973041637e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 61/71 | LOSS: 7.176347160918428e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 62/71 | LOSS: 7.149064936490345e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 63/71 | LOSS: 7.1487686909677e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 64/71 | LOSS: 7.112813971336716e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 65/71 | LOSS: 7.09121791398061e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 66/71 | LOSS: 7.064858990775268e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 67/71 | LOSS: 7.152239558148293e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 68/71 | LOSS: 7.149589894823663e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 69/71 | LOSS: 7.11726747665255e-05\n",
      "TRAIN: EPOCH 33/1000 | BATCH 70/71 | LOSS: 7.091810771847852e-05\n",
      "VAL: EPOCH 33/1000 | BATCH 0/8 | LOSS: 9.512614633422345e-05\n",
      "VAL: EPOCH 33/1000 | BATCH 1/8 | LOSS: 7.545449443568941e-05\n",
      "VAL: EPOCH 33/1000 | BATCH 2/8 | LOSS: 7.657911798257071e-05\n",
      "VAL: EPOCH 33/1000 | BATCH 3/8 | LOSS: 6.955801200092537e-05\n",
      "VAL: EPOCH 33/1000 | BATCH 4/8 | LOSS: 6.704988263663835e-05\n",
      "VAL: EPOCH 33/1000 | BATCH 5/8 | LOSS: 6.785367501530952e-05\n",
      "VAL: EPOCH 33/1000 | BATCH 6/8 | LOSS: 6.755117283319123e-05\n",
      "VAL: EPOCH 33/1000 | BATCH 7/8 | LOSS: 6.611335993511602e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 0/71 | LOSS: 0.00013200112152844667\n",
      "TRAIN: EPOCH 34/1000 | BATCH 1/71 | LOSS: 9.905176193569787e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 2/71 | LOSS: 8.480670415641119e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 3/71 | LOSS: 7.924248166091274e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 4/71 | LOSS: 8.963048312580213e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 5/71 | LOSS: 8.381590669159777e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 6/71 | LOSS: 8.56122704655198e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 7/71 | LOSS: 8.290733330795774e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 8/71 | LOSS: 8.078577148909163e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 9/71 | LOSS: 7.823525520507246e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 10/71 | LOSS: 7.772378210740334e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 11/71 | LOSS: 8.013566609103388e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 12/71 | LOSS: 7.70120112485109e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 13/71 | LOSS: 7.498218214355543e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 14/71 | LOSS: 7.247061633582537e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 15/71 | LOSS: 7.07130739101558e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 16/71 | LOSS: 7.12142269442077e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 17/71 | LOSS: 6.997166888646057e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 18/71 | LOSS: 7.026130946255044e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 19/71 | LOSS: 6.90012973791454e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 20/71 | LOSS: 6.825682032199221e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 21/71 | LOSS: 6.748162856886417e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 22/71 | LOSS: 6.873065095809896e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 23/71 | LOSS: 6.902443753157665e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 24/71 | LOSS: 6.819688162067904e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 25/71 | LOSS: 6.742677900407356e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 26/71 | LOSS: 6.738277025525113e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 27/71 | LOSS: 6.789781361606271e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 28/71 | LOSS: 6.95426237582759e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 29/71 | LOSS: 6.929420475595785e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 30/71 | LOSS: 7.068049883180779e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 31/71 | LOSS: 7.164161741002317e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 32/71 | LOSS: 7.100031948401451e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 33/71 | LOSS: 7.038199574333917e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 34/71 | LOSS: 7.079640397153395e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 35/71 | LOSS: 7.023338306074341e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 36/71 | LOSS: 7.045268560161915e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 37/71 | LOSS: 7.001479829532879e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 38/71 | LOSS: 6.982265749217894e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 39/71 | LOSS: 7.032634130155202e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 40/71 | LOSS: 7.00378073904686e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 41/71 | LOSS: 6.975333304754237e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 42/71 | LOSS: 6.941600051589397e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 43/71 | LOSS: 6.856113379233813e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 44/71 | LOSS: 6.938006586602165e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 45/71 | LOSS: 6.872446054172622e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 46/71 | LOSS: 6.841173447529844e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 47/71 | LOSS: 6.830015566568666e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 48/71 | LOSS: 6.85120615585498e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 49/71 | LOSS: 6.823889772931579e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 50/71 | LOSS: 6.820966354651176e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 51/71 | LOSS: 6.778746928130904e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 52/71 | LOSS: 6.788845286860553e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 53/71 | LOSS: 6.837925664310913e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 54/71 | LOSS: 6.810360361917199e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 55/71 | LOSS: 6.770866502847639e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 56/71 | LOSS: 6.844059431674203e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 57/71 | LOSS: 6.831977708347479e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 58/71 | LOSS: 6.805478465216534e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 59/71 | LOSS: 6.807657088453804e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 60/71 | LOSS: 6.762053799618905e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 61/71 | LOSS: 6.749987932703187e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 62/71 | LOSS: 6.72930989475433e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 63/71 | LOSS: 6.685428161290474e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 64/71 | LOSS: 6.654777204788004e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 65/71 | LOSS: 6.617219674587955e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 66/71 | LOSS: 6.600701356125968e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 67/71 | LOSS: 6.608510043674066e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 68/71 | LOSS: 6.599165826101208e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 69/71 | LOSS: 6.580014383611602e-05\n",
      "TRAIN: EPOCH 34/1000 | BATCH 70/71 | LOSS: 6.577242009012147e-05\n",
      "VAL: EPOCH 34/1000 | BATCH 0/8 | LOSS: 8.334128506248817e-05\n",
      "VAL: EPOCH 34/1000 | BATCH 1/8 | LOSS: 6.530348400701769e-05\n",
      "VAL: EPOCH 34/1000 | BATCH 2/8 | LOSS: 6.772706547053531e-05\n",
      "VAL: EPOCH 34/1000 | BATCH 3/8 | LOSS: 6.210883111634757e-05\n",
      "VAL: EPOCH 34/1000 | BATCH 4/8 | LOSS: 6.009735516272485e-05\n",
      "VAL: EPOCH 34/1000 | BATCH 5/8 | LOSS: 6.099802461297562e-05\n",
      "VAL: EPOCH 34/1000 | BATCH 6/8 | LOSS: 5.996166146360338e-05\n",
      "VAL: EPOCH 34/1000 | BATCH 7/8 | LOSS: 5.885101290914463e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 0/71 | LOSS: 4.933009768137708e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 1/71 | LOSS: 4.789544254890643e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 2/71 | LOSS: 4.6888754392663636e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 3/71 | LOSS: 4.811090457224054e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 4/71 | LOSS: 4.866204180871136e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 5/71 | LOSS: 5.104414655458337e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 6/71 | LOSS: 5.810084206002232e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 7/71 | LOSS: 5.82316151849227e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 8/71 | LOSS: 5.70678768983473e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 9/71 | LOSS: 5.9237492314423436e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 10/71 | LOSS: 6.119973874195817e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 11/71 | LOSS: 6.042393791479602e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 12/71 | LOSS: 5.93031970940781e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 13/71 | LOSS: 5.89323268026679e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 14/71 | LOSS: 6.289289910152243e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 15/71 | LOSS: 6.212927337401197e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 16/71 | LOSS: 6.180010463782203e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 17/71 | LOSS: 6.492159930833925e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 18/71 | LOSS: 6.43626006326246e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 19/71 | LOSS: 6.657040758000221e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 20/71 | LOSS: 6.59124771551606e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 21/71 | LOSS: 6.504723817422267e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 22/71 | LOSS: 6.464822992869733e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 23/71 | LOSS: 6.363437857241176e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 24/71 | LOSS: 6.473449364420958e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 25/71 | LOSS: 6.65223945431465e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 26/71 | LOSS: 6.765187262553136e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 27/71 | LOSS: 6.746337632128936e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 28/71 | LOSS: 6.786114794647738e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 29/71 | LOSS: 6.707122908361877e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 30/71 | LOSS: 6.69175177820087e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 31/71 | LOSS: 6.647220573086088e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 32/71 | LOSS: 6.595799630193858e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 33/71 | LOSS: 6.56636983716025e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 34/71 | LOSS: 6.524316444743558e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 35/71 | LOSS: 6.472447512351209e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 36/71 | LOSS: 6.41792051995966e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 37/71 | LOSS: 6.358758850853376e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 38/71 | LOSS: 6.337682311706699e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 39/71 | LOSS: 6.270678159125964e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 40/71 | LOSS: 6.249936259558947e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 41/71 | LOSS: 6.268785095709886e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 42/71 | LOSS: 6.194239464347511e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 43/71 | LOSS: 6.177915326010076e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 44/71 | LOSS: 6.252290824906796e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 45/71 | LOSS: 6.230811026319355e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 46/71 | LOSS: 6.206885479920683e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 47/71 | LOSS: 6.185704197984403e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 48/71 | LOSS: 6.150966554723338e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 49/71 | LOSS: 6.148372842289972e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 50/71 | LOSS: 6.147866616968303e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 51/71 | LOSS: 6.230119813083617e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 52/71 | LOSS: 6.19797805793153e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 53/71 | LOSS: 6.193427887208397e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 54/71 | LOSS: 6.153905362060124e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 55/71 | LOSS: 6.162201446256534e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 56/71 | LOSS: 6.124070517998597e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 57/71 | LOSS: 6.203911013064247e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 58/71 | LOSS: 6.200353071385585e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 59/71 | LOSS: 6.19039485779164e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 60/71 | LOSS: 6.141846168874076e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 61/71 | LOSS: 6.197339978591058e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 62/71 | LOSS: 6.155226404348107e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 63/71 | LOSS: 6.131207186399479e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 64/71 | LOSS: 6.171485130415441e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 65/71 | LOSS: 6.145599185071082e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 66/71 | LOSS: 6.131686905742998e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 67/71 | LOSS: 6.123340004063827e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 68/71 | LOSS: 6.143767936062405e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 69/71 | LOSS: 6.147082598805095e-05\n",
      "TRAIN: EPOCH 35/1000 | BATCH 70/71 | LOSS: 6.125279735859064e-05\n",
      "VAL: EPOCH 35/1000 | BATCH 0/8 | LOSS: 7.747812196612358e-05\n",
      "VAL: EPOCH 35/1000 | BATCH 1/8 | LOSS: 6.378758735081647e-05\n",
      "VAL: EPOCH 35/1000 | BATCH 2/8 | LOSS: 6.772501728846692e-05\n",
      "VAL: EPOCH 35/1000 | BATCH 3/8 | LOSS: 6.451197168644285e-05\n",
      "VAL: EPOCH 35/1000 | BATCH 4/8 | LOSS: 6.301991888904012e-05\n",
      "VAL: EPOCH 35/1000 | BATCH 5/8 | LOSS: 6.2954354386117e-05\n",
      "VAL: EPOCH 35/1000 | BATCH 6/8 | LOSS: 6.234306991765541e-05\n",
      "VAL: EPOCH 35/1000 | BATCH 7/8 | LOSS: 6.109102423579316e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 0/71 | LOSS: 4.94493760925252e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 1/71 | LOSS: 4.551393067231402e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 2/71 | LOSS: 5.595581023953855e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 3/71 | LOSS: 5.813216193928383e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 4/71 | LOSS: 5.534766605705954e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 5/71 | LOSS: 5.7211597474330724e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 6/71 | LOSS: 5.543351497700704e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 7/71 | LOSS: 5.590919045062037e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 8/71 | LOSS: 5.440060057379823e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 9/71 | LOSS: 5.38626602065051e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 10/71 | LOSS: 5.1905135478591546e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 11/71 | LOSS: 5.138416448365509e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 12/71 | LOSS: 5.119655291379716e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 13/71 | LOSS: 5.2730836354645104e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 14/71 | LOSS: 5.1973119601219274e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 15/71 | LOSS: 5.624060463560454e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 16/71 | LOSS: 5.528166837072657e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 17/71 | LOSS: 5.4947438911767676e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 18/71 | LOSS: 5.668462912772635e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 19/71 | LOSS: 5.6975499319378287e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 20/71 | LOSS: 5.711684882823777e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 21/71 | LOSS: 5.686911588947458e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 22/71 | LOSS: 6.005024941101589e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 23/71 | LOSS: 5.972019152977737e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 24/71 | LOSS: 6.138044729596004e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 25/71 | LOSS: 6.110758980727181e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 26/71 | LOSS: 6.050342253055768e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 27/71 | LOSS: 5.971434360877278e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 28/71 | LOSS: 5.925220341004174e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 29/71 | LOSS: 5.951917143344569e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 30/71 | LOSS: 5.9142936562796335e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 31/71 | LOSS: 5.9160006117053854e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 32/71 | LOSS: 5.934113483597299e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 33/71 | LOSS: 5.8796292625415575e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 34/71 | LOSS: 5.994042928380493e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 35/71 | LOSS: 5.9584818296166806e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 36/71 | LOSS: 6.073828272574984e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 37/71 | LOSS: 6.0467911584725554e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 38/71 | LOSS: 5.989143317445921e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 39/71 | LOSS: 5.98338908275764e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 40/71 | LOSS: 5.974108752547536e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 41/71 | LOSS: 5.9496439245690236e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 42/71 | LOSS: 5.954925350230295e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 43/71 | LOSS: 5.977434565548637e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 44/71 | LOSS: 6.023946358861091e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 45/71 | LOSS: 6.0043777051285595e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 46/71 | LOSS: 6.0111227209377595e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 47/71 | LOSS: 6.0024105475046476e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 48/71 | LOSS: 6.075080210041274e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 49/71 | LOSS: 6.039930703991558e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 50/71 | LOSS: 5.9871723280201536e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 51/71 | LOSS: 5.966222397546517e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 52/71 | LOSS: 5.925964590992322e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 53/71 | LOSS: 5.905900562302796e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 54/71 | LOSS: 5.88363843483173e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 55/71 | LOSS: 5.906897916117616e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 56/71 | LOSS: 5.8972528086885355e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 57/71 | LOSS: 5.897331796185081e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 58/71 | LOSS: 5.8826425305658444e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 59/71 | LOSS: 5.874462440260686e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 60/71 | LOSS: 5.8739063698325126e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 61/71 | LOSS: 5.8552012975973585e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 62/71 | LOSS: 5.849535054516136e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 63/71 | LOSS: 5.83491060410779e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 64/71 | LOSS: 5.839852232244224e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 65/71 | LOSS: 5.8149384296319276e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 66/71 | LOSS: 5.8041005484401875e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 67/71 | LOSS: 5.813751624159255e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 68/71 | LOSS: 5.805170334549958e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 69/71 | LOSS: 5.821411952118589e-05\n",
      "TRAIN: EPOCH 36/1000 | BATCH 70/71 | LOSS: 5.805472376867867e-05\n",
      "VAL: EPOCH 36/1000 | BATCH 0/8 | LOSS: 7.152222678996623e-05\n",
      "VAL: EPOCH 36/1000 | BATCH 1/8 | LOSS: 5.847161082783714e-05\n",
      "VAL: EPOCH 36/1000 | BATCH 2/8 | LOSS: 6.039456153909365e-05\n",
      "VAL: EPOCH 36/1000 | BATCH 3/8 | LOSS: 5.585332382906927e-05\n",
      "VAL: EPOCH 36/1000 | BATCH 4/8 | LOSS: 5.39901498996187e-05\n",
      "VAL: EPOCH 36/1000 | BATCH 5/8 | LOSS: 5.486368475734101e-05\n",
      "VAL: EPOCH 36/1000 | BATCH 6/8 | LOSS: 5.4003012857200315e-05\n",
      "VAL: EPOCH 36/1000 | BATCH 7/8 | LOSS: 5.280449568090262e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 0/71 | LOSS: 4.2921979911625385e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 1/71 | LOSS: 4.9736057917471044e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 2/71 | LOSS: 4.9275404307991266e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 3/71 | LOSS: 4.479562358028488e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 4/71 | LOSS: 5.297916432027705e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 5/71 | LOSS: 5.355564765826178e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 6/71 | LOSS: 5.5908111140264995e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 7/71 | LOSS: 5.630321174976416e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 8/71 | LOSS: 5.384394899010658e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 9/71 | LOSS: 5.402476745075546e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 10/71 | LOSS: 5.2422588024373084e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 11/71 | LOSS: 5.2611367512630146e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 12/71 | LOSS: 5.2007257741374466e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 13/71 | LOSS: 5.5434550761544544e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 14/71 | LOSS: 5.437534030837317e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 15/71 | LOSS: 5.380440143198939e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 16/71 | LOSS: 5.4545608530615404e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 17/71 | LOSS: 5.4242926934320065e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 18/71 | LOSS: 5.3529481719076435e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 19/71 | LOSS: 5.6145643247873524e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 20/71 | LOSS: 5.619528868313258e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 21/71 | LOSS: 5.63608919037506e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 22/71 | LOSS: 5.625020212787406e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 23/71 | LOSS: 5.5976244160168186e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 24/71 | LOSS: 5.5352226336253804e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 25/71 | LOSS: 5.4766398674333825e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 26/71 | LOSS: 5.425365210006324e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 27/71 | LOSS: 5.437025252571662e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 28/71 | LOSS: 5.3861070071058024e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 29/71 | LOSS: 5.354071715070556e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 30/71 | LOSS: 5.4149764999296636e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 31/71 | LOSS: 5.406766331361723e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 32/71 | LOSS: 5.398013231707878e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 33/71 | LOSS: 5.4011531520860875e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 34/71 | LOSS: 5.38101747224573e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 35/71 | LOSS: 5.3306201961176055e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 36/71 | LOSS: 5.299978049053509e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 37/71 | LOSS: 5.295982268044578e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 38/71 | LOSS: 5.2883236770494244e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 39/71 | LOSS: 5.269460707495455e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 40/71 | LOSS: 5.3061357459405486e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 41/71 | LOSS: 5.2818450751060265e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 42/71 | LOSS: 5.328120037014488e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 43/71 | LOSS: 5.329077934716638e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 44/71 | LOSS: 5.356430313743961e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 45/71 | LOSS: 5.301035474511319e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 46/71 | LOSS: 5.285417899629349e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 47/71 | LOSS: 5.363730106940542e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 48/71 | LOSS: 5.36432781904741e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 49/71 | LOSS: 5.3799971465196e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 50/71 | LOSS: 5.344984606827162e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 51/71 | LOSS: 5.3446320390111607e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 52/71 | LOSS: 5.5267977689443386e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 53/71 | LOSS: 5.503389160434448e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 54/71 | LOSS: 5.495757441994184e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 55/71 | LOSS: 5.463805431255813e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 56/71 | LOSS: 5.4431970662941843e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 57/71 | LOSS: 5.4638669192580236e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 58/71 | LOSS: 5.457646795425177e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 59/71 | LOSS: 5.4656306595764666e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 60/71 | LOSS: 5.464026101241766e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 61/71 | LOSS: 5.4519738613077693e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 62/71 | LOSS: 5.4577918807996276e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 63/71 | LOSS: 5.4382992601631486e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 64/71 | LOSS: 5.4308093156298406e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 65/71 | LOSS: 5.422798436815025e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 66/71 | LOSS: 5.420784061664277e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 67/71 | LOSS: 5.40517462468415e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 68/71 | LOSS: 5.4076164261736586e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 69/71 | LOSS: 5.429121668255123e-05\n",
      "TRAIN: EPOCH 37/1000 | BATCH 70/71 | LOSS: 5.423089386964984e-05\n",
      "VAL: EPOCH 37/1000 | BATCH 0/8 | LOSS: 6.613055302295834e-05\n",
      "VAL: EPOCH 37/1000 | BATCH 1/8 | LOSS: 5.633279215544462e-05\n",
      "VAL: EPOCH 37/1000 | BATCH 2/8 | LOSS: 5.817737595255797e-05\n",
      "VAL: EPOCH 37/1000 | BATCH 3/8 | LOSS: 5.422153299150523e-05\n",
      "VAL: EPOCH 37/1000 | BATCH 4/8 | LOSS: 5.283053906168789e-05\n",
      "VAL: EPOCH 37/1000 | BATCH 5/8 | LOSS: 5.290924552051971e-05\n",
      "VAL: EPOCH 37/1000 | BATCH 6/8 | LOSS: 5.217049329075962e-05\n",
      "VAL: EPOCH 37/1000 | BATCH 7/8 | LOSS: 5.155727512828889e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 0/71 | LOSS: 5.2632447477662936e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 1/71 | LOSS: 5.93607564951526e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 2/71 | LOSS: 5.496665835380554e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 3/71 | LOSS: 5.941581730439793e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 4/71 | LOSS: 5.59839594643563e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 5/71 | LOSS: 5.2492549002636224e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 6/71 | LOSS: 5.091989805805497e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 7/71 | LOSS: 5.161891613170155e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 8/71 | LOSS: 4.9043076109632646e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 9/71 | LOSS: 4.7506306509603745e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 10/71 | LOSS: 4.7964001913152806e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 11/71 | LOSS: 5.141614656167803e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 12/71 | LOSS: 5.107163647503162e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 13/71 | LOSS: 5.285826695658865e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 14/71 | LOSS: 5.3216694262422e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 15/71 | LOSS: 5.384941846386937e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 16/71 | LOSS: 5.379229953498854e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 17/71 | LOSS: 5.329646672988828e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 18/71 | LOSS: 5.467121311659483e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 19/71 | LOSS: 5.428810727607925e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 20/71 | LOSS: 5.3750648519434476e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 21/71 | LOSS: 5.31807740613162e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 22/71 | LOSS: 5.277992576062072e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 23/71 | LOSS: 5.287490572906487e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 24/71 | LOSS: 5.283071950543672e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 25/71 | LOSS: 5.330895678176043e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 26/71 | LOSS: 5.4187332150629824e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 27/71 | LOSS: 5.417961671940118e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 28/71 | LOSS: 5.415450467463133e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 29/71 | LOSS: 5.4064160455406336e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 30/71 | LOSS: 5.384542823923121e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 31/71 | LOSS: 5.32328729150322e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 32/71 | LOSS: 5.2702736535469406e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 33/71 | LOSS: 5.268416551938144e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 34/71 | LOSS: 5.267230163943688e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 35/71 | LOSS: 5.282903647538559e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 36/71 | LOSS: 5.3197190985933095e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 37/71 | LOSS: 5.3284515639748666e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 38/71 | LOSS: 5.2940186343711015e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 39/71 | LOSS: 5.259446497802855e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 40/71 | LOSS: 5.34752661024402e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 41/71 | LOSS: 5.3567735449178144e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 42/71 | LOSS: 5.352722740120276e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 43/71 | LOSS: 5.340179632185027e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 44/71 | LOSS: 5.3029565929642155e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 45/71 | LOSS: 5.2773218909430355e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 46/71 | LOSS: 5.300578204890158e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 47/71 | LOSS: 5.293058408521271e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 48/71 | LOSS: 5.268509289171376e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 49/71 | LOSS: 5.296603463648353e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 50/71 | LOSS: 5.273850105118518e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 51/71 | LOSS: 5.262735640616693e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 52/71 | LOSS: 5.252858531059486e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 53/71 | LOSS: 5.26386402607723e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 54/71 | LOSS: 5.241231490138241e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 55/71 | LOSS: 5.22850948852595e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 56/71 | LOSS: 5.228784306254647e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 57/71 | LOSS: 5.237939663418039e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 58/71 | LOSS: 5.252855358560475e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 59/71 | LOSS: 5.241054380652107e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 60/71 | LOSS: 5.226680120552478e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 61/71 | LOSS: 5.204042516256337e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 62/71 | LOSS: 5.176631453824008e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 63/71 | LOSS: 5.1510050980141386e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 64/71 | LOSS: 5.139957944405838e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 65/71 | LOSS: 5.198397670800191e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 66/71 | LOSS: 5.1970054558825806e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 67/71 | LOSS: 5.184900793906041e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 68/71 | LOSS: 5.168424853025868e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 69/71 | LOSS: 5.180725347599946e-05\n",
      "TRAIN: EPOCH 38/1000 | BATCH 70/71 | LOSS: 5.162604117083927e-05\n",
      "VAL: EPOCH 38/1000 | BATCH 0/8 | LOSS: 6.314050551736727e-05\n",
      "VAL: EPOCH 38/1000 | BATCH 1/8 | LOSS: 5.230005263001658e-05\n",
      "VAL: EPOCH 38/1000 | BATCH 2/8 | LOSS: 5.3515340065738805e-05\n",
      "VAL: EPOCH 38/1000 | BATCH 3/8 | LOSS: 4.922602602164261e-05\n",
      "VAL: EPOCH 38/1000 | BATCH 4/8 | LOSS: 4.7888082190183925e-05\n",
      "VAL: EPOCH 38/1000 | BATCH 5/8 | LOSS: 4.848635520223373e-05\n",
      "VAL: EPOCH 38/1000 | BATCH 6/8 | LOSS: 4.7392976349718606e-05\n",
      "VAL: EPOCH 38/1000 | BATCH 7/8 | LOSS: 4.672840623243246e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 0/71 | LOSS: 5.037769005866721e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 1/71 | LOSS: 5.192307617107872e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 2/71 | LOSS: 4.932194012023198e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 3/71 | LOSS: 4.8445755055581685e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 4/71 | LOSS: 4.616477599483915e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 5/71 | LOSS: 4.894727377783662e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 6/71 | LOSS: 4.7789595035802836e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 7/71 | LOSS: 4.824205871045706e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 8/71 | LOSS: 4.666064382440204e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 9/71 | LOSS: 5.082911702629645e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 10/71 | LOSS: 4.947454396063801e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 11/71 | LOSS: 5.0318617165127456e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 12/71 | LOSS: 4.9466966279746535e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 13/71 | LOSS: 4.901950783927792e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 14/71 | LOSS: 4.855620451659585e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 15/71 | LOSS: 4.772393617713533e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 16/71 | LOSS: 4.7158520935815485e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 17/71 | LOSS: 4.7591128855452145e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 18/71 | LOSS: 4.751491590133427e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 19/71 | LOSS: 4.6673580800415945e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 20/71 | LOSS: 4.7261138423068804e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 21/71 | LOSS: 4.675703431447883e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 22/71 | LOSS: 4.689739390691418e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 23/71 | LOSS: 4.713552167838012e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 24/71 | LOSS: 4.754292938741855e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 25/71 | LOSS: 4.743115879626622e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 26/71 | LOSS: 4.808583441815615e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 27/71 | LOSS: 4.7447836875237825e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 28/71 | LOSS: 4.7548328501730354e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 29/71 | LOSS: 4.730996412642222e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 30/71 | LOSS: 4.759005529312192e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 31/71 | LOSS: 4.856631386473964e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 32/71 | LOSS: 4.849326515713156e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 33/71 | LOSS: 4.9880055485118646e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 34/71 | LOSS: 4.966280700630575e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 35/71 | LOSS: 5.043637505271666e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 36/71 | LOSS: 5.023968121865953e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 37/71 | LOSS: 5.052792187741847e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 38/71 | LOSS: 5.02231545490362e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 39/71 | LOSS: 5.026276535318175e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 40/71 | LOSS: 4.996697800113286e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 41/71 | LOSS: 5.02932915055897e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 42/71 | LOSS: 5.002902198188971e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 43/71 | LOSS: 5.0021599277401535e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 44/71 | LOSS: 4.96319233611252e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 45/71 | LOSS: 4.978918446874519e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 46/71 | LOSS: 4.962323552543703e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 47/71 | LOSS: 4.953757771393915e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 48/71 | LOSS: 4.959606470650642e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 49/71 | LOSS: 4.937533001793781e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 50/71 | LOSS: 5.00496101748842e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 51/71 | LOSS: 4.993335072028388e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 52/71 | LOSS: 4.9629628847469576e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 53/71 | LOSS: 4.960814803106822e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 54/71 | LOSS: 4.946487609810323e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 55/71 | LOSS: 4.939529315249404e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 56/71 | LOSS: 4.92959511336123e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 57/71 | LOSS: 4.933051300683693e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 58/71 | LOSS: 4.9078766219821064e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 59/71 | LOSS: 4.897866841323169e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 60/71 | LOSS: 4.911376893941168e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 61/71 | LOSS: 4.9041406254541326e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 62/71 | LOSS: 4.897274147734852e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 63/71 | LOSS: 4.886412668270168e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 64/71 | LOSS: 4.929319455606254e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 65/71 | LOSS: 4.964117262275205e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 66/71 | LOSS: 4.9418827522947424e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 67/71 | LOSS: 4.930353505936664e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 68/71 | LOSS: 4.91846542148699e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 69/71 | LOSS: 4.903048125665269e-05\n",
      "TRAIN: EPOCH 39/1000 | BATCH 70/71 | LOSS: 4.8767414983545603e-05\n",
      "VAL: EPOCH 39/1000 | BATCH 0/8 | LOSS: 6.020684304530732e-05\n",
      "VAL: EPOCH 39/1000 | BATCH 1/8 | LOSS: 5.309411608322989e-05\n",
      "VAL: EPOCH 39/1000 | BATCH 2/8 | LOSS: 5.471068410164056e-05\n",
      "VAL: EPOCH 39/1000 | BATCH 3/8 | LOSS: 5.1039321078860667e-05\n",
      "VAL: EPOCH 39/1000 | BATCH 4/8 | LOSS: 4.945076652802527e-05\n",
      "VAL: EPOCH 39/1000 | BATCH 5/8 | LOSS: 4.925134574781017e-05\n",
      "VAL: EPOCH 39/1000 | BATCH 6/8 | LOSS: 4.88624583730208e-05\n",
      "VAL: EPOCH 39/1000 | BATCH 7/8 | LOSS: 4.772108059114544e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 0/71 | LOSS: 5.5435855756513774e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 1/71 | LOSS: 5.006710853194818e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 2/71 | LOSS: 4.86082280986011e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 3/71 | LOSS: 4.705702485807706e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 4/71 | LOSS: 4.53446111350786e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 5/71 | LOSS: 4.557326792564709e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 6/71 | LOSS: 4.595219226238052e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 7/71 | LOSS: 4.712639747594949e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 8/71 | LOSS: 4.685206658905372e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 9/71 | LOSS: 4.9107321683550256e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 10/71 | LOSS: 4.949978359615091e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 11/71 | LOSS: 4.915603221888887e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 12/71 | LOSS: 4.779822568077809e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 13/71 | LOSS: 4.742577532722082e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 14/71 | LOSS: 4.766692751824546e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 15/71 | LOSS: 4.741955513054563e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 16/71 | LOSS: 4.7070851852528425e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 17/71 | LOSS: 4.667896892998316e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 18/71 | LOSS: 4.651617372904806e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 19/71 | LOSS: 4.603124452842167e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 20/71 | LOSS: 4.645776356309874e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 21/71 | LOSS: 4.678563990720167e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 22/71 | LOSS: 4.6271344098900244e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 23/71 | LOSS: 4.648792476776483e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 24/71 | LOSS: 4.6362084103748204e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 25/71 | LOSS: 4.614464719802177e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 26/71 | LOSS: 4.614243449328502e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 27/71 | LOSS: 4.5731694236954877e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 28/71 | LOSS: 4.6022168229181515e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 29/71 | LOSS: 4.620152697801435e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 30/71 | LOSS: 4.58859337208372e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 31/71 | LOSS: 4.665691608352063e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 32/71 | LOSS: 4.660177519051076e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 33/71 | LOSS: 4.683170396567541e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 34/71 | LOSS: 4.665453504588056e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 35/71 | LOSS: 4.614408003868043e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 36/71 | LOSS: 4.597291213891318e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 37/71 | LOSS: 4.5578494935302604e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 38/71 | LOSS: 4.593086263272338e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 39/71 | LOSS: 4.568594331431086e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 40/71 | LOSS: 4.5433359024705504e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 41/71 | LOSS: 4.5594828580284406e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 42/71 | LOSS: 4.627510768545488e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 43/71 | LOSS: 4.681999301035169e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 44/71 | LOSS: 4.658342554143423e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 45/71 | LOSS: 4.637788760706114e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 46/71 | LOSS: 4.606553549355669e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 47/71 | LOSS: 4.606179913935193e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 48/71 | LOSS: 4.6495312652004195e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 49/71 | LOSS: 4.63920681795571e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 50/71 | LOSS: 4.635136635853069e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 51/71 | LOSS: 4.634835281299624e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 52/71 | LOSS: 4.6252320316853e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 53/71 | LOSS: 4.628489868655042e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 54/71 | LOSS: 4.6197990542912686e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 55/71 | LOSS: 4.627969039055253e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 56/71 | LOSS: 4.61072308928379e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 57/71 | LOSS: 4.602562016927926e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 58/71 | LOSS: 4.5785202974773216e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 59/71 | LOSS: 4.580984429291372e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 60/71 | LOSS: 4.618085681539044e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 61/71 | LOSS: 4.7403527200766536e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 62/71 | LOSS: 4.72327534110278e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 63/71 | LOSS: 4.707834551709311e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 64/71 | LOSS: 4.698494477242303e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 65/71 | LOSS: 4.6997512430552334e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 66/71 | LOSS: 4.696819347888926e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 67/71 | LOSS: 4.683340022679391e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 68/71 | LOSS: 4.6774080028231054e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 69/71 | LOSS: 4.67420145079294e-05\n",
      "TRAIN: EPOCH 40/1000 | BATCH 70/71 | LOSS: 4.64650637068061e-05\n",
      "VAL: EPOCH 40/1000 | BATCH 0/8 | LOSS: 5.4841726523591205e-05\n",
      "VAL: EPOCH 40/1000 | BATCH 1/8 | LOSS: 4.724460086436011e-05\n",
      "VAL: EPOCH 40/1000 | BATCH 2/8 | LOSS: 4.8690015925482534e-05\n",
      "VAL: EPOCH 40/1000 | BATCH 3/8 | LOSS: 4.472327964322176e-05\n",
      "VAL: EPOCH 40/1000 | BATCH 4/8 | LOSS: 4.361665342003107e-05\n",
      "VAL: EPOCH 40/1000 | BATCH 5/8 | LOSS: 4.399007957545109e-05\n",
      "VAL: EPOCH 40/1000 | BATCH 6/8 | LOSS: 4.295301239056114e-05\n",
      "VAL: EPOCH 40/1000 | BATCH 7/8 | LOSS: 4.247807100909995e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 0/71 | LOSS: 4.873824946116656e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 1/71 | LOSS: 4.6978926548035815e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 2/71 | LOSS: 4.418186654220335e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 3/71 | LOSS: 4.121014808333712e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 4/71 | LOSS: 4.1384020732948557e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 5/71 | LOSS: 4.081366872317934e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 6/71 | LOSS: 4.104938696920206e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 7/71 | LOSS: 4.2267178741894895e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 8/71 | LOSS: 4.180516771157272e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 9/71 | LOSS: 4.104832660232205e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 10/71 | LOSS: 4.187185186310671e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 11/71 | LOSS: 4.326327659024779e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 12/71 | LOSS: 4.3050202713437524e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 13/71 | LOSS: 4.429471378638742e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 14/71 | LOSS: 4.429190327452185e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 15/71 | LOSS: 4.362190770734742e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 16/71 | LOSS: 4.3663062944474135e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 17/71 | LOSS: 4.419100494285683e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 18/71 | LOSS: 4.406439237707098e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 19/71 | LOSS: 4.370372589619364e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 20/71 | LOSS: 4.292162096438309e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 21/71 | LOSS: 4.293367485595147e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 22/71 | LOSS: 4.281297926512628e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 23/71 | LOSS: 4.2904559601690075e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 24/71 | LOSS: 4.2727065447252245e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 25/71 | LOSS: 4.254063661881866e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 26/71 | LOSS: 4.301516768194873e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 27/71 | LOSS: 4.269117549223925e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 28/71 | LOSS: 4.22763254830678e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 29/71 | LOSS: 4.1801578481681646e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 30/71 | LOSS: 4.152540995780709e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 31/71 | LOSS: 4.143287128499651e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 32/71 | LOSS: 4.139215183606597e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 33/71 | LOSS: 4.149350549114923e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 34/71 | LOSS: 4.126018331070165e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 35/71 | LOSS: 4.125989562453469e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 36/71 | LOSS: 4.1822995577555e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 37/71 | LOSS: 4.260328463906741e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 38/71 | LOSS: 4.256234919571748e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 39/71 | LOSS: 4.226102109896601e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 40/71 | LOSS: 4.241442019985875e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 41/71 | LOSS: 4.3308142093814064e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 42/71 | LOSS: 4.416517617226968e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 43/71 | LOSS: 4.4270912439969834e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 44/71 | LOSS: 4.457825505394592e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 45/71 | LOSS: 4.44363548645608e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 46/71 | LOSS: 4.4341510882541696e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 47/71 | LOSS: 4.410762221596087e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 48/71 | LOSS: 4.4054411142608344e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 49/71 | LOSS: 4.405927073094062e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 50/71 | LOSS: 4.40023957743702e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 51/71 | LOSS: 4.403931751977115e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 52/71 | LOSS: 4.37866099604915e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 53/71 | LOSS: 4.368278017931583e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 54/71 | LOSS: 4.358808457089419e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 55/71 | LOSS: 4.37716570818988e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 56/71 | LOSS: 4.361192061384045e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 57/71 | LOSS: 4.34738089850349e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 58/71 | LOSS: 4.335458208392453e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 59/71 | LOSS: 4.32762892160099e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 60/71 | LOSS: 4.3328502275084035e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 61/71 | LOSS: 4.432377359227112e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 62/71 | LOSS: 4.40912357705187e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 63/71 | LOSS: 4.4243088268558495e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 64/71 | LOSS: 4.417796858783381e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 65/71 | LOSS: 4.4083143979095795e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 66/71 | LOSS: 4.404501017763405e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 67/71 | LOSS: 4.423035689422464e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 68/71 | LOSS: 4.420073789821101e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 69/71 | LOSS: 4.4230261846678334e-05\n",
      "TRAIN: EPOCH 41/1000 | BATCH 70/71 | LOSS: 4.398116255634051e-05\n",
      "VAL: EPOCH 41/1000 | BATCH 0/8 | LOSS: 5.225310451351106e-05\n",
      "VAL: EPOCH 41/1000 | BATCH 1/8 | LOSS: 4.532975435722619e-05\n",
      "VAL: EPOCH 41/1000 | BATCH 2/8 | LOSS: 4.703295538395954e-05\n",
      "VAL: EPOCH 41/1000 | BATCH 3/8 | LOSS: 4.355669625510927e-05\n",
      "VAL: EPOCH 41/1000 | BATCH 4/8 | LOSS: 4.220320406602696e-05\n",
      "VAL: EPOCH 41/1000 | BATCH 5/8 | LOSS: 4.210932274872903e-05\n",
      "VAL: EPOCH 41/1000 | BATCH 6/8 | LOSS: 4.138747161570271e-05\n",
      "VAL: EPOCH 41/1000 | BATCH 7/8 | LOSS: 4.071100966029917e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 0/71 | LOSS: 3.532299888320267e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 1/71 | LOSS: 3.279841348557966e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 2/71 | LOSS: 3.681529597088229e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 3/71 | LOSS: 3.744302603081451e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 4/71 | LOSS: 4.14943369833054e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 5/71 | LOSS: 4.1047581968693216e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 6/71 | LOSS: 4.08196072904892e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 7/71 | LOSS: 4.196609984319366e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 8/71 | LOSS: 4.1280010312524006e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 9/71 | LOSS: 4.181691674602917e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 10/71 | LOSS: 4.252042295278939e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 11/71 | LOSS: 4.275569047725488e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 12/71 | LOSS: 4.4322466447528524e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 13/71 | LOSS: 4.5668769416806754e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 14/71 | LOSS: 4.5177699576015584e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 15/71 | LOSS: 4.437219001829362e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 16/71 | LOSS: 4.380401076238794e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 17/71 | LOSS: 4.4087274747855394e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 18/71 | LOSS: 4.3329162795807384e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 19/71 | LOSS: 4.289373719075229e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 20/71 | LOSS: 4.348792104532809e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 21/71 | LOSS: 4.327862354164774e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 22/71 | LOSS: 4.3052688377666407e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 23/71 | LOSS: 4.2721053584197456e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 24/71 | LOSS: 4.2403388797538354e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 25/71 | LOSS: 4.4062892461974676e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 26/71 | LOSS: 4.353859373993516e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 27/71 | LOSS: 4.324442501716216e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 28/71 | LOSS: 4.2974059334514534e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 29/71 | LOSS: 4.3242090760031716e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 30/71 | LOSS: 4.3246929929028413e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 31/71 | LOSS: 4.40110324007037e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 32/71 | LOSS: 4.4836373766002275e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 33/71 | LOSS: 4.460510246965843e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 34/71 | LOSS: 4.437636224403312e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 35/71 | LOSS: 4.388662910059793e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 36/71 | LOSS: 4.3684409418445385e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 37/71 | LOSS: 4.3526507253868896e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 38/71 | LOSS: 4.347128495469522e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 39/71 | LOSS: 4.3387667483330004e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 40/71 | LOSS: 4.3058419023731315e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 41/71 | LOSS: 4.3030636290842226e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 42/71 | LOSS: 4.289960041967069e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 43/71 | LOSS: 4.258536302421073e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 44/71 | LOSS: 4.2557853723539866e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 45/71 | LOSS: 4.25393804028357e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 46/71 | LOSS: 4.234165874337084e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 47/71 | LOSS: 4.223089869507627e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 48/71 | LOSS: 4.211268338733543e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 49/71 | LOSS: 4.1961144088418226e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 50/71 | LOSS: 4.205554446967903e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 51/71 | LOSS: 4.1931083335209856e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 52/71 | LOSS: 4.17596336019243e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 53/71 | LOSS: 4.167339917600017e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 54/71 | LOSS: 4.159920565805144e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 55/71 | LOSS: 4.145340049685079e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 56/71 | LOSS: 4.152405713204899e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 57/71 | LOSS: 4.1485947992347566e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 58/71 | LOSS: 4.156956535969245e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 59/71 | LOSS: 4.1471493629311834e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 60/71 | LOSS: 4.127693116105711e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 61/71 | LOSS: 4.123626439972976e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 62/71 | LOSS: 4.139015771446764e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 63/71 | LOSS: 4.136609175020567e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 64/71 | LOSS: 4.1266539035579906e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 65/71 | LOSS: 4.130309802845163e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 66/71 | LOSS: 4.110596501728895e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 67/71 | LOSS: 4.1215026131420234e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 68/71 | LOSS: 4.189269278920593e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 69/71 | LOSS: 4.206170004701042e-05\n",
      "TRAIN: EPOCH 42/1000 | BATCH 70/71 | LOSS: 4.2035849949404023e-05\n",
      "VAL: EPOCH 42/1000 | BATCH 0/8 | LOSS: 5.137034895597026e-05\n",
      "VAL: EPOCH 42/1000 | BATCH 1/8 | LOSS: 4.552999780571554e-05\n",
      "VAL: EPOCH 42/1000 | BATCH 2/8 | LOSS: 4.6351330335407205e-05\n",
      "VAL: EPOCH 42/1000 | BATCH 3/8 | LOSS: 4.1905336274794536e-05\n",
      "VAL: EPOCH 42/1000 | BATCH 4/8 | LOSS: 4.021344684588257e-05\n",
      "VAL: EPOCH 42/1000 | BATCH 5/8 | LOSS: 4.0364698179473635e-05\n",
      "VAL: EPOCH 42/1000 | BATCH 6/8 | LOSS: 3.973807724833023e-05\n",
      "VAL: EPOCH 42/1000 | BATCH 7/8 | LOSS: 3.8859895767018315e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 0/71 | LOSS: 2.9702674510190263e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 1/71 | LOSS: 3.612680848164018e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 2/71 | LOSS: 4.217099800977545e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 3/71 | LOSS: 4.112876740691718e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 4/71 | LOSS: 4.095802578376606e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 5/71 | LOSS: 4.387347144074738e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 6/71 | LOSS: 4.166112220056155e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 7/71 | LOSS: 4.377410300548945e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 8/71 | LOSS: 4.967980951025513e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 9/71 | LOSS: 4.8638946282153483e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 10/71 | LOSS: 4.775726235741538e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 11/71 | LOSS: 4.7038259405477824e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 12/71 | LOSS: 4.6919045556793346e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 13/71 | LOSS: 4.600449535376226e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 14/71 | LOSS: 4.5387817347849095e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 15/71 | LOSS: 4.420236916757858e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 16/71 | LOSS: 4.463330622135853e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 17/71 | LOSS: 4.397233058423606e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 18/71 | LOSS: 4.389136062711355e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 19/71 | LOSS: 4.332544440330821e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 20/71 | LOSS: 4.289269652092896e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 21/71 | LOSS: 4.270430368209914e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 22/71 | LOSS: 4.2199314323269114e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 23/71 | LOSS: 4.22706064000522e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 24/71 | LOSS: 4.1964520278270356e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 25/71 | LOSS: 4.1521242914873605e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 26/71 | LOSS: 4.126178127653138e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 27/71 | LOSS: 4.098498318074105e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 28/71 | LOSS: 4.093704974993357e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 29/71 | LOSS: 4.091550702772414e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 30/71 | LOSS: 4.069436099305148e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 31/71 | LOSS: 4.037431477854625e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 32/71 | LOSS: 4.046338648494742e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 33/71 | LOSS: 4.131617590567604e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 34/71 | LOSS: 4.128329856030177e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 35/71 | LOSS: 4.1302923566238154e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 36/71 | LOSS: 4.1282841118880094e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 37/71 | LOSS: 4.0945494650881496e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 38/71 | LOSS: 4.079599640159032e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 39/71 | LOSS: 4.074644984939368e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 40/71 | LOSS: 4.063065281981125e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 41/71 | LOSS: 4.074465782434258e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 42/71 | LOSS: 4.0467103919877976e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 43/71 | LOSS: 4.0339244109914446e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 44/71 | LOSS: 4.036338565735302e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 45/71 | LOSS: 4.0809516774417084e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 46/71 | LOSS: 4.109810659893481e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 47/71 | LOSS: 4.111817145258101e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 48/71 | LOSS: 4.107307165900331e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 49/71 | LOSS: 4.1031141627172474e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 50/71 | LOSS: 4.104153316909694e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 51/71 | LOSS: 4.095390246360554e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 52/71 | LOSS: 4.0748904555001435e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 53/71 | LOSS: 4.0453075160729454e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 54/71 | LOSS: 4.071038162113506e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 55/71 | LOSS: 4.046100385884139e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 56/71 | LOSS: 4.037193873061864e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 57/71 | LOSS: 4.0176138444904015e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 58/71 | LOSS: 3.991869158137164e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 59/71 | LOSS: 3.9948545357522865e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 60/71 | LOSS: 3.9844125553156386e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 61/71 | LOSS: 3.996866073594579e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 62/71 | LOSS: 3.979583146117095e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 63/71 | LOSS: 3.988748881056381e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 64/71 | LOSS: 3.987390071135731e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 65/71 | LOSS: 3.9743970222788804e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 66/71 | LOSS: 3.9756834477015927e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 67/71 | LOSS: 3.987679273824837e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 68/71 | LOSS: 3.9915990255658336e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 69/71 | LOSS: 3.991890324479235e-05\n",
      "TRAIN: EPOCH 43/1000 | BATCH 70/71 | LOSS: 3.988162479543743e-05\n",
      "VAL: EPOCH 43/1000 | BATCH 0/8 | LOSS: 4.985231134924106e-05\n",
      "VAL: EPOCH 43/1000 | BATCH 1/8 | LOSS: 4.392625669424888e-05\n",
      "VAL: EPOCH 43/1000 | BATCH 2/8 | LOSS: 4.5324793366792924e-05\n",
      "VAL: EPOCH 43/1000 | BATCH 3/8 | LOSS: 4.2291232603020035e-05\n",
      "VAL: EPOCH 43/1000 | BATCH 4/8 | LOSS: 4.057479818584397e-05\n",
      "VAL: EPOCH 43/1000 | BATCH 5/8 | LOSS: 4.047678036537642e-05\n",
      "VAL: EPOCH 43/1000 | BATCH 6/8 | LOSS: 3.9504218355952094e-05\n",
      "VAL: EPOCH 43/1000 | BATCH 7/8 | LOSS: 3.828167018582462e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 0/71 | LOSS: 3.451858356129378e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 1/71 | LOSS: 3.6911475035594776e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 2/71 | LOSS: 3.6318556036955364e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 3/71 | LOSS: 3.630223545769695e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 4/71 | LOSS: 3.531491165631451e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 5/71 | LOSS: 3.399329580133781e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 6/71 | LOSS: 3.8362144550774246e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 7/71 | LOSS: 3.752210886887042e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 8/71 | LOSS: 3.7758304743975816e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 9/71 | LOSS: 3.811446476902347e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 10/71 | LOSS: 3.722492345497647e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 11/71 | LOSS: 3.679308171437393e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 12/71 | LOSS: 3.6654223619664735e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 13/71 | LOSS: 3.598157582018757e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 14/71 | LOSS: 3.5946570157345074e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 15/71 | LOSS: 3.6222974244992656e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 16/71 | LOSS: 3.577939060050994e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 17/71 | LOSS: 3.532900781364232e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 18/71 | LOSS: 3.688801696530151e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 19/71 | LOSS: 3.7783203879371285e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 20/71 | LOSS: 3.7530254000254594e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 21/71 | LOSS: 3.803558187924368e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 22/71 | LOSS: 3.798638483757436e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 23/71 | LOSS: 3.877684866893105e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 24/71 | LOSS: 3.8372529816115274e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 25/71 | LOSS: 3.821457315867659e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 26/71 | LOSS: 3.814471683990969e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 27/71 | LOSS: 3.798723732221073e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 28/71 | LOSS: 3.870315965192778e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 29/71 | LOSS: 3.8650023634545504e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 30/71 | LOSS: 3.873166978366733e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 31/71 | LOSS: 3.8485315940306464e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 32/71 | LOSS: 3.8327497432791546e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 33/71 | LOSS: 3.829399803875918e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 34/71 | LOSS: 3.811599932045543e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 35/71 | LOSS: 3.783876278854829e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 36/71 | LOSS: 3.879788936181875e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 37/71 | LOSS: 3.8614315168448034e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 38/71 | LOSS: 3.8636216721407924e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 39/71 | LOSS: 3.855832483168342e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 40/71 | LOSS: 3.899117122850063e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 41/71 | LOSS: 3.8818483344151724e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 42/71 | LOSS: 3.8965525756032356e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 43/71 | LOSS: 3.9238186383789234e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 44/71 | LOSS: 3.927899783270227e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 45/71 | LOSS: 3.902343988833625e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 46/71 | LOSS: 3.887940754871221e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 47/71 | LOSS: 3.9373473100567935e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 48/71 | LOSS: 3.9386497519444674e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 49/71 | LOSS: 3.9589574735146014e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 50/71 | LOSS: 3.96380979937556e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 51/71 | LOSS: 3.958762002954385e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 52/71 | LOSS: 3.9483046662699395e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 53/71 | LOSS: 3.936112201093541e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 54/71 | LOSS: 3.932679623672315e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 55/71 | LOSS: 3.909755462570631e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 56/71 | LOSS: 3.922475189667015e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 57/71 | LOSS: 3.9760895153198486e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 58/71 | LOSS: 3.9568556469206114e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 59/71 | LOSS: 3.93720456486335e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 60/71 | LOSS: 3.915700176921597e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 61/71 | LOSS: 3.9141735009428474e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 62/71 | LOSS: 3.89644247329598e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 63/71 | LOSS: 3.8889898888783136e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 64/71 | LOSS: 3.8961262600003884e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 65/71 | LOSS: 3.904545213696441e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 66/71 | LOSS: 3.891945192690264e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 67/71 | LOSS: 3.88664854440054e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 68/71 | LOSS: 3.878576132157595e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 69/71 | LOSS: 3.863738851837947e-05\n",
      "TRAIN: EPOCH 44/1000 | BATCH 70/71 | LOSS: 3.8554444318820896e-05\n",
      "VAL: EPOCH 44/1000 | BATCH 0/8 | LOSS: 4.434498259797692e-05\n",
      "VAL: EPOCH 44/1000 | BATCH 1/8 | LOSS: 3.939665657526348e-05\n",
      "VAL: EPOCH 44/1000 | BATCH 2/8 | LOSS: 4.1807861028549574e-05\n",
      "VAL: EPOCH 44/1000 | BATCH 3/8 | LOSS: 3.9134820326580666e-05\n",
      "VAL: EPOCH 44/1000 | BATCH 4/8 | LOSS: 3.789903421420604e-05\n",
      "VAL: EPOCH 44/1000 | BATCH 5/8 | LOSS: 3.800176030684573e-05\n",
      "VAL: EPOCH 44/1000 | BATCH 6/8 | LOSS: 3.734457277460024e-05\n",
      "VAL: EPOCH 44/1000 | BATCH 7/8 | LOSS: 3.6806916341447504e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 0/71 | LOSS: 3.436461338424124e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 1/71 | LOSS: 3.8161359043442644e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 2/71 | LOSS: 3.539470587080965e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 3/71 | LOSS: 3.493166241241852e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 4/71 | LOSS: 3.503700572764501e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 5/71 | LOSS: 4.313595006048369e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 6/71 | LOSS: 4.61472778364883e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 7/71 | LOSS: 4.4764517951989546e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 8/71 | LOSS: 4.364300710019759e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 9/71 | LOSS: 4.294612917874474e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 10/71 | LOSS: 4.220807981751436e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 11/71 | LOSS: 4.2612373666391555e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 12/71 | LOSS: 4.1684049322681785e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 13/71 | LOSS: 4.0685966398866314e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 14/71 | LOSS: 4.089710807117323e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 15/71 | LOSS: 3.963218273383973e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 16/71 | LOSS: 4.046022192650365e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 17/71 | LOSS: 3.978141062867103e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 18/71 | LOSS: 3.895356770188204e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 19/71 | LOSS: 3.893103530572262e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 20/71 | LOSS: 3.885871687782041e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 21/71 | LOSS: 3.8704118527345024e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 22/71 | LOSS: 3.8235820481887735e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 23/71 | LOSS: 3.7910200868888445e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 24/71 | LOSS: 3.7878197617828845e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 25/71 | LOSS: 3.7555247147863083e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 26/71 | LOSS: 3.8503474772571484e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 27/71 | LOSS: 3.816467298228028e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 28/71 | LOSS: 3.864650603520266e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 29/71 | LOSS: 3.824837003776338e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 30/71 | LOSS: 3.8049147171418994e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 31/71 | LOSS: 3.807201687777706e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 32/71 | LOSS: 3.794429049417941e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 33/71 | LOSS: 3.784405207090244e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 34/71 | LOSS: 3.7759784962483016e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 35/71 | LOSS: 3.779216694965726e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 36/71 | LOSS: 3.768033639062196e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 37/71 | LOSS: 3.7566534860831636e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 38/71 | LOSS: 3.747682552411555e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 39/71 | LOSS: 3.7604143926728284e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 40/71 | LOSS: 3.750924925708848e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 41/71 | LOSS: 3.7507731879789695e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 42/71 | LOSS: 3.728175833266293e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 43/71 | LOSS: 3.7581876065153416e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 44/71 | LOSS: 3.754638124115041e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 45/71 | LOSS: 3.738584006856874e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 46/71 | LOSS: 3.736438346559806e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 47/71 | LOSS: 3.715952099507073e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 48/71 | LOSS: 3.7244184160063384e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 49/71 | LOSS: 3.7156372927711344e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 50/71 | LOSS: 3.7247714085513544e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 51/71 | LOSS: 3.7121226761463244e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 52/71 | LOSS: 3.732228445719471e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 53/71 | LOSS: 3.726059636661645e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 54/71 | LOSS: 3.706945710953071e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 55/71 | LOSS: 3.741192209158076e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 56/71 | LOSS: 3.733541668400887e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 57/71 | LOSS: 3.723710098460413e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 58/71 | LOSS: 3.7104621558189706e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 59/71 | LOSS: 3.712458953183765e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 60/71 | LOSS: 3.70998435546873e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 61/71 | LOSS: 3.6995574895803245e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 62/71 | LOSS: 3.701537022448426e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 63/71 | LOSS: 3.700137744999665e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 64/71 | LOSS: 3.702930273273243e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 65/71 | LOSS: 3.717261962733711e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 66/71 | LOSS: 3.720476367439616e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 67/71 | LOSS: 3.703401002627181e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 68/71 | LOSS: 3.703940461307049e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 69/71 | LOSS: 3.69456829243323e-05\n",
      "TRAIN: EPOCH 45/1000 | BATCH 70/71 | LOSS: 3.6876149159736446e-05\n",
      "VAL: EPOCH 45/1000 | BATCH 0/8 | LOSS: 4.749228173750453e-05\n",
      "VAL: EPOCH 45/1000 | BATCH 1/8 | LOSS: 4.2815427150344476e-05\n",
      "VAL: EPOCH 45/1000 | BATCH 2/8 | LOSS: 4.296855695429258e-05\n",
      "VAL: EPOCH 45/1000 | BATCH 3/8 | LOSS: 3.943739329770324e-05\n",
      "VAL: EPOCH 45/1000 | BATCH 4/8 | LOSS: 3.7440714731928895e-05\n",
      "VAL: EPOCH 45/1000 | BATCH 5/8 | LOSS: 3.7416478941546906e-05\n",
      "VAL: EPOCH 45/1000 | BATCH 6/8 | LOSS: 3.673663351427032e-05\n",
      "VAL: EPOCH 45/1000 | BATCH 7/8 | LOSS: 3.5314159731569816e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 0/71 | LOSS: 2.942279570561368e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 1/71 | LOSS: 3.500731418171199e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 2/71 | LOSS: 3.35365251279048e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 3/71 | LOSS: 3.22011619573459e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 4/71 | LOSS: 3.285528800915927e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 5/71 | LOSS: 3.2648227109651394e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 6/71 | LOSS: 3.322050075179764e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 7/71 | LOSS: 3.510440092213685e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 8/71 | LOSS: 3.4252178819668996e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 9/71 | LOSS: 3.369617807038594e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 10/71 | LOSS: 3.419033501731147e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 11/71 | LOSS: 3.542482742583767e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 12/71 | LOSS: 3.443209397328946e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 13/71 | LOSS: 3.428574252341475e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 14/71 | LOSS: 3.3717325519925606e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 15/71 | LOSS: 3.3325735330436146e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 16/71 | LOSS: 3.3438098291619954e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 17/71 | LOSS: 3.3729090015791975e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 18/71 | LOSS: 3.377645235128799e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 19/71 | LOSS: 3.4041977778542785e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 20/71 | LOSS: 3.3952330081124925e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 21/71 | LOSS: 3.3930441449311644e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 22/71 | LOSS: 3.3902665840603575e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 23/71 | LOSS: 3.380898942850763e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 24/71 | LOSS: 3.367461846210063e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 25/71 | LOSS: 3.3412465339866823e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 26/71 | LOSS: 3.3806129067670554e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 27/71 | LOSS: 3.4903285008372874e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 28/71 | LOSS: 3.465795703834436e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 29/71 | LOSS: 3.5040297128337745e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 30/71 | LOSS: 3.4995622216581155e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 31/71 | LOSS: 3.4912047226498544e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 32/71 | LOSS: 3.505630505087814e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 33/71 | LOSS: 3.492847591748132e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 34/71 | LOSS: 3.536163556938326e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 35/71 | LOSS: 3.510978280650711e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 36/71 | LOSS: 3.516246805382842e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 37/71 | LOSS: 3.518200327035677e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 38/71 | LOSS: 3.5176983380015605e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 39/71 | LOSS: 3.539775034369086e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 40/71 | LOSS: 3.5273230179616184e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 41/71 | LOSS: 3.522214822060916e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 42/71 | LOSS: 3.524655208863361e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 43/71 | LOSS: 3.540052818028595e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 44/71 | LOSS: 3.525535115234864e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 45/71 | LOSS: 3.528409987389434e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 46/71 | LOSS: 3.5202581552476524e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 47/71 | LOSS: 3.5146776932985326e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 48/71 | LOSS: 3.502217953851237e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 49/71 | LOSS: 3.513375901093241e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 50/71 | LOSS: 3.5224125966516454e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 51/71 | LOSS: 3.518869415039752e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 52/71 | LOSS: 3.511195190751679e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 53/71 | LOSS: 3.52162599786728e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 54/71 | LOSS: 3.507335670953828e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 55/71 | LOSS: 3.500210852637663e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 56/71 | LOSS: 3.490019723055108e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 57/71 | LOSS: 3.523172359438894e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 58/71 | LOSS: 3.588345415186792e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 59/71 | LOSS: 3.584338977210185e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 60/71 | LOSS: 3.593063381425159e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 61/71 | LOSS: 3.5940915187248106e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 62/71 | LOSS: 3.589966209325385e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 63/71 | LOSS: 3.5897824346875495e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 64/71 | LOSS: 3.575334672999676e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 65/71 | LOSS: 3.5675445195176344e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 66/71 | LOSS: 3.552607713165561e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 67/71 | LOSS: 3.5617741907438304e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 68/71 | LOSS: 3.5651838133808404e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 69/71 | LOSS: 3.5688007405302155e-05\n",
      "TRAIN: EPOCH 46/1000 | BATCH 70/71 | LOSS: 3.550719108242332e-05\n",
      "VAL: EPOCH 46/1000 | BATCH 0/8 | LOSS: 3.923439726349898e-05\n",
      "VAL: EPOCH 46/1000 | BATCH 1/8 | LOSS: 3.576050767151173e-05\n",
      "VAL: EPOCH 46/1000 | BATCH 2/8 | LOSS: 3.7341062125051394e-05\n",
      "VAL: EPOCH 46/1000 | BATCH 3/8 | LOSS: 3.5617999856185634e-05\n",
      "VAL: EPOCH 46/1000 | BATCH 4/8 | LOSS: 3.4648956352612e-05\n",
      "VAL: EPOCH 46/1000 | BATCH 5/8 | LOSS: 3.44270247296663e-05\n",
      "VAL: EPOCH 46/1000 | BATCH 6/8 | LOSS: 3.349808814943701e-05\n",
      "VAL: EPOCH 46/1000 | BATCH 7/8 | LOSS: 3.2855800327524776e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 0/71 | LOSS: 2.8260157705517486e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 1/71 | LOSS: 3.2420748539152555e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 2/71 | LOSS: 3.348108657519333e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 3/71 | LOSS: 3.241324975533644e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 4/71 | LOSS: 3.277702708146535e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 5/71 | LOSS: 3.5370415692644506e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 6/71 | LOSS: 3.5426070618476455e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 7/71 | LOSS: 3.5595742247096496e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 8/71 | LOSS: 3.486219530815207e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 9/71 | LOSS: 3.3950294528040105e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 10/71 | LOSS: 3.300639995855322e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 11/71 | LOSS: 3.212584442735533e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 12/71 | LOSS: 3.196457314166205e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 13/71 | LOSS: 3.289231361642513e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 14/71 | LOSS: 3.2608204128337094e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 15/71 | LOSS: 3.3665717978692555e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 16/71 | LOSS: 3.348351813602836e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 17/71 | LOSS: 3.301406155110776e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 18/71 | LOSS: 3.3079816268199415e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 19/71 | LOSS: 3.283285595898633e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 20/71 | LOSS: 3.2734719795934366e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 21/71 | LOSS: 3.2979097837100696e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 22/71 | LOSS: 3.2821682591178536e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 23/71 | LOSS: 3.259182517467707e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 24/71 | LOSS: 3.249375455197878e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 25/71 | LOSS: 3.3235358317212486e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 26/71 | LOSS: 3.3262238031494676e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 27/71 | LOSS: 3.3572748309649925e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 28/71 | LOSS: 3.34100312039513e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 29/71 | LOSS: 3.4039421370835045e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 30/71 | LOSS: 3.376321183168329e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 31/71 | LOSS: 3.3663339877421095e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 32/71 | LOSS: 3.365040240830488e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 33/71 | LOSS: 3.367962884059285e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 34/71 | LOSS: 3.359124925087339e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 35/71 | LOSS: 3.3644302776439064e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 36/71 | LOSS: 3.3561595341165525e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 37/71 | LOSS: 3.3698162664414234e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 38/71 | LOSS: 3.367606190388473e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 39/71 | LOSS: 3.3502270889584905e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 40/71 | LOSS: 3.337397250040184e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 41/71 | LOSS: 3.3384388665408116e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 42/71 | LOSS: 3.307845948912409e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 43/71 | LOSS: 3.3067773281138877e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 44/71 | LOSS: 3.3213069683471174e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 45/71 | LOSS: 3.308513481423016e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 46/71 | LOSS: 3.305673260214501e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 47/71 | LOSS: 3.3623111600415236e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 48/71 | LOSS: 3.355588152623327e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 49/71 | LOSS: 3.3517385709274095e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 50/71 | LOSS: 3.343293407370391e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 51/71 | LOSS: 3.385404264778257e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 52/71 | LOSS: 3.3897998603280495e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 53/71 | LOSS: 3.390150851811524e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 54/71 | LOSS: 3.404436230828816e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 55/71 | LOSS: 3.383675695139183e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 56/71 | LOSS: 3.365960883297304e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 57/71 | LOSS: 3.352900832145804e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 58/71 | LOSS: 3.342731653932695e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 59/71 | LOSS: 3.336121635584277e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 60/71 | LOSS: 3.33600602345064e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 61/71 | LOSS: 3.311685644979251e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 62/71 | LOSS: 3.341484645918374e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 63/71 | LOSS: 3.3315436439806945e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 64/71 | LOSS: 3.3205977636568534e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 65/71 | LOSS: 3.308730976476165e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 66/71 | LOSS: 3.313361306556626e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 67/71 | LOSS: 3.3069781586485024e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 68/71 | LOSS: 3.305354099817729e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 69/71 | LOSS: 3.309203922461685e-05\n",
      "TRAIN: EPOCH 47/1000 | BATCH 70/71 | LOSS: 3.3124957323233385e-05\n",
      "VAL: EPOCH 47/1000 | BATCH 0/8 | LOSS: 3.8051435694796965e-05\n",
      "VAL: EPOCH 47/1000 | BATCH 1/8 | LOSS: 3.476385791145731e-05\n",
      "VAL: EPOCH 47/1000 | BATCH 2/8 | LOSS: 3.672818153669747e-05\n",
      "VAL: EPOCH 47/1000 | BATCH 3/8 | LOSS: 3.475432185950922e-05\n",
      "VAL: EPOCH 47/1000 | BATCH 4/8 | LOSS: 3.3277379770879634e-05\n",
      "VAL: EPOCH 47/1000 | BATCH 5/8 | LOSS: 3.301391704250515e-05\n",
      "VAL: EPOCH 47/1000 | BATCH 6/8 | LOSS: 3.243441564596391e-05\n",
      "VAL: EPOCH 47/1000 | BATCH 7/8 | LOSS: 3.156060779474501e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 0/71 | LOSS: 3.858156196656637e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 1/71 | LOSS: 3.187364836776396e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 2/71 | LOSS: 3.212274350516964e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 3/71 | LOSS: 3.1148830203164835e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 4/71 | LOSS: 2.991995534102898e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 5/71 | LOSS: 3.4974132783342306e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 6/71 | LOSS: 3.42488825221413e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 7/71 | LOSS: 3.292284964118153e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 8/71 | LOSS: 3.233921557289755e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 9/71 | LOSS: 3.173359036736656e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 10/71 | LOSS: 3.197753374644724e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 11/71 | LOSS: 3.180865648270507e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 12/71 | LOSS: 3.310494307348003e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 13/71 | LOSS: 3.2676181945134886e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 14/71 | LOSS: 3.251962843933143e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 15/71 | LOSS: 3.255697333770513e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 16/71 | LOSS: 3.2539803214604035e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 17/71 | LOSS: 3.259432095445744e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 18/71 | LOSS: 3.2561769713586395e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 19/71 | LOSS: 3.2316264696419236e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 20/71 | LOSS: 3.211072899354067e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 21/71 | LOSS: 3.286980319733795e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 22/71 | LOSS: 3.28006389400284e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 23/71 | LOSS: 3.269508465564286e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 24/71 | LOSS: 3.234460047679022e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 25/71 | LOSS: 3.2671393665413445e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 26/71 | LOSS: 3.2375967647466394e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 27/71 | LOSS: 3.2475274565513246e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 28/71 | LOSS: 3.2458323528709147e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 29/71 | LOSS: 3.223669021584404e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 30/71 | LOSS: 3.263672777158659e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 31/71 | LOSS: 3.261079723415605e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 32/71 | LOSS: 3.2371705773667515e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 33/71 | LOSS: 3.231886426767553e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 34/71 | LOSS: 3.240963789202007e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 35/71 | LOSS: 3.275407390093379e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 36/71 | LOSS: 3.277660810429527e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 37/71 | LOSS: 3.2719954489086954e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 38/71 | LOSS: 3.276835466236569e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 39/71 | LOSS: 3.261609499531915e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 40/71 | LOSS: 3.239197199369167e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 41/71 | LOSS: 3.216075540313752e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 42/71 | LOSS: 3.254697498219727e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 43/71 | LOSS: 3.248870631399438e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 44/71 | LOSS: 3.2618334646233256e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 45/71 | LOSS: 3.255231212792948e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 46/71 | LOSS: 3.247201133722519e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 47/71 | LOSS: 3.240294908361344e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 48/71 | LOSS: 3.305240248404776e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 49/71 | LOSS: 3.3002371674228924e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 50/71 | LOSS: 3.284957307971804e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 51/71 | LOSS: 3.2723530554134275e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 52/71 | LOSS: 3.279908220442934e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 53/71 | LOSS: 3.284888175080091e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 54/71 | LOSS: 3.2741400388080035e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 55/71 | LOSS: 3.273841135913764e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 56/71 | LOSS: 3.2740272919845504e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 57/71 | LOSS: 3.282175601208141e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 58/71 | LOSS: 3.272349040652428e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 59/71 | LOSS: 3.2570542316534556e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 60/71 | LOSS: 3.270102239244419e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 61/71 | LOSS: 3.264891297623117e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 62/71 | LOSS: 3.2560498636615064e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 63/71 | LOSS: 3.243267784114323e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 64/71 | LOSS: 3.240640236561796e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 65/71 | LOSS: 3.2304869240430314e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 66/71 | LOSS: 3.221999824419791e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 67/71 | LOSS: 3.213342447719804e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 68/71 | LOSS: 3.22235564849309e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 69/71 | LOSS: 3.2160501853338377e-05\n",
      "TRAIN: EPOCH 48/1000 | BATCH 70/71 | LOSS: 3.1998170861313576e-05\n",
      "VAL: EPOCH 48/1000 | BATCH 0/8 | LOSS: 3.7956393498461694e-05\n",
      "VAL: EPOCH 48/1000 | BATCH 1/8 | LOSS: 3.510776878101751e-05\n",
      "VAL: EPOCH 48/1000 | BATCH 2/8 | LOSS: 3.538029826207397e-05\n",
      "VAL: EPOCH 48/1000 | BATCH 3/8 | LOSS: 3.277372888987884e-05\n",
      "VAL: EPOCH 48/1000 | BATCH 4/8 | LOSS: 3.1172556191449985e-05\n",
      "VAL: EPOCH 48/1000 | BATCH 5/8 | LOSS: 3.1034043180018976e-05\n",
      "VAL: EPOCH 48/1000 | BATCH 6/8 | LOSS: 3.0270490762112396e-05\n",
      "VAL: EPOCH 48/1000 | BATCH 7/8 | LOSS: 2.937562862825871e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 0/71 | LOSS: 5.45443435839843e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 1/71 | LOSS: 5.475329453474842e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 2/71 | LOSS: 4.600380028326375e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 3/71 | LOSS: 4.1371662518940866e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 4/71 | LOSS: 3.779281323659234e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 5/71 | LOSS: 3.6177296654689904e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 6/71 | LOSS: 3.422113435850146e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 7/71 | LOSS: 3.3182834386025206e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 8/71 | LOSS: 3.2695265973517155e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 9/71 | LOSS: 3.2097851180878935e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 10/71 | LOSS: 3.167337457522411e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 11/71 | LOSS: 3.1191353324781325e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 12/71 | LOSS: 3.2168793180832065e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 13/71 | LOSS: 3.171807163328465e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 14/71 | LOSS: 3.180013203139727e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 15/71 | LOSS: 3.132273604933289e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 16/71 | LOSS: 3.1259617411278136e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 17/71 | LOSS: 3.1414327673297114e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 18/71 | LOSS: 3.12810981612481e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 19/71 | LOSS: 3.0803270783508195e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 20/71 | LOSS: 3.1034047770801754e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 21/71 | LOSS: 3.0995585058752276e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 22/71 | LOSS: 3.08047322544988e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 23/71 | LOSS: 3.072976005569217e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 24/71 | LOSS: 3.095930864219554e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 25/71 | LOSS: 3.0676494414085304e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 26/71 | LOSS: 3.0522823686219956e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 27/71 | LOSS: 3.0307477962716283e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 28/71 | LOSS: 3.0304020489485742e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 29/71 | LOSS: 3.040836672880687e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 30/71 | LOSS: 3.0211975054924316e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 31/71 | LOSS: 3.0142837772473285e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 32/71 | LOSS: 3.002138449422394e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 33/71 | LOSS: 2.978471553326338e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 34/71 | LOSS: 2.9704804182983936e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 35/71 | LOSS: 2.9762512389829175e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 36/71 | LOSS: 2.9740127403685522e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 37/71 | LOSS: 3.00747621000291e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 38/71 | LOSS: 3.051590680022342e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 39/71 | LOSS: 3.0397839509532787e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 40/71 | LOSS: 3.0350260159338644e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 41/71 | LOSS: 3.0286301288080203e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 42/71 | LOSS: 3.0280012821137016e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 43/71 | LOSS: 3.041564175212195e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 44/71 | LOSS: 3.053829936511142e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 45/71 | LOSS: 3.0460778994576845e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 46/71 | LOSS: 3.0352781323017552e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 47/71 | LOSS: 3.0340843522935756e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 48/71 | LOSS: 3.0379349779223606e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 49/71 | LOSS: 3.0449771584244447e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 50/71 | LOSS: 3.038730816721606e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 51/71 | LOSS: 3.0259756735736468e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 52/71 | LOSS: 3.010001397205139e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 53/71 | LOSS: 2.9962677992309046e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 54/71 | LOSS: 2.9879201974836178e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 55/71 | LOSS: 2.989699744050865e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 56/71 | LOSS: 2.9797937825430398e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 57/71 | LOSS: 2.9766000430942675e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 58/71 | LOSS: 2.9863216541806172e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 59/71 | LOSS: 3.0016640569859494e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 60/71 | LOSS: 2.9942912973280325e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 61/71 | LOSS: 2.988564166674135e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 62/71 | LOSS: 3.017834601378704e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 63/71 | LOSS: 3.015980939835572e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 64/71 | LOSS: 3.0116065150090995e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 65/71 | LOSS: 3.015283348786673e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 66/71 | LOSS: 3.0132693541757483e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 67/71 | LOSS: 3.0091860668000983e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 68/71 | LOSS: 3.0343971351006378e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 69/71 | LOSS: 3.0489076587920344e-05\n",
      "TRAIN: EPOCH 49/1000 | BATCH 70/71 | LOSS: 3.0370815065779567e-05\n",
      "VAL: EPOCH 49/1000 | BATCH 0/8 | LOSS: 3.501092578517273e-05\n",
      "VAL: EPOCH 49/1000 | BATCH 1/8 | LOSS: 3.2219507374975365e-05\n",
      "VAL: EPOCH 49/1000 | BATCH 2/8 | LOSS: 3.342241143400315e-05\n",
      "VAL: EPOCH 49/1000 | BATCH 3/8 | LOSS: 3.149640042465762e-05\n",
      "VAL: EPOCH 49/1000 | BATCH 4/8 | LOSS: 3.0025361047592015e-05\n",
      "VAL: EPOCH 49/1000 | BATCH 5/8 | LOSS: 2.9870791574163984e-05\n",
      "VAL: EPOCH 49/1000 | BATCH 6/8 | LOSS: 2.921491613960825e-05\n",
      "VAL: EPOCH 49/1000 | BATCH 7/8 | LOSS: 2.847728478627687e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 0/71 | LOSS: 2.9476330382749438e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 1/71 | LOSS: 2.9410871320578735e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 2/71 | LOSS: 2.883126217056997e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 3/71 | LOSS: 3.0171072467055637e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 4/71 | LOSS: 2.9230664222268387e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 5/71 | LOSS: 2.7674875733888864e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 6/71 | LOSS: 2.8357511317673406e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 7/71 | LOSS: 2.8498130859588855e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 8/71 | LOSS: 2.932222216461216e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 9/71 | LOSS: 2.910214061557781e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 10/71 | LOSS: 2.872737604775466e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 11/71 | LOSS: 2.864377878116405e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 12/71 | LOSS: 2.7986042998516215e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 13/71 | LOSS: 2.820130664206642e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 14/71 | LOSS: 2.9273610562086105e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 15/71 | LOSS: 3.018879419869336e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 16/71 | LOSS: 3.0089044765404917e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 17/71 | LOSS: 3.10423147311667e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 18/71 | LOSS: 3.077990990469085e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 19/71 | LOSS: 3.0328085995279252e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 20/71 | LOSS: 3.0272548298983437e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 21/71 | LOSS: 3.010078142489709e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 22/71 | LOSS: 3.0091160505949318e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 23/71 | LOSS: 2.9877864714459672e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 24/71 | LOSS: 2.9504138801712543e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 25/71 | LOSS: 2.9131564238363913e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 26/71 | LOSS: 2.906968149074129e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 27/71 | LOSS: 2.912751298416489e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 28/71 | LOSS: 2.8956022325818876e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 29/71 | LOSS: 2.9541360284686864e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 30/71 | LOSS: 2.9629126842220074e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 31/71 | LOSS: 2.9362683960698632e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 32/71 | LOSS: 2.9511545962481193e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 33/71 | LOSS: 2.9379899923001442e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 34/71 | LOSS: 2.9598337641800752e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 35/71 | LOSS: 2.9498507097337602e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 36/71 | LOSS: 2.9568138415925205e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 37/71 | LOSS: 2.941977417274182e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 38/71 | LOSS: 2.9274855664996907e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 39/71 | LOSS: 2.923758215729322e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 40/71 | LOSS: 2.927848516087468e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 41/71 | LOSS: 2.9646324254004175e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 42/71 | LOSS: 2.9580103220596778e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 43/71 | LOSS: 2.9541026792272035e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 44/71 | LOSS: 2.9736987764610805e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 45/71 | LOSS: 2.975381927250925e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 46/71 | LOSS: 2.9628582853276502e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 47/71 | LOSS: 2.9480744956345006e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 48/71 | LOSS: 2.956187652634238e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 49/71 | LOSS: 2.9482021818694194e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 50/71 | LOSS: 2.9441689441548934e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 51/71 | LOSS: 2.9353084231003708e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 52/71 | LOSS: 2.9503771553929665e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 53/71 | LOSS: 2.942371210573819e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 54/71 | LOSS: 2.963329226821026e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 55/71 | LOSS: 2.960075448754651e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 56/71 | LOSS: 2.9572625128805573e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 57/71 | LOSS: 2.947749905207933e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 58/71 | LOSS: 2.9585749069741784e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 59/71 | LOSS: 2.9558690433380736e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 60/71 | LOSS: 2.9659966696540398e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 61/71 | LOSS: 2.9655947400897276e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 62/71 | LOSS: 2.9742861789556974e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 63/71 | LOSS: 2.9723199816089618e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 64/71 | LOSS: 2.968657180854979e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 65/71 | LOSS: 2.9678750555225026e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 66/71 | LOSS: 2.972921030769653e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 67/71 | LOSS: 2.975700863316888e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 68/71 | LOSS: 3.004632589695773e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 69/71 | LOSS: 3.0028033818650457e-05\n",
      "TRAIN: EPOCH 50/1000 | BATCH 70/71 | LOSS: 3.056274347817263e-05\n",
      "VAL: EPOCH 50/1000 | BATCH 0/8 | LOSS: 3.315182766527869e-05\n",
      "VAL: EPOCH 50/1000 | BATCH 1/8 | LOSS: 3.084294257860165e-05\n",
      "VAL: EPOCH 50/1000 | BATCH 2/8 | LOSS: 3.20956921010899e-05\n",
      "VAL: EPOCH 50/1000 | BATCH 3/8 | LOSS: 3.0351678105944302e-05\n",
      "VAL: EPOCH 50/1000 | BATCH 4/8 | LOSS: 2.8969335107831285e-05\n",
      "VAL: EPOCH 50/1000 | BATCH 5/8 | LOSS: 2.8935034000217758e-05\n",
      "VAL: EPOCH 50/1000 | BATCH 6/8 | LOSS: 2.8425531646852115e-05\n",
      "VAL: EPOCH 50/1000 | BATCH 7/8 | LOSS: 2.77727083357604e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 0/71 | LOSS: 2.3644108296139166e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 1/71 | LOSS: 2.7163821869180538e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 2/71 | LOSS: 2.630638603780729e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 3/71 | LOSS: 2.5948384063667618e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 4/71 | LOSS: 2.6607796098687685e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 5/71 | LOSS: 2.7637531275104266e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 6/71 | LOSS: 2.7616084220686128e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 7/71 | LOSS: 2.7869144560099812e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 8/71 | LOSS: 2.7568963560042903e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 9/71 | LOSS: 2.7710257927537896e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 10/71 | LOSS: 3.146199841549705e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 11/71 | LOSS: 3.125983403151622e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 12/71 | LOSS: 3.068592741328757e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 13/71 | LOSS: 3.0404438933016665e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 14/71 | LOSS: 2.997455121658277e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 15/71 | LOSS: 2.9422324928418675e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 16/71 | LOSS: 3.0259721504134015e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 17/71 | LOSS: 3.059546088479692e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 18/71 | LOSS: 3.0348299141654645e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 19/71 | LOSS: 3.024053194167209e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 20/71 | LOSS: 3.0528095088638586e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 21/71 | LOSS: 3.023356682454257e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 22/71 | LOSS: 3.0414213735797524e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 23/71 | LOSS: 3.073177238851107e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 24/71 | LOSS: 3.04153702018084e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 25/71 | LOSS: 3.0154725629052755e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 26/71 | LOSS: 3.072200538385746e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 27/71 | LOSS: 3.068809896181587e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 28/71 | LOSS: 3.05499762707141e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 29/71 | LOSS: 3.0375174962197585e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 30/71 | LOSS: 3.0489548847864142e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 31/71 | LOSS: 3.0430676019932434e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 32/71 | LOSS: 3.0461918923361793e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 33/71 | LOSS: 3.0063239993229645e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 34/71 | LOSS: 2.98268528110514e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 35/71 | LOSS: 2.9669928001870478e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 36/71 | LOSS: 2.9643245520797006e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 37/71 | LOSS: 2.9833426675682667e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 38/71 | LOSS: 2.9707080988699978e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 39/71 | LOSS: 2.9771312483717338e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 40/71 | LOSS: 2.9754693702959296e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 41/71 | LOSS: 2.9689824865878716e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 42/71 | LOSS: 2.971248008983567e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 43/71 | LOSS: 2.959268604999207e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 44/71 | LOSS: 2.9672198797925377e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 45/71 | LOSS: 2.9580344586144708e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 46/71 | LOSS: 2.9540213251900068e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 47/71 | LOSS: 2.9400954682993568e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 48/71 | LOSS: 2.9431695018464947e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 49/71 | LOSS: 2.937467892479617e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 50/71 | LOSS: 2.925428969890573e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 51/71 | LOSS: 2.938627333675798e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 52/71 | LOSS: 2.933718205208864e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 53/71 | LOSS: 2.9274853693129478e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 54/71 | LOSS: 2.9274097985101188e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 55/71 | LOSS: 2.9256726130760008e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 56/71 | LOSS: 2.9195658945562236e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 57/71 | LOSS: 2.914136339000081e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 58/71 | LOSS: 2.897375178058687e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 59/71 | LOSS: 2.9000750176540655e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 60/71 | LOSS: 2.8920544164215185e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 61/71 | LOSS: 2.884382009201105e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 62/71 | LOSS: 2.889462343268355e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 63/71 | LOSS: 2.8900458602265644e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 64/71 | LOSS: 2.8867299456928428e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 65/71 | LOSS: 2.8903243522411987e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 66/71 | LOSS: 2.877088516117456e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 67/71 | LOSS: 2.8764883689505307e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 68/71 | LOSS: 2.8793093841396395e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 69/71 | LOSS: 2.8796097415449497e-05\n",
      "TRAIN: EPOCH 51/1000 | BATCH 70/71 | LOSS: 2.8660985419623437e-05\n",
      "VAL: EPOCH 51/1000 | BATCH 0/8 | LOSS: 3.500406819512136e-05\n",
      "VAL: EPOCH 51/1000 | BATCH 1/8 | LOSS: 3.2802367059048265e-05\n",
      "VAL: EPOCH 51/1000 | BATCH 2/8 | LOSS: 3.289151694237565e-05\n",
      "VAL: EPOCH 51/1000 | BATCH 3/8 | LOSS: 3.068485239055008e-05\n",
      "VAL: EPOCH 51/1000 | BATCH 4/8 | LOSS: 2.9040405934210867e-05\n",
      "VAL: EPOCH 51/1000 | BATCH 5/8 | LOSS: 2.8739012122969143e-05\n",
      "VAL: EPOCH 51/1000 | BATCH 6/8 | LOSS: 2.8329553190685276e-05\n",
      "VAL: EPOCH 51/1000 | BATCH 7/8 | LOSS: 2.737353770498885e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 0/71 | LOSS: 2.416720599285327e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 1/71 | LOSS: 2.6101299226866104e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 2/71 | LOSS: 2.7369638701202348e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 3/71 | LOSS: 2.8364238460198976e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 4/71 | LOSS: 2.7807903825305404e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 5/71 | LOSS: 2.811629171143674e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 6/71 | LOSS: 2.8302079954301007e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 7/71 | LOSS: 2.7720288244381663e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 8/71 | LOSS: 2.6957584648496573e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 9/71 | LOSS: 2.7790701278718188e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 10/71 | LOSS: 2.8110218739708547e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 11/71 | LOSS: 2.864648831746308e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 12/71 | LOSS: 2.8367104487887656e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 13/71 | LOSS: 2.8368586754368152e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 14/71 | LOSS: 2.8513465925546672e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 15/71 | LOSS: 2.811809429204004e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 16/71 | LOSS: 2.860956149507889e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 17/71 | LOSS: 2.93542884214225e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 18/71 | LOSS: 2.888018417778719e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 19/71 | LOSS: 2.857066756405402e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 20/71 | LOSS: 2.8391692120792522e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 21/71 | LOSS: 2.806728595963002e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 22/71 | LOSS: 2.8025294380748402e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 23/71 | LOSS: 2.8066447460635875e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 24/71 | LOSS: 2.8063470090273767e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 25/71 | LOSS: 2.7850022958703517e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 26/71 | LOSS: 2.786232269297923e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 27/71 | LOSS: 2.785480234836411e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 28/71 | LOSS: 2.7893296207586337e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 29/71 | LOSS: 2.7994551116231983e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 30/71 | LOSS: 2.7968964258757148e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 31/71 | LOSS: 2.8065127821719216e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 32/71 | LOSS: 2.828583577259754e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 33/71 | LOSS: 2.828278582669822e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 34/71 | LOSS: 2.8546114377344826e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 35/71 | LOSS: 2.8189234853925882e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 36/71 | LOSS: 2.795064835014715e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 37/71 | LOSS: 2.826552970777572e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 38/71 | LOSS: 2.819497505977499e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 39/71 | LOSS: 2.8429758594938902e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 40/71 | LOSS: 2.839968568345861e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 41/71 | LOSS: 2.8269363177796115e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 42/71 | LOSS: 2.818373506543562e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 43/71 | LOSS: 2.8205823706560874e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 44/71 | LOSS: 2.8090384269792897e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 45/71 | LOSS: 2.811942593562771e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 46/71 | LOSS: 2.790754556468876e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 47/71 | LOSS: 2.7818000224518375e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 48/71 | LOSS: 2.779252674966357e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 49/71 | LOSS: 2.7915578939428088e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 50/71 | LOSS: 2.783522255662783e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 51/71 | LOSS: 2.779234924114112e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 52/71 | LOSS: 2.7679142184975064e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 53/71 | LOSS: 2.7699667751019457e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 54/71 | LOSS: 2.7574837936712852e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 55/71 | LOSS: 2.7558426121946207e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 56/71 | LOSS: 2.7391404696493083e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 57/71 | LOSS: 2.7338127657803224e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 58/71 | LOSS: 2.7295201215241892e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 59/71 | LOSS: 2.739147857937496e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 60/71 | LOSS: 2.7289988017892922e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 61/71 | LOSS: 2.7333585685776215e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 62/71 | LOSS: 2.744835128107788e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 63/71 | LOSS: 2.7285845760616212e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 64/71 | LOSS: 2.730453989687913e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 65/71 | LOSS: 2.7478904871024884e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 66/71 | LOSS: 2.746405554527014e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 67/71 | LOSS: 2.7361787514939136e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 68/71 | LOSS: 2.7330250146484975e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 69/71 | LOSS: 2.7373706585162186e-05\n",
      "TRAIN: EPOCH 52/1000 | BATCH 70/71 | LOSS: 2.728723030857919e-05\n",
      "VAL: EPOCH 52/1000 | BATCH 0/8 | LOSS: 2.9849965358152986e-05\n",
      "VAL: EPOCH 52/1000 | BATCH 1/8 | LOSS: 2.7694385607901495e-05\n",
      "VAL: EPOCH 52/1000 | BATCH 2/8 | LOSS: 2.8362475253137138e-05\n",
      "VAL: EPOCH 52/1000 | BATCH 3/8 | LOSS: 2.720151724133757e-05\n",
      "VAL: EPOCH 52/1000 | BATCH 4/8 | LOSS: 2.616251258586999e-05\n",
      "VAL: EPOCH 52/1000 | BATCH 5/8 | LOSS: 2.6288412300345954e-05\n",
      "VAL: EPOCH 52/1000 | BATCH 6/8 | LOSS: 2.5573732013331858e-05\n",
      "VAL: EPOCH 52/1000 | BATCH 7/8 | LOSS: 2.5230672235920792e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 0/71 | LOSS: 2.929145011876244e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 1/71 | LOSS: 2.721001237659948e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 2/71 | LOSS: 2.6832845833268948e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 3/71 | LOSS: 2.760480492725037e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 4/71 | LOSS: 2.913651696871966e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 5/71 | LOSS: 2.933367856409556e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 6/71 | LOSS: 2.9631426254387145e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 7/71 | LOSS: 2.917234360211296e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 8/71 | LOSS: 2.9621014821006814e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 9/71 | LOSS: 2.8933938665431924e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 10/71 | LOSS: 2.8967845454727385e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 11/71 | LOSS: 2.848337347434911e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 12/71 | LOSS: 2.7998111853286482e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 13/71 | LOSS: 2.8762372364456367e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 14/71 | LOSS: 2.89317509062433e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 15/71 | LOSS: 2.8349141757644247e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 16/71 | LOSS: 2.805936716921518e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 17/71 | LOSS: 2.791450889263716e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 18/71 | LOSS: 2.7513395635699118e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 19/71 | LOSS: 2.720525271797669e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 20/71 | LOSS: 2.740069188196988e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 21/71 | LOSS: 2.7291794553589583e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 22/71 | LOSS: 2.7129645509969283e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 23/71 | LOSS: 2.6911826125797234e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 24/71 | LOSS: 2.680072262592148e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 25/71 | LOSS: 2.6613901214799487e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 26/71 | LOSS: 2.6590971982740382e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 27/71 | LOSS: 2.650046073020868e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 28/71 | LOSS: 2.657103448592383e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 29/71 | LOSS: 2.6552296124767356e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 30/71 | LOSS: 2.6453039202850404e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 31/71 | LOSS: 2.646183509114053e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 32/71 | LOSS: 2.6631064952887368e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 33/71 | LOSS: 2.647932684768046e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 34/71 | LOSS: 2.6506586228996248e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 35/71 | LOSS: 2.6282076052868637e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 36/71 | LOSS: 2.6344602579106198e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 37/71 | LOSS: 2.6436579949069327e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 38/71 | LOSS: 2.6602313599328343e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 39/71 | LOSS: 2.6560760215943446e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 40/71 | LOSS: 2.6554999948996005e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 41/71 | LOSS: 2.645168214734267e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 42/71 | LOSS: 2.6427504857311697e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 43/71 | LOSS: 2.642579139236742e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 44/71 | LOSS: 2.6418926346296857e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 45/71 | LOSS: 2.6387845339385144e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 46/71 | LOSS: 2.6609111026776557e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 47/71 | LOSS: 2.6654875606861122e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 48/71 | LOSS: 2.6688748870843222e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 49/71 | LOSS: 2.6672397507354616e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 50/71 | LOSS: 2.689070663786968e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 51/71 | LOSS: 2.6889754036136976e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 52/71 | LOSS: 2.68328523198538e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 53/71 | LOSS: 2.6933427256557883e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 54/71 | LOSS: 2.687421401788015e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 55/71 | LOSS: 2.681967781167519e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 56/71 | LOSS: 2.6729380630984035e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 57/71 | LOSS: 2.6887004643303325e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 58/71 | LOSS: 2.7133467992209292e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 59/71 | LOSS: 2.717906775918285e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 60/71 | LOSS: 2.714198505048846e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 61/71 | LOSS: 2.715617862239953e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 62/71 | LOSS: 2.7088946202032577e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 63/71 | LOSS: 2.7023786685731466e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 64/71 | LOSS: 2.7002277951956224e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 65/71 | LOSS: 2.6942068953675598e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 66/71 | LOSS: 2.693317010334116e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 67/71 | LOSS: 2.6901128020493643e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 68/71 | LOSS: 2.6838949992636934e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 69/71 | LOSS: 2.681446941486294e-05\n",
      "TRAIN: EPOCH 53/1000 | BATCH 70/71 | LOSS: 2.6849233518718597e-05\n",
      "VAL: EPOCH 53/1000 | BATCH 0/8 | LOSS: 2.6826215616893023e-05\n",
      "VAL: EPOCH 53/1000 | BATCH 1/8 | LOSS: 2.5398970137757715e-05\n",
      "VAL: EPOCH 53/1000 | BATCH 2/8 | LOSS: 2.6454312925731454e-05\n",
      "VAL: EPOCH 53/1000 | BATCH 3/8 | LOSS: 2.571177901700139e-05\n",
      "VAL: EPOCH 53/1000 | BATCH 4/8 | LOSS: 2.484710785211064e-05\n",
      "VAL: EPOCH 53/1000 | BATCH 5/8 | LOSS: 2.5344019377371296e-05\n",
      "VAL: EPOCH 53/1000 | BATCH 6/8 | LOSS: 2.4513541055577142e-05\n",
      "VAL: EPOCH 53/1000 | BATCH 7/8 | LOSS: 2.422489706077613e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 0/71 | LOSS: 2.4736504201428033e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 1/71 | LOSS: 2.770756236714078e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 2/71 | LOSS: 2.4975276877133485e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 3/71 | LOSS: 2.712269588300842e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 4/71 | LOSS: 2.58410978858592e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 5/71 | LOSS: 2.5139329106120083e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 6/71 | LOSS: 2.5976082854737926e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 7/71 | LOSS: 2.648773806868121e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 8/71 | LOSS: 2.8219560287349548e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 9/71 | LOSS: 2.803675197355915e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 10/71 | LOSS: 2.8415116089904174e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 11/71 | LOSS: 2.7752819429830804e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 12/71 | LOSS: 2.7611879956496592e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 13/71 | LOSS: 2.724668216355245e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 14/71 | LOSS: 2.742153337749187e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 15/71 | LOSS: 2.750836904397147e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 16/71 | LOSS: 2.7281454629915327e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 17/71 | LOSS: 2.7019225045579435e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 18/71 | LOSS: 2.6740604401933715e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 19/71 | LOSS: 2.6607173458614854e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 20/71 | LOSS: 2.658594582628991e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 21/71 | LOSS: 2.631679424005349e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 22/71 | LOSS: 2.6212211156709362e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 23/71 | LOSS: 2.6139592515998327e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 24/71 | LOSS: 2.6195158061455003e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 25/71 | LOSS: 2.6189381969743408e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 26/71 | LOSS: 2.5988159770214998e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 27/71 | LOSS: 2.643451937599041e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 28/71 | LOSS: 2.64005407973008e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 29/71 | LOSS: 2.6156145819792678e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 30/71 | LOSS: 2.631028620535583e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 31/71 | LOSS: 2.624718416655014e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 32/71 | LOSS: 2.62849885488818e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 33/71 | LOSS: 2.6657686462653254e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 34/71 | LOSS: 2.6579330265771465e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 35/71 | LOSS: 2.6478877417604155e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 36/71 | LOSS: 2.6509535756132038e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 37/71 | LOSS: 2.638245678099338e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 38/71 | LOSS: 2.6255225095202408e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 39/71 | LOSS: 2.6540438693700707e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 40/71 | LOSS: 2.652568649110191e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 41/71 | LOSS: 2.657106644965963e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 42/71 | LOSS: 2.6793389076564656e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 43/71 | LOSS: 2.6723094048396558e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 44/71 | LOSS: 2.668503511813469e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 45/71 | LOSS: 2.6546131345600337e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 46/71 | LOSS: 2.640687454642946e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 47/71 | LOSS: 2.639649627174852e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 48/71 | LOSS: 2.6262249460215777e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 49/71 | LOSS: 2.6165878807660192e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 50/71 | LOSS: 2.606096060452389e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 51/71 | LOSS: 2.6022684918503866e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 52/71 | LOSS: 2.5901409142808813e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 53/71 | LOSS: 2.5773371244550044e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 54/71 | LOSS: 2.586032632362648e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 55/71 | LOSS: 2.5885243400937596e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 56/71 | LOSS: 2.5809562175071048e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 57/71 | LOSS: 2.5775413256457703e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 58/71 | LOSS: 2.5904450536719366e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 59/71 | LOSS: 2.587682286806133e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 60/71 | LOSS: 2.5946626621708808e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 61/71 | LOSS: 2.5984830772945463e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 62/71 | LOSS: 2.59010223147463e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 63/71 | LOSS: 2.6046179328886865e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 64/71 | LOSS: 2.6145054373325996e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 65/71 | LOSS: 2.6080319388931844e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 66/71 | LOSS: 2.59699558662407e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 67/71 | LOSS: 2.5868138864633e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 68/71 | LOSS: 2.577144664098961e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 69/71 | LOSS: 2.5732126414368394e-05\n",
      "TRAIN: EPOCH 54/1000 | BATCH 70/71 | LOSS: 2.5956629209579493e-05\n",
      "VAL: EPOCH 54/1000 | BATCH 0/8 | LOSS: 2.7182508347323164e-05\n",
      "VAL: EPOCH 54/1000 | BATCH 1/8 | LOSS: 2.632279029057827e-05\n",
      "VAL: EPOCH 54/1000 | BATCH 2/8 | LOSS: 2.6925269291192915e-05\n",
      "VAL: EPOCH 54/1000 | BATCH 3/8 | LOSS: 2.5859229936031625e-05\n",
      "VAL: EPOCH 54/1000 | BATCH 4/8 | LOSS: 2.4662054784130305e-05\n",
      "VAL: EPOCH 54/1000 | BATCH 5/8 | LOSS: 2.4839157655757543e-05\n",
      "VAL: EPOCH 54/1000 | BATCH 6/8 | LOSS: 2.412970830586606e-05\n",
      "VAL: EPOCH 54/1000 | BATCH 7/8 | LOSS: 2.3628127109986963e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 0/71 | LOSS: 2.2620026356889866e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 1/71 | LOSS: 2.3256719032360706e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 2/71 | LOSS: 2.2878768504597247e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 3/71 | LOSS: 2.1896581984037766e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 4/71 | LOSS: 2.318263941560872e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 5/71 | LOSS: 2.2902893154726673e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 6/71 | LOSS: 2.270223298442683e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 7/71 | LOSS: 2.2302426714304602e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 8/71 | LOSS: 2.2653676043975996e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 9/71 | LOSS: 2.2884191093908157e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 10/71 | LOSS: 2.30064607339128e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 11/71 | LOSS: 2.381872597349381e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 12/71 | LOSS: 2.452349270662615e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 13/71 | LOSS: 2.4414065332426355e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 14/71 | LOSS: 2.433700271164222e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 15/71 | LOSS: 2.4179839442695084e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 16/71 | LOSS: 2.5117653529057006e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 17/71 | LOSS: 2.5253225430788007e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 18/71 | LOSS: 2.531900846454511e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 19/71 | LOSS: 2.527050510252593e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 20/71 | LOSS: 2.5155912919130752e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 21/71 | LOSS: 2.5200025778427847e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 22/71 | LOSS: 2.523956739067849e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 23/71 | LOSS: 2.5743014324082953e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 24/71 | LOSS: 2.6093086999026126e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 25/71 | LOSS: 2.6377719922703152e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 26/71 | LOSS: 2.6559489722583545e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 27/71 | LOSS: 2.636216562937729e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 28/71 | LOSS: 2.623089858388994e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 29/71 | LOSS: 2.6177314854673265e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 30/71 | LOSS: 2.6403639323337215e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 31/71 | LOSS: 2.6371502315214457e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 32/71 | LOSS: 2.6542646578904666e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 33/71 | LOSS: 2.6526594945821937e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 34/71 | LOSS: 2.630703300902886e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 35/71 | LOSS: 2.6213447300607288e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 36/71 | LOSS: 2.6151406190537444e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 37/71 | LOSS: 2.617576491933218e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 38/71 | LOSS: 2.602185253262854e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 39/71 | LOSS: 2.5967942838178716e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 40/71 | LOSS: 2.616037649781125e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 41/71 | LOSS: 2.605097458962562e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 42/71 | LOSS: 2.60048874584162e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 43/71 | LOSS: 2.5951879954597892e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 44/71 | LOSS: 2.5956434344859897e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 45/71 | LOSS: 2.5854387390806906e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 46/71 | LOSS: 2.583692432690828e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 47/71 | LOSS: 2.5829014248301974e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 48/71 | LOSS: 2.579184261516535e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 49/71 | LOSS: 2.570097982243169e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 50/71 | LOSS: 2.5649825307910386e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 51/71 | LOSS: 2.5567055567640076e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 52/71 | LOSS: 2.5473630472024866e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 53/71 | LOSS: 2.539716532973452e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 54/71 | LOSS: 2.529850912238048e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 55/71 | LOSS: 2.5195277982805108e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 56/71 | LOSS: 2.522893271561132e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 57/71 | LOSS: 2.522368698731137e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 58/71 | LOSS: 2.513270229135711e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 59/71 | LOSS: 2.5140594243566738e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 60/71 | LOSS: 2.5221824947292306e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 61/71 | LOSS: 2.5174428306091877e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 62/71 | LOSS: 2.5177710562101786e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 63/71 | LOSS: 2.5260918420144662e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 64/71 | LOSS: 2.5255701649047506e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 65/71 | LOSS: 2.5205331053946406e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 66/71 | LOSS: 2.5220901513977824e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 67/71 | LOSS: 2.5249475316634154e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 68/71 | LOSS: 2.5111132134148594e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 69/71 | LOSS: 2.5065275985980405e-05\n",
      "TRAIN: EPOCH 55/1000 | BATCH 70/71 | LOSS: 2.5381528476046554e-05\n",
      "VAL: EPOCH 55/1000 | BATCH 0/8 | LOSS: 2.8074182409909554e-05\n",
      "VAL: EPOCH 55/1000 | BATCH 1/8 | LOSS: 2.7136510652781e-05\n",
      "VAL: EPOCH 55/1000 | BATCH 2/8 | LOSS: 2.7929412681260146e-05\n",
      "VAL: EPOCH 55/1000 | BATCH 3/8 | LOSS: 2.6861086098506348e-05\n",
      "VAL: EPOCH 55/1000 | BATCH 4/8 | LOSS: 2.573186247900594e-05\n",
      "VAL: EPOCH 55/1000 | BATCH 5/8 | LOSS: 2.558580824067273e-05\n",
      "VAL: EPOCH 55/1000 | BATCH 6/8 | LOSS: 2.4978577909808208e-05\n",
      "VAL: EPOCH 55/1000 | BATCH 7/8 | LOSS: 2.4595442027930403e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 0/71 | LOSS: 3.332381311338395e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 1/71 | LOSS: 2.79957184829982e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 2/71 | LOSS: 2.9503999636896577e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 3/71 | LOSS: 2.9880979127483442e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 4/71 | LOSS: 2.857633589883335e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 5/71 | LOSS: 2.8529554583656136e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 6/71 | LOSS: 2.9263189259966437e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 7/71 | LOSS: 2.9439702984745963e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 8/71 | LOSS: 2.8719090753131444e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 9/71 | LOSS: 2.873553712561261e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 10/71 | LOSS: 2.8619163872049697e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 11/71 | LOSS: 2.8429967187548755e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 12/71 | LOSS: 2.8086324583945007e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 13/71 | LOSS: 2.7976703937123864e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 14/71 | LOSS: 2.785990933868258e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 15/71 | LOSS: 2.7607213155533827e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 16/71 | LOSS: 2.7214629025205367e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 17/71 | LOSS: 2.6777186778619783e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 18/71 | LOSS: 2.6412309747875522e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 19/71 | LOSS: 2.616128094814485e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 20/71 | LOSS: 2.6148433746476213e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 21/71 | LOSS: 2.5772482248695187e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 22/71 | LOSS: 2.5386132540955693e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 23/71 | LOSS: 2.515060585513614e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 24/71 | LOSS: 2.5092590585700237e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 25/71 | LOSS: 2.5746049826781156e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 26/71 | LOSS: 2.6056241107720848e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 27/71 | LOSS: 2.6074757118164727e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 28/71 | LOSS: 2.6107929004175622e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 29/71 | LOSS: 2.5967233038196962e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 30/71 | LOSS: 2.578538407115341e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 31/71 | LOSS: 2.5622312648465595e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 32/71 | LOSS: 2.5624794227152243e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 33/71 | LOSS: 2.604324163257843e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 34/71 | LOSS: 2.608462481085943e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 35/71 | LOSS: 2.6306532719091694e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 36/71 | LOSS: 2.639117337821517e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 37/71 | LOSS: 2.6345510042646263e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 38/71 | LOSS: 2.638554281311838e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 39/71 | LOSS: 2.6275293430444434e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 40/71 | LOSS: 2.618722819755842e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 41/71 | LOSS: 2.620093900553738e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 42/71 | LOSS: 2.6076062102558436e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 43/71 | LOSS: 2.5968940752301766e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 44/71 | LOSS: 2.6114144444970102e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 45/71 | LOSS: 2.5953461653957873e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 46/71 | LOSS: 2.577248000926239e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 47/71 | LOSS: 2.5707611560695415e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 48/71 | LOSS: 2.580872758847604e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 49/71 | LOSS: 2.600093044748064e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 50/71 | LOSS: 2.5970421262957413e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 51/71 | LOSS: 2.588474084264957e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 52/71 | LOSS: 2.581111732310229e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 53/71 | LOSS: 2.5642520961776393e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 54/71 | LOSS: 2.5557613877357323e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 55/71 | LOSS: 2.548142661648204e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 56/71 | LOSS: 2.5465966724655836e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 57/71 | LOSS: 2.5469379109161872e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 58/71 | LOSS: 2.5417836526284485e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 59/71 | LOSS: 2.5356753121741348e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 60/71 | LOSS: 2.5292691611494014e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 61/71 | LOSS: 2.52680532162964e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 62/71 | LOSS: 2.5199974645927015e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 63/71 | LOSS: 2.513426483119474e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 64/71 | LOSS: 2.512229209689674e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 65/71 | LOSS: 2.5131867584026292e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 66/71 | LOSS: 2.5046081428803547e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 67/71 | LOSS: 2.502905448743738e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 68/71 | LOSS: 2.499356151679092e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 69/71 | LOSS: 2.491459727025358e-05\n",
      "TRAIN: EPOCH 56/1000 | BATCH 70/71 | LOSS: 2.4811485224783842e-05\n",
      "VAL: EPOCH 56/1000 | BATCH 0/8 | LOSS: 2.7828358724946156e-05\n",
      "VAL: EPOCH 56/1000 | BATCH 1/8 | LOSS: 2.6955571229336783e-05\n",
      "VAL: EPOCH 56/1000 | BATCH 2/8 | LOSS: 2.8125619792263024e-05\n",
      "VAL: EPOCH 56/1000 | BATCH 3/8 | LOSS: 2.726111188167124e-05\n",
      "VAL: EPOCH 56/1000 | BATCH 4/8 | LOSS: 2.5914005891536363e-05\n",
      "VAL: EPOCH 56/1000 | BATCH 5/8 | LOSS: 2.5438179667010747e-05\n",
      "VAL: EPOCH 56/1000 | BATCH 6/8 | LOSS: 2.4707347360422966e-05\n",
      "VAL: EPOCH 56/1000 | BATCH 7/8 | LOSS: 2.385467519161466e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 0/71 | LOSS: 3.955753709306009e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 1/71 | LOSS: 3.045158791792346e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 2/71 | LOSS: 2.7162793533837732e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 3/71 | LOSS: 2.4158883661584696e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 4/71 | LOSS: 2.496717424946837e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 5/71 | LOSS: 2.602782175623967e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 6/71 | LOSS: 2.6144847392320765e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 7/71 | LOSS: 2.6298766442778287e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 8/71 | LOSS: 2.5707160805015723e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 9/71 | LOSS: 2.5121714134002105e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 10/71 | LOSS: 2.5414181932732887e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 11/71 | LOSS: 2.576584802227444e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 12/71 | LOSS: 2.5980527145573153e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 13/71 | LOSS: 2.5252116237035288e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 14/71 | LOSS: 2.5678385281935334e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 15/71 | LOSS: 2.5651992928032996e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 16/71 | LOSS: 2.559382953811163e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 17/71 | LOSS: 2.5851574618071834e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 18/71 | LOSS: 2.588817158922259e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 19/71 | LOSS: 2.555392129579559e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 20/71 | LOSS: 2.5299822352410827e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 21/71 | LOSS: 2.5253269251896366e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 22/71 | LOSS: 2.5069411687740207e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 23/71 | LOSS: 2.488113257944254e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 24/71 | LOSS: 2.522387330827769e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 25/71 | LOSS: 2.5449197752235566e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 26/71 | LOSS: 2.5374753359183496e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 27/71 | LOSS: 2.520408289845883e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 28/71 | LOSS: 2.4954804569555062e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 29/71 | LOSS: 2.526974118760942e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 30/71 | LOSS: 2.5454892766579326e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 31/71 | LOSS: 2.5233426924842206e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 32/71 | LOSS: 2.5133721988602314e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 33/71 | LOSS: 2.4885064436495988e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 34/71 | LOSS: 2.4720073841828187e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 35/71 | LOSS: 2.4528442938592183e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 36/71 | LOSS: 2.4712075714519046e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 37/71 | LOSS: 2.453623681886759e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 38/71 | LOSS: 2.457321398907926e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 39/71 | LOSS: 2.4647569898661458e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 40/71 | LOSS: 2.4608076317945675e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 41/71 | LOSS: 2.477699629144765e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 42/71 | LOSS: 2.4581331249358957e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 43/71 | LOSS: 2.4542155188100878e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 44/71 | LOSS: 2.4474704575065212e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 45/71 | LOSS: 2.441310220013332e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 46/71 | LOSS: 2.441780545384663e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 47/71 | LOSS: 2.430425259566012e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 48/71 | LOSS: 2.425381312697974e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 49/71 | LOSS: 2.4171280820155517e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 50/71 | LOSS: 2.4043715586908637e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 51/71 | LOSS: 2.417632332743289e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 52/71 | LOSS: 2.4108086846645852e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 53/71 | LOSS: 2.4065760650652617e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 54/71 | LOSS: 2.410909854171967e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 55/71 | LOSS: 2.424521296364089e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 56/71 | LOSS: 2.4183442675214456e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 57/71 | LOSS: 2.412869726585682e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 58/71 | LOSS: 2.4197321828855282e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 59/71 | LOSS: 2.411705011885109e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 60/71 | LOSS: 2.432889068288324e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 61/71 | LOSS: 2.429881327947031e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 62/71 | LOSS: 2.4391005440737078e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 63/71 | LOSS: 2.444363738618449e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 64/71 | LOSS: 2.45276827906939e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 65/71 | LOSS: 2.446791922645361e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 66/71 | LOSS: 2.4474388452806274e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 67/71 | LOSS: 2.444959478880799e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 68/71 | LOSS: 2.4455898407419834e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 69/71 | LOSS: 2.4422332847149976e-05\n",
      "TRAIN: EPOCH 57/1000 | BATCH 70/71 | LOSS: 2.4461255006698736e-05\n",
      "VAL: EPOCH 57/1000 | BATCH 0/8 | LOSS: 2.457425944157876e-05\n",
      "VAL: EPOCH 57/1000 | BATCH 1/8 | LOSS: 2.3617759325134102e-05\n",
      "VAL: EPOCH 57/1000 | BATCH 2/8 | LOSS: 2.535152210233112e-05\n",
      "VAL: EPOCH 57/1000 | BATCH 3/8 | LOSS: 2.4952991225291044e-05\n",
      "VAL: EPOCH 57/1000 | BATCH 4/8 | LOSS: 2.3895514459582046e-05\n",
      "VAL: EPOCH 57/1000 | BATCH 5/8 | LOSS: 2.3559982461544376e-05\n",
      "VAL: EPOCH 57/1000 | BATCH 6/8 | LOSS: 2.2852343265965048e-05\n",
      "VAL: EPOCH 57/1000 | BATCH 7/8 | LOSS: 2.2272620526564424e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 0/71 | LOSS: 2.5574761821189895e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 1/71 | LOSS: 2.3873004465713166e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 2/71 | LOSS: 2.4434005657288555e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 3/71 | LOSS: 2.3298813175642863e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 4/71 | LOSS: 2.2273016656981782e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 5/71 | LOSS: 2.2731760206321876e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 6/71 | LOSS: 2.2790467482991517e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 7/71 | LOSS: 2.3306342200157815e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 8/71 | LOSS: 2.383634374483437e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 9/71 | LOSS: 2.4192990713345354e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 10/71 | LOSS: 2.4417564418399706e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 11/71 | LOSS: 2.552804107835982e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 12/71 | LOSS: 2.5176647166237952e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 13/71 | LOSS: 2.4924553144956008e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 14/71 | LOSS: 2.486025223333854e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 15/71 | LOSS: 2.4853963395798928e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 16/71 | LOSS: 2.5050658330335008e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 17/71 | LOSS: 2.4783356517824966e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 18/71 | LOSS: 2.462031165793489e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 19/71 | LOSS: 2.4778441002126783e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 20/71 | LOSS: 2.4641626147231798e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 21/71 | LOSS: 2.4478778620753225e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 22/71 | LOSS: 2.4212640552002046e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 23/71 | LOSS: 2.4033652304448577e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 24/71 | LOSS: 2.371452981606126e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 25/71 | LOSS: 2.3513117412221618e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 26/71 | LOSS: 2.3575152833169948e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 27/71 | LOSS: 2.333638427184529e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 28/71 | LOSS: 2.325558370071592e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 29/71 | LOSS: 2.3213689867892148e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 30/71 | LOSS: 2.3123592625544858e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 31/71 | LOSS: 2.316438400384868e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 32/71 | LOSS: 2.306023254112171e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 33/71 | LOSS: 2.3193856898561696e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 34/71 | LOSS: 2.3343004431808368e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 35/71 | LOSS: 2.359961369317413e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 36/71 | LOSS: 2.3585222004692548e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 37/71 | LOSS: 2.359410691018351e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 38/71 | LOSS: 2.369530678455097e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 39/71 | LOSS: 2.385193802183494e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 40/71 | LOSS: 2.3701588272560024e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 41/71 | LOSS: 2.3660451569455853e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 42/71 | LOSS: 2.3639349058422065e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 43/71 | LOSS: 2.3678637262491975e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 44/71 | LOSS: 2.3776316043545698e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 45/71 | LOSS: 2.3608780516635466e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 46/71 | LOSS: 2.3490778829077596e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 47/71 | LOSS: 2.3570295184072165e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 48/71 | LOSS: 2.3616917485701473e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 49/71 | LOSS: 2.3605087590112818e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 50/71 | LOSS: 2.3515675588985723e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 51/71 | LOSS: 2.3454341387080673e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 52/71 | LOSS: 2.3432416196985013e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 53/71 | LOSS: 2.3322958272560585e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 54/71 | LOSS: 2.3241146382960407e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 55/71 | LOSS: 2.3262220176028287e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 56/71 | LOSS: 2.3341107072682415e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 57/71 | LOSS: 2.3351320406022593e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 58/71 | LOSS: 2.3312986340281016e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 59/71 | LOSS: 2.3344080151825134e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 60/71 | LOSS: 2.3385014278350445e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 61/71 | LOSS: 2.3338783736811997e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 62/71 | LOSS: 2.3257427153386366e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 63/71 | LOSS: 2.3292149279541263e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 64/71 | LOSS: 2.32198557429141e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 65/71 | LOSS: 2.311756149235104e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 66/71 | LOSS: 2.3102197226409717e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 67/71 | LOSS: 2.313800555117161e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 68/71 | LOSS: 2.3110749732923992e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 69/71 | LOSS: 2.3063589183688497e-05\n",
      "TRAIN: EPOCH 58/1000 | BATCH 70/71 | LOSS: 2.3080637335406723e-05\n",
      "VAL: EPOCH 58/1000 | BATCH 0/8 | LOSS: 2.293827674293425e-05\n",
      "VAL: EPOCH 58/1000 | BATCH 1/8 | LOSS: 2.161640986741986e-05\n",
      "VAL: EPOCH 58/1000 | BATCH 2/8 | LOSS: 2.356819944300999e-05\n",
      "VAL: EPOCH 58/1000 | BATCH 3/8 | LOSS: 2.40355884670862e-05\n",
      "VAL: EPOCH 58/1000 | BATCH 4/8 | LOSS: 2.3630559735465794e-05\n",
      "VAL: EPOCH 58/1000 | BATCH 5/8 | LOSS: 2.3851167497923598e-05\n",
      "VAL: EPOCH 58/1000 | BATCH 6/8 | LOSS: 2.308088395303847e-05\n",
      "VAL: EPOCH 58/1000 | BATCH 7/8 | LOSS: 2.2810440896137152e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 0/71 | LOSS: 2.2314574380288832e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 1/71 | LOSS: 2.170303832826903e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 2/71 | LOSS: 2.1497869965969585e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 3/71 | LOSS: 2.302327447978314e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 4/71 | LOSS: 2.205689379479736e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 5/71 | LOSS: 2.3795948436600156e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 6/71 | LOSS: 2.3673332179896533e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 7/71 | LOSS: 2.342463858440169e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 8/71 | LOSS: 2.303731081257057e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 9/71 | LOSS: 2.314780685992446e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 10/71 | LOSS: 2.277976322643967e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 11/71 | LOSS: 2.3262874189337406e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 12/71 | LOSS: 2.3046125604126315e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 13/71 | LOSS: 2.279278851347044e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 14/71 | LOSS: 2.2723475922248326e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 15/71 | LOSS: 2.334956263894128e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 16/71 | LOSS: 2.309916314569658e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 17/71 | LOSS: 2.2675556566456282e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 18/71 | LOSS: 2.2340000884325588e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 19/71 | LOSS: 2.2099628495197976e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 20/71 | LOSS: 2.2149328461160794e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 21/71 | LOSS: 2.2324473899392284e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 22/71 | LOSS: 2.2396829713809144e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 23/71 | LOSS: 2.237379514250885e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 24/71 | LOSS: 2.2168567811604588e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 25/71 | LOSS: 2.217296647498957e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 26/71 | LOSS: 2.205631231104403e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 27/71 | LOSS: 2.1895632016821764e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 28/71 | LOSS: 2.1913026761592784e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 29/71 | LOSS: 2.1894328771547104e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 30/71 | LOSS: 2.1999212725448514e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 31/71 | LOSS: 2.186320057262492e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 32/71 | LOSS: 2.200656429825661e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 33/71 | LOSS: 2.2205559991348043e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 34/71 | LOSS: 2.2191753877060754e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 35/71 | LOSS: 2.2123956164755833e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 36/71 | LOSS: 2.219996526577774e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 37/71 | LOSS: 2.2295655119370694e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 38/71 | LOSS: 2.2269511296494435e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 39/71 | LOSS: 2.2161579590829205e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 40/71 | LOSS: 2.2142907299694785e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 41/71 | LOSS: 2.2150664898661143e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 42/71 | LOSS: 2.215257666458181e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 43/71 | LOSS: 2.2103262050553564e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 44/71 | LOSS: 2.2096086548925893e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 45/71 | LOSS: 2.2057078003063392e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 46/71 | LOSS: 2.204516200203835e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 47/71 | LOSS: 2.194151591083937e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 48/71 | LOSS: 2.1860185778659425e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 49/71 | LOSS: 2.2027897684893106e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 50/71 | LOSS: 2.210183244160897e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 51/71 | LOSS: 2.2102303354096457e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 52/71 | LOSS: 2.2027316175945947e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 53/71 | LOSS: 2.2009473401272703e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 54/71 | LOSS: 2.1949287673289127e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 55/71 | LOSS: 2.1973675367397455e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 56/71 | LOSS: 2.1932380740620654e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 57/71 | LOSS: 2.193950864427185e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 58/71 | LOSS: 2.1904966762137472e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 59/71 | LOSS: 2.197446680535601e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 60/71 | LOSS: 2.184396327988981e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 61/71 | LOSS: 2.1796370102463463e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 62/71 | LOSS: 2.174465770088044e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 63/71 | LOSS: 2.171854551136221e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 64/71 | LOSS: 2.1809056558525367e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 65/71 | LOSS: 2.1787423355073603e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 66/71 | LOSS: 2.1951143243137238e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 67/71 | LOSS: 2.1890207482797184e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 68/71 | LOSS: 2.1878600858579613e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 69/71 | LOSS: 2.1911479507252807e-05\n",
      "TRAIN: EPOCH 59/1000 | BATCH 70/71 | LOSS: 2.193097020288303e-05\n",
      "VAL: EPOCH 59/1000 | BATCH 0/8 | LOSS: 2.2750002244720235e-05\n",
      "VAL: EPOCH 59/1000 | BATCH 1/8 | LOSS: 2.1764933990198188e-05\n",
      "VAL: EPOCH 59/1000 | BATCH 2/8 | LOSS: 2.2999341429870885e-05\n",
      "VAL: EPOCH 59/1000 | BATCH 3/8 | LOSS: 2.2527385681314627e-05\n",
      "VAL: EPOCH 59/1000 | BATCH 4/8 | LOSS: 2.1663499501300977e-05\n",
      "VAL: EPOCH 59/1000 | BATCH 5/8 | LOSS: 2.176857621331389e-05\n",
      "VAL: EPOCH 59/1000 | BATCH 6/8 | LOSS: 2.0984590394488934e-05\n",
      "VAL: EPOCH 59/1000 | BATCH 7/8 | LOSS: 2.062100043076498e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 0/71 | LOSS: 1.69072827702621e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 1/71 | LOSS: 1.9285157577542122e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 2/71 | LOSS: 2.0526416847133078e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 3/71 | LOSS: 2.207749957960914e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 4/71 | LOSS: 2.1162682969588786e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 5/71 | LOSS: 2.1252227876781642e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 6/71 | LOSS: 2.172781257416188e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 7/71 | LOSS: 2.1710936380259227e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 8/71 | LOSS: 2.17917365969495e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 9/71 | LOSS: 2.2017846276867203e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 10/71 | LOSS: 2.202491305069998e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 11/71 | LOSS: 2.1653331562750584e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 12/71 | LOSS: 2.1528975804148315e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 13/71 | LOSS: 2.1213669892209248e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 14/71 | LOSS: 2.1392885658618375e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 15/71 | LOSS: 2.175817314764572e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 16/71 | LOSS: 2.16757945214768e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 17/71 | LOSS: 2.191501193414701e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 18/71 | LOSS: 2.2131295209922092e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 19/71 | LOSS: 2.21679758396931e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 20/71 | LOSS: 2.2044080001900772e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 21/71 | LOSS: 2.2077664322172165e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 22/71 | LOSS: 2.2097505912508655e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 23/71 | LOSS: 2.213152841553286e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 24/71 | LOSS: 2.1824708092026413e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 25/71 | LOSS: 2.1725319759123242e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 26/71 | LOSS: 2.1892004817211137e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 27/71 | LOSS: 2.1974802945935933e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 28/71 | LOSS: 2.1807901028860843e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 29/71 | LOSS: 2.181295737197312e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 30/71 | LOSS: 2.1708805551527128e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 31/71 | LOSS: 2.18812247680944e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 32/71 | LOSS: 2.1738504404890012e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 33/71 | LOSS: 2.1794883490228504e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 34/71 | LOSS: 2.1838253451278435e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 35/71 | LOSS: 2.169512552225367e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 36/71 | LOSS: 2.175142799387686e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 37/71 | LOSS: 2.1753547068663848e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 38/71 | LOSS: 2.1746300589233542e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 39/71 | LOSS: 2.1696668500226225e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 40/71 | LOSS: 2.1775344865135395e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 41/71 | LOSS: 2.1855558207746417e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 42/71 | LOSS: 2.1868315787625844e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 43/71 | LOSS: 2.1890568818476325e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 44/71 | LOSS: 2.1938948945413963e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 45/71 | LOSS: 2.1990355099146456e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 46/71 | LOSS: 2.2061514910285736e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 47/71 | LOSS: 2.1889404592911887e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 48/71 | LOSS: 2.1786237437259027e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 49/71 | LOSS: 2.1650597991538233e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 50/71 | LOSS: 2.1611372087100592e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 51/71 | LOSS: 2.1624202767830306e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 52/71 | LOSS: 2.167096866991037e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 53/71 | LOSS: 2.1688129385842734e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 54/71 | LOSS: 2.1752296313273043e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 55/71 | LOSS: 2.175431125449125e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 56/71 | LOSS: 2.1786364812757248e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 57/71 | LOSS: 2.1820521145980758e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 58/71 | LOSS: 2.1746828694585157e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 59/71 | LOSS: 2.193661666751723e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 60/71 | LOSS: 2.192228966436471e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 61/71 | LOSS: 2.2006652216077782e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 62/71 | LOSS: 2.1978826124622072e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 63/71 | LOSS: 2.2115048551540895e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 64/71 | LOSS: 2.2195347348595253e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 65/71 | LOSS: 2.216451227440936e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 66/71 | LOSS: 2.2290694062969883e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 67/71 | LOSS: 2.2221577744537225e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 68/71 | LOSS: 2.227763642609268e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 69/71 | LOSS: 2.2179878682696393e-05\n",
      "TRAIN: EPOCH 60/1000 | BATCH 70/71 | LOSS: 2.21457685000414e-05\n",
      "VAL: EPOCH 60/1000 | BATCH 0/8 | LOSS: 2.4302258680108935e-05\n",
      "VAL: EPOCH 60/1000 | BATCH 1/8 | LOSS: 2.340147057111608e-05\n",
      "VAL: EPOCH 60/1000 | BATCH 2/8 | LOSS: 2.596637023088988e-05\n",
      "VAL: EPOCH 60/1000 | BATCH 3/8 | LOSS: 2.6804403205460403e-05\n",
      "VAL: EPOCH 60/1000 | BATCH 4/8 | LOSS: 2.6455667830305173e-05\n",
      "VAL: EPOCH 60/1000 | BATCH 5/8 | LOSS: 2.61177635062874e-05\n",
      "VAL: EPOCH 60/1000 | BATCH 6/8 | LOSS: 2.543306618463248e-05\n",
      "VAL: EPOCH 60/1000 | BATCH 7/8 | LOSS: 2.47362045229238e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 0/71 | LOSS: 2.6845316824619658e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 1/71 | LOSS: 2.5606510462239385e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 2/71 | LOSS: 2.4700386347831227e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 3/71 | LOSS: 2.266894762215088e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 4/71 | LOSS: 2.3975561998668126e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 5/71 | LOSS: 2.281217439303873e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 6/71 | LOSS: 2.3361959132931327e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 7/71 | LOSS: 2.2720563492839574e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 8/71 | LOSS: 2.2502197073966574e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 9/71 | LOSS: 2.2603666911891197e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 10/71 | LOSS: 2.2512190050540745e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 11/71 | LOSS: 2.2442005653526092e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 12/71 | LOSS: 2.2521041291139016e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 13/71 | LOSS: 2.2355010774585287e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 14/71 | LOSS: 2.276621041043351e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 15/71 | LOSS: 2.2631386059401848e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 16/71 | LOSS: 2.265700844445211e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 17/71 | LOSS: 2.2355069429775336e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 18/71 | LOSS: 2.2459840501572837e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 19/71 | LOSS: 2.2582569181395228e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 20/71 | LOSS: 2.2563604250622337e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 21/71 | LOSS: 2.2846761070964433e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 22/71 | LOSS: 2.276094243785812e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 23/71 | LOSS: 2.2562722885292413e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 24/71 | LOSS: 2.2362995950970797e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 25/71 | LOSS: 2.233412249971969e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 26/71 | LOSS: 2.2153959367575158e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 27/71 | LOSS: 2.2117667023329496e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 28/71 | LOSS: 2.2096272067724855e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 29/71 | LOSS: 2.2075595734349917e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 30/71 | LOSS: 2.226676887432639e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 31/71 | LOSS: 2.2096909162883094e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 32/71 | LOSS: 2.230858185750108e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 33/71 | LOSS: 2.232320832049214e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 34/71 | LOSS: 2.2309427523785938e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 35/71 | LOSS: 2.2321029064187314e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 36/71 | LOSS: 2.2312477872096555e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 37/71 | LOSS: 2.2316238899197504e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 38/71 | LOSS: 2.225837847501493e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 39/71 | LOSS: 2.2374769469024613e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 40/71 | LOSS: 2.2227556633524507e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 41/71 | LOSS: 2.206130108918712e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 42/71 | LOSS: 2.2096995899330576e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 43/71 | LOSS: 2.2064907728814763e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 44/71 | LOSS: 2.21750550432868e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 45/71 | LOSS: 2.2162751381825807e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 46/71 | LOSS: 2.2083341873633585e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 47/71 | LOSS: 2.2003768075743817e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 48/71 | LOSS: 2.1922628622298954e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 49/71 | LOSS: 2.1996082450641554e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 50/71 | LOSS: 2.190773006184506e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 51/71 | LOSS: 2.188089395251434e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 52/71 | LOSS: 2.189687632569573e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 53/71 | LOSS: 2.191250925736515e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 54/71 | LOSS: 2.1933516555608632e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 55/71 | LOSS: 2.2010969571186122e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 56/71 | LOSS: 2.1935935189519607e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 57/71 | LOSS: 2.194537665704486e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 58/71 | LOSS: 2.192309667576811e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 59/71 | LOSS: 2.1893945677220473e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 60/71 | LOSS: 2.1865979164669893e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 61/71 | LOSS: 2.1748619239529475e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 62/71 | LOSS: 2.1785049789792134e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 63/71 | LOSS: 2.1743901697846013e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 64/71 | LOSS: 2.1736337545847234e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 65/71 | LOSS: 2.173726578334798e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 66/71 | LOSS: 2.1657654773417875e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 67/71 | LOSS: 2.1634763657750547e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 68/71 | LOSS: 2.1587740142738607e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 69/71 | LOSS: 2.154000581608021e-05\n",
      "TRAIN: EPOCH 61/1000 | BATCH 70/71 | LOSS: 2.194749814266352e-05\n",
      "VAL: EPOCH 61/1000 | BATCH 0/8 | LOSS: 2.3550976038677618e-05\n",
      "VAL: EPOCH 61/1000 | BATCH 1/8 | LOSS: 2.214711366832489e-05\n",
      "VAL: EPOCH 61/1000 | BATCH 2/8 | LOSS: 2.304441477463115e-05\n",
      "VAL: EPOCH 61/1000 | BATCH 3/8 | LOSS: 2.245267478429014e-05\n",
      "VAL: EPOCH 61/1000 | BATCH 4/8 | LOSS: 2.183438264182769e-05\n",
      "VAL: EPOCH 61/1000 | BATCH 5/8 | LOSS: 2.2064681312864803e-05\n",
      "VAL: EPOCH 61/1000 | BATCH 6/8 | LOSS: 2.1284205753805247e-05\n",
      "VAL: EPOCH 61/1000 | BATCH 7/8 | LOSS: 2.1199749198785867e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 0/71 | LOSS: 2.4178341845981777e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 1/71 | LOSS: 2.2475409423350357e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 2/71 | LOSS: 2.1523418884801988e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 3/71 | LOSS: 2.1722389192291303e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 4/71 | LOSS: 2.1074689720990135e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 5/71 | LOSS: 2.0602768017852213e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 6/71 | LOSS: 2.135508033721375e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 7/71 | LOSS: 2.1986531692164135e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 8/71 | LOSS: 2.2380199071550225e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 9/71 | LOSS: 2.266463707201183e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 10/71 | LOSS: 2.239482736595991e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 11/71 | LOSS: 2.1932654211317033e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 12/71 | LOSS: 2.1976543603858983e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 13/71 | LOSS: 2.1899764890674434e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 14/71 | LOSS: 2.196310827760802e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 15/71 | LOSS: 2.1920638232586498e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 16/71 | LOSS: 2.2132382903638407e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 17/71 | LOSS: 2.227048561407072e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 18/71 | LOSS: 2.2123504286834138e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 19/71 | LOSS: 2.1918395032116677e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 20/71 | LOSS: 2.1712803832737597e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 21/71 | LOSS: 2.159361131568651e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 22/71 | LOSS: 2.15046606488455e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 23/71 | LOSS: 2.1535619528852596e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 24/71 | LOSS: 2.1706457700929605e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 25/71 | LOSS: 2.190336950801653e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 26/71 | LOSS: 2.1677570893424906e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 27/71 | LOSS: 2.1689700003792366e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 28/71 | LOSS: 2.1717852962018666e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 29/71 | LOSS: 2.1724436192016582e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 30/71 | LOSS: 2.1496450508786965e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 31/71 | LOSS: 2.1501791934497305e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 32/71 | LOSS: 2.15712911030454e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 33/71 | LOSS: 2.1396551453696756e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 34/71 | LOSS: 2.126136278093327e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 35/71 | LOSS: 2.107069086479088e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 36/71 | LOSS: 2.1100479923031367e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 37/71 | LOSS: 2.0995508700587762e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 38/71 | LOSS: 2.1137761462178343e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 39/71 | LOSS: 2.1241869990262786e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 40/71 | LOSS: 2.1179937415049276e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 41/71 | LOSS: 2.121734585324746e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 42/71 | LOSS: 2.113333171427109e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 43/71 | LOSS: 2.116726380501694e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 44/71 | LOSS: 2.1050688115388363e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 45/71 | LOSS: 2.1040213378791638e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 46/71 | LOSS: 2.1014461186070472e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 47/71 | LOSS: 2.1019656287535327e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 48/71 | LOSS: 2.099506751359773e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 49/71 | LOSS: 2.0935856628057082e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 50/71 | LOSS: 2.0934136117313184e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 51/71 | LOSS: 2.0959949919275598e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 52/71 | LOSS: 2.0999148065764632e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 53/71 | LOSS: 2.1042658840155195e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 54/71 | LOSS: 2.101153966743203e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 55/71 | LOSS: 2.0897119952678622e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 56/71 | LOSS: 2.0875532786783624e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 57/71 | LOSS: 2.0852284674690274e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 58/71 | LOSS: 2.0834707695831964e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 59/71 | LOSS: 2.07970237624977e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 60/71 | LOSS: 2.0710593147311154e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 61/71 | LOSS: 2.0655592482310773e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 62/71 | LOSS: 2.0622241858550945e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 63/71 | LOSS: 2.061023413091334e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 64/71 | LOSS: 2.0566598812785323e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 65/71 | LOSS: 2.061771166347281e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 66/71 | LOSS: 2.0661533103996034e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 67/71 | LOSS: 2.0628253035765202e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 68/71 | LOSS: 2.0637851920965012e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 69/71 | LOSS: 2.06084459802826e-05\n",
      "TRAIN: EPOCH 62/1000 | BATCH 70/71 | LOSS: 2.0543683834622357e-05\n",
      "VAL: EPOCH 62/1000 | BATCH 0/8 | LOSS: 2.1597086742985994e-05\n",
      "VAL: EPOCH 62/1000 | BATCH 1/8 | LOSS: 2.099650191667024e-05\n",
      "VAL: EPOCH 62/1000 | BATCH 2/8 | LOSS: 2.1986216476458747e-05\n",
      "VAL: EPOCH 62/1000 | BATCH 3/8 | LOSS: 2.1704199298255844e-05\n",
      "VAL: EPOCH 62/1000 | BATCH 4/8 | LOSS: 2.090343004965689e-05\n",
      "VAL: EPOCH 62/1000 | BATCH 5/8 | LOSS: 2.0787172312945284e-05\n",
      "VAL: EPOCH 62/1000 | BATCH 6/8 | LOSS: 1.9958894102143988e-05\n",
      "VAL: EPOCH 62/1000 | BATCH 7/8 | LOSS: 1.947289695181098e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 0/71 | LOSS: 1.8145696230931208e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 1/71 | LOSS: 1.9009996321983635e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 2/71 | LOSS: 1.9159598499148462e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 3/71 | LOSS: 1.8305954654351808e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 4/71 | LOSS: 1.8022715084953234e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 5/71 | LOSS: 1.878320987695285e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 6/71 | LOSS: 1.9522235951236716e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 7/71 | LOSS: 1.9249415572630824e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 8/71 | LOSS: 1.9303202255590197e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 9/71 | LOSS: 1.92639279703144e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 10/71 | LOSS: 1.9260779398874465e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 11/71 | LOSS: 1.9157350986157933e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 12/71 | LOSS: 1.9629376625883968e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 13/71 | LOSS: 1.9644593911445035e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 14/71 | LOSS: 2.0726439834106715e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 15/71 | LOSS: 2.0771562276422628e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 16/71 | LOSS: 2.075037734945962e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 17/71 | LOSS: 2.12289522904838e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 18/71 | LOSS: 2.1171812668877753e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 19/71 | LOSS: 2.119109421983012e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 20/71 | LOSS: 2.100751477146765e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 21/71 | LOSS: 2.1006138013035525e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 22/71 | LOSS: 2.0876880270130325e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 23/71 | LOSS: 2.1322342414957045e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 24/71 | LOSS: 2.1411615089164117e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 25/71 | LOSS: 2.1312961717530226e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 26/71 | LOSS: 2.1180971560641882e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 27/71 | LOSS: 2.1045822190249703e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 28/71 | LOSS: 2.0943048090477282e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 29/71 | LOSS: 2.083591559009316e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 30/71 | LOSS: 2.0807836629119853e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 31/71 | LOSS: 2.1065018700028304e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 32/71 | LOSS: 2.1072254171348035e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 33/71 | LOSS: 2.10174846014364e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 34/71 | LOSS: 2.0891342137474568e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 35/71 | LOSS: 2.073848797105408e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 36/71 | LOSS: 2.069001631163111e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 37/71 | LOSS: 2.07003623041888e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 38/71 | LOSS: 2.069300372125163e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 39/71 | LOSS: 2.079631490232714e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 40/71 | LOSS: 2.0817108745133027e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 41/71 | LOSS: 2.080233878673642e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 42/71 | LOSS: 2.0727990992126315e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 43/71 | LOSS: 2.0786710386098342e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 44/71 | LOSS: 2.0682136689881896e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 45/71 | LOSS: 2.058650051881357e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 46/71 | LOSS: 2.0666732335156027e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 47/71 | LOSS: 2.0607276307297678e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 48/71 | LOSS: 2.061763148141873e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 49/71 | LOSS: 2.0604919336619786e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 50/71 | LOSS: 2.0532591602828936e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 51/71 | LOSS: 2.0609396415238734e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 52/71 | LOSS: 2.0642626382913528e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 53/71 | LOSS: 2.0598623181016324e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 54/71 | LOSS: 2.0573784951755607e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 55/71 | LOSS: 2.0513668070114882e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 56/71 | LOSS: 2.0529842865806514e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 57/71 | LOSS: 2.0590255503364083e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 58/71 | LOSS: 2.066389618957439e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 59/71 | LOSS: 2.0790988946828294e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 60/71 | LOSS: 2.079343939243938e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 61/71 | LOSS: 2.0809935420167618e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 62/71 | LOSS: 2.081365972925495e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 63/71 | LOSS: 2.0850403672056927e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 64/71 | LOSS: 2.0801802747882903e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 65/71 | LOSS: 2.0749604504310287e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 66/71 | LOSS: 2.0794773847038455e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 67/71 | LOSS: 2.0876963390037417e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 68/71 | LOSS: 2.0858006541527793e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 69/71 | LOSS: 2.0831600288927023e-05\n",
      "TRAIN: EPOCH 63/1000 | BATCH 70/71 | LOSS: 2.0747628017053217e-05\n",
      "VAL: EPOCH 63/1000 | BATCH 0/8 | LOSS: 2.3578297259518877e-05\n",
      "VAL: EPOCH 63/1000 | BATCH 1/8 | LOSS: 2.364916781516513e-05\n",
      "VAL: EPOCH 63/1000 | BATCH 2/8 | LOSS: 2.4619958518693846e-05\n",
      "VAL: EPOCH 63/1000 | BATCH 3/8 | LOSS: 2.3932862404763e-05\n",
      "VAL: EPOCH 63/1000 | BATCH 4/8 | LOSS: 2.2875289141666143e-05\n",
      "VAL: EPOCH 63/1000 | BATCH 5/8 | LOSS: 2.236556550390863e-05\n",
      "VAL: EPOCH 63/1000 | BATCH 6/8 | LOSS: 2.1708760739004773e-05\n",
      "VAL: EPOCH 63/1000 | BATCH 7/8 | LOSS: 2.1070412685730844e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 0/71 | LOSS: 2.6225930923828855e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 1/71 | LOSS: 2.4632267013657838e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 2/71 | LOSS: 2.4280425350298174e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 3/71 | LOSS: 2.16261501009285e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 4/71 | LOSS: 2.280988519487437e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 5/71 | LOSS: 2.316695796859373e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 6/71 | LOSS: 2.2726291198133758e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 7/71 | LOSS: 2.274742723784584e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 8/71 | LOSS: 2.26744040345592e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 9/71 | LOSS: 2.297665851074271e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 10/71 | LOSS: 2.2649535656620916e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 11/71 | LOSS: 2.255178469567909e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 12/71 | LOSS: 2.2320689962585813e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 13/71 | LOSS: 2.232106297534691e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 14/71 | LOSS: 2.2424782461409146e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 15/71 | LOSS: 2.21774811279829e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 16/71 | LOSS: 2.203136425711872e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 17/71 | LOSS: 2.1685553696847313e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 18/71 | LOSS: 2.1464515444485616e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 19/71 | LOSS: 2.127635216311319e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 20/71 | LOSS: 2.10843486240178e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 21/71 | LOSS: 2.099030726557513e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 22/71 | LOSS: 2.0901657436574485e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 23/71 | LOSS: 2.103148419034066e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 24/71 | LOSS: 2.1025881287641824e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 25/71 | LOSS: 2.1020926606769746e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 26/71 | LOSS: 2.0835722049620625e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 27/71 | LOSS: 2.0628187030524714e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 28/71 | LOSS: 2.0600720055317024e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 29/71 | LOSS: 2.0721047440019903e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 30/71 | LOSS: 2.0848203113869644e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 31/71 | LOSS: 2.078898251056671e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 32/71 | LOSS: 2.0692959548217583e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 33/71 | LOSS: 2.0655224836766993e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 34/71 | LOSS: 2.0692559025649513e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 35/71 | LOSS: 2.0634077625598162e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 36/71 | LOSS: 2.0637233811194654e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 37/71 | LOSS: 2.0594316945215197e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 38/71 | LOSS: 2.045755144745971e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 39/71 | LOSS: 2.0431210214155725e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 40/71 | LOSS: 2.0342246521400606e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 41/71 | LOSS: 2.0202908563944566e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 42/71 | LOSS: 2.0177778487566176e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 43/71 | LOSS: 2.0122546443830785e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 44/71 | LOSS: 2.0199372112175397e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 45/71 | LOSS: 2.0151409463996178e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 46/71 | LOSS: 2.017931870176789e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 47/71 | LOSS: 2.0096098126032302e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 48/71 | LOSS: 2.003133836204105e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 49/71 | LOSS: 1.991421378988889e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 50/71 | LOSS: 1.9779494929631863e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 51/71 | LOSS: 1.989478407547442e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 52/71 | LOSS: 1.9804455285270278e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 53/71 | LOSS: 1.976239349864449e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 54/71 | LOSS: 1.9759610090246e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 55/71 | LOSS: 1.9804716397889672e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 56/71 | LOSS: 1.9849301169048422e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 57/71 | LOSS: 1.9789099747742573e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 58/71 | LOSS: 1.9845795592981768e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 59/71 | LOSS: 1.9955116931669183e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 60/71 | LOSS: 1.9930394851367874e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 61/71 | LOSS: 1.988983779982288e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 62/71 | LOSS: 1.9808053005521837e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 63/71 | LOSS: 1.9831251933055682e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 64/71 | LOSS: 1.9852978193039935e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 65/71 | LOSS: 1.9885083791891564e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 66/71 | LOSS: 1.9826873241656166e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 67/71 | LOSS: 1.9869482430829737e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 68/71 | LOSS: 1.981009312578902e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 69/71 | LOSS: 1.9801649952049567e-05\n",
      "TRAIN: EPOCH 64/1000 | BATCH 70/71 | LOSS: 1.9799959875559633e-05\n",
      "VAL: EPOCH 64/1000 | BATCH 0/8 | LOSS: 1.9727736798813567e-05\n",
      "VAL: EPOCH 64/1000 | BATCH 1/8 | LOSS: 1.9696031813509762e-05\n",
      "VAL: EPOCH 64/1000 | BATCH 2/8 | LOSS: 2.1064792235847563e-05\n",
      "VAL: EPOCH 64/1000 | BATCH 3/8 | LOSS: 2.0941725779266562e-05\n",
      "VAL: EPOCH 64/1000 | BATCH 4/8 | LOSS: 2.028836461249739e-05\n",
      "VAL: EPOCH 64/1000 | BATCH 5/8 | LOSS: 2.017391792226893e-05\n",
      "VAL: EPOCH 64/1000 | BATCH 6/8 | LOSS: 1.9516379844779813e-05\n",
      "VAL: EPOCH 64/1000 | BATCH 7/8 | LOSS: 1.9110073708361597e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 0/71 | LOSS: 2.08054952963721e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 1/71 | LOSS: 1.768602169249789e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 2/71 | LOSS: 1.780444684603329e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 3/71 | LOSS: 1.8605444211061695e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 4/71 | LOSS: 1.8679419554246124e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 5/71 | LOSS: 1.878355017955376e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 6/71 | LOSS: 1.924560053469447e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 7/71 | LOSS: 1.9830388623631734e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 8/71 | LOSS: 1.9714534472667663e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 9/71 | LOSS: 2.0439745185285574e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 10/71 | LOSS: 2.0418004169187043e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 11/71 | LOSS: 1.99991137227092e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 12/71 | LOSS: 1.9905508941939424e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 13/71 | LOSS: 2.0055184060246184e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 14/71 | LOSS: 2.0129993754380848e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 15/71 | LOSS: 2.017539810594826e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 16/71 | LOSS: 2.02602441812119e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 17/71 | LOSS: 2.0027054688398493e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 18/71 | LOSS: 1.9915459520952456e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 19/71 | LOSS: 1.9907332580260116e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 20/71 | LOSS: 1.954823623929683e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 21/71 | LOSS: 1.9592910906463988e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 22/71 | LOSS: 1.9622733262899008e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 23/71 | LOSS: 1.948816005400052e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 24/71 | LOSS: 1.9508196637616492e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 25/71 | LOSS: 1.9452792912488803e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 26/71 | LOSS: 1.9336849719467056e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 27/71 | LOSS: 1.9138739649601382e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 28/71 | LOSS: 1.909366826645243e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 29/71 | LOSS: 1.920745853567496e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 30/71 | LOSS: 1.927859275676911e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 31/71 | LOSS: 1.936365373467197e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 32/71 | LOSS: 1.932177088526783e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 33/71 | LOSS: 1.94234035541954e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 34/71 | LOSS: 1.9295774732849428e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 35/71 | LOSS: 1.9280235089051228e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 36/71 | LOSS: 1.953823934977786e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 37/71 | LOSS: 1.944038194778841e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 38/71 | LOSS: 1.94382180500095e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 39/71 | LOSS: 1.938621526278439e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 40/71 | LOSS: 1.93794812065146e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 41/71 | LOSS: 1.935769541021381e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 42/71 | LOSS: 1.9341729522809973e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 43/71 | LOSS: 1.9318671671829758e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 44/71 | LOSS: 1.9382366614687876e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 45/71 | LOSS: 1.946982264717403e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 46/71 | LOSS: 1.9514387570373574e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 47/71 | LOSS: 1.9516717619202e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 48/71 | LOSS: 1.9468315202880612e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 49/71 | LOSS: 1.955294454091927e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 50/71 | LOSS: 1.9467256374899116e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 51/71 | LOSS: 1.9650272196696067e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 52/71 | LOSS: 1.968640812402503e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 53/71 | LOSS: 1.9612214466043816e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 54/71 | LOSS: 1.9535531332208352e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 55/71 | LOSS: 1.956978553607769e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 56/71 | LOSS: 1.954184054946445e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 57/71 | LOSS: 1.9437999291202968e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 58/71 | LOSS: 1.9521538718944906e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 59/71 | LOSS: 1.9567275709656922e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 60/71 | LOSS: 1.9578357363362096e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 61/71 | LOSS: 1.950948600067697e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 62/71 | LOSS: 1.946756593598467e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 63/71 | LOSS: 1.9482993934616388e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 64/71 | LOSS: 1.953532594668034e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 65/71 | LOSS: 1.955683772834935e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 66/71 | LOSS: 1.960809437259198e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 67/71 | LOSS: 1.9596477934957127e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 68/71 | LOSS: 1.956184565062648e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 69/71 | LOSS: 1.9565190190665557e-05\n",
      "TRAIN: EPOCH 65/1000 | BATCH 70/71 | LOSS: 1.9662983722297776e-05\n",
      "VAL: EPOCH 65/1000 | BATCH 0/8 | LOSS: 2.0127667085034773e-05\n",
      "VAL: EPOCH 65/1000 | BATCH 1/8 | LOSS: 1.9612873074947856e-05\n",
      "VAL: EPOCH 65/1000 | BATCH 2/8 | LOSS: 2.1233547158772126e-05\n",
      "VAL: EPOCH 65/1000 | BATCH 3/8 | LOSS: 2.1069070498924702e-05\n",
      "VAL: EPOCH 65/1000 | BATCH 4/8 | LOSS: 2.0296996081015094e-05\n",
      "VAL: EPOCH 65/1000 | BATCH 5/8 | LOSS: 1.9960055700115237e-05\n",
      "VAL: EPOCH 65/1000 | BATCH 6/8 | LOSS: 1.928366327774711e-05\n",
      "VAL: EPOCH 65/1000 | BATCH 7/8 | LOSS: 1.8670554595701105e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 0/71 | LOSS: 1.6158885046024807e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 1/71 | LOSS: 1.5534104022663087e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 2/71 | LOSS: 2.0084993593627587e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 3/71 | LOSS: 2.0594592569977976e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 4/71 | LOSS: 2.0204176689730956e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 5/71 | LOSS: 1.955946330175114e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 6/71 | LOSS: 1.8907076342397238e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 7/71 | LOSS: 1.9271573819423793e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 8/71 | LOSS: 1.958171883921346e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 9/71 | LOSS: 1.9797393724729774e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 10/71 | LOSS: 1.9569823879547503e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 11/71 | LOSS: 1.9448797653846366e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 12/71 | LOSS: 1.9818889576037272e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 13/71 | LOSS: 1.9832042718397653e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 14/71 | LOSS: 2.0078338881527694e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 15/71 | LOSS: 1.9841053358504723e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 16/71 | LOSS: 1.9914729444494963e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 17/71 | LOSS: 1.999493128500439e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 18/71 | LOSS: 1.99662175107637e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 19/71 | LOSS: 2.0003504505439196e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 20/71 | LOSS: 1.998347484221172e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 21/71 | LOSS: 2.0051056956369642e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 22/71 | LOSS: 2.0005299476273965e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 23/71 | LOSS: 1.987040946005436e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 24/71 | LOSS: 1.9932879440602845e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 25/71 | LOSS: 1.994902000800581e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 26/71 | LOSS: 2.0113081147967562e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 27/71 | LOSS: 2.0019670370986985e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 28/71 | LOSS: 1.9999807936356177e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 29/71 | LOSS: 2.003592277712111e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 30/71 | LOSS: 1.9962291903099077e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 31/71 | LOSS: 1.9859150654610858e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 32/71 | LOSS: 1.9781931934465486e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 33/71 | LOSS: 1.9805722706009608e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 34/71 | LOSS: 1.975009025565149e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 35/71 | LOSS: 1.9641608307412956e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 36/71 | LOSS: 1.9559112204021675e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 37/71 | LOSS: 1.9687351691812327e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 38/71 | LOSS: 1.9563183275003656e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 39/71 | LOSS: 1.9551891591618185e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 40/71 | LOSS: 1.9441965866085743e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 41/71 | LOSS: 1.9312020954419583e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 42/71 | LOSS: 1.9380495550014825e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 43/71 | LOSS: 1.9320117168965886e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 44/71 | LOSS: 1.930043122734383e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 45/71 | LOSS: 1.9224926271145588e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 46/71 | LOSS: 1.929642482362861e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 47/71 | LOSS: 1.92433802984245e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 48/71 | LOSS: 1.9203514688231327e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 49/71 | LOSS: 1.917445833896636e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 50/71 | LOSS: 1.9322410979857167e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 51/71 | LOSS: 1.927435066416085e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 52/71 | LOSS: 1.9265790625759797e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 53/71 | LOSS: 1.9250941932114074e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 54/71 | LOSS: 1.9178264317981136e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 55/71 | LOSS: 1.92208194351094e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 56/71 | LOSS: 1.9260955620363045e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 57/71 | LOSS: 1.9237902302722494e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 58/71 | LOSS: 1.9243737138606514e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 59/71 | LOSS: 1.9214689897732264e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 60/71 | LOSS: 1.91802051840121e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 61/71 | LOSS: 1.9186815851782974e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 62/71 | LOSS: 1.9213765838122482e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 63/71 | LOSS: 1.9127676182506548e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 64/71 | LOSS: 1.9104198369090087e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 65/71 | LOSS: 1.9055404471035935e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 66/71 | LOSS: 1.904257105571329e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 67/71 | LOSS: 1.9074724524911933e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 68/71 | LOSS: 1.9143317568586614e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 69/71 | LOSS: 1.909350500812122e-05\n",
      "TRAIN: EPOCH 66/1000 | BATCH 70/71 | LOSS: 1.9131617412146535e-05\n",
      "VAL: EPOCH 66/1000 | BATCH 0/8 | LOSS: 1.9972605514340103e-05\n",
      "VAL: EPOCH 66/1000 | BATCH 1/8 | LOSS: 1.9113086636934895e-05\n",
      "VAL: EPOCH 66/1000 | BATCH 2/8 | LOSS: 2.0698954661687214e-05\n",
      "VAL: EPOCH 66/1000 | BATCH 3/8 | LOSS: 2.083223716908833e-05\n",
      "VAL: EPOCH 66/1000 | BATCH 4/8 | LOSS: 2.0422347370185888e-05\n",
      "VAL: EPOCH 66/1000 | BATCH 5/8 | LOSS: 2.0924678816906333e-05\n",
      "VAL: EPOCH 66/1000 | BATCH 6/8 | LOSS: 2.0506393411778845e-05\n",
      "VAL: EPOCH 66/1000 | BATCH 7/8 | LOSS: 2.055213963103597e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 0/71 | LOSS: 2.418454096186906e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 1/71 | LOSS: 2.15962700167438e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 2/71 | LOSS: 1.916516915419682e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 3/71 | LOSS: 2.029961228799948e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 4/71 | LOSS: 1.981068871828029e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 5/71 | LOSS: 1.961105787510557e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 6/71 | LOSS: 1.9157949152161435e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 7/71 | LOSS: 1.9009122638635745e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 8/71 | LOSS: 1.944302888053547e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 9/71 | LOSS: 1.9368638550076867e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 10/71 | LOSS: 1.920546004508982e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 11/71 | LOSS: 1.9672103993192042e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 12/71 | LOSS: 1.9548385912807695e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 13/71 | LOSS: 1.956042115125456e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 14/71 | LOSS: 1.9904885084542913e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 15/71 | LOSS: 1.9726199127489963e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 16/71 | LOSS: 1.990599909785327e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 17/71 | LOSS: 1.9946281554439338e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 18/71 | LOSS: 1.973903277087773e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 19/71 | LOSS: 1.971115020751313e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 20/71 | LOSS: 1.9718095158861527e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 21/71 | LOSS: 1.9720204156990142e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 22/71 | LOSS: 1.9729791900950342e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 23/71 | LOSS: 1.9736112600791483e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 24/71 | LOSS: 1.9832241487165448e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 25/71 | LOSS: 1.9966179129719072e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 26/71 | LOSS: 1.9847297321744178e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 27/71 | LOSS: 1.9760262180820325e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 28/71 | LOSS: 1.9703823893686257e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 29/71 | LOSS: 1.9719149501421878e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 30/71 | LOSS: 1.965795534840342e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 31/71 | LOSS: 1.950826816710105e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 32/71 | LOSS: 1.938477731583231e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 33/71 | LOSS: 1.924988699850955e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 34/71 | LOSS: 1.9170549473658737e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 35/71 | LOSS: 1.9146100587224486e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 36/71 | LOSS: 1.9171561812195374e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 37/71 | LOSS: 1.9126413941481396e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 38/71 | LOSS: 1.9114721511962274e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 39/71 | LOSS: 1.90856383596838e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 40/71 | LOSS: 1.9068715252016283e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 41/71 | LOSS: 1.9183879019346067e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 42/71 | LOSS: 1.9202401618226865e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 43/71 | LOSS: 1.920930096523741e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 44/71 | LOSS: 1.9236072249542405e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 45/71 | LOSS: 1.9281003836152628e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 46/71 | LOSS: 1.9361275027473855e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 47/71 | LOSS: 1.929789395186769e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 48/71 | LOSS: 1.923430306992579e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 49/71 | LOSS: 1.9252887832408304e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 50/71 | LOSS: 1.923580170756288e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 51/71 | LOSS: 1.9201108863433965e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 52/71 | LOSS: 1.9195861002443497e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 53/71 | LOSS: 1.929739887095315e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 54/71 | LOSS: 1.9329722272232174e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 55/71 | LOSS: 1.932971177406476e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 56/71 | LOSS: 1.936099805900819e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 57/71 | LOSS: 1.9452162005422748e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 58/71 | LOSS: 1.934930705983326e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 59/71 | LOSS: 1.9314804376335815e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 60/71 | LOSS: 1.9314490377017464e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 61/71 | LOSS: 1.9246028709711673e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 62/71 | LOSS: 1.916373158954749e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 63/71 | LOSS: 1.913726242719349e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 64/71 | LOSS: 1.914218877433226e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 65/71 | LOSS: 1.9140364091788687e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 66/71 | LOSS: 1.913213513469247e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 67/71 | LOSS: 1.914102832041167e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 68/71 | LOSS: 1.9116158369613096e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 69/71 | LOSS: 1.9104433912226732e-05\n",
      "TRAIN: EPOCH 67/1000 | BATCH 70/71 | LOSS: 1.908108393942878e-05\n",
      "VAL: EPOCH 67/1000 | BATCH 0/8 | LOSS: 2.4833661882439628e-05\n",
      "VAL: EPOCH 67/1000 | BATCH 1/8 | LOSS: 2.526426123949932e-05\n",
      "VAL: EPOCH 67/1000 | BATCH 2/8 | LOSS: 2.6490401675497804e-05\n",
      "VAL: EPOCH 67/1000 | BATCH 3/8 | LOSS: 2.572087987573468e-05\n",
      "VAL: EPOCH 67/1000 | BATCH 4/8 | LOSS: 2.4582475089118815e-05\n",
      "VAL: EPOCH 67/1000 | BATCH 5/8 | LOSS: 2.3962895890387397e-05\n",
      "VAL: EPOCH 67/1000 | BATCH 6/8 | LOSS: 2.336130533616857e-05\n",
      "VAL: EPOCH 67/1000 | BATCH 7/8 | LOSS: 2.2407328515328118e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 0/71 | LOSS: 2.5053621357074007e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 1/71 | LOSS: 2.0858366042375565e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 2/71 | LOSS: 2.0335060374539655e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 3/71 | LOSS: 1.980299884962733e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 4/71 | LOSS: 2.0326183948782274e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 5/71 | LOSS: 2.023799879680155e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 6/71 | LOSS: 2.0368741388665512e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 7/71 | LOSS: 2.0510096419457113e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 8/71 | LOSS: 2.0907690971701715e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 9/71 | LOSS: 2.1240856949589215e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 10/71 | LOSS: 2.0928507596677676e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 11/71 | LOSS: 2.075188164477974e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 12/71 | LOSS: 2.057043429186496e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 13/71 | LOSS: 2.03539826283564e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 14/71 | LOSS: 2.0015027015081917e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 15/71 | LOSS: 1.994281649331242e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 16/71 | LOSS: 1.9933128415312455e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 17/71 | LOSS: 1.982366332716386e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 18/71 | LOSS: 1.9810569842535953e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 19/71 | LOSS: 1.998761408685823e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 20/71 | LOSS: 1.9999547605818536e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 21/71 | LOSS: 1.9879075476968534e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 22/71 | LOSS: 1.967561705817428e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 23/71 | LOSS: 1.985119479286368e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 24/71 | LOSS: 1.972227721125819e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 25/71 | LOSS: 1.972336604712137e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 26/71 | LOSS: 1.9667195980825152e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 27/71 | LOSS: 1.9539966160664335e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 28/71 | LOSS: 1.9566294087843713e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 29/71 | LOSS: 1.9453728661270968e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 30/71 | LOSS: 1.9402439999882312e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 31/71 | LOSS: 1.932775757040872e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 32/71 | LOSS: 1.9276463827929657e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 33/71 | LOSS: 1.9213110189251465e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 34/71 | LOSS: 1.9172903090033547e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 35/71 | LOSS: 1.9167405096393646e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 36/71 | LOSS: 1.910359984518633e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 37/71 | LOSS: 1.9058138269955587e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 38/71 | LOSS: 1.9163058310275515e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 39/71 | LOSS: 1.9156589360136422e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 40/71 | LOSS: 1.9101284878405105e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 41/71 | LOSS: 1.90273170422491e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 42/71 | LOSS: 1.9007322449862502e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 43/71 | LOSS: 1.899984417005643e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 44/71 | LOSS: 1.8917718585321887e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 45/71 | LOSS: 1.8810784715005074e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 46/71 | LOSS: 1.8781148258874075e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 47/71 | LOSS: 1.873766063908988e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 48/71 | LOSS: 1.866056105167705e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 49/71 | LOSS: 1.857096833191463e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 50/71 | LOSS: 1.8641667178171166e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 51/71 | LOSS: 1.85756135369257e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 52/71 | LOSS: 1.852826120876619e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 53/71 | LOSS: 1.8557157663626304e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 54/71 | LOSS: 1.8491682402038183e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 55/71 | LOSS: 1.8469440906950956e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 56/71 | LOSS: 1.8419128409413244e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 57/71 | LOSS: 1.8383899636131084e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 58/71 | LOSS: 1.8391160249277966e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 59/71 | LOSS: 1.842233235341458e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 60/71 | LOSS: 1.8409105935267583e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 61/71 | LOSS: 1.83956494325802e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 62/71 | LOSS: 1.8524367092934895e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 63/71 | LOSS: 1.8506301387333224e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 64/71 | LOSS: 1.8538055491472522e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 65/71 | LOSS: 1.8593856690462086e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 66/71 | LOSS: 1.8561405137115256e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 67/71 | LOSS: 1.8459319455680944e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 68/71 | LOSS: 1.8464439299482297e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 69/71 | LOSS: 1.8462933933603512e-05\n",
      "TRAIN: EPOCH 68/1000 | BATCH 70/71 | LOSS: 1.8407737218400196e-05\n",
      "VAL: EPOCH 68/1000 | BATCH 0/8 | LOSS: 1.7306221707258373e-05\n",
      "VAL: EPOCH 68/1000 | BATCH 1/8 | LOSS: 1.6843260709720198e-05\n",
      "VAL: EPOCH 68/1000 | BATCH 2/8 | LOSS: 1.852958848758135e-05\n",
      "VAL: EPOCH 68/1000 | BATCH 3/8 | LOSS: 1.8939511846838286e-05\n",
      "VAL: EPOCH 68/1000 | BATCH 4/8 | LOSS: 1.8544935664976946e-05\n",
      "VAL: EPOCH 68/1000 | BATCH 5/8 | LOSS: 1.8811918986709013e-05\n",
      "VAL: EPOCH 68/1000 | BATCH 6/8 | LOSS: 1.8309323942438433e-05\n",
      "VAL: EPOCH 68/1000 | BATCH 7/8 | LOSS: 1.8013473663813784e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 0/71 | LOSS: 1.7007492715492845e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 1/71 | LOSS: 1.6498519471497275e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 2/71 | LOSS: 1.6542903419273596e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 3/71 | LOSS: 1.602445718162926e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 4/71 | LOSS: 1.531889247416984e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 5/71 | LOSS: 1.5862421605561394e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 6/71 | LOSS: 1.5711438339037288e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 7/71 | LOSS: 1.557754649184062e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 8/71 | LOSS: 1.5558647444575196e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 9/71 | LOSS: 1.5713038919784594e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 10/71 | LOSS: 1.60660820256453e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 11/71 | LOSS: 1.6078945160794927e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 12/71 | LOSS: 1.6109332556460195e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 13/71 | LOSS: 1.5904006301882744e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 14/71 | LOSS: 1.61829719824406e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 15/71 | LOSS: 1.6112313801386335e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 16/71 | LOSS: 1.6369700995924443e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 17/71 | LOSS: 1.637441538251652e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 18/71 | LOSS: 1.6441106804689442e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 19/71 | LOSS: 1.6697216869943078e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 20/71 | LOSS: 1.6706597307347693e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 21/71 | LOSS: 1.6718429254648402e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 22/71 | LOSS: 1.6623252478261154e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 23/71 | LOSS: 1.660859265939507e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 24/71 | LOSS: 1.672168837103527e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 25/71 | LOSS: 1.6995253365446562e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 26/71 | LOSS: 1.708401524175079e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 27/71 | LOSS: 1.709648950054543e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 28/71 | LOSS: 1.7132556704695914e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 29/71 | LOSS: 1.6924506780924276e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 30/71 | LOSS: 1.712829051088662e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 31/71 | LOSS: 1.712312655399728e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 32/71 | LOSS: 1.6962423339436732e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 33/71 | LOSS: 1.7065244131064153e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 34/71 | LOSS: 1.6933512727389045e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 35/71 | LOSS: 1.7026547311616014e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 36/71 | LOSS: 1.702570392120145e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 37/71 | LOSS: 1.7134734446843043e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 38/71 | LOSS: 1.70881035396283e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 39/71 | LOSS: 1.7319704534202172e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 40/71 | LOSS: 1.7352119446114535e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 41/71 | LOSS: 1.7306057673392517e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 42/71 | LOSS: 1.722528736007557e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 43/71 | LOSS: 1.738258273772689e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 44/71 | LOSS: 1.7454205084909012e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 45/71 | LOSS: 1.7589084900069082e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 46/71 | LOSS: 1.765922282155519e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 47/71 | LOSS: 1.7662301331711205e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 48/71 | LOSS: 1.7685146025430687e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 49/71 | LOSS: 1.7716023157845485e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 50/71 | LOSS: 1.7788805784149708e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 51/71 | LOSS: 1.7875735658470567e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 52/71 | LOSS: 1.795054123369551e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 53/71 | LOSS: 1.7938794388572677e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 54/71 | LOSS: 1.7989864516659344e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 55/71 | LOSS: 1.795080595456966e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 56/71 | LOSS: 1.7962185841481603e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 57/71 | LOSS: 1.808129172786721e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 58/71 | LOSS: 1.8124520835753776e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 59/71 | LOSS: 1.8047854685695103e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 60/71 | LOSS: 1.803660322112875e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 61/71 | LOSS: 1.8097175202016052e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 62/71 | LOSS: 1.817048575056048e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 63/71 | LOSS: 1.8143823425020855e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 64/71 | LOSS: 1.8098986975718827e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 65/71 | LOSS: 1.80769807781017e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 66/71 | LOSS: 1.812319542703118e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 67/71 | LOSS: 1.8142912093775094e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 68/71 | LOSS: 1.8174260599218964e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 69/71 | LOSS: 1.8146566188599015e-05\n",
      "TRAIN: EPOCH 69/1000 | BATCH 70/71 | LOSS: 1.815334570905256e-05\n",
      "VAL: EPOCH 69/1000 | BATCH 0/8 | LOSS: 1.77275542228017e-05\n",
      "VAL: EPOCH 69/1000 | BATCH 1/8 | LOSS: 1.7296070836891886e-05\n",
      "VAL: EPOCH 69/1000 | BATCH 2/8 | LOSS: 1.866170411328009e-05\n",
      "VAL: EPOCH 69/1000 | BATCH 3/8 | LOSS: 1.8642416762304492e-05\n",
      "VAL: EPOCH 69/1000 | BATCH 4/8 | LOSS: 1.8064472169498912e-05\n",
      "VAL: EPOCH 69/1000 | BATCH 5/8 | LOSS: 1.817692312518678e-05\n",
      "VAL: EPOCH 69/1000 | BATCH 6/8 | LOSS: 1.7615808766484925e-05\n",
      "VAL: EPOCH 69/1000 | BATCH 7/8 | LOSS: 1.7335408983853995e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 0/71 | LOSS: 1.5993618944776244e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 1/71 | LOSS: 1.5656091818527784e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 2/71 | LOSS: 1.6825018368156936e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 3/71 | LOSS: 1.70300158970349e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 4/71 | LOSS: 1.830311994126532e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 5/71 | LOSS: 1.908736218562505e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 6/71 | LOSS: 1.864772769165159e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 7/71 | LOSS: 1.8501719750929624e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 8/71 | LOSS: 1.9095280448608617e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 9/71 | LOSS: 1.8452580206940183e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 10/71 | LOSS: 1.8193577075594064e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 11/71 | LOSS: 1.803554118851025e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 12/71 | LOSS: 1.8081279029921952e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 13/71 | LOSS: 1.7883835230999727e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 14/71 | LOSS: 1.7848294919531327e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 15/71 | LOSS: 1.7731241371166107e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 16/71 | LOSS: 1.754074946802575e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 17/71 | LOSS: 1.765052618591451e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 18/71 | LOSS: 1.7704381350899655e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 19/71 | LOSS: 1.762594720275956e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 20/71 | LOSS: 1.755451514673907e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 21/71 | LOSS: 1.741595402067601e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 22/71 | LOSS: 1.7346110839950953e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 23/71 | LOSS: 1.7401818126927537e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 24/71 | LOSS: 1.747565849655075e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 25/71 | LOSS: 1.7614915157957093e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 26/71 | LOSS: 1.7620876591266738e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 27/71 | LOSS: 1.7580429195212283e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 28/71 | LOSS: 1.7536687319706886e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 29/71 | LOSS: 1.769723630786757e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 30/71 | LOSS: 1.7745177736346264e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 31/71 | LOSS: 1.767853538581221e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 32/71 | LOSS: 1.7770442492760733e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 33/71 | LOSS: 1.785477570592775e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 34/71 | LOSS: 1.7830748324091214e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 35/71 | LOSS: 1.7872296186598964e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 36/71 | LOSS: 1.7824664044727233e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 37/71 | LOSS: 1.7926487209096147e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 38/71 | LOSS: 1.802025683271215e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 39/71 | LOSS: 1.8010863664130737e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 40/71 | LOSS: 1.7934296654236344e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 41/71 | LOSS: 1.7829793857041903e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 42/71 | LOSS: 1.788580981198276e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 43/71 | LOSS: 1.7803765786084114e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 44/71 | LOSS: 1.7958102600661934e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 45/71 | LOSS: 1.795496317082206e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 46/71 | LOSS: 1.8065593355865358e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 47/71 | LOSS: 1.799740925889637e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 48/71 | LOSS: 1.7921620012054278e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 49/71 | LOSS: 1.7894909797178114e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 50/71 | LOSS: 1.7908293539578297e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 51/71 | LOSS: 1.788480578677711e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 52/71 | LOSS: 1.786847462666766e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 53/71 | LOSS: 1.7807653438029337e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 54/71 | LOSS: 1.776263897309863e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 55/71 | LOSS: 1.774405026415999e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 56/71 | LOSS: 1.7634207500107057e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 57/71 | LOSS: 1.7661747404355114e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 58/71 | LOSS: 1.7612484043638688e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 59/71 | LOSS: 1.7618440521497782e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 60/71 | LOSS: 1.7657684588439374e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 61/71 | LOSS: 1.7720514631125092e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 62/71 | LOSS: 1.77296181550085e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 63/71 | LOSS: 1.781128132449794e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 64/71 | LOSS: 1.7807943745887873e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 65/71 | LOSS: 1.78279811927606e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 66/71 | LOSS: 1.7852523423746096e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 67/71 | LOSS: 1.7919492660546574e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 68/71 | LOSS: 1.795638450014361e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 69/71 | LOSS: 1.7922012882211545e-05\n",
      "TRAIN: EPOCH 70/1000 | BATCH 70/71 | LOSS: 1.7989493193146675e-05\n",
      "VAL: EPOCH 70/1000 | BATCH 0/8 | LOSS: 1.9741151845664717e-05\n",
      "VAL: EPOCH 70/1000 | BATCH 1/8 | LOSS: 1.9762769625231158e-05\n",
      "VAL: EPOCH 70/1000 | BATCH 2/8 | LOSS: 2.032368926544829e-05\n",
      "VAL: EPOCH 70/1000 | BATCH 3/8 | LOSS: 2.011208698604605e-05\n",
      "VAL: EPOCH 70/1000 | BATCH 4/8 | LOSS: 1.9249278921051882e-05\n",
      "VAL: EPOCH 70/1000 | BATCH 5/8 | LOSS: 1.914816190643857e-05\n",
      "VAL: EPOCH 70/1000 | BATCH 6/8 | LOSS: 1.860822833675359e-05\n",
      "VAL: EPOCH 70/1000 | BATCH 7/8 | LOSS: 1.8014719557868375e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 0/71 | LOSS: 1.5518251530011185e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 1/71 | LOSS: 1.5396755770780146e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 2/71 | LOSS: 1.5872635534227204e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 3/71 | LOSS: 1.57575955199718e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 4/71 | LOSS: 1.572900582686998e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 5/71 | LOSS: 1.623054170825829e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 6/71 | LOSS: 1.6090623310966684e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 7/71 | LOSS: 1.5656929576834955e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 8/71 | LOSS: 1.5604032543908237e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 9/71 | LOSS: 1.6300602237606653e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 10/71 | LOSS: 1.6730914963277016e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 11/71 | LOSS: 1.648590515893981e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 12/71 | LOSS: 1.648093858910635e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 13/71 | LOSS: 1.7051875862047643e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 14/71 | LOSS: 1.69610859908668e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 15/71 | LOSS: 1.6878905000794475e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 16/71 | LOSS: 1.6879464804814607e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 17/71 | LOSS: 1.6797367556541252e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 18/71 | LOSS: 1.6643615230063197e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 19/71 | LOSS: 1.6491548740305008e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 20/71 | LOSS: 1.653264440565614e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 21/71 | LOSS: 1.6612269725804005e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 22/71 | LOSS: 1.6518395252363838e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 23/71 | LOSS: 1.6620190460040856e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 24/71 | LOSS: 1.66218926460715e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 25/71 | LOSS: 1.6702339123115122e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 26/71 | LOSS: 1.6679784123509847e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 27/71 | LOSS: 1.6805118320917245e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 28/71 | LOSS: 1.677691561153865e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 29/71 | LOSS: 1.6867538640023365e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 30/71 | LOSS: 1.6902119598178673e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 31/71 | LOSS: 1.6858872413649806e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 32/71 | LOSS: 1.684425925222141e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 33/71 | LOSS: 1.683567300942206e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 34/71 | LOSS: 1.69361598506969e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 35/71 | LOSS: 1.6943759823738623e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 36/71 | LOSS: 1.690265295061132e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 37/71 | LOSS: 1.701448021968214e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 38/71 | LOSS: 1.6935451920009935e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 39/71 | LOSS: 1.6919743711696355e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 40/71 | LOSS: 1.695503408150327e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 41/71 | LOSS: 1.69830581009072e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 42/71 | LOSS: 1.6960655557340942e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 43/71 | LOSS: 1.6943280611898942e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 44/71 | LOSS: 1.7088184540625663e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 45/71 | LOSS: 1.709726834435613e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 46/71 | LOSS: 1.70300865763838e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 47/71 | LOSS: 1.71960705301899e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 48/71 | LOSS: 1.727232227828924e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 49/71 | LOSS: 1.7329745496681425e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 50/71 | LOSS: 1.7364709314207693e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 51/71 | LOSS: 1.7366062435870004e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 52/71 | LOSS: 1.7479945388501773e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 53/71 | LOSS: 1.7506778864049852e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 54/71 | LOSS: 1.7531930015221324e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 55/71 | LOSS: 1.7568243915125743e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 56/71 | LOSS: 1.750413353045332e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 57/71 | LOSS: 1.7491083248199267e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 58/71 | LOSS: 1.749335169117361e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 59/71 | LOSS: 1.7669606192309097e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 60/71 | LOSS: 1.764325420257681e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 61/71 | LOSS: 1.768286705144771e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 62/71 | LOSS: 1.7645516616915027e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 63/71 | LOSS: 1.7687413134126473e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 64/71 | LOSS: 1.770956427767837e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 65/71 | LOSS: 1.77415454207662e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 66/71 | LOSS: 1.7779773390511355e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 67/71 | LOSS: 1.7777307274467175e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 68/71 | LOSS: 1.7822476207663346e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 69/71 | LOSS: 1.785407829239765e-05\n",
      "TRAIN: EPOCH 71/1000 | BATCH 70/71 | LOSS: 1.7822922120718484e-05\n",
      "VAL: EPOCH 71/1000 | BATCH 0/8 | LOSS: 2.4192078853957355e-05\n",
      "VAL: EPOCH 71/1000 | BATCH 1/8 | LOSS: 2.4966515411506407e-05\n",
      "VAL: EPOCH 71/1000 | BATCH 2/8 | LOSS: 2.519937334000133e-05\n",
      "VAL: EPOCH 71/1000 | BATCH 3/8 | LOSS: 2.43057802435942e-05\n",
      "VAL: EPOCH 71/1000 | BATCH 4/8 | LOSS: 2.3340947518590837e-05\n",
      "VAL: EPOCH 71/1000 | BATCH 5/8 | LOSS: 2.2908976461621933e-05\n",
      "VAL: EPOCH 71/1000 | BATCH 6/8 | LOSS: 2.2440935936174355e-05\n",
      "VAL: EPOCH 71/1000 | BATCH 7/8 | LOSS: 2.1809246391057968e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 0/71 | LOSS: 2.612969728943426e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 1/71 | LOSS: 2.459489587636199e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 2/71 | LOSS: 2.138910804205807e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 3/71 | LOSS: 2.08887411190517e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 4/71 | LOSS: 2.1270129036565776e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 5/71 | LOSS: 2.080025130150413e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 6/71 | LOSS: 2.042887834769707e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 7/71 | LOSS: 1.9651089019134815e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 8/71 | LOSS: 1.9347944645333984e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 9/71 | LOSS: 1.923354384416598e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 10/71 | LOSS: 1.9461975088316567e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 11/71 | LOSS: 1.9154750665014337e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 12/71 | LOSS: 1.9403310034249444e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 13/71 | LOSS: 1.9406287265155697e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 14/71 | LOSS: 1.911836267633286e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 15/71 | LOSS: 1.9218784188979043e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 16/71 | LOSS: 1.8972248279383432e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 17/71 | LOSS: 1.8712762893200204e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 18/71 | LOSS: 1.8547278686080398e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 19/71 | LOSS: 1.8387268983133254e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 20/71 | LOSS: 1.8530509026147358e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 21/71 | LOSS: 1.8324275226239646e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 22/71 | LOSS: 1.8520900005608816e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 23/71 | LOSS: 1.83864448975631e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 24/71 | LOSS: 1.8208123183285352e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 25/71 | LOSS: 1.824762830437976e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 26/71 | LOSS: 1.815502410964756e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 27/71 | LOSS: 1.8117762627818073e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 28/71 | LOSS: 1.7975059751639993e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 29/71 | LOSS: 1.8159544530741793e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 30/71 | LOSS: 1.801023559129399e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 31/71 | LOSS: 1.8126552475905555e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 32/71 | LOSS: 1.8100504458936594e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 33/71 | LOSS: 1.8073894165461432e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 34/71 | LOSS: 1.8054932791398773e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 35/71 | LOSS: 1.8119983476733774e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 36/71 | LOSS: 1.8076315132635205e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 37/71 | LOSS: 1.8021940784245472e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 38/71 | LOSS: 1.8010993232728186e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 39/71 | LOSS: 1.81071172164593e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 40/71 | LOSS: 1.8110741842885943e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 41/71 | LOSS: 1.8133905563272058e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 42/71 | LOSS: 1.8023454157268725e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 43/71 | LOSS: 1.800458694974209e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 44/71 | LOSS: 1.809080048486875e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 45/71 | LOSS: 1.809846662581549e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 46/71 | LOSS: 1.8101441902275682e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 47/71 | LOSS: 1.8138046338359953e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 48/71 | LOSS: 1.8106289625931672e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 49/71 | LOSS: 1.8132369823433692e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 50/71 | LOSS: 1.815539495759831e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 51/71 | LOSS: 1.8073362490702912e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 52/71 | LOSS: 1.806539744135972e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 53/71 | LOSS: 1.8023179766007364e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 54/71 | LOSS: 1.7994785081431144e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 55/71 | LOSS: 1.7936940496708432e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 56/71 | LOSS: 1.7941309411789436e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 57/71 | LOSS: 1.7993999226119143e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 58/71 | LOSS: 1.8030546154970917e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 59/71 | LOSS: 1.80388924794291e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 60/71 | LOSS: 1.8002465456775693e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 61/71 | LOSS: 1.8040462999769878e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 62/71 | LOSS: 1.8067502034162836e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 63/71 | LOSS: 1.8107048902038514e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 64/71 | LOSS: 1.805616187854097e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 65/71 | LOSS: 1.8038876712098222e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 66/71 | LOSS: 1.79824131946453e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 67/71 | LOSS: 1.8015565662126927e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 68/71 | LOSS: 1.8019343730449453e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 69/71 | LOSS: 1.802134794551031e-05\n",
      "TRAIN: EPOCH 72/1000 | BATCH 70/71 | LOSS: 1.8110351691216813e-05\n",
      "VAL: EPOCH 72/1000 | BATCH 0/8 | LOSS: 2.0578012481564656e-05\n",
      "VAL: EPOCH 72/1000 | BATCH 1/8 | LOSS: 2.09874524443876e-05\n",
      "VAL: EPOCH 72/1000 | BATCH 2/8 | LOSS: 2.222595382287788e-05\n",
      "VAL: EPOCH 72/1000 | BATCH 3/8 | LOSS: 2.231976031907834e-05\n",
      "VAL: EPOCH 72/1000 | BATCH 4/8 | LOSS: 2.1299233412719332e-05\n",
      "VAL: EPOCH 72/1000 | BATCH 5/8 | LOSS: 2.0791383576579392e-05\n",
      "VAL: EPOCH 72/1000 | BATCH 6/8 | LOSS: 2.0280632140514043e-05\n",
      "VAL: EPOCH 72/1000 | BATCH 7/8 | LOSS: 1.9518766521287034e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 0/71 | LOSS: 2.0263882106519304e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 1/71 | LOSS: 1.9640620848804247e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 2/71 | LOSS: 1.8140322936233133e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 3/71 | LOSS: 1.8092463506036438e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 4/71 | LOSS: 1.8222725702798926e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 5/71 | LOSS: 1.795434218365699e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 6/71 | LOSS: 1.748597865766247e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 7/71 | LOSS: 1.7192373320540355e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 8/71 | LOSS: 1.7680438456106156e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 9/71 | LOSS: 1.745833806126029e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 10/71 | LOSS: 1.7874042979118794e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 11/71 | LOSS: 1.8173397544766583e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 12/71 | LOSS: 1.790479599059416e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 13/71 | LOSS: 1.7870559661657482e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 14/71 | LOSS: 1.7813243296889898e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 15/71 | LOSS: 1.7826086491368187e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 16/71 | LOSS: 1.8194336863577037e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 17/71 | LOSS: 1.829553932313704e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 18/71 | LOSS: 1.8172034335867992e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 19/71 | LOSS: 1.818011960494914e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 20/71 | LOSS: 1.7849550550592886e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 21/71 | LOSS: 1.7657276368316328e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 22/71 | LOSS: 1.754561783499135e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 23/71 | LOSS: 1.7474356316900714e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 24/71 | LOSS: 1.7687773470242975e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 25/71 | LOSS: 1.7705635995158584e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 26/71 | LOSS: 1.7810795051681687e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 27/71 | LOSS: 1.7768650877769687e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 28/71 | LOSS: 1.7691793361556284e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 29/71 | LOSS: 1.7738544647727396e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 30/71 | LOSS: 1.7839777155454465e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 31/71 | LOSS: 1.7745317563822027e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 32/71 | LOSS: 1.7801105334938533e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 33/71 | LOSS: 1.7958770887857743e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 34/71 | LOSS: 1.790870020548547e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 35/71 | LOSS: 1.7843989856676974e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 36/71 | LOSS: 1.788878334745941e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 37/71 | LOSS: 1.797083496057894e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 38/71 | LOSS: 1.7949720737912383e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 39/71 | LOSS: 1.7962687252293108e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 40/71 | LOSS: 1.804150127920765e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 41/71 | LOSS: 1.8056502735167408e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 42/71 | LOSS: 1.804758852982601e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 43/71 | LOSS: 1.8040834427004235e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 44/71 | LOSS: 1.8106048365653905e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 45/71 | LOSS: 1.8036707071745617e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 46/71 | LOSS: 1.7992141422044058e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 47/71 | LOSS: 1.7996649736081356e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 48/71 | LOSS: 1.7932105019637525e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 49/71 | LOSS: 1.790100412108586e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 50/71 | LOSS: 1.786045602642059e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 51/71 | LOSS: 1.7848672699144048e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 52/71 | LOSS: 1.7891149856847545e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 53/71 | LOSS: 1.7860076544214582e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 54/71 | LOSS: 1.795328166322592e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 55/71 | LOSS: 1.7927821042056686e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 56/71 | LOSS: 1.7867770079567236e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 57/71 | LOSS: 1.7784729999252234e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 58/71 | LOSS: 1.7769048500374963e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 59/71 | LOSS: 1.7839240551135543e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 60/71 | LOSS: 1.7816977158306188e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 61/71 | LOSS: 1.7817753257165144e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 62/71 | LOSS: 1.7799722006621157e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 63/71 | LOSS: 1.7817082195392686e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 64/71 | LOSS: 1.783131221880187e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 65/71 | LOSS: 1.7782095614110762e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 66/71 | LOSS: 1.773557049552858e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 67/71 | LOSS: 1.76807722362034e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 68/71 | LOSS: 1.7708694189344026e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 69/71 | LOSS: 1.7758673324611404e-05\n",
      "TRAIN: EPOCH 73/1000 | BATCH 70/71 | LOSS: 1.7730944780620557e-05\n",
      "VAL: EPOCH 73/1000 | BATCH 0/8 | LOSS: 1.8596609152154997e-05\n",
      "VAL: EPOCH 73/1000 | BATCH 1/8 | LOSS: 1.8189246475230902e-05\n",
      "VAL: EPOCH 73/1000 | BATCH 2/8 | LOSS: 2.0240364998850662e-05\n",
      "VAL: EPOCH 73/1000 | BATCH 3/8 | LOSS: 2.1011483113397844e-05\n",
      "VAL: EPOCH 73/1000 | BATCH 4/8 | LOSS: 2.0984114235034214e-05\n",
      "VAL: EPOCH 73/1000 | BATCH 5/8 | LOSS: 2.1729110206554953e-05\n",
      "VAL: EPOCH 73/1000 | BATCH 6/8 | LOSS: 2.132340003819471e-05\n",
      "VAL: EPOCH 73/1000 | BATCH 7/8 | LOSS: 2.1395438352556084e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 0/71 | LOSS: 2.3113152565201744e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 1/71 | LOSS: 2.0600003153958824e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 2/71 | LOSS: 1.972514170726451e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 3/71 | LOSS: 1.9155146219418384e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 4/71 | LOSS: 1.8309615006728565e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 5/71 | LOSS: 1.8903005942168722e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 6/71 | LOSS: 1.821935516740528e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 7/71 | LOSS: 1.8182761095886235e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 8/71 | LOSS: 1.8409387242475834e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 9/71 | LOSS: 1.828759104682831e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 10/71 | LOSS: 1.7923712551816028e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 11/71 | LOSS: 1.790835813153535e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 12/71 | LOSS: 1.809487786466399e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 13/71 | LOSS: 1.8142144784048598e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 14/71 | LOSS: 1.808792488494267e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 15/71 | LOSS: 1.7794267478166148e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 16/71 | LOSS: 1.7764543051324674e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 17/71 | LOSS: 1.750165994154587e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 18/71 | LOSS: 1.7589154738647938e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 19/71 | LOSS: 1.7858941419035546e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 20/71 | LOSS: 1.788833248740827e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 21/71 | LOSS: 1.7950826283843277e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 22/71 | LOSS: 1.7946803382073995e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 23/71 | LOSS: 1.7801573108044977e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 24/71 | LOSS: 1.7696246977720876e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 25/71 | LOSS: 1.7619925144502373e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 26/71 | LOSS: 1.7790903055154355e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 27/71 | LOSS: 1.787673958071017e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 28/71 | LOSS: 1.7935064251701236e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 29/71 | LOSS: 1.802958143647023e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 30/71 | LOSS: 1.789781908296244e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 31/71 | LOSS: 1.782188741117352e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 32/71 | LOSS: 1.7847117774319045e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 33/71 | LOSS: 1.7851482719860916e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 34/71 | LOSS: 1.765383276506327e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 35/71 | LOSS: 1.7784085078649998e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 36/71 | LOSS: 1.7918065733848944e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 37/71 | LOSS: 1.785582753655035e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 38/71 | LOSS: 1.7824046620099136e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 39/71 | LOSS: 1.780055922608881e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 40/71 | LOSS: 1.7736856623342624e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 41/71 | LOSS: 1.794363132440984e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 42/71 | LOSS: 1.7860096472019038e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 43/71 | LOSS: 1.7925667986095174e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 44/71 | LOSS: 1.789492317685573e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 45/71 | LOSS: 1.785894969346017e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 46/71 | LOSS: 1.7929563039209088e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 47/71 | LOSS: 1.7909592334793462e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 48/71 | LOSS: 1.7923037988538806e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 49/71 | LOSS: 1.7959810447791826e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 50/71 | LOSS: 1.7977441940256896e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 51/71 | LOSS: 1.7935611367647653e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 52/71 | LOSS: 1.7874696205358048e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 53/71 | LOSS: 1.7921278430111232e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 54/71 | LOSS: 1.7947003272473177e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 55/71 | LOSS: 1.7975263533896524e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 56/71 | LOSS: 1.793700145450809e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 57/71 | LOSS: 1.7912395268749918e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 58/71 | LOSS: 1.7866493674203305e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 59/71 | LOSS: 1.7858673451579914e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 60/71 | LOSS: 1.7968792876597976e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 61/71 | LOSS: 1.7908860730042967e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 62/71 | LOSS: 1.7901041232223165e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 63/71 | LOSS: 1.7840264519008997e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 64/71 | LOSS: 1.7805158243460866e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 65/71 | LOSS: 1.781031636655274e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 66/71 | LOSS: 1.7701492910038807e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 67/71 | LOSS: 1.7676411891229425e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 68/71 | LOSS: 1.77178470935821e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 69/71 | LOSS: 1.7687086541887506e-05\n",
      "TRAIN: EPOCH 74/1000 | BATCH 70/71 | LOSS: 1.7666451294388307e-05\n",
      "VAL: EPOCH 74/1000 | BATCH 0/8 | LOSS: 1.616054942132905e-05\n",
      "VAL: EPOCH 74/1000 | BATCH 1/8 | LOSS: 1.545369423183729e-05\n",
      "VAL: EPOCH 74/1000 | BATCH 2/8 | LOSS: 1.7080730382682912e-05\n",
      "VAL: EPOCH 74/1000 | BATCH 3/8 | LOSS: 1.727750009195006e-05\n",
      "VAL: EPOCH 74/1000 | BATCH 4/8 | LOSS: 1.6843445155245718e-05\n",
      "VAL: EPOCH 74/1000 | BATCH 5/8 | LOSS: 1.703332494192485e-05\n",
      "VAL: EPOCH 74/1000 | BATCH 6/8 | LOSS: 1.656829174732723e-05\n",
      "VAL: EPOCH 74/1000 | BATCH 7/8 | LOSS: 1.6270506648652372e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 0/71 | LOSS: 1.8420636479277164e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 1/71 | LOSS: 1.8455647477821913e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 2/71 | LOSS: 1.8494786975982908e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 3/71 | LOSS: 1.9027313555852743e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 4/71 | LOSS: 1.9306597459944895e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 5/71 | LOSS: 1.8824925973603968e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 6/71 | LOSS: 1.8564951980286943e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 7/71 | LOSS: 1.84325044756406e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 8/71 | LOSS: 1.8121119662989964e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 9/71 | LOSS: 1.788582594599575e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 10/71 | LOSS: 1.750559981287965e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 11/71 | LOSS: 1.7302885908065946e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 12/71 | LOSS: 1.7196315359167504e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 13/71 | LOSS: 1.703822239765681e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 14/71 | LOSS: 1.6940572277235332e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 15/71 | LOSS: 1.6774554694620747e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 16/71 | LOSS: 1.6957184444061783e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 17/71 | LOSS: 1.7077563168438952e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 18/71 | LOSS: 1.7155989218353123e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 19/71 | LOSS: 1.7250990958928013e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 20/71 | LOSS: 1.7230075476012036e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 21/71 | LOSS: 1.708960632062041e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 22/71 | LOSS: 1.6948639590143852e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 23/71 | LOSS: 1.7070829509672574e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 24/71 | LOSS: 1.6959811546257697e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 25/71 | LOSS: 1.6935638646268322e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 26/71 | LOSS: 1.693795712964385e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 27/71 | LOSS: 1.6937157917189844e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 28/71 | LOSS: 1.6822580462381854e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 29/71 | LOSS: 1.667920241743559e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 30/71 | LOSS: 1.6615543215941158e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 31/71 | LOSS: 1.657250101061436e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 32/71 | LOSS: 1.6624489558814798e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 33/71 | LOSS: 1.6635415758013123e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 34/71 | LOSS: 1.663416743602803e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 35/71 | LOSS: 1.675654852483098e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 36/71 | LOSS: 1.6734512452810146e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 37/71 | LOSS: 1.6750040327008863e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 38/71 | LOSS: 1.666922334828623e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 39/71 | LOSS: 1.678847345374379e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 40/71 | LOSS: 1.6799500837983143e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 41/71 | LOSS: 1.6766308687822728e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 42/71 | LOSS: 1.6843654542077105e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 43/71 | LOSS: 1.6801680306922538e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 44/71 | LOSS: 1.6720670686885975e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 45/71 | LOSS: 1.6718022834538715e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 46/71 | LOSS: 1.6936123137519585e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 47/71 | LOSS: 1.6948592720685458e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 48/71 | LOSS: 1.7029350230438227e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 49/71 | LOSS: 1.7084650044125737e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 50/71 | LOSS: 1.7190578724765538e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 51/71 | LOSS: 1.7109870581267525e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 52/71 | LOSS: 1.712749021090479e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 53/71 | LOSS: 1.7089347154439935e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 54/71 | LOSS: 1.710769265628187e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 55/71 | LOSS: 1.7148802482682057e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 56/71 | LOSS: 1.709452266240668e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 57/71 | LOSS: 1.704318746588201e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 58/71 | LOSS: 1.7010619727844347e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 59/71 | LOSS: 1.6955884181394746e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 60/71 | LOSS: 1.7010613251453624e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 61/71 | LOSS: 1.7068805611357206e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 62/71 | LOSS: 1.7143597919101243e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 63/71 | LOSS: 1.719542215994352e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 64/71 | LOSS: 1.715154171856389e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 65/71 | LOSS: 1.7204273235344743e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 66/71 | LOSS: 1.718361184416676e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 67/71 | LOSS: 1.7205437215499626e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 68/71 | LOSS: 1.717207647163667e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 69/71 | LOSS: 1.7234246622267944e-05\n",
      "TRAIN: EPOCH 75/1000 | BATCH 70/71 | LOSS: 1.722233354113586e-05\n",
      "VAL: EPOCH 75/1000 | BATCH 0/8 | LOSS: 1.4648481737822294e-05\n",
      "VAL: EPOCH 75/1000 | BATCH 1/8 | LOSS: 1.4757928965991596e-05\n",
      "VAL: EPOCH 75/1000 | BATCH 2/8 | LOSS: 1.637918133686374e-05\n",
      "VAL: EPOCH 75/1000 | BATCH 3/8 | LOSS: 1.67119999332499e-05\n",
      "VAL: EPOCH 75/1000 | BATCH 4/8 | LOSS: 1.626388602744555e-05\n",
      "VAL: EPOCH 75/1000 | BATCH 5/8 | LOSS: 1.637684620921694e-05\n",
      "VAL: EPOCH 75/1000 | BATCH 6/8 | LOSS: 1.592555638905781e-05\n",
      "VAL: EPOCH 75/1000 | BATCH 7/8 | LOSS: 1.549237663311942e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 0/71 | LOSS: 1.3088976629660465e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 1/71 | LOSS: 1.635860371607123e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 2/71 | LOSS: 1.5988977186983295e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 3/71 | LOSS: 1.5957866480675875e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 4/71 | LOSS: 1.6122446504596154e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 5/71 | LOSS: 1.61787597789953e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 6/71 | LOSS: 1.65886640388635e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 7/71 | LOSS: 1.6660525830047845e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 8/71 | LOSS: 1.6573362295781004e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 9/71 | LOSS: 1.7228037268068875e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 10/71 | LOSS: 1.7380397201288194e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 11/71 | LOSS: 1.732498223342797e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 12/71 | LOSS: 1.723606559468093e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 13/71 | LOSS: 1.7491735174449525e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 14/71 | LOSS: 1.7440366051838888e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 15/71 | LOSS: 1.737909593657605e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 16/71 | LOSS: 1.718901335955118e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 17/71 | LOSS: 1.7073335736010147e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 18/71 | LOSS: 1.714118465507023e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 19/71 | LOSS: 1.692966247901495e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 20/71 | LOSS: 1.6987853563304745e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 21/71 | LOSS: 1.712099041965716e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 22/71 | LOSS: 1.7146272472493127e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 23/71 | LOSS: 1.7125133543534805e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 24/71 | LOSS: 1.7308769492956344e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 25/71 | LOSS: 1.7386141420362175e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 26/71 | LOSS: 1.7330392067917813e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 27/71 | LOSS: 1.7213159058623466e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 28/71 | LOSS: 1.72819725399865e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 29/71 | LOSS: 1.7237959976531177e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 30/71 | LOSS: 1.716029841660319e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 31/71 | LOSS: 1.7129781355151863e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 32/71 | LOSS: 1.7186068610251162e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 33/71 | LOSS: 1.7093189265270828e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 34/71 | LOSS: 1.707093490819846e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 35/71 | LOSS: 1.701835377490271e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 36/71 | LOSS: 1.7040260162674884e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 37/71 | LOSS: 1.696783996713317e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 38/71 | LOSS: 1.6930410450955744e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 39/71 | LOSS: 1.6836151166899073e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 40/71 | LOSS: 1.6879294179954917e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 41/71 | LOSS: 1.6908806975109092e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 42/71 | LOSS: 1.6823058674987786e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 43/71 | LOSS: 1.6757201344510328e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 44/71 | LOSS: 1.6800604579556318e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 45/71 | LOSS: 1.6816357975071497e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 46/71 | LOSS: 1.6875669779379833e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 47/71 | LOSS: 1.6875544815775356e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 48/71 | LOSS: 1.6850139478005337e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 49/71 | LOSS: 1.6892699895834085e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 50/71 | LOSS: 1.694937878603335e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 51/71 | LOSS: 1.6861802578205243e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 52/71 | LOSS: 1.6865366836460658e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 53/71 | LOSS: 1.6808607732080554e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 54/71 | LOSS: 1.6858810357172678e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 55/71 | LOSS: 1.677495843718394e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 56/71 | LOSS: 1.6797325913916882e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 57/71 | LOSS: 1.6824352848022407e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 58/71 | LOSS: 1.669905785832779e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 59/71 | LOSS: 1.6679349467570622e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 60/71 | LOSS: 1.6637320514433428e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 61/71 | LOSS: 1.663710030874016e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 62/71 | LOSS: 1.6633577312556252e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 63/71 | LOSS: 1.664188049232962e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 64/71 | LOSS: 1.663739580040923e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 65/71 | LOSS: 1.6583767911938732e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 66/71 | LOSS: 1.6603597235990125e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 67/71 | LOSS: 1.6655407240678495e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 68/71 | LOSS: 1.6611412636690226e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 69/71 | LOSS: 1.658280866000236e-05\n",
      "TRAIN: EPOCH 76/1000 | BATCH 70/71 | LOSS: 1.660633251061958e-05\n",
      "VAL: EPOCH 76/1000 | BATCH 0/8 | LOSS: 1.4721942534379195e-05\n",
      "VAL: EPOCH 76/1000 | BATCH 1/8 | LOSS: 1.4821934200881515e-05\n",
      "VAL: EPOCH 76/1000 | BATCH 2/8 | LOSS: 1.6014055897054884e-05\n",
      "VAL: EPOCH 76/1000 | BATCH 3/8 | LOSS: 1.5989806797733763e-05\n",
      "VAL: EPOCH 76/1000 | BATCH 4/8 | LOSS: 1.5520221495535225e-05\n",
      "VAL: EPOCH 76/1000 | BATCH 5/8 | LOSS: 1.5787798777940527e-05\n",
      "VAL: EPOCH 76/1000 | BATCH 6/8 | LOSS: 1.5372019333881326e-05\n",
      "VAL: EPOCH 76/1000 | BATCH 7/8 | LOSS: 1.5051335935822863e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 0/71 | LOSS: 1.293641071242746e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 1/71 | LOSS: 1.4230024135031272e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 2/71 | LOSS: 1.4654521388971867e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 3/71 | LOSS: 1.4731119790667435e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 4/71 | LOSS: 1.4622294838773086e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 5/71 | LOSS: 1.4649262084276415e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 6/71 | LOSS: 1.5491611650629367e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 7/71 | LOSS: 1.5235607634167536e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 8/71 | LOSS: 1.4627736365784787e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 9/71 | LOSS: 1.4910516074451153e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 10/71 | LOSS: 1.4789894934934141e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 11/71 | LOSS: 1.4708493533059178e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 12/71 | LOSS: 1.4824515136961754e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 13/71 | LOSS: 1.524497383798007e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 14/71 | LOSS: 1.5384031818636383e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 15/71 | LOSS: 1.5245514418893436e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 16/71 | LOSS: 1.5181953880100456e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 17/71 | LOSS: 1.5235904862088824e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 18/71 | LOSS: 1.5374955278514934e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 19/71 | LOSS: 1.5375160182884427e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 20/71 | LOSS: 1.542833429301113e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 21/71 | LOSS: 1.5433649175313555e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 22/71 | LOSS: 1.5512372714431915e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 23/71 | LOSS: 1.5458441074163904e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 24/71 | LOSS: 1.54674720033654e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 25/71 | LOSS: 1.555113511550679e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 26/71 | LOSS: 1.5516562295185093e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 27/71 | LOSS: 1.5436753236437134e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 28/71 | LOSS: 1.5302957418216153e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 29/71 | LOSS: 1.545760414956021e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 30/71 | LOSS: 1.560717696373749e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 31/71 | LOSS: 1.562269179089526e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 32/71 | LOSS: 1.5701567931638706e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 33/71 | LOSS: 1.5793235032855163e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 34/71 | LOSS: 1.5797606543596234e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 35/71 | LOSS: 1.5868585630111436e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 36/71 | LOSS: 1.5965278770316808e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 37/71 | LOSS: 1.6037319901921653e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 38/71 | LOSS: 1.6122646975884727e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 39/71 | LOSS: 1.609249179637118e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 40/71 | LOSS: 1.6195491187362478e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 41/71 | LOSS: 1.61348573653543e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 42/71 | LOSS: 1.6121039913932057e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 43/71 | LOSS: 1.608507473478808e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 44/71 | LOSS: 1.6124130322876023e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 45/71 | LOSS: 1.612616654750562e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 46/71 | LOSS: 1.6266797579288672e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 47/71 | LOSS: 1.6209018042445678e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 48/71 | LOSS: 1.621396744184847e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 49/71 | LOSS: 1.6226080788328545e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 50/71 | LOSS: 1.633490089315113e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 51/71 | LOSS: 1.6303927185827677e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 52/71 | LOSS: 1.6262577206224756e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 53/71 | LOSS: 1.6194346564454327e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 54/71 | LOSS: 1.6198167494704565e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 55/71 | LOSS: 1.6178160470709762e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 56/71 | LOSS: 1.6174512574364424e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 57/71 | LOSS: 1.6254770265543904e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 58/71 | LOSS: 1.6324424473553998e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 59/71 | LOSS: 1.6267311472499084e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 60/71 | LOSS: 1.6242963134632715e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 61/71 | LOSS: 1.6214467919898226e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 62/71 | LOSS: 1.623644252431982e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 63/71 | LOSS: 1.6259276051755478e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 64/71 | LOSS: 1.6315998338821092e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 65/71 | LOSS: 1.63149826552068e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 66/71 | LOSS: 1.6329168938139434e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 67/71 | LOSS: 1.632651796011059e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 68/71 | LOSS: 1.6322897323149938e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 69/71 | LOSS: 1.6342302829538573e-05\n",
      "TRAIN: EPOCH 77/1000 | BATCH 70/71 | LOSS: 1.6330963967127798e-05\n",
      "VAL: EPOCH 77/1000 | BATCH 0/8 | LOSS: 1.4733152056578547e-05\n",
      "VAL: EPOCH 77/1000 | BATCH 1/8 | LOSS: 1.4982110315031605e-05\n",
      "VAL: EPOCH 77/1000 | BATCH 2/8 | LOSS: 1.6437181632985205e-05\n",
      "VAL: EPOCH 77/1000 | BATCH 3/8 | LOSS: 1.6476452628921834e-05\n",
      "VAL: EPOCH 77/1000 | BATCH 4/8 | LOSS: 1.6136779777298216e-05\n",
      "VAL: EPOCH 77/1000 | BATCH 5/8 | LOSS: 1.6377089953797015e-05\n",
      "VAL: EPOCH 77/1000 | BATCH 6/8 | LOSS: 1.5914609191116013e-05\n",
      "VAL: EPOCH 77/1000 | BATCH 7/8 | LOSS: 1.564287003930076e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 0/71 | LOSS: 2.0866646082140505e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 1/71 | LOSS: 1.8008428924076725e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 2/71 | LOSS: 1.8228509967836242e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 3/71 | LOSS: 1.782931576599367e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 4/71 | LOSS: 1.7318236859864556e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 5/71 | LOSS: 1.752508190596321e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 6/71 | LOSS: 1.8029176706997013e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 7/71 | LOSS: 1.815435439311841e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 8/71 | LOSS: 1.775764152019595e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 9/71 | LOSS: 1.8096416351909283e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 10/71 | LOSS: 1.808342230188745e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 11/71 | LOSS: 1.8213117376338534e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 12/71 | LOSS: 1.791945396689698e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 13/71 | LOSS: 1.811909065249243e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 14/71 | LOSS: 1.8195584198110736e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 15/71 | LOSS: 1.807081309834757e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 16/71 | LOSS: 1.7788334469728663e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 17/71 | LOSS: 1.767265146352454e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 18/71 | LOSS: 1.7860103980638087e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 19/71 | LOSS: 1.7757135901774744e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 20/71 | LOSS: 1.763659881925421e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 21/71 | LOSS: 1.756109697287849e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 22/71 | LOSS: 1.7636203032768957e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 23/71 | LOSS: 1.744537568508046e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 24/71 | LOSS: 1.7370197165291758e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 25/71 | LOSS: 1.729654468363151e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 26/71 | LOSS: 1.733173155150359e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 27/71 | LOSS: 1.737404662500402e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 28/71 | LOSS: 1.7211912281395756e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 29/71 | LOSS: 1.7070210257467504e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 30/71 | LOSS: 1.699291765215739e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 31/71 | LOSS: 1.682286827531243e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 32/71 | LOSS: 1.67020834300219e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 33/71 | LOSS: 1.6636924957471338e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 34/71 | LOSS: 1.6511277421419175e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 35/71 | LOSS: 1.6435973066109e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 36/71 | LOSS: 1.6428261989022832e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 37/71 | LOSS: 1.641295339619606e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 38/71 | LOSS: 1.6454007215006874e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 39/71 | LOSS: 1.6382510239054682e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 40/71 | LOSS: 1.644420644435918e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 41/71 | LOSS: 1.6406758515499642e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 42/71 | LOSS: 1.6383976608901935e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 43/71 | LOSS: 1.6340755337296287e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 44/71 | LOSS: 1.640762319665454e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 45/71 | LOSS: 1.6475758892066434e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 46/71 | LOSS: 1.640235451362814e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 47/71 | LOSS: 1.633788175089042e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 48/71 | LOSS: 1.640904605553761e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 49/71 | LOSS: 1.6392984543927015e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 50/71 | LOSS: 1.6388947045140642e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 51/71 | LOSS: 1.6359879299880748e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 52/71 | LOSS: 1.6323462007693587e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 53/71 | LOSS: 1.6340771236611073e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 54/71 | LOSS: 1.633761767152464e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 55/71 | LOSS: 1.6302460202106367e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 56/71 | LOSS: 1.6239625103097704e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 57/71 | LOSS: 1.620962506547522e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 58/71 | LOSS: 1.6180096558932515e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 59/71 | LOSS: 1.6069341108959633e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 60/71 | LOSS: 1.6091061819470718e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 61/71 | LOSS: 1.603210679487318e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 62/71 | LOSS: 1.6108031068431464e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 63/71 | LOSS: 1.6118678928478403e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 64/71 | LOSS: 1.6124327717429528e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 65/71 | LOSS: 1.6127247363328934e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 66/71 | LOSS: 1.6148832061722415e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 67/71 | LOSS: 1.6161062670589136e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 68/71 | LOSS: 1.613616128135776e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 69/71 | LOSS: 1.6142859619451754e-05\n",
      "TRAIN: EPOCH 78/1000 | BATCH 70/71 | LOSS: 1.612248649161955e-05\n",
      "VAL: EPOCH 78/1000 | BATCH 0/8 | LOSS: 1.6442922060377896e-05\n",
      "VAL: EPOCH 78/1000 | BATCH 1/8 | LOSS: 1.6392443285440095e-05\n",
      "VAL: EPOCH 78/1000 | BATCH 2/8 | LOSS: 1.7529084288980812e-05\n",
      "VAL: EPOCH 78/1000 | BATCH 3/8 | LOSS: 1.7068590750568546e-05\n",
      "VAL: EPOCH 78/1000 | BATCH 4/8 | LOSS: 1.6364568000426517e-05\n",
      "VAL: EPOCH 78/1000 | BATCH 5/8 | LOSS: 1.62570164926971e-05\n",
      "VAL: EPOCH 78/1000 | BATCH 6/8 | LOSS: 1.5710457254109705e-05\n",
      "VAL: EPOCH 78/1000 | BATCH 7/8 | LOSS: 1.5299926076295378e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 0/71 | LOSS: 1.3748957826464903e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 1/71 | LOSS: 1.64479256454797e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 2/71 | LOSS: 1.6434001433178007e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 3/71 | LOSS: 1.579506101734296e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 4/71 | LOSS: 1.5442546282429247e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 5/71 | LOSS: 1.5774726610591944e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 6/71 | LOSS: 1.601085464894173e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 7/71 | LOSS: 1.5875537656029337e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 8/71 | LOSS: 1.606434206122584e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 9/71 | LOSS: 1.624552041903371e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 10/71 | LOSS: 1.632490487033713e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 11/71 | LOSS: 1.6333826958240632e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 12/71 | LOSS: 1.6229871177459314e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 13/71 | LOSS: 1.635673671834021e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 14/71 | LOSS: 1.6076308505337995e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 15/71 | LOSS: 1.6174116922229587e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 16/71 | LOSS: 1.647731401402887e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 17/71 | LOSS: 1.6595699283546612e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 18/71 | LOSS: 1.638375727596117e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 19/71 | LOSS: 1.642611155148188e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 20/71 | LOSS: 1.6448666125749394e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 21/71 | LOSS: 1.6334880456270184e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 22/71 | LOSS: 1.6284671129023597e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 23/71 | LOSS: 1.638515417804835e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 24/71 | LOSS: 1.6456010889669415e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 25/71 | LOSS: 1.6498169770784443e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 26/71 | LOSS: 1.6482315745465542e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 27/71 | LOSS: 1.660390954124783e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 28/71 | LOSS: 1.6578263371072885e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 29/71 | LOSS: 1.6469073337551284e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 30/71 | LOSS: 1.65524808776794e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 31/71 | LOSS: 1.666033998048988e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 32/71 | LOSS: 1.6733010880037028e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 33/71 | LOSS: 1.6775896229399438e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 34/71 | LOSS: 1.691942257561355e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 35/71 | LOSS: 1.69914349549395e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 36/71 | LOSS: 1.701959182183428e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 37/71 | LOSS: 1.690877115834155e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 38/71 | LOSS: 1.7021310855059895e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 39/71 | LOSS: 1.703832074326783e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 40/71 | LOSS: 1.7098292784528272e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 41/71 | LOSS: 1.7123409179475857e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 42/71 | LOSS: 1.7020616174112398e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 43/71 | LOSS: 1.694585193945386e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 44/71 | LOSS: 1.7036153015700013e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 45/71 | LOSS: 1.713200014779906e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 46/71 | LOSS: 1.7148789199981888e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 47/71 | LOSS: 1.716102294343121e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 48/71 | LOSS: 1.710384916879441e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 49/71 | LOSS: 1.704908485407941e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 50/71 | LOSS: 1.698694218337586e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 51/71 | LOSS: 1.7010175602431096e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 52/71 | LOSS: 1.6900370130040858e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 53/71 | LOSS: 1.693130074003151e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 54/71 | LOSS: 1.695910842417189e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 55/71 | LOSS: 1.6987016871488387e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 56/71 | LOSS: 1.691343657097477e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 57/71 | LOSS: 1.6873288132046582e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 58/71 | LOSS: 1.6811917487495397e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 59/71 | LOSS: 1.683113191575103e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 60/71 | LOSS: 1.6865011558798334e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 61/71 | LOSS: 1.684485703428377e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 62/71 | LOSS: 1.6831242588985275e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 63/71 | LOSS: 1.6806330577878725e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 64/71 | LOSS: 1.6845060781633946e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 65/71 | LOSS: 1.686812258798675e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 66/71 | LOSS: 1.680964812036489e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 67/71 | LOSS: 1.6813567138823974e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 68/71 | LOSS: 1.6778003041311067e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 69/71 | LOSS: 1.673823717283085e-05\n",
      "TRAIN: EPOCH 79/1000 | BATCH 70/71 | LOSS: 1.6655556746499457e-05\n",
      "VAL: EPOCH 79/1000 | BATCH 0/8 | LOSS: 1.7912745533976704e-05\n",
      "VAL: EPOCH 79/1000 | BATCH 1/8 | LOSS: 1.8098628061125055e-05\n",
      "VAL: EPOCH 79/1000 | BATCH 2/8 | LOSS: 1.9234169788736228e-05\n",
      "VAL: EPOCH 79/1000 | BATCH 3/8 | LOSS: 1.8742524389381288e-05\n",
      "VAL: EPOCH 79/1000 | BATCH 4/8 | LOSS: 1.7888010006572586e-05\n",
      "VAL: EPOCH 79/1000 | BATCH 5/8 | LOSS: 1.755641566584624e-05\n",
      "VAL: EPOCH 79/1000 | BATCH 6/8 | LOSS: 1.702325114040702e-05\n",
      "VAL: EPOCH 79/1000 | BATCH 7/8 | LOSS: 1.6432160009571817e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 0/71 | LOSS: 1.2893069651909173e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 1/71 | LOSS: 1.769247501215432e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 2/71 | LOSS: 1.7759089435761172e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 3/71 | LOSS: 1.7369734905514633e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 4/71 | LOSS: 1.859757503552828e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 5/71 | LOSS: 1.788082499842858e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 6/71 | LOSS: 1.7540291602407316e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 7/71 | LOSS: 1.913434152811533e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 8/71 | LOSS: 1.929050308212431e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 9/71 | LOSS: 1.8797675875248387e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 10/71 | LOSS: 1.8831292519197714e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 11/71 | LOSS: 1.8865496410095755e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 12/71 | LOSS: 1.8469361468585423e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 13/71 | LOSS: 1.8371302498313264e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 14/71 | LOSS: 1.8519778556462067e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 15/71 | LOSS: 1.809253978990455e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 16/71 | LOSS: 1.7906496235381756e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 17/71 | LOSS: 1.8029309507659895e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 18/71 | LOSS: 1.806056312297618e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 19/71 | LOSS: 1.767897856552736e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 20/71 | LOSS: 1.7616397859196047e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 21/71 | LOSS: 1.7749735310313884e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 22/71 | LOSS: 1.7709435348439477e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 23/71 | LOSS: 1.7574157292680564e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 24/71 | LOSS: 1.7395881441188978e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 25/71 | LOSS: 1.7229573061810628e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 26/71 | LOSS: 1.7118729610282805e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 27/71 | LOSS: 1.7006888583637192e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 28/71 | LOSS: 1.694487736099761e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 29/71 | LOSS: 1.696028997078732e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 30/71 | LOSS: 1.7073369851840974e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 31/71 | LOSS: 1.7133403986235862e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 32/71 | LOSS: 1.7159088367241733e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 33/71 | LOSS: 1.71507836860067e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 34/71 | LOSS: 1.698672627493839e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 35/71 | LOSS: 1.691926099233064e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 36/71 | LOSS: 1.6823665818437065e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 37/71 | LOSS: 1.676319968617846e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 38/71 | LOSS: 1.669510467577194e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 39/71 | LOSS: 1.658971445976931e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 40/71 | LOSS: 1.6498633446711365e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 41/71 | LOSS: 1.6439417582400244e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 42/71 | LOSS: 1.640997663005607e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 43/71 | LOSS: 1.6326973578047315e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 44/71 | LOSS: 1.6311491122501642e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 45/71 | LOSS: 1.6300293697460564e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 46/71 | LOSS: 1.6226187712008857e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 47/71 | LOSS: 1.6175958421627e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 48/71 | LOSS: 1.6240513420542606e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 49/71 | LOSS: 1.6208234956138768e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 50/71 | LOSS: 1.6261800827159475e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 51/71 | LOSS: 1.620950299114226e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 52/71 | LOSS: 1.6226535441297905e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 53/71 | LOSS: 1.6202340669190528e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 54/71 | LOSS: 1.6201281224229287e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 55/71 | LOSS: 1.617168387773355e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 56/71 | LOSS: 1.6202466564177404e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 57/71 | LOSS: 1.6154681236065668e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 58/71 | LOSS: 1.615774111750228e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 59/71 | LOSS: 1.6179973044927465e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 60/71 | LOSS: 1.618984120408432e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 61/71 | LOSS: 1.6185322883463796e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 62/71 | LOSS: 1.6131192939018734e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 63/71 | LOSS: 1.6109081798276748e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 64/71 | LOSS: 1.6063218992293024e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 65/71 | LOSS: 1.6101377199943684e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 66/71 | LOSS: 1.615404227180337e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 67/71 | LOSS: 1.6143925998168177e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 68/71 | LOSS: 1.6150632712895565e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 69/71 | LOSS: 1.616467805953497e-05\n",
      "TRAIN: EPOCH 80/1000 | BATCH 70/71 | LOSS: 1.6185522557426453e-05\n",
      "VAL: EPOCH 80/1000 | BATCH 0/8 | LOSS: 2.02060145966243e-05\n",
      "VAL: EPOCH 80/1000 | BATCH 1/8 | LOSS: 2.0822425540245604e-05\n",
      "VAL: EPOCH 80/1000 | BATCH 2/8 | LOSS: 2.2475187506643124e-05\n",
      "VAL: EPOCH 80/1000 | BATCH 3/8 | LOSS: 2.210686352555058e-05\n",
      "VAL: EPOCH 80/1000 | BATCH 4/8 | LOSS: 2.107940563291777e-05\n",
      "VAL: EPOCH 80/1000 | BATCH 5/8 | LOSS: 2.0465498588843427e-05\n",
      "VAL: EPOCH 80/1000 | BATCH 6/8 | LOSS: 1.992466994644409e-05\n",
      "VAL: EPOCH 80/1000 | BATCH 7/8 | LOSS: 1.9135335605824366e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 0/71 | LOSS: 1.859884650912136e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 1/71 | LOSS: 1.7835192920756526e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 2/71 | LOSS: 1.742646963975858e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 3/71 | LOSS: 1.709728985588299e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 4/71 | LOSS: 1.692491860012524e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 5/71 | LOSS: 1.7117166256260436e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 6/71 | LOSS: 1.6793302035823998e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 7/71 | LOSS: 1.7058533785530017e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 8/71 | LOSS: 1.7243949211357783e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 9/71 | LOSS: 1.7827590636443346e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 10/71 | LOSS: 1.7632635552383196e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 11/71 | LOSS: 1.7983790864188148e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 12/71 | LOSS: 1.7648698443596142e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 13/71 | LOSS: 1.761078647177783e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 14/71 | LOSS: 1.7640162877796683e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 15/71 | LOSS: 1.766876283681995e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 16/71 | LOSS: 1.7468749319959898e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 17/71 | LOSS: 1.7599208806334194e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 18/71 | LOSS: 1.7631133759048106e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 19/71 | LOSS: 1.7485997022959055e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 20/71 | LOSS: 1.7341361854050774e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 21/71 | LOSS: 1.7519961188967997e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 22/71 | LOSS: 1.7593529304102287e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 23/71 | LOSS: 1.7463059483209992e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 24/71 | LOSS: 1.7917426775966304e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 25/71 | LOSS: 1.7893226763240258e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 26/71 | LOSS: 1.773454510435436e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 27/71 | LOSS: 1.8196918745161383e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 28/71 | LOSS: 1.8356498191584752e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 29/71 | LOSS: 1.8207708520397622e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 30/71 | LOSS: 1.817770701182458e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 31/71 | LOSS: 1.8232253950145605e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 32/71 | LOSS: 1.8210065608207405e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 33/71 | LOSS: 1.8060932688968023e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 34/71 | LOSS: 1.8131381070166494e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 35/71 | LOSS: 1.8075800224121646e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 36/71 | LOSS: 1.7918984200593047e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 37/71 | LOSS: 1.7849112761101232e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 38/71 | LOSS: 1.78319392384415e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 39/71 | LOSS: 1.7760827199708727e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 40/71 | LOSS: 1.7757517215192444e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 41/71 | LOSS: 1.780796323406061e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 42/71 | LOSS: 1.7740171177476524e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 43/71 | LOSS: 1.7698191248365962e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 44/71 | LOSS: 1.7692023397507404e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 45/71 | LOSS: 1.7711305842923416e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 46/71 | LOSS: 1.770481850008822e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 47/71 | LOSS: 1.769754311453653e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 48/71 | LOSS: 1.7787085532606342e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 49/71 | LOSS: 1.778259311322472e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 50/71 | LOSS: 1.7699998370160612e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 51/71 | LOSS: 1.7654026036185453e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 52/71 | LOSS: 1.7570999896736982e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 53/71 | LOSS: 1.7481408576713875e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 54/71 | LOSS: 1.746651505527552e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 55/71 | LOSS: 1.736804134517505e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 56/71 | LOSS: 1.733974343216304e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 57/71 | LOSS: 1.727408698570798e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 58/71 | LOSS: 1.7324647684537183e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 59/71 | LOSS: 1.7208719130697623e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 60/71 | LOSS: 1.715205414003605e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 61/71 | LOSS: 1.706617412046105e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 62/71 | LOSS: 1.700465428358358e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 63/71 | LOSS: 1.6998324554151623e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 64/71 | LOSS: 1.69705232110573e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 65/71 | LOSS: 1.686676600773794e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 66/71 | LOSS: 1.677373779445466e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 67/71 | LOSS: 1.6772427162483355e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 68/71 | LOSS: 1.6762679162797134e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 69/71 | LOSS: 1.6722583040973404e-05\n",
      "TRAIN: EPOCH 81/1000 | BATCH 70/71 | LOSS: 1.6764122193935818e-05\n",
      "VAL: EPOCH 81/1000 | BATCH 0/8 | LOSS: 1.4163563719193917e-05\n",
      "VAL: EPOCH 81/1000 | BATCH 1/8 | LOSS: 1.3937866697233403e-05\n",
      "VAL: EPOCH 81/1000 | BATCH 2/8 | LOSS: 1.5433527551067527e-05\n",
      "VAL: EPOCH 81/1000 | BATCH 3/8 | LOSS: 1.545577447359392e-05\n",
      "VAL: EPOCH 81/1000 | BATCH 4/8 | LOSS: 1.5075833834998775e-05\n",
      "VAL: EPOCH 81/1000 | BATCH 5/8 | LOSS: 1.5111326774785994e-05\n",
      "VAL: EPOCH 81/1000 | BATCH 6/8 | LOSS: 1.4654382149663953e-05\n",
      "VAL: EPOCH 81/1000 | BATCH 7/8 | LOSS: 1.4273354736360488e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 0/71 | LOSS: 1.6027184756239876e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 1/71 | LOSS: 1.4726780136697926e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 2/71 | LOSS: 1.4053330839184733e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 3/71 | LOSS: 1.5046464341139654e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 4/71 | LOSS: 1.4936478692106903e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 5/71 | LOSS: 1.4785903658776078e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 6/71 | LOSS: 1.4953822756069712e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 7/71 | LOSS: 1.4746771398677083e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 8/71 | LOSS: 1.466888948521551e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 9/71 | LOSS: 1.4684454345115228e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 10/71 | LOSS: 1.5156930534100287e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 11/71 | LOSS: 1.5286930117023683e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 12/71 | LOSS: 1.540831929630188e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 13/71 | LOSS: 1.5185478917244057e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 14/71 | LOSS: 1.5303127838706132e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 15/71 | LOSS: 1.525273393099269e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 16/71 | LOSS: 1.5546941861376743e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 17/71 | LOSS: 1.5501816607057764e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 18/71 | LOSS: 1.541226040564269e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 19/71 | LOSS: 1.5553656112388126e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 20/71 | LOSS: 1.5876237956122385e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 21/71 | LOSS: 1.5818043927786427e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 22/71 | LOSS: 1.58074072200891e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 23/71 | LOSS: 1.5890001691332145e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 24/71 | LOSS: 1.5795267281646374e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 25/71 | LOSS: 1.5799931239038975e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 26/71 | LOSS: 1.5847445663692066e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 27/71 | LOSS: 1.597602363290207e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 28/71 | LOSS: 1.5869793624633218e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 29/71 | LOSS: 1.5794241062394575e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 30/71 | LOSS: 1.5716348365634574e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 31/71 | LOSS: 1.567559723980594e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 32/71 | LOSS: 1.572829762263802e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 33/71 | LOSS: 1.5737172789693647e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 34/71 | LOSS: 1.5698814578562244e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 35/71 | LOSS: 1.5812329744827974e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 36/71 | LOSS: 1.5887508419148838e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 37/71 | LOSS: 1.5895217960573255e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 38/71 | LOSS: 1.5793675489616224e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 39/71 | LOSS: 1.580812395332032e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 40/71 | LOSS: 1.5832286075370885e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 41/71 | LOSS: 1.5771742536820793e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 42/71 | LOSS: 1.5775743291711943e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 43/71 | LOSS: 1.5652549812082064e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 44/71 | LOSS: 1.566219331531708e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 45/71 | LOSS: 1.565446842979999e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 46/71 | LOSS: 1.5598991341632545e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 47/71 | LOSS: 1.5567562115847977e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 48/71 | LOSS: 1.5561686936683943e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 49/71 | LOSS: 1.550949999000295e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 50/71 | LOSS: 1.5535805722916564e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 51/71 | LOSS: 1.548985476172395e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 52/71 | LOSS: 1.5445758590259465e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 53/71 | LOSS: 1.5410614280510884e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 54/71 | LOSS: 1.543719054924705e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 55/71 | LOSS: 1.547886881780869e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 56/71 | LOSS: 1.553545859535102e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 57/71 | LOSS: 1.550715440881222e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 58/71 | LOSS: 1.557282016388638e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 59/71 | LOSS: 1.5544995843204864e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 60/71 | LOSS: 1.5581331622417534e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 61/71 | LOSS: 1.5553964394418854e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 62/71 | LOSS: 1.566673930187429e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 63/71 | LOSS: 1.568281945196759e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 64/71 | LOSS: 1.5668675909042718e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 65/71 | LOSS: 1.571214822130666e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 66/71 | LOSS: 1.5702291271573717e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 67/71 | LOSS: 1.5716644812153606e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 68/71 | LOSS: 1.5763616104685344e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 69/71 | LOSS: 1.582904389059487e-05\n",
      "TRAIN: EPOCH 82/1000 | BATCH 70/71 | LOSS: 1.5765922373094314e-05\n",
      "VAL: EPOCH 82/1000 | BATCH 0/8 | LOSS: 1.3481296264217235e-05\n",
      "VAL: EPOCH 82/1000 | BATCH 1/8 | LOSS: 1.3536197911889758e-05\n",
      "VAL: EPOCH 82/1000 | BATCH 2/8 | LOSS: 1.498073349163557e-05\n",
      "VAL: EPOCH 82/1000 | BATCH 3/8 | LOSS: 1.4909152923792135e-05\n",
      "VAL: EPOCH 82/1000 | BATCH 4/8 | LOSS: 1.4597704102925491e-05\n",
      "VAL: EPOCH 82/1000 | BATCH 5/8 | LOSS: 1.4903254699068688e-05\n",
      "VAL: EPOCH 82/1000 | BATCH 6/8 | LOSS: 1.4472082771784958e-05\n",
      "VAL: EPOCH 82/1000 | BATCH 7/8 | LOSS: 1.4258172882364306e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 0/71 | LOSS: 1.652717764955014e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 1/71 | LOSS: 1.7200083675561473e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 2/71 | LOSS: 1.6855042607251864e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 3/71 | LOSS: 1.597190157553996e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 4/71 | LOSS: 1.602956253918819e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 5/71 | LOSS: 1.6022951361568023e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 6/71 | LOSS: 1.5786876117220216e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 7/71 | LOSS: 1.5558175277874398e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 8/71 | LOSS: 1.5611880472634868e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 9/71 | LOSS: 1.577893535795738e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 10/71 | LOSS: 1.5466437286125835e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 11/71 | LOSS: 1.5945705248062343e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 12/71 | LOSS: 1.5885949533884952e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 13/71 | LOSS: 1.5937991585295614e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 14/71 | LOSS: 1.5858790175116155e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 15/71 | LOSS: 1.5782416028287116e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 16/71 | LOSS: 1.5802206743340803e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 17/71 | LOSS: 1.583139121268889e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 18/71 | LOSS: 1.6038952708186116e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 19/71 | LOSS: 1.6136146086864755e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 20/71 | LOSS: 1.5970479883738637e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 21/71 | LOSS: 1.5864707000384275e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 22/71 | LOSS: 1.5845635536369745e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 23/71 | LOSS: 1.5645585108359228e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 24/71 | LOSS: 1.5611081580573226e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 25/71 | LOSS: 1.5424189314217074e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 26/71 | LOSS: 1.5406803800875355e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 27/71 | LOSS: 1.5303214987787734e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 28/71 | LOSS: 1.526463451039584e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 29/71 | LOSS: 1.5211544359772234e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 30/71 | LOSS: 1.5210240528704748e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 31/71 | LOSS: 1.5233060423724964e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 32/71 | LOSS: 1.5231635574530104e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 33/71 | LOSS: 1.5177152764569173e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 34/71 | LOSS: 1.5131968367703458e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 35/71 | LOSS: 1.5197141212613335e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 36/71 | LOSS: 1.5309295392669172e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 37/71 | LOSS: 1.5234890270221513e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 38/71 | LOSS: 1.531366895226081e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 39/71 | LOSS: 1.5177840168689727e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 40/71 | LOSS: 1.5201986992373926e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 41/71 | LOSS: 1.520259506616553e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 42/71 | LOSS: 1.5243207556967147e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 43/71 | LOSS: 1.5198051683414219e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 44/71 | LOSS: 1.5243669056669912e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 45/71 | LOSS: 1.51985056974176e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 46/71 | LOSS: 1.5117533176306368e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 47/71 | LOSS: 1.498828146395681e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 48/71 | LOSS: 1.4930253108067685e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 49/71 | LOSS: 1.489355225203326e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 50/71 | LOSS: 1.4837726042720069e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 51/71 | LOSS: 1.4898146358833541e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 52/71 | LOSS: 1.4880667326649e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 53/71 | LOSS: 1.4876609940175615e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 54/71 | LOSS: 1.4896974525155118e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 55/71 | LOSS: 1.4879459253539348e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 56/71 | LOSS: 1.486017830948664e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 57/71 | LOSS: 1.4904523407588794e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 58/71 | LOSS: 1.4895652615666485e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 59/71 | LOSS: 1.4853197383975687e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 60/71 | LOSS: 1.492367393111039e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 61/71 | LOSS: 1.4891142376244158e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 62/71 | LOSS: 1.4944430187738145e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 63/71 | LOSS: 1.4901595307037496e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 64/71 | LOSS: 1.4853545172417608e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 65/71 | LOSS: 1.489121236910719e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 66/71 | LOSS: 1.4902970192384031e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 67/71 | LOSS: 1.4911552598812919e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 68/71 | LOSS: 1.4927342119230214e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 69/71 | LOSS: 1.4932460804370099e-05\n",
      "TRAIN: EPOCH 83/1000 | BATCH 70/71 | LOSS: 1.5031598336399455e-05\n",
      "VAL: EPOCH 83/1000 | BATCH 0/8 | LOSS: 1.585645259183366e-05\n",
      "VAL: EPOCH 83/1000 | BATCH 1/8 | LOSS: 1.6410863281635102e-05\n",
      "VAL: EPOCH 83/1000 | BATCH 2/8 | LOSS: 1.700024646803892e-05\n",
      "VAL: EPOCH 83/1000 | BATCH 3/8 | LOSS: 1.653945400903467e-05\n",
      "VAL: EPOCH 83/1000 | BATCH 4/8 | LOSS: 1.5926900596241465e-05\n",
      "VAL: EPOCH 83/1000 | BATCH 5/8 | LOSS: 1.5946936097558744e-05\n",
      "VAL: EPOCH 83/1000 | BATCH 6/8 | LOSS: 1.553452213037027e-05\n",
      "VAL: EPOCH 83/1000 | BATCH 7/8 | LOSS: 1.5091077102624695e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 0/71 | LOSS: 1.4115614249021746e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 1/71 | LOSS: 1.5684387108194642e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 2/71 | LOSS: 1.5213960068649612e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 3/71 | LOSS: 1.5530857581325108e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 4/71 | LOSS: 1.6054907246143556e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 5/71 | LOSS: 1.561161934660049e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 6/71 | LOSS: 1.5971816018074086e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 7/71 | LOSS: 1.6109131820485345e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 8/71 | LOSS: 1.5793021298628042e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 9/71 | LOSS: 1.5704332417953994e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 10/71 | LOSS: 1.5474145171042025e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 11/71 | LOSS: 1.5178992346894423e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 12/71 | LOSS: 1.4989383789025642e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 13/71 | LOSS: 1.5168565174203e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 14/71 | LOSS: 1.5384028241290556e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 15/71 | LOSS: 1.5350068792940874e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 16/71 | LOSS: 1.5196813632535528e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 17/71 | LOSS: 1.5439976070613033e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 18/71 | LOSS: 1.5571429685223848e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 19/71 | LOSS: 1.558594276502845e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 20/71 | LOSS: 1.5686079374669742e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 21/71 | LOSS: 1.568057616003154e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 22/71 | LOSS: 1.57272515934892e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 23/71 | LOSS: 1.572676630227458e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 24/71 | LOSS: 1.5775603460497224e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 25/71 | LOSS: 1.567162388920014e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 26/71 | LOSS: 1.561909845838306e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 27/71 | LOSS: 1.5640313709549707e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 28/71 | LOSS: 1.5568214276472333e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 29/71 | LOSS: 1.553271946856209e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 30/71 | LOSS: 1.553677107186823e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 31/71 | LOSS: 1.56710594865217e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 32/71 | LOSS: 1.5607025020334262e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 33/71 | LOSS: 1.569186050640864e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 34/71 | LOSS: 1.570780324462768e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 35/71 | LOSS: 1.5751614000691916e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 36/71 | LOSS: 1.579146887583979e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 37/71 | LOSS: 1.577737506714153e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 38/71 | LOSS: 1.5822297451123654e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 39/71 | LOSS: 1.5883339005995368e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 40/71 | LOSS: 1.5846369578645042e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 41/71 | LOSS: 1.5964114243702387e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 42/71 | LOSS: 1.589106021890265e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 43/71 | LOSS: 1.601112465075196e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 44/71 | LOSS: 1.5957859735256836e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 45/71 | LOSS: 1.5913129474305922e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 46/71 | LOSS: 1.6062243203316725e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 47/71 | LOSS: 1.607200672045413e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 48/71 | LOSS: 1.596103405119015e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 49/71 | LOSS: 1.595377103512874e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 50/71 | LOSS: 1.5954629584573958e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 51/71 | LOSS: 1.5889038457736697e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 52/71 | LOSS: 1.5772171142123386e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 53/71 | LOSS: 1.5720638489087663e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 54/71 | LOSS: 1.5647230743938548e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 55/71 | LOSS: 1.562639207414967e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 56/71 | LOSS: 1.5695777141158224e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 57/71 | LOSS: 1.5682497742718312e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 58/71 | LOSS: 1.5676234181875647e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 59/71 | LOSS: 1.5678295433948126e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 60/71 | LOSS: 1.564963075325664e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 61/71 | LOSS: 1.5668507184008994e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 62/71 | LOSS: 1.5671762455548604e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 63/71 | LOSS: 1.560570278513751e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 64/71 | LOSS: 1.562248296697642e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 65/71 | LOSS: 1.5757457359458467e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 66/71 | LOSS: 1.5705600583429776e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 67/71 | LOSS: 1.572657852728512e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 68/71 | LOSS: 1.572636003824089e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 69/71 | LOSS: 1.570803462007981e-05\n",
      "TRAIN: EPOCH 84/1000 | BATCH 70/71 | LOSS: 1.575854061946968e-05\n",
      "VAL: EPOCH 84/1000 | BATCH 0/8 | LOSS: 1.6906917153391987e-05\n",
      "VAL: EPOCH 84/1000 | BATCH 1/8 | LOSS: 1.7782691429601982e-05\n",
      "VAL: EPOCH 84/1000 | BATCH 2/8 | LOSS: 1.9040169718209654e-05\n",
      "VAL: EPOCH 84/1000 | BATCH 3/8 | LOSS: 1.8960637135023717e-05\n",
      "VAL: EPOCH 84/1000 | BATCH 4/8 | LOSS: 1.829278808145318e-05\n",
      "VAL: EPOCH 84/1000 | BATCH 5/8 | LOSS: 1.7793963100605954e-05\n",
      "VAL: EPOCH 84/1000 | BATCH 6/8 | LOSS: 1.7487763346122976e-05\n",
      "VAL: EPOCH 84/1000 | BATCH 7/8 | LOSS: 1.6881543274394062e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 0/71 | LOSS: 1.5062936654430814e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 1/71 | LOSS: 1.6573550965404138e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 2/71 | LOSS: 1.688759948592633e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 3/71 | LOSS: 1.765841352607822e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 4/71 | LOSS: 1.7486811702838168e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 5/71 | LOSS: 1.7281579251478735e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 6/71 | LOSS: 1.7918293971368776e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 7/71 | LOSS: 1.7839860447566025e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 8/71 | LOSS: 1.7178648199155254e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 9/71 | LOSS: 1.740213365337695e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 10/71 | LOSS: 1.7380981262249406e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 11/71 | LOSS: 1.7467000134274713e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 12/71 | LOSS: 1.7508894276173894e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 13/71 | LOSS: 1.735042074609997e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 14/71 | LOSS: 1.7540473527333235e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 15/71 | LOSS: 1.7528110731745983e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 16/71 | LOSS: 1.739234090457424e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 17/71 | LOSS: 1.7271900813082335e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 18/71 | LOSS: 1.7023912541366083e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 19/71 | LOSS: 1.6835924498082022e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 20/71 | LOSS: 1.6978741898908212e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 21/71 | LOSS: 1.665086388542973e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 22/71 | LOSS: 1.654956656199394e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 23/71 | LOSS: 1.644896125678012e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 24/71 | LOSS: 1.6354167528334073e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 25/71 | LOSS: 1.63568716883534e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 26/71 | LOSS: 1.6272649999235808e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 27/71 | LOSS: 1.6209813825948265e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 28/71 | LOSS: 1.6071103314531903e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 29/71 | LOSS: 1.6055519821141692e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 30/71 | LOSS: 1.597189757082942e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 31/71 | LOSS: 1.59120947955671e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 32/71 | LOSS: 1.59650551336888e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 33/71 | LOSS: 1.5868328666650032e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 34/71 | LOSS: 1.5855008885929627e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 35/71 | LOSS: 1.581443454294155e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 36/71 | LOSS: 1.5765880291567283e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 37/71 | LOSS: 1.5712439730314625e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 38/71 | LOSS: 1.5658450114969106e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 39/71 | LOSS: 1.5597084689034092e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 40/71 | LOSS: 1.5570747808208293e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 41/71 | LOSS: 1.5474928334567396e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 42/71 | LOSS: 1.5395491551993642e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 43/71 | LOSS: 1.536257995708597e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 44/71 | LOSS: 1.531808425675586e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 45/71 | LOSS: 1.5237737304984046e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 46/71 | LOSS: 1.5219282876716262e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 47/71 | LOSS: 1.5185948446590677e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 48/71 | LOSS: 1.513350611164743e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 49/71 | LOSS: 1.5160457714955554e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 50/71 | LOSS: 1.5211508909095416e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 51/71 | LOSS: 1.517566686113871e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 52/71 | LOSS: 1.5158743890252773e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 53/71 | LOSS: 1.5113493696440451e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 54/71 | LOSS: 1.509092761063419e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 55/71 | LOSS: 1.5053934004884337e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 56/71 | LOSS: 1.504344035901452e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 57/71 | LOSS: 1.5084711949071035e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 58/71 | LOSS: 1.5078286797704048e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 59/71 | LOSS: 1.506079030756761e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 60/71 | LOSS: 1.5137203785134708e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 61/71 | LOSS: 1.5059485870170324e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 62/71 | LOSS: 1.5104149280800594e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 63/71 | LOSS: 1.5136835614271149e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 64/71 | LOSS: 1.5132229810450763e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 65/71 | LOSS: 1.5175692403841602e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 66/71 | LOSS: 1.5179979106790991e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 67/71 | LOSS: 1.5163506344940876e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 68/71 | LOSS: 1.5156707627613576e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 69/71 | LOSS: 1.5172186340350891e-05\n",
      "TRAIN: EPOCH 85/1000 | BATCH 70/71 | LOSS: 1.5086254292404653e-05\n",
      "VAL: EPOCH 85/1000 | BATCH 0/8 | LOSS: 1.3964878235128708e-05\n",
      "VAL: EPOCH 85/1000 | BATCH 1/8 | LOSS: 1.3643958482134622e-05\n",
      "VAL: EPOCH 85/1000 | BATCH 2/8 | LOSS: 1.5059267752803862e-05\n",
      "VAL: EPOCH 85/1000 | BATCH 3/8 | LOSS: 1.5027000927148038e-05\n",
      "VAL: EPOCH 85/1000 | BATCH 4/8 | LOSS: 1.4621068658016156e-05\n",
      "VAL: EPOCH 85/1000 | BATCH 5/8 | LOSS: 1.482940676093373e-05\n",
      "VAL: EPOCH 85/1000 | BATCH 6/8 | LOSS: 1.450243858666259e-05\n",
      "VAL: EPOCH 85/1000 | BATCH 7/8 | LOSS: 1.4206643868419633e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 0/71 | LOSS: 1.5133278793655336e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 1/71 | LOSS: 1.5101744793355465e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 2/71 | LOSS: 1.555672376222598e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 3/71 | LOSS: 1.4603490853914991e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 4/71 | LOSS: 1.4580534843844361e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 5/71 | LOSS: 1.4753369214304257e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 6/71 | LOSS: 1.4827312043053098e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 7/71 | LOSS: 1.5053160268507781e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 8/71 | LOSS: 1.4820922034333409e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 9/71 | LOSS: 1.5158126461756184e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 10/71 | LOSS: 1.5046261565161826e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 11/71 | LOSS: 1.4911396495638959e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 12/71 | LOSS: 1.5056513802846894e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 13/71 | LOSS: 1.4964775540907535e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 14/71 | LOSS: 1.4655734654904032e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 15/71 | LOSS: 1.4670236623715027e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 16/71 | LOSS: 1.4766330423299223e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 17/71 | LOSS: 1.4655887652123864e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 18/71 | LOSS: 1.4815798606018929e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 19/71 | LOSS: 1.486145256421878e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 20/71 | LOSS: 1.4786992757026815e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 21/71 | LOSS: 1.4812157050156118e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 22/71 | LOSS: 1.4924214591029221e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 23/71 | LOSS: 1.4877889346583592e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 24/71 | LOSS: 1.4815746144449804e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 25/71 | LOSS: 1.4879392403180156e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 26/71 | LOSS: 1.5003144485321914e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 27/71 | LOSS: 1.4944603500615422e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 28/71 | LOSS: 1.5098292508276966e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 29/71 | LOSS: 1.5042217971010056e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 30/71 | LOSS: 1.5114772542156355e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 31/71 | LOSS: 1.5005129256451255e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 32/71 | LOSS: 1.5015876559835196e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 33/71 | LOSS: 1.501630464083385e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 34/71 | LOSS: 1.5005049967190383e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 35/71 | LOSS: 1.4946173703517868e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 36/71 | LOSS: 1.4860895896925138e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 37/71 | LOSS: 1.4870891103974396e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 38/71 | LOSS: 1.4804488903162284e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 39/71 | LOSS: 1.4752364336345636e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 40/71 | LOSS: 1.4816099326530892e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 41/71 | LOSS: 1.4846739434558134e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 42/71 | LOSS: 1.4840210729575078e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 43/71 | LOSS: 1.4804765318000467e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 44/71 | LOSS: 1.4775296040170361e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 45/71 | LOSS: 1.4837870189101364e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 46/71 | LOSS: 1.476077954492289e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 47/71 | LOSS: 1.475461342200409e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 48/71 | LOSS: 1.478133161034915e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 49/71 | LOSS: 1.4723517178936163e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 50/71 | LOSS: 1.4673508749361717e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 51/71 | LOSS: 1.4594369111784124e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 52/71 | LOSS: 1.4557695925210488e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 53/71 | LOSS: 1.4598226850163579e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 54/71 | LOSS: 1.4609106156885074e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 55/71 | LOSS: 1.4545063452585185e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 56/71 | LOSS: 1.4614815690468899e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 57/71 | LOSS: 1.4602883656434956e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 58/71 | LOSS: 1.4550966147522806e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 59/71 | LOSS: 1.4564965385943652e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 60/71 | LOSS: 1.4566315809640743e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 61/71 | LOSS: 1.4576298986691156e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 62/71 | LOSS: 1.4589189843732923e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 63/71 | LOSS: 1.4559843720007848e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 64/71 | LOSS: 1.4557843888514281e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 65/71 | LOSS: 1.4534533888901493e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 66/71 | LOSS: 1.4494769477177718e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 67/71 | LOSS: 1.4458805373899796e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 68/71 | LOSS: 1.4428283382427333e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 69/71 | LOSS: 1.4429432056723662e-05\n",
      "TRAIN: EPOCH 86/1000 | BATCH 70/71 | LOSS: 1.4378955557718577e-05\n",
      "VAL: EPOCH 86/1000 | BATCH 0/8 | LOSS: 1.3163722542230971e-05\n",
      "VAL: EPOCH 86/1000 | BATCH 1/8 | LOSS: 1.3213802958489396e-05\n",
      "VAL: EPOCH 86/1000 | BATCH 2/8 | LOSS: 1.4763633468343565e-05\n",
      "VAL: EPOCH 86/1000 | BATCH 3/8 | LOSS: 1.469768290007778e-05\n",
      "VAL: EPOCH 86/1000 | BATCH 4/8 | LOSS: 1.4296738481789361e-05\n",
      "VAL: EPOCH 86/1000 | BATCH 5/8 | LOSS: 1.4258079166514412e-05\n",
      "VAL: EPOCH 86/1000 | BATCH 6/8 | LOSS: 1.3892251185357704e-05\n",
      "VAL: EPOCH 86/1000 | BATCH 7/8 | LOSS: 1.344271788639162e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 0/71 | LOSS: 1.4751305570825934e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 1/71 | LOSS: 1.3734613730775891e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 2/71 | LOSS: 1.4530678223915553e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 3/71 | LOSS: 1.5122590639293776e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 4/71 | LOSS: 1.4933724196453113e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 5/71 | LOSS: 1.456078355962139e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 6/71 | LOSS: 1.4341360318732249e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 7/71 | LOSS: 1.3796146618005878e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 8/71 | LOSS: 1.3591983891577305e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 9/71 | LOSS: 1.3763895458396291e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 10/71 | LOSS: 1.3289525585688269e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 11/71 | LOSS: 1.3466068821799126e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 12/71 | LOSS: 1.3379926153552682e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 13/71 | LOSS: 1.3487255923142324e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 14/71 | LOSS: 1.366432285673606e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 15/71 | LOSS: 1.3609935308522836e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 16/71 | LOSS: 1.3408541717046105e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 17/71 | LOSS: 1.318536341285734e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 18/71 | LOSS: 1.3380438327945548e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 19/71 | LOSS: 1.3339344104679184e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 20/71 | LOSS: 1.3341884715330144e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 21/71 | LOSS: 1.3313272616373565e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 22/71 | LOSS: 1.3237665845498787e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 23/71 | LOSS: 1.3153947899506116e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 24/71 | LOSS: 1.3270850977278315e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 25/71 | LOSS: 1.3267026518476017e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 26/71 | LOSS: 1.3268901360182403e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 27/71 | LOSS: 1.324416228141802e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 28/71 | LOSS: 1.3267388818223544e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 29/71 | LOSS: 1.325976888134998e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 30/71 | LOSS: 1.3155838572739384e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 31/71 | LOSS: 1.3126189543299915e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 32/71 | LOSS: 1.3179275315037971e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 33/71 | LOSS: 1.3280598790622795e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 34/71 | LOSS: 1.3234825045011738e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 35/71 | LOSS: 1.325622952208505e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 36/71 | LOSS: 1.3248471200436265e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 37/71 | LOSS: 1.3327798330321553e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 38/71 | LOSS: 1.3300923805777878e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 39/71 | LOSS: 1.3419206584330823e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 40/71 | LOSS: 1.3445045313834803e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 41/71 | LOSS: 1.3482270116496476e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 42/71 | LOSS: 1.3514867321138683e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 43/71 | LOSS: 1.3519675390356992e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 44/71 | LOSS: 1.3509919638939511e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 45/71 | LOSS: 1.3625048189208863e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 46/71 | LOSS: 1.3685897508192749e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 47/71 | LOSS: 1.3789922491014295e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 48/71 | LOSS: 1.3850218869154152e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 49/71 | LOSS: 1.3858211150363787e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 50/71 | LOSS: 1.392221125285027e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 51/71 | LOSS: 1.39202587573751e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 52/71 | LOSS: 1.403066622549646e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 53/71 | LOSS: 1.4033799524676633e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 54/71 | LOSS: 1.4054665619121145e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 55/71 | LOSS: 1.4146738505717491e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 56/71 | LOSS: 1.4152739200462269e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 57/71 | LOSS: 1.4187721858590897e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 58/71 | LOSS: 1.4183132669255059e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 59/71 | LOSS: 1.4210400574180918e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 60/71 | LOSS: 1.422322357477063e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 61/71 | LOSS: 1.4241221423059958e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 62/71 | LOSS: 1.4231938578069231e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 63/71 | LOSS: 1.425876972405149e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 64/71 | LOSS: 1.4263789671531412e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 65/71 | LOSS: 1.4269248519364433e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 66/71 | LOSS: 1.4248664649755089e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 67/71 | LOSS: 1.426980806814647e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 68/71 | LOSS: 1.4251165233925322e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 69/71 | LOSS: 1.4237339928513393e-05\n",
      "TRAIN: EPOCH 87/1000 | BATCH 70/71 | LOSS: 1.417095468305363e-05\n",
      "VAL: EPOCH 87/1000 | BATCH 0/8 | LOSS: 1.3678681170858908e-05\n",
      "VAL: EPOCH 87/1000 | BATCH 1/8 | LOSS: 1.4387984720087843e-05\n",
      "VAL: EPOCH 87/1000 | BATCH 2/8 | LOSS: 1.5151721830382788e-05\n",
      "VAL: EPOCH 87/1000 | BATCH 3/8 | LOSS: 1.4748731473446242e-05\n",
      "VAL: EPOCH 87/1000 | BATCH 4/8 | LOSS: 1.4356796054926235e-05\n",
      "VAL: EPOCH 87/1000 | BATCH 5/8 | LOSS: 1.4357786767504876e-05\n",
      "VAL: EPOCH 87/1000 | BATCH 6/8 | LOSS: 1.4087574137582643e-05\n",
      "VAL: EPOCH 87/1000 | BATCH 7/8 | LOSS: 1.3762325238531048e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 0/71 | LOSS: 1.4104690308158752e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 1/71 | LOSS: 1.5412551420013187e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 2/71 | LOSS: 1.600260444926486e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 3/71 | LOSS: 1.546673161101353e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 4/71 | LOSS: 1.4597269364458043e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 5/71 | LOSS: 1.4412485294694003e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 6/71 | LOSS: 1.5114352338839257e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 7/71 | LOSS: 1.5270180597326544e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 8/71 | LOSS: 1.5240912944136653e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 9/71 | LOSS: 1.55899812853022e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 10/71 | LOSS: 1.5693655437975064e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 11/71 | LOSS: 1.5823308558537974e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 12/71 | LOSS: 1.5890701014051538e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 13/71 | LOSS: 1.5631577753083548e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 14/71 | LOSS: 1.546797660315254e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 15/71 | LOSS: 1.5478014233849535e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 16/71 | LOSS: 1.5483235769994173e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 17/71 | LOSS: 1.5210501462408703e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 18/71 | LOSS: 1.5210932408655553e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 19/71 | LOSS: 1.5137926993702422e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 20/71 | LOSS: 1.5153494998577072e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 21/71 | LOSS: 1.5118939038736492e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 22/71 | LOSS: 1.5045944604460839e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 23/71 | LOSS: 1.5055622952786507e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 24/71 | LOSS: 1.5017944788269233e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 25/71 | LOSS: 1.5019878691628737e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 26/71 | LOSS: 1.4941209580380624e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 27/71 | LOSS: 1.4833607045667513e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 28/71 | LOSS: 1.4806627405050676e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 29/71 | LOSS: 1.4774127521377522e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 30/71 | LOSS: 1.4737886416579357e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 31/71 | LOSS: 1.4829947474481742e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 32/71 | LOSS: 1.4845547246460972e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 33/71 | LOSS: 1.4830176817614461e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 34/71 | LOSS: 1.4680498861707746e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 35/71 | LOSS: 1.4603507730094458e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 36/71 | LOSS: 1.462117197169809e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 37/71 | LOSS: 1.4674724633024246e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 38/71 | LOSS: 1.455167390910598e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 39/71 | LOSS: 1.4475671741820407e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 40/71 | LOSS: 1.4486363264639782e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 41/71 | LOSS: 1.450866864702437e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 42/71 | LOSS: 1.4416995729207397e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 43/71 | LOSS: 1.4358804409394823e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 44/71 | LOSS: 1.4352819279237236e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 45/71 | LOSS: 1.4361011341426764e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 46/71 | LOSS: 1.4319730809725018e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 47/71 | LOSS: 1.4277347853900816e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 48/71 | LOSS: 1.4268848756227252e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 49/71 | LOSS: 1.4266663165471981e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 50/71 | LOSS: 1.4233219154252142e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 51/71 | LOSS: 1.4224754295355524e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 52/71 | LOSS: 1.4272748111974545e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 53/71 | LOSS: 1.4292474841365927e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 54/71 | LOSS: 1.4282499449274673e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 55/71 | LOSS: 1.4346190547647503e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 56/71 | LOSS: 1.431137168669681e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 57/71 | LOSS: 1.4334733883703753e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 58/71 | LOSS: 1.435443023106249e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 59/71 | LOSS: 1.4356655295462891e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 60/71 | LOSS: 1.4376820411702968e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 61/71 | LOSS: 1.4373120144051838e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 62/71 | LOSS: 1.4353244787055282e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 63/71 | LOSS: 1.4365569910523845e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 64/71 | LOSS: 1.4339330650377983e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 65/71 | LOSS: 1.4340846808574332e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 66/71 | LOSS: 1.4301579187249716e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 67/71 | LOSS: 1.4261681585861987e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 68/71 | LOSS: 1.4245942981253155e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 69/71 | LOSS: 1.4183969610063026e-05\n",
      "TRAIN: EPOCH 88/1000 | BATCH 70/71 | LOSS: 1.4138350681451799e-05\n",
      "VAL: EPOCH 88/1000 | BATCH 0/8 | LOSS: 1.3195196515880525e-05\n",
      "VAL: EPOCH 88/1000 | BATCH 1/8 | LOSS: 1.3128355476510478e-05\n",
      "VAL: EPOCH 88/1000 | BATCH 2/8 | LOSS: 1.4048156723826347e-05\n",
      "VAL: EPOCH 88/1000 | BATCH 3/8 | LOSS: 1.3596594953924068e-05\n",
      "VAL: EPOCH 88/1000 | BATCH 4/8 | LOSS: 1.3342354395717849e-05\n",
      "VAL: EPOCH 88/1000 | BATCH 5/8 | LOSS: 1.358244981020107e-05\n",
      "VAL: EPOCH 88/1000 | BATCH 6/8 | LOSS: 1.3212353613718214e-05\n",
      "VAL: EPOCH 88/1000 | BATCH 7/8 | LOSS: 1.2977356732335465e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 0/71 | LOSS: 1.0015030056820251e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 1/71 | LOSS: 1.1197765161341522e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 2/71 | LOSS: 1.2758897961854624e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 3/71 | LOSS: 1.2910292753076646e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 4/71 | LOSS: 1.2985022840439342e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 5/71 | LOSS: 1.3069719746757377e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 6/71 | LOSS: 1.2710010196315125e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 7/71 | LOSS: 1.303848557654419e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 8/71 | LOSS: 1.2766351169880687e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 9/71 | LOSS: 1.269038621103391e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 10/71 | LOSS: 1.2780678265368227e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 11/71 | LOSS: 1.3085700174997328e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 12/71 | LOSS: 1.2975943508746031e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 13/71 | LOSS: 1.3211167372771473e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 14/71 | LOSS: 1.3355027112993411e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 15/71 | LOSS: 1.3458487273965147e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 16/71 | LOSS: 1.3535692179335884e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 17/71 | LOSS: 1.3743201129222547e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 18/71 | LOSS: 1.3855282304575667e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 19/71 | LOSS: 1.4013349482411285e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 20/71 | LOSS: 1.4070828910917044e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 21/71 | LOSS: 1.393940944829162e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 22/71 | LOSS: 1.398951241318845e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 23/71 | LOSS: 1.405120224262646e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 24/71 | LOSS: 1.3892465940443799e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 25/71 | LOSS: 1.4119070608733007e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 26/71 | LOSS: 1.4118626832323909e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 27/71 | LOSS: 1.4198843278531317e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 28/71 | LOSS: 1.41129513739825e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 29/71 | LOSS: 1.4050621151303252e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 30/71 | LOSS: 1.4006060683318684e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 31/71 | LOSS: 1.4034813148100511e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 32/71 | LOSS: 1.4024557458469644e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 33/71 | LOSS: 1.402213571482742e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 34/71 | LOSS: 1.4028115011959536e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 35/71 | LOSS: 1.3983723192723119e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 36/71 | LOSS: 1.404788843108297e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 37/71 | LOSS: 1.4049213835900637e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 38/71 | LOSS: 1.3990182593167843e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 39/71 | LOSS: 1.3939148288955039e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 40/71 | LOSS: 1.3941133080906429e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 41/71 | LOSS: 1.3930440023354354e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 42/71 | LOSS: 1.3852380974174932e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 43/71 | LOSS: 1.387684886305413e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 44/71 | LOSS: 1.3843901110198608e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 45/71 | LOSS: 1.3860921369442899e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 46/71 | LOSS: 1.3903697997212726e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 47/71 | LOSS: 1.3807684638322826e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 48/71 | LOSS: 1.3823233066246683e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 49/71 | LOSS: 1.3768295757472514e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 50/71 | LOSS: 1.369979077471904e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 51/71 | LOSS: 1.3674821450247411e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 52/71 | LOSS: 1.362383838293784e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 53/71 | LOSS: 1.3610424334560608e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 54/71 | LOSS: 1.363088925824162e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 55/71 | LOSS: 1.3625511931942519e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 56/71 | LOSS: 1.3569094016131936e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 57/71 | LOSS: 1.3522507225420198e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 58/71 | LOSS: 1.3520706334482815e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 59/71 | LOSS: 1.3476412732416065e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 60/71 | LOSS: 1.3422382015378986e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 61/71 | LOSS: 1.3450223861817054e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 62/71 | LOSS: 1.3477975901170996e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 63/71 | LOSS: 1.3475663578788044e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 64/71 | LOSS: 1.3513457969775817e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 65/71 | LOSS: 1.345364537910703e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 66/71 | LOSS: 1.352904396231109e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 67/71 | LOSS: 1.3519378800243858e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 68/71 | LOSS: 1.3516100214315964e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 69/71 | LOSS: 1.3558035386397803e-05\n",
      "TRAIN: EPOCH 89/1000 | BATCH 70/71 | LOSS: 1.3547613825337489e-05\n",
      "VAL: EPOCH 89/1000 | BATCH 0/8 | LOSS: 1.6029836842790246e-05\n",
      "VAL: EPOCH 89/1000 | BATCH 1/8 | LOSS: 1.7055222997441888e-05\n",
      "VAL: EPOCH 89/1000 | BATCH 2/8 | LOSS: 1.7397386424515087e-05\n",
      "VAL: EPOCH 89/1000 | BATCH 3/8 | LOSS: 1.664098795117752e-05\n",
      "VAL: EPOCH 89/1000 | BATCH 4/8 | LOSS: 1.60605517521617e-05\n",
      "VAL: EPOCH 89/1000 | BATCH 5/8 | LOSS: 1.58834313879197e-05\n",
      "VAL: EPOCH 89/1000 | BATCH 6/8 | LOSS: 1.557968490877621e-05\n",
      "VAL: EPOCH 89/1000 | BATCH 7/8 | LOSS: 1.5064822719068616e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 0/71 | LOSS: 1.4878964066156186e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 1/71 | LOSS: 1.4607005596189993e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 2/71 | LOSS: 1.4890028069203254e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 3/71 | LOSS: 1.4975819112805766e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 4/71 | LOSS: 1.5893437193881255e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 5/71 | LOSS: 1.647221521731505e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 6/71 | LOSS: 1.5739245879688368e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 7/71 | LOSS: 1.5825299556126993e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 8/71 | LOSS: 1.5904647827685243e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 9/71 | LOSS: 1.5459570477105444e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 10/71 | LOSS: 1.5469781167418908e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 11/71 | LOSS: 1.535181195322366e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 12/71 | LOSS: 1.578876680678849e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 13/71 | LOSS: 1.5856520738972385e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 14/71 | LOSS: 1.573514564370271e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 15/71 | LOSS: 1.5466335071323556e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 16/71 | LOSS: 1.544666541603339e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 17/71 | LOSS: 1.5409035364023617e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 18/71 | LOSS: 1.5426995129306115e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 19/71 | LOSS: 1.5301396342692896e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 20/71 | LOSS: 1.5125473336887635e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 21/71 | LOSS: 1.5187752813455353e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 22/71 | LOSS: 1.5144096165465232e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 23/71 | LOSS: 1.4983424080128316e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 24/71 | LOSS: 1.5018655540188775e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 25/71 | LOSS: 1.4938989841959063e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 26/71 | LOSS: 1.4938014592863274e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 27/71 | LOSS: 1.482524450174034e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 28/71 | LOSS: 1.4748760428307738e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 29/71 | LOSS: 1.4792743786529173e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 30/71 | LOSS: 1.474045277591945e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 31/71 | LOSS: 1.4629546313926767e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 32/71 | LOSS: 1.4508079099565899e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 33/71 | LOSS: 1.4448325172826812e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 34/71 | LOSS: 1.4443910393830655e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 35/71 | LOSS: 1.4439893170169347e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 36/71 | LOSS: 1.4494635841991703e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 37/71 | LOSS: 1.4463733996786973e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 38/71 | LOSS: 1.4538838536865138e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 39/71 | LOSS: 1.4524794664794172e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 40/71 | LOSS: 1.447114650878442e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 41/71 | LOSS: 1.4456107345765867e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 42/71 | LOSS: 1.450106476621058e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 43/71 | LOSS: 1.4492225442485175e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 44/71 | LOSS: 1.4553557391789379e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 45/71 | LOSS: 1.4523026592265207e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 46/71 | LOSS: 1.4478300553046792e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 47/71 | LOSS: 1.4451913348996944e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 48/71 | LOSS: 1.4421914747057303e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 49/71 | LOSS: 1.4400065174413612e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 50/71 | LOSS: 1.438446778432562e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 51/71 | LOSS: 1.4412826116173304e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 52/71 | LOSS: 1.441271215642117e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 53/71 | LOSS: 1.4363308966696624e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 54/71 | LOSS: 1.4368767998265949e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 55/71 | LOSS: 1.4385901749847108e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 56/71 | LOSS: 1.4409353944325917e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 57/71 | LOSS: 1.4399364317796426e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 58/71 | LOSS: 1.4424845237769305e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 59/71 | LOSS: 1.4465271412215468e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 60/71 | LOSS: 1.4458247681089188e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 61/71 | LOSS: 1.4428961901191336e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 62/71 | LOSS: 1.4377603538233662e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 63/71 | LOSS: 1.4409593788400343e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 64/71 | LOSS: 1.439917917965571e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 65/71 | LOSS: 1.4347946944326395e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 66/71 | LOSS: 1.4336104529198266e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 67/71 | LOSS: 1.4309823320539127e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 68/71 | LOSS: 1.4273315041530134e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 69/71 | LOSS: 1.4259002377262472e-05\n",
      "TRAIN: EPOCH 90/1000 | BATCH 70/71 | LOSS: 1.4171248744467346e-05\n",
      "VAL: EPOCH 90/1000 | BATCH 0/8 | LOSS: 1.3140735973138362e-05\n",
      "VAL: EPOCH 90/1000 | BATCH 1/8 | LOSS: 1.3613233022624627e-05\n",
      "VAL: EPOCH 90/1000 | BATCH 2/8 | LOSS: 1.4902001566952094e-05\n",
      "VAL: EPOCH 90/1000 | BATCH 3/8 | LOSS: 1.484525978412421e-05\n",
      "VAL: EPOCH 90/1000 | BATCH 4/8 | LOSS: 1.4323209870781285e-05\n",
      "VAL: EPOCH 90/1000 | BATCH 5/8 | LOSS: 1.4176991498970892e-05\n",
      "VAL: EPOCH 90/1000 | BATCH 6/8 | LOSS: 1.3869545260344499e-05\n",
      "VAL: EPOCH 90/1000 | BATCH 7/8 | LOSS: 1.3314826446730876e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 0/71 | LOSS: 1.1880750207637902e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 1/71 | LOSS: 1.0651290267560398e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 2/71 | LOSS: 1.13853545068802e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 3/71 | LOSS: 1.1791985798481619e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 4/71 | LOSS: 1.2544979472295382e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 5/71 | LOSS: 1.2708984286291525e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 6/71 | LOSS: 1.2320541697720597e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 7/71 | LOSS: 1.2316775837462046e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 8/71 | LOSS: 1.2922444309677101e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 9/71 | LOSS: 1.3235483129392378e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 10/71 | LOSS: 1.3353403119253926e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 11/71 | LOSS: 1.3465071788232308e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 12/71 | LOSS: 1.3481906884967779e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 13/71 | LOSS: 1.3360689146273736e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 14/71 | LOSS: 1.350818511127727e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 15/71 | LOSS: 1.3608674521492503e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 16/71 | LOSS: 1.3675664483932505e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 17/71 | LOSS: 1.358083894350178e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 18/71 | LOSS: 1.3557632452207863e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 19/71 | LOSS: 1.3477788797899848e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 20/71 | LOSS: 1.3577115602120535e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 21/71 | LOSS: 1.3633460267473394e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 22/71 | LOSS: 1.3693659876361894e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 23/71 | LOSS: 1.3600385348884933e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 24/71 | LOSS: 1.3574714030255563e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 25/71 | LOSS: 1.3360986136271655e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 26/71 | LOSS: 1.3643579492323894e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 27/71 | LOSS: 1.3904687485981932e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 28/71 | LOSS: 1.3830063929570565e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 29/71 | LOSS: 1.3887008117308142e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 30/71 | LOSS: 1.3923131958900711e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 31/71 | LOSS: 1.3925306660667047e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 32/71 | LOSS: 1.386492735808574e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 33/71 | LOSS: 1.3795159996815902e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 34/71 | LOSS: 1.3846675834169479e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 35/71 | LOSS: 1.382100425770558e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 36/71 | LOSS: 1.3770559038479209e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 37/71 | LOSS: 1.3685972733487448e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 38/71 | LOSS: 1.3617664197413549e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 39/71 | LOSS: 1.3658131160809717e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 40/71 | LOSS: 1.367868263492355e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 41/71 | LOSS: 1.3652110218182706e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 42/71 | LOSS: 1.3668971764990454e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 43/71 | LOSS: 1.361283226618649e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 44/71 | LOSS: 1.3660073616645403e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 45/71 | LOSS: 1.3753103461488575e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 46/71 | LOSS: 1.3775440454489224e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 47/71 | LOSS: 1.374988541632168e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 48/71 | LOSS: 1.3784363304415708e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 49/71 | LOSS: 1.3821950215060497e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 50/71 | LOSS: 1.3767454701712054e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 51/71 | LOSS: 1.389431493127911e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 52/71 | LOSS: 1.3846625974248076e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 53/71 | LOSS: 1.3889733694398913e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 54/71 | LOSS: 1.3936477295250039e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 55/71 | LOSS: 1.3970868102270678e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 56/71 | LOSS: 1.4027641985816636e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 57/71 | LOSS: 1.4021248606977993e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 58/71 | LOSS: 1.3972095938836485e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 59/71 | LOSS: 1.4048112082794736e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 60/71 | LOSS: 1.4065000839048584e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 61/71 | LOSS: 1.4106440821412815e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 62/71 | LOSS: 1.4070679840404496e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 63/71 | LOSS: 1.4040042628948868e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 64/71 | LOSS: 1.4065755525594935e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 65/71 | LOSS: 1.4032516777330205e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 66/71 | LOSS: 1.3957963306187485e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 67/71 | LOSS: 1.3909950345859055e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 68/71 | LOSS: 1.394276907798274e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 69/71 | LOSS: 1.3936658667392164e-05\n",
      "TRAIN: EPOCH 91/1000 | BATCH 70/71 | LOSS: 1.387073281337917e-05\n",
      "VAL: EPOCH 91/1000 | BATCH 0/8 | LOSS: 1.2500711818574928e-05\n",
      "VAL: EPOCH 91/1000 | BATCH 1/8 | LOSS: 1.2855654858867638e-05\n",
      "VAL: EPOCH 91/1000 | BATCH 2/8 | LOSS: 1.3471560426599657e-05\n",
      "VAL: EPOCH 91/1000 | BATCH 3/8 | LOSS: 1.3174493233236717e-05\n",
      "VAL: EPOCH 91/1000 | BATCH 4/8 | LOSS: 1.2999806858715602e-05\n",
      "VAL: EPOCH 91/1000 | BATCH 5/8 | LOSS: 1.3220909446924148e-05\n",
      "VAL: EPOCH 91/1000 | BATCH 6/8 | LOSS: 1.2953807786938601e-05\n",
      "VAL: EPOCH 91/1000 | BATCH 7/8 | LOSS: 1.2718662674160441e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 0/71 | LOSS: 1.3827237125951797e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 1/71 | LOSS: 1.3096327620587545e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 2/71 | LOSS: 1.233228734539201e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 3/71 | LOSS: 1.2948576568305725e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 4/71 | LOSS: 1.4007484787725844e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 5/71 | LOSS: 1.3816435360543741e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 6/71 | LOSS: 1.380559453017278e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 7/71 | LOSS: 1.4218401247489965e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 8/71 | LOSS: 1.4099783382132753e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 9/71 | LOSS: 1.398338226863416e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 10/71 | LOSS: 1.4174596964783797e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 11/71 | LOSS: 1.4254236778773096e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 12/71 | LOSS: 1.4089298509553862e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 13/71 | LOSS: 1.3861066690878943e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 14/71 | LOSS: 1.3833289024963354e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 15/71 | LOSS: 1.3883403539693973e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 16/71 | LOSS: 1.385434231505154e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 17/71 | LOSS: 1.3752009686464185e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 18/71 | LOSS: 1.3729116912336873e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 19/71 | LOSS: 1.3642011072079186e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 20/71 | LOSS: 1.3611133302523134e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 21/71 | LOSS: 1.3660433523909887e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 22/71 | LOSS: 1.364745109996242e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 23/71 | LOSS: 1.3645307528046638e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 24/71 | LOSS: 1.365380630886648e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 25/71 | LOSS: 1.365365939748769e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 26/71 | LOSS: 1.3676030431708097e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 27/71 | LOSS: 1.3703409065003922e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 28/71 | LOSS: 1.3789143693053309e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 29/71 | LOSS: 1.3824408567112793e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 30/71 | LOSS: 1.3756399312114809e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 31/71 | LOSS: 1.3661141025522738e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 32/71 | LOSS: 1.3721778652159793e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 33/71 | LOSS: 1.3655915475229609e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 34/71 | LOSS: 1.3568154957153769e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 35/71 | LOSS: 1.3581522352978936e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 36/71 | LOSS: 1.3674216928405724e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 37/71 | LOSS: 1.3624931350358038e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 38/71 | LOSS: 1.351747635990017e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 39/71 | LOSS: 1.3540914073928434e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 40/71 | LOSS: 1.3549276445701253e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 41/71 | LOSS: 1.3523982791715147e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 42/71 | LOSS: 1.3494820640791954e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 43/71 | LOSS: 1.3435512445622324e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 44/71 | LOSS: 1.3395219684753102e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 45/71 | LOSS: 1.341823160502194e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 46/71 | LOSS: 1.3506929675752466e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 47/71 | LOSS: 1.3515704949895735e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 48/71 | LOSS: 1.3632689749974073e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 49/71 | LOSS: 1.365109603284509e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 50/71 | LOSS: 1.3684064416499014e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 51/71 | LOSS: 1.3627747543628524e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 52/71 | LOSS: 1.3721703685147969e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 53/71 | LOSS: 1.3666813208180463e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 54/71 | LOSS: 1.365925294191005e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 55/71 | LOSS: 1.3646120594655389e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 56/71 | LOSS: 1.3687451667608816e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 57/71 | LOSS: 1.3698086510477628e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 58/71 | LOSS: 1.3673644481691477e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 59/71 | LOSS: 1.3593633032845295e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 60/71 | LOSS: 1.3609453088095313e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 61/71 | LOSS: 1.358735210399778e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 62/71 | LOSS: 1.353831974859257e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 63/71 | LOSS: 1.354816559739902e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 64/71 | LOSS: 1.3490515588133488e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 65/71 | LOSS: 1.351778181549779e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 66/71 | LOSS: 1.3466102639391303e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 67/71 | LOSS: 1.3449317186635145e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 68/71 | LOSS: 1.3387491982438656e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 69/71 | LOSS: 1.335383746793793e-05\n",
      "TRAIN: EPOCH 92/1000 | BATCH 70/71 | LOSS: 1.341769486418958e-05\n",
      "VAL: EPOCH 92/1000 | BATCH 0/8 | LOSS: 1.2401532330841292e-05\n",
      "VAL: EPOCH 92/1000 | BATCH 1/8 | LOSS: 1.286808037548326e-05\n",
      "VAL: EPOCH 92/1000 | BATCH 2/8 | LOSS: 1.3595312035855992e-05\n",
      "VAL: EPOCH 92/1000 | BATCH 3/8 | LOSS: 1.3258446415420622e-05\n",
      "VAL: EPOCH 92/1000 | BATCH 4/8 | LOSS: 1.2891644291812555e-05\n",
      "VAL: EPOCH 92/1000 | BATCH 5/8 | LOSS: 1.2868713990125494e-05\n",
      "VAL: EPOCH 92/1000 | BATCH 6/8 | LOSS: 1.2590267033374403e-05\n",
      "VAL: EPOCH 92/1000 | BATCH 7/8 | LOSS: 1.2191733503641444e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 0/71 | LOSS: 1.3651218068844173e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 1/71 | LOSS: 1.2202480320411269e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 2/71 | LOSS: 1.2432667062967084e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 3/71 | LOSS: 1.3014866453886498e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 4/71 | LOSS: 1.2072830941178836e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 5/71 | LOSS: 1.2170488844276406e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 6/71 | LOSS: 1.2091546912935363e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 7/71 | LOSS: 1.2058767879352672e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 8/71 | LOSS: 1.1976088899245951e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 9/71 | LOSS: 1.1930418531846954e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 10/71 | LOSS: 1.2339232274495192e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 11/71 | LOSS: 1.2416388396256176e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 12/71 | LOSS: 1.2412612634831861e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 13/71 | LOSS: 1.2138042166043306e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 14/71 | LOSS: 1.2161957844606756e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 15/71 | LOSS: 1.2132507492879085e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 16/71 | LOSS: 1.2230239489701275e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 17/71 | LOSS: 1.225795201915187e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 18/71 | LOSS: 1.2422849019793303e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 19/71 | LOSS: 1.2494858538047993e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 20/71 | LOSS: 1.2603197406168051e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 21/71 | LOSS: 1.2748070978638928e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 22/71 | LOSS: 1.280499307777854e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 23/71 | LOSS: 1.297055041504791e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 24/71 | LOSS: 1.33766996441409e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 25/71 | LOSS: 1.3509103295151503e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 26/71 | LOSS: 1.3437874528150402e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 27/71 | LOSS: 1.3743170614231661e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 28/71 | LOSS: 1.3756793719196515e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 29/71 | LOSS: 1.3723474906631357e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 30/71 | LOSS: 1.3808211171577099e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 31/71 | LOSS: 1.3920286477286936e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 32/71 | LOSS: 1.3949534150015097e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 33/71 | LOSS: 1.4028434019136886e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 34/71 | LOSS: 1.395118270011153e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 35/71 | LOSS: 1.3903701604552529e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 36/71 | LOSS: 1.3984017223756874e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 37/71 | LOSS: 1.399810482900265e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 38/71 | LOSS: 1.3960426771611823e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 39/71 | LOSS: 1.3857767726221937e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 40/71 | LOSS: 1.3963398021410174e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 41/71 | LOSS: 1.3915287408530102e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 42/71 | LOSS: 1.398363284076981e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 43/71 | LOSS: 1.3975736051592817e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 44/71 | LOSS: 1.386744679621188e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 45/71 | LOSS: 1.3798581755469533e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 46/71 | LOSS: 1.3767290704678051e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 47/71 | LOSS: 1.3691671104728206e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 48/71 | LOSS: 1.3674017096417054e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 49/71 | LOSS: 1.3675781665369869e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 50/71 | LOSS: 1.3741760067561842e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 51/71 | LOSS: 1.373617068076363e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 52/71 | LOSS: 1.3718454592547133e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 53/71 | LOSS: 1.3688288803408526e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 54/71 | LOSS: 1.3675534418408378e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 55/71 | LOSS: 1.3641214260522766e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 56/71 | LOSS: 1.363395796560278e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 57/71 | LOSS: 1.3645510879823946e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 58/71 | LOSS: 1.3608963268718257e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 59/71 | LOSS: 1.355749038035962e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 60/71 | LOSS: 1.3506212378503778e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 61/71 | LOSS: 1.3485305448886074e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 62/71 | LOSS: 1.3481199583486794e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 63/71 | LOSS: 1.3429696153366422e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 64/71 | LOSS: 1.3408304794910901e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 65/71 | LOSS: 1.3397549509770187e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 66/71 | LOSS: 1.3413659654537723e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 67/71 | LOSS: 1.3374422544931054e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 68/71 | LOSS: 1.3389827156219246e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 69/71 | LOSS: 1.3435771415970521e-05\n",
      "TRAIN: EPOCH 93/1000 | BATCH 70/71 | LOSS: 1.3496574660755766e-05\n",
      "VAL: EPOCH 93/1000 | BATCH 0/8 | LOSS: 1.2610982594196685e-05\n",
      "VAL: EPOCH 93/1000 | BATCH 1/8 | LOSS: 1.354983442070079e-05\n",
      "VAL: EPOCH 93/1000 | BATCH 2/8 | LOSS: 1.4138801816443447e-05\n",
      "VAL: EPOCH 93/1000 | BATCH 3/8 | LOSS: 1.3784183693132945e-05\n",
      "VAL: EPOCH 93/1000 | BATCH 4/8 | LOSS: 1.3314459465618711e-05\n",
      "VAL: EPOCH 93/1000 | BATCH 5/8 | LOSS: 1.3175875513600962e-05\n",
      "VAL: EPOCH 93/1000 | BATCH 6/8 | LOSS: 1.2941607045442132e-05\n",
      "VAL: EPOCH 93/1000 | BATCH 7/8 | LOSS: 1.2512270700426598e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 0/71 | LOSS: 1.152720687969122e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 1/71 | LOSS: 1.2694124052359257e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 2/71 | LOSS: 1.1546476040772783e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 3/71 | LOSS: 1.1188586086063879e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 4/71 | LOSS: 1.1302331768092699e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 5/71 | LOSS: 1.2476502585438235e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 6/71 | LOSS: 1.2805184139454337e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 7/71 | LOSS: 1.2967028055754781e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 8/71 | LOSS: 1.3228928360654714e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 9/71 | LOSS: 1.3242878685559844e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 10/71 | LOSS: 1.3564712059054397e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 11/71 | LOSS: 1.3561090706086057e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 12/71 | LOSS: 1.3529727066970036e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 13/71 | LOSS: 1.3578541776431458e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 14/71 | LOSS: 1.3533209918629533e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 15/71 | LOSS: 1.3368086968057469e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 16/71 | LOSS: 1.3363393062557888e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 17/71 | LOSS: 1.3388054766336507e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 18/71 | LOSS: 1.3305148441807097e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 19/71 | LOSS: 1.3576851733887451e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 20/71 | LOSS: 1.366473376644232e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 21/71 | LOSS: 1.3549279918317387e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 22/71 | LOSS: 1.3614676113913605e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 23/71 | LOSS: 1.3416700956743929e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 24/71 | LOSS: 1.3479107801686041e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 25/71 | LOSS: 1.3458517199838892e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 26/71 | LOSS: 1.3394256812817832e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 27/71 | LOSS: 1.3215004530918253e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 28/71 | LOSS: 1.3262520638065701e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 29/71 | LOSS: 1.3248000595922348e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 30/71 | LOSS: 1.3239921328579751e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 31/71 | LOSS: 1.3190326797030139e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 32/71 | LOSS: 1.3113387384774713e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 33/71 | LOSS: 1.3035940382371123e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 34/71 | LOSS: 1.3083841700530944e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 35/71 | LOSS: 1.3124209797549864e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 36/71 | LOSS: 1.3131108504682276e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 37/71 | LOSS: 1.3154816363232002e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 38/71 | LOSS: 1.3156646043801e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 39/71 | LOSS: 1.3181672784412513e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 40/71 | LOSS: 1.3176245951532421e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 41/71 | LOSS: 1.3169250082899539e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 42/71 | LOSS: 1.3184869605411646e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 43/71 | LOSS: 1.318017108463788e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 44/71 | LOSS: 1.31642024724796e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 45/71 | LOSS: 1.315979856688342e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 46/71 | LOSS: 1.3271653033177457e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 47/71 | LOSS: 1.320613974561032e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 48/71 | LOSS: 1.32151614651571e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 49/71 | LOSS: 1.3175697276892606e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 50/71 | LOSS: 1.3172907957221683e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 51/71 | LOSS: 1.3142678688801914e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 52/71 | LOSS: 1.3186516607331797e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 53/71 | LOSS: 1.3100618481850554e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 54/71 | LOSS: 1.3066916736451358e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 55/71 | LOSS: 1.3023720254880442e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 56/71 | LOSS: 1.3033297731007016e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 57/71 | LOSS: 1.3042186514337964e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 58/71 | LOSS: 1.3044621806268155e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 59/71 | LOSS: 1.3043204065373478e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 60/71 | LOSS: 1.2975140947912491e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 61/71 | LOSS: 1.3048024415653462e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 62/71 | LOSS: 1.3058164926954464e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 63/71 | LOSS: 1.3062816535125421e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 64/71 | LOSS: 1.3045239910962454e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 65/71 | LOSS: 1.3002807642778587e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 66/71 | LOSS: 1.3027298323390484e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 67/71 | LOSS: 1.3017726480350521e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 68/71 | LOSS: 1.3036018519441301e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 69/71 | LOSS: 1.3031113979065724e-05\n",
      "TRAIN: EPOCH 94/1000 | BATCH 70/71 | LOSS: 1.2993425462156384e-05\n",
      "VAL: EPOCH 94/1000 | BATCH 0/8 | LOSS: 1.5235425962600857e-05\n",
      "VAL: EPOCH 94/1000 | BATCH 1/8 | LOSS: 1.5938176147756167e-05\n",
      "VAL: EPOCH 94/1000 | BATCH 2/8 | LOSS: 1.6523377780686133e-05\n",
      "VAL: EPOCH 94/1000 | BATCH 3/8 | LOSS: 1.601283088348282e-05\n",
      "VAL: EPOCH 94/1000 | BATCH 4/8 | LOSS: 1.5279307262971997e-05\n",
      "VAL: EPOCH 94/1000 | BATCH 5/8 | LOSS: 1.4897409376620393e-05\n",
      "VAL: EPOCH 94/1000 | BATCH 6/8 | LOSS: 1.4635192850879061e-05\n",
      "VAL: EPOCH 94/1000 | BATCH 7/8 | LOSS: 1.3989989156470983e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 0/71 | LOSS: 1.3032377864874434e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 1/71 | LOSS: 1.192240915770526e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 2/71 | LOSS: 1.4051899597689044e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 3/71 | LOSS: 1.4148756690701703e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 4/71 | LOSS: 1.4728050518897363e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 5/71 | LOSS: 1.5124111390226366e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 6/71 | LOSS: 1.4625573645129667e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 7/71 | LOSS: 1.453785603189317e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 8/71 | LOSS: 1.4297654262110074e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 9/71 | LOSS: 1.4203311002347618e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 10/71 | LOSS: 1.4008168363943696e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 11/71 | LOSS: 1.3890028033832399e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 12/71 | LOSS: 1.4183676075145531e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 13/71 | LOSS: 1.4258107122649172e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 14/71 | LOSS: 1.4179430824394027e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 15/71 | LOSS: 1.4281230505730491e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 16/71 | LOSS: 1.4370725051883388e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 17/71 | LOSS: 1.4381057452636822e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 18/71 | LOSS: 1.4277075407383228e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 19/71 | LOSS: 1.4474984709522687e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 20/71 | LOSS: 1.4362939293191413e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 21/71 | LOSS: 1.4381236972192049e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 22/71 | LOSS: 1.4299582724798592e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 23/71 | LOSS: 1.4283212863119843e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 24/71 | LOSS: 1.4169599198794458e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 25/71 | LOSS: 1.4152491215869444e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 26/71 | LOSS: 1.415889326179038e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 27/71 | LOSS: 1.40203390271511e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 28/71 | LOSS: 1.3974878882059973e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 29/71 | LOSS: 1.3905405618667524e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 30/71 | LOSS: 1.379904695626934e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 31/71 | LOSS: 1.3688833462310868e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 32/71 | LOSS: 1.3731142285960782e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 33/71 | LOSS: 1.3687813380409566e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 34/71 | LOSS: 1.3663176001240831e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 35/71 | LOSS: 1.3609662523271658e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 36/71 | LOSS: 1.3546637903247657e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 37/71 | LOSS: 1.3552515946158613e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 38/71 | LOSS: 1.3460171897186694e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 39/71 | LOSS: 1.3455323801281338e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 40/71 | LOSS: 1.343249490806867e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 41/71 | LOSS: 1.332646384287814e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 42/71 | LOSS: 1.3308210560184265e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 43/71 | LOSS: 1.3308862166509657e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 44/71 | LOSS: 1.3317425711850067e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 45/71 | LOSS: 1.3327304397693217e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 46/71 | LOSS: 1.3296937295501963e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 47/71 | LOSS: 1.3335915525658493e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 48/71 | LOSS: 1.3317098113920833e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 49/71 | LOSS: 1.3316527256392874e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 50/71 | LOSS: 1.3350943007418549e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 51/71 | LOSS: 1.3300494944045427e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 52/71 | LOSS: 1.3279939182366263e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 53/71 | LOSS: 1.3221040885751049e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 54/71 | LOSS: 1.320840383414179e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 55/71 | LOSS: 1.3221021455527599e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 56/71 | LOSS: 1.3233833414504484e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 57/71 | LOSS: 1.3252848287359892e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 58/71 | LOSS: 1.3293694360457928e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 59/71 | LOSS: 1.3269972972314767e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 60/71 | LOSS: 1.329677523572769e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 61/71 | LOSS: 1.3259762674293128e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 62/71 | LOSS: 1.3320453526541805e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 63/71 | LOSS: 1.3299232662689064e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 64/71 | LOSS: 1.324871640323321e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 65/71 | LOSS: 1.3234660552067604e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 66/71 | LOSS: 1.3261501177356564e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 67/71 | LOSS: 1.3260091209838934e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 68/71 | LOSS: 1.3253757535257256e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 69/71 | LOSS: 1.327509557346015e-05\n",
      "TRAIN: EPOCH 95/1000 | BATCH 70/71 | LOSS: 1.3313331781857004e-05\n",
      "VAL: EPOCH 95/1000 | BATCH 0/8 | LOSS: 1.2134997632529121e-05\n",
      "VAL: EPOCH 95/1000 | BATCH 1/8 | LOSS: 1.2428421541699208e-05\n",
      "VAL: EPOCH 95/1000 | BATCH 2/8 | LOSS: 1.3234093178956149e-05\n",
      "VAL: EPOCH 95/1000 | BATCH 3/8 | LOSS: 1.290312525270565e-05\n",
      "VAL: EPOCH 95/1000 | BATCH 4/8 | LOSS: 1.2492349014792126e-05\n",
      "VAL: EPOCH 95/1000 | BATCH 5/8 | LOSS: 1.2457717730285367e-05\n",
      "VAL: EPOCH 95/1000 | BATCH 6/8 | LOSS: 1.215336098019699e-05\n",
      "VAL: EPOCH 95/1000 | BATCH 7/8 | LOSS: 1.1740285572159337e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 0/71 | LOSS: 9.778957064554561e-06\n",
      "TRAIN: EPOCH 96/1000 | BATCH 1/71 | LOSS: 1.1063562396884663e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 2/71 | LOSS: 1.1846620509459171e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 3/71 | LOSS: 1.2462219274311792e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 4/71 | LOSS: 1.2384136425680481e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 5/71 | LOSS: 1.235353647643933e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 6/71 | LOSS: 1.261464642863887e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 7/71 | LOSS: 1.2271944456188066e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 8/71 | LOSS: 1.2259982617656027e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 9/71 | LOSS: 1.2168898501840886e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 10/71 | LOSS: 1.2028775513912974e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 11/71 | LOSS: 1.2196862371638417e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 12/71 | LOSS: 1.2131515457160556e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 13/71 | LOSS: 1.2040890071927737e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 14/71 | LOSS: 1.2102966623691221e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 15/71 | LOSS: 1.2019034386412386e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 16/71 | LOSS: 1.1932667803193908e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 17/71 | LOSS: 1.2044148863626308e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 18/71 | LOSS: 1.206621048990392e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 19/71 | LOSS: 1.2086703191016567e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 20/71 | LOSS: 1.2224723702404714e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 21/71 | LOSS: 1.2259744132527636e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 22/71 | LOSS: 1.2339452397376906e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 23/71 | LOSS: 1.2263738994988671e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 24/71 | LOSS: 1.22676169485203e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 25/71 | LOSS: 1.2280561000247522e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 26/71 | LOSS: 1.2233873247977397e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 27/71 | LOSS: 1.2255708920097927e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 28/71 | LOSS: 1.2230991505303196e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 29/71 | LOSS: 1.2238834581997556e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 30/71 | LOSS: 1.2313231045188712e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 31/71 | LOSS: 1.24746604228676e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 32/71 | LOSS: 1.246053254748624e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 33/71 | LOSS: 1.2537854699450477e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 34/71 | LOSS: 1.2603964647236613e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 35/71 | LOSS: 1.2590699472841354e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 36/71 | LOSS: 1.2707245897783306e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 37/71 | LOSS: 1.2713840222108716e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 38/71 | LOSS: 1.2743077977808216e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 39/71 | LOSS: 1.2884825673609157e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 40/71 | LOSS: 1.2925455493819843e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 41/71 | LOSS: 1.3045459105515398e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 42/71 | LOSS: 1.307077656197497e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 43/71 | LOSS: 1.3106040507244126e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 44/71 | LOSS: 1.314624454001508e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 45/71 | LOSS: 1.3152773273917694e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 46/71 | LOSS: 1.3150523877715039e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 47/71 | LOSS: 1.310650717793275e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 48/71 | LOSS: 1.3174613017401163e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 49/71 | LOSS: 1.3248191098682583e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 50/71 | LOSS: 1.3300531374589613e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 51/71 | LOSS: 1.3387521518710015e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 52/71 | LOSS: 1.3411101222890517e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 53/71 | LOSS: 1.3476496557511078e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 54/71 | LOSS: 1.3449370321309702e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 55/71 | LOSS: 1.3466825391463186e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 56/71 | LOSS: 1.3610192885001501e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 57/71 | LOSS: 1.3575655398669589e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 58/71 | LOSS: 1.3585109596626075e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 59/71 | LOSS: 1.3577247045759577e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 60/71 | LOSS: 1.3547105630945613e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 61/71 | LOSS: 1.3482642228360242e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 62/71 | LOSS: 1.347444018999037e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 63/71 | LOSS: 1.3514925441882042e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 64/71 | LOSS: 1.3507832647099769e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 65/71 | LOSS: 1.3524657443860743e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 66/71 | LOSS: 1.3511533392945116e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 67/71 | LOSS: 1.3443869072622928e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 68/71 | LOSS: 1.3438541946411336e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 69/71 | LOSS: 1.3443476694062285e-05\n",
      "TRAIN: EPOCH 96/1000 | BATCH 70/71 | LOSS: 1.3428449472510846e-05\n",
      "VAL: EPOCH 96/1000 | BATCH 0/8 | LOSS: 1.5675010217819363e-05\n",
      "VAL: EPOCH 96/1000 | BATCH 1/8 | LOSS: 1.6770497495599557e-05\n",
      "VAL: EPOCH 96/1000 | BATCH 2/8 | LOSS: 1.7140583925841685e-05\n",
      "VAL: EPOCH 96/1000 | BATCH 3/8 | LOSS: 1.6685944501659833e-05\n",
      "VAL: EPOCH 96/1000 | BATCH 4/8 | LOSS: 1.6102712106658145e-05\n",
      "VAL: EPOCH 96/1000 | BATCH 5/8 | LOSS: 1.570546510265558e-05\n",
      "VAL: EPOCH 96/1000 | BATCH 6/8 | LOSS: 1.5607825194560325e-05\n",
      "VAL: EPOCH 96/1000 | BATCH 7/8 | LOSS: 1.5060470218486444e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 0/71 | LOSS: 1.6110087017295882e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 1/71 | LOSS: 1.534955163151608e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 2/71 | LOSS: 1.5382298746165663e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 3/71 | LOSS: 1.5158140513449325e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 4/71 | LOSS: 1.5025243010313716e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 5/71 | LOSS: 1.5030032197197821e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 6/71 | LOSS: 1.5101549771705841e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 7/71 | LOSS: 1.532983935703669e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 8/71 | LOSS: 1.4834752214988435e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 9/71 | LOSS: 1.5105673173820833e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 10/71 | LOSS: 1.565094209416367e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 11/71 | LOSS: 1.5446721818079823e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 12/71 | LOSS: 1.5663974199224656e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 13/71 | LOSS: 1.6035287509501878e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 14/71 | LOSS: 1.588109201596429e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 15/71 | LOSS: 1.5922087527542317e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 16/71 | LOSS: 1.579108870560851e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 17/71 | LOSS: 1.5743561157756227e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 18/71 | LOSS: 1.5456962596784396e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 19/71 | LOSS: 1.5413134042319145e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 20/71 | LOSS: 1.5355230538088583e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 21/71 | LOSS: 1.5307136023197515e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 22/71 | LOSS: 1.557590515047828e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 23/71 | LOSS: 1.5468569586118974e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 24/71 | LOSS: 1.5384554972115437e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 25/71 | LOSS: 1.5449127658939688e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 26/71 | LOSS: 1.5397440788715527e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 27/71 | LOSS: 1.5286709705313634e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 28/71 | LOSS: 1.5231895868682527e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 29/71 | LOSS: 1.5183021332632052e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 30/71 | LOSS: 1.5227989552518157e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 31/71 | LOSS: 1.5173196345585893e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 32/71 | LOSS: 1.5064556547176009e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 33/71 | LOSS: 1.5023846625648302e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 34/71 | LOSS: 1.4914393458249314e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 35/71 | LOSS: 1.4778082762253083e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 36/71 | LOSS: 1.4719063123315258e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 37/71 | LOSS: 1.4766964652249284e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 38/71 | LOSS: 1.4727063246074324e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 39/71 | LOSS: 1.4818334238952958e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 40/71 | LOSS: 1.4819315144516777e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 41/71 | LOSS: 1.4808882692055444e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 42/71 | LOSS: 1.4783432709728638e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 43/71 | LOSS: 1.468077754328657e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 44/71 | LOSS: 1.4632955551658395e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 45/71 | LOSS: 1.4583304520151021e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 46/71 | LOSS: 1.4540271896545705e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 47/71 | LOSS: 1.445130034956795e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 48/71 | LOSS: 1.4412500326104738e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 49/71 | LOSS: 1.4376577746588737e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 50/71 | LOSS: 1.4318137105232489e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 51/71 | LOSS: 1.4229150337996543e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 52/71 | LOSS: 1.4249066483006453e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 53/71 | LOSS: 1.4179846284942297e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 54/71 | LOSS: 1.4179535306042949e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 55/71 | LOSS: 1.4149635522439244e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 56/71 | LOSS: 1.4126644505028563e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 57/71 | LOSS: 1.4137758281039764e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 58/71 | LOSS: 1.4157766738701296e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 59/71 | LOSS: 1.4102899255400795e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 60/71 | LOSS: 1.416126630575774e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 61/71 | LOSS: 1.4146271232382259e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 62/71 | LOSS: 1.4132873585613053e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 63/71 | LOSS: 1.4119672101742253e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 64/71 | LOSS: 1.4107965146719764e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 65/71 | LOSS: 1.4123296130657598e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 66/71 | LOSS: 1.4170232758776515e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 67/71 | LOSS: 1.4117777121047016e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 68/71 | LOSS: 1.4089337457625188e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 69/71 | LOSS: 1.4177300116508768e-05\n",
      "TRAIN: EPOCH 97/1000 | BATCH 70/71 | LOSS: 1.4140500355443039e-05\n",
      "VAL: EPOCH 97/1000 | BATCH 0/8 | LOSS: 1.229183362738695e-05\n",
      "VAL: EPOCH 97/1000 | BATCH 1/8 | LOSS: 1.2576742392411688e-05\n",
      "VAL: EPOCH 97/1000 | BATCH 2/8 | LOSS: 1.3268400228601726e-05\n",
      "VAL: EPOCH 97/1000 | BATCH 3/8 | LOSS: 1.2928305068271584e-05\n",
      "VAL: EPOCH 97/1000 | BATCH 4/8 | LOSS: 1.2674048230110202e-05\n",
      "VAL: EPOCH 97/1000 | BATCH 5/8 | LOSS: 1.2848642503134519e-05\n",
      "VAL: EPOCH 97/1000 | BATCH 6/8 | LOSS: 1.2494277013632069e-05\n",
      "VAL: EPOCH 97/1000 | BATCH 7/8 | LOSS: 1.2217435482853034e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 0/71 | LOSS: 1.1715510481735691e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 1/71 | LOSS: 1.2985327884962317e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 2/71 | LOSS: 1.1852882380480878e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 3/71 | LOSS: 1.2645206197703374e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 4/71 | LOSS: 1.2321803114900832e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 5/71 | LOSS: 1.258642851098557e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 6/71 | LOSS: 1.2350364938486433e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 7/71 | LOSS: 1.2481146541176713e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 8/71 | LOSS: 1.2556162699992354e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 9/71 | LOSS: 1.2573606181831564e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 10/71 | LOSS: 1.2529821932813237e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 11/71 | LOSS: 1.2778226391674252e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 12/71 | LOSS: 1.2928986437215757e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 13/71 | LOSS: 1.2827254457598819e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 14/71 | LOSS: 1.250895117361021e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 15/71 | LOSS: 1.2561907794861327e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 16/71 | LOSS: 1.233940870674012e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 17/71 | LOSS: 1.2420261504707418e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 18/71 | LOSS: 1.2327628849705338e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 19/71 | LOSS: 1.2242916091054212e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 20/71 | LOSS: 1.2208963285264049e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 21/71 | LOSS: 1.2329584437380122e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 22/71 | LOSS: 1.2313129406285714e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 23/71 | LOSS: 1.2250696272531059e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 24/71 | LOSS: 1.22997495054733e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 25/71 | LOSS: 1.2241321760834231e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 26/71 | LOSS: 1.2248227668115524e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 27/71 | LOSS: 1.2142529645383807e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 28/71 | LOSS: 1.2255146649640037e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 29/71 | LOSS: 1.2308102941460675e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 30/71 | LOSS: 1.235890701331476e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 31/71 | LOSS: 1.2619037050853876e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 32/71 | LOSS: 1.261141151741013e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 33/71 | LOSS: 1.2706740964109913e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 34/71 | LOSS: 1.2750581429697506e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 35/71 | LOSS: 1.2831529880309568e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 36/71 | LOSS: 1.2823380406975835e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 37/71 | LOSS: 1.3088600792941993e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 38/71 | LOSS: 1.3014658855890724e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 39/71 | LOSS: 1.3036321388426585e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 40/71 | LOSS: 1.3089120773479297e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 41/71 | LOSS: 1.308434756148407e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 42/71 | LOSS: 1.3095698831907506e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 43/71 | LOSS: 1.3093242593888796e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 44/71 | LOSS: 1.3043462907565602e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 45/71 | LOSS: 1.3071338067358896e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 46/71 | LOSS: 1.31809483525316e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 47/71 | LOSS: 1.3217286133719123e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 48/71 | LOSS: 1.3216910980302099e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 49/71 | LOSS: 1.3315057149156928e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 50/71 | LOSS: 1.332973656912252e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 51/71 | LOSS: 1.339316244971437e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 52/71 | LOSS: 1.3544365843562497e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 53/71 | LOSS: 1.354261670443251e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 54/71 | LOSS: 1.3588727779278997e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 55/71 | LOSS: 1.3592156506092579e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 56/71 | LOSS: 1.356696787281595e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 57/71 | LOSS: 1.3635587884923082e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 58/71 | LOSS: 1.3601020620657345e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 59/71 | LOSS: 1.3552628009468512e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 60/71 | LOSS: 1.3497006801146915e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 61/71 | LOSS: 1.349214857503783e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 62/71 | LOSS: 1.3431487772438216e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 63/71 | LOSS: 1.34076674953576e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 64/71 | LOSS: 1.342265316522501e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 65/71 | LOSS: 1.3398103323142454e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 66/71 | LOSS: 1.3307959388432426e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 67/71 | LOSS: 1.3335294745368574e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 68/71 | LOSS: 1.3347781110707624e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 69/71 | LOSS: 1.3318702128474666e-05\n",
      "TRAIN: EPOCH 98/1000 | BATCH 70/71 | LOSS: 1.3357331495876359e-05\n",
      "VAL: EPOCH 98/1000 | BATCH 0/8 | LOSS: 1.1860427548526786e-05\n",
      "VAL: EPOCH 98/1000 | BATCH 1/8 | LOSS: 1.2099127161491197e-05\n",
      "VAL: EPOCH 98/1000 | BATCH 2/8 | LOSS: 1.2738015357172117e-05\n",
      "VAL: EPOCH 98/1000 | BATCH 3/8 | LOSS: 1.2498007890826557e-05\n",
      "VAL: EPOCH 98/1000 | BATCH 4/8 | LOSS: 1.2184951810922939e-05\n",
      "VAL: EPOCH 98/1000 | BATCH 5/8 | LOSS: 1.21791810367237e-05\n",
      "VAL: EPOCH 98/1000 | BATCH 6/8 | LOSS: 1.1890740097442176e-05\n",
      "VAL: EPOCH 98/1000 | BATCH 7/8 | LOSS: 1.1549605233085458e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 0/71 | LOSS: 1.2590048754645977e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 1/71 | LOSS: 1.3415300145425135e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 2/71 | LOSS: 1.4131710183088822e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 3/71 | LOSS: 1.2991200264877989e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 4/71 | LOSS: 1.2851570681959857e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 5/71 | LOSS: 1.3189851263935756e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 6/71 | LOSS: 1.2949767811473326e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 7/71 | LOSS: 1.2891972119177808e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 8/71 | LOSS: 1.3240807473064504e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 9/71 | LOSS: 1.2955330748809502e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 10/71 | LOSS: 1.2937340216012672e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 11/71 | LOSS: 1.318916110903956e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 12/71 | LOSS: 1.3321837356600623e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 13/71 | LOSS: 1.3518507980084646e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 14/71 | LOSS: 1.3652987763634884e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 15/71 | LOSS: 1.349569510011861e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 16/71 | LOSS: 1.3788818625257323e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 17/71 | LOSS: 1.3704554071915078e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 18/71 | LOSS: 1.3618050903268772e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 19/71 | LOSS: 1.384564179716108e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 20/71 | LOSS: 1.4008065937516567e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 21/71 | LOSS: 1.3905624779330207e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 22/71 | LOSS: 1.388656864551392e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 23/71 | LOSS: 1.3873282265800905e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 24/71 | LOSS: 1.382855960400775e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 25/71 | LOSS: 1.374640721853491e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 26/71 | LOSS: 1.3714751090716433e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 27/71 | LOSS: 1.3830543269120557e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 28/71 | LOSS: 1.3841915868648232e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 29/71 | LOSS: 1.3877357014280279e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 30/71 | LOSS: 1.381967194902245e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 31/71 | LOSS: 1.3903198407660966e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 32/71 | LOSS: 1.3891576076496152e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 33/71 | LOSS: 1.387024285042357e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 34/71 | LOSS: 1.3872626004740595e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 35/71 | LOSS: 1.3894563406008981e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 36/71 | LOSS: 1.3784739560317059e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 37/71 | LOSS: 1.3708703471294078e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 38/71 | LOSS: 1.3616960714921917e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 39/71 | LOSS: 1.3524427572519926e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 40/71 | LOSS: 1.3557836831662316e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 41/71 | LOSS: 1.3407823124517953e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 42/71 | LOSS: 1.3357256838664693e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 43/71 | LOSS: 1.336913909323233e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 44/71 | LOSS: 1.3361911179446098e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 45/71 | LOSS: 1.3327781229948059e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 46/71 | LOSS: 1.3303585353454802e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 47/71 | LOSS: 1.3242636953236797e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 48/71 | LOSS: 1.3216458293235129e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 49/71 | LOSS: 1.3151489201845835e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 50/71 | LOSS: 1.3072871671979288e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 51/71 | LOSS: 1.2988499415419377e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 52/71 | LOSS: 1.294391335730599e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 53/71 | LOSS: 1.2900334459792012e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 54/71 | LOSS: 1.286662262083899e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 55/71 | LOSS: 1.2891120848377926e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 56/71 | LOSS: 1.2860127610206531e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 57/71 | LOSS: 1.282162895285151e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 58/71 | LOSS: 1.2749694671922844e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 59/71 | LOSS: 1.2716776852054561e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 60/71 | LOSS: 1.2750778639246327e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 61/71 | LOSS: 1.2758244514360183e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 62/71 | LOSS: 1.2822041176222561e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 63/71 | LOSS: 1.2795903217011073e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 64/71 | LOSS: 1.2754961706097955e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 65/71 | LOSS: 1.2752908360474095e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 66/71 | LOSS: 1.276228336357588e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 67/71 | LOSS: 1.2726969223020955e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 68/71 | LOSS: 1.2728229233724289e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 69/71 | LOSS: 1.270559337821656e-05\n",
      "TRAIN: EPOCH 99/1000 | BATCH 70/71 | LOSS: 1.2663971881946207e-05\n",
      "VAL: EPOCH 99/1000 | BATCH 0/8 | LOSS: 1.3920170204073656e-05\n",
      "VAL: EPOCH 99/1000 | BATCH 1/8 | LOSS: 1.4192072740115691e-05\n",
      "VAL: EPOCH 99/1000 | BATCH 2/8 | LOSS: 1.4596358596463688e-05\n",
      "VAL: EPOCH 99/1000 | BATCH 3/8 | LOSS: 1.4152650237519993e-05\n",
      "VAL: EPOCH 99/1000 | BATCH 4/8 | LOSS: 1.3710893108509481e-05\n",
      "VAL: EPOCH 99/1000 | BATCH 5/8 | LOSS: 1.3639369020287026e-05\n",
      "VAL: EPOCH 99/1000 | BATCH 6/8 | LOSS: 1.342635018123214e-05\n",
      "VAL: EPOCH 99/1000 | BATCH 7/8 | LOSS: 1.3006548101657245e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 0/71 | LOSS: 1.4546057172992732e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 1/71 | LOSS: 1.2839620467275381e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 2/71 | LOSS: 1.2791121359138439e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 3/71 | LOSS: 1.2955356851307442e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 4/71 | LOSS: 1.2635559687623755e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 5/71 | LOSS: 1.2398515385333061e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 6/71 | LOSS: 1.2012256125412282e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 7/71 | LOSS: 1.1467418403299234e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 8/71 | LOSS: 1.1280767062594856e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 9/71 | LOSS: 1.1472588903416181e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 10/71 | LOSS: 1.155358784837352e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 11/71 | LOSS: 1.1659452866297215e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 12/71 | LOSS: 1.1669879803286256e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 13/71 | LOSS: 1.1826302202929842e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 14/71 | LOSS: 1.1761477435356937e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 15/71 | LOSS: 1.1999729849776486e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 16/71 | LOSS: 1.2051267998792943e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 17/71 | LOSS: 1.2115391705770486e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 18/71 | LOSS: 1.2109094649004922e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 19/71 | LOSS: 1.2138569218222983e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 20/71 | LOSS: 1.2114154309355876e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 21/71 | LOSS: 1.2061341625336684e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 22/71 | LOSS: 1.1984386773015697e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 23/71 | LOSS: 1.1883813272106636e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 24/71 | LOSS: 1.1880901911354159e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 25/71 | LOSS: 1.18278834494189e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 26/71 | LOSS: 1.1760361303458922e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 27/71 | LOSS: 1.1884578075945943e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 28/71 | LOSS: 1.1921042736001505e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 29/71 | LOSS: 1.1890147591960461e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 30/71 | LOSS: 1.178867252395233e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 31/71 | LOSS: 1.1827160449229268e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 32/71 | LOSS: 1.1893846076969622e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 33/71 | LOSS: 1.193991513847483e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 34/71 | LOSS: 1.195899736298348e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 35/71 | LOSS: 1.1934041822314612e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 36/71 | LOSS: 1.1965547536100211e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 37/71 | LOSS: 1.209071069752099e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 38/71 | LOSS: 1.2116236039452278e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 39/71 | LOSS: 1.2100161075068173e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 40/71 | LOSS: 1.2066326146340565e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 41/71 | LOSS: 1.20444548147567e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 42/71 | LOSS: 1.20917899504664e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 43/71 | LOSS: 1.2080472825718145e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 44/71 | LOSS: 1.2074298259297697e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 45/71 | LOSS: 1.2046732713908726e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 46/71 | LOSS: 1.20099562952825e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 47/71 | LOSS: 1.1985184812601801e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 48/71 | LOSS: 1.196112381356735e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 49/71 | LOSS: 1.1951983833569101e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 50/71 | LOSS: 1.1912083655439418e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 51/71 | LOSS: 1.1906884345500237e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 52/71 | LOSS: 1.1903919121838078e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 53/71 | LOSS: 1.1911902590697907e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 54/71 | LOSS: 1.1881265295828185e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 55/71 | LOSS: 1.1863446892285928e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 56/71 | LOSS: 1.1866936004305006e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 57/71 | LOSS: 1.1839188479417976e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 58/71 | LOSS: 1.1864101922537113e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 59/71 | LOSS: 1.181578119637076e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 60/71 | LOSS: 1.1834432534967953e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 61/71 | LOSS: 1.1822749735485207e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 62/71 | LOSS: 1.1845953070275885e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 63/71 | LOSS: 1.1797014593639688e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 64/71 | LOSS: 1.1835018113533107e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 65/71 | LOSS: 1.1810797910915095e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 66/71 | LOSS: 1.1814758520935655e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 67/71 | LOSS: 1.1800673224998344e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 68/71 | LOSS: 1.1867063999554796e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 69/71 | LOSS: 1.1911267094027217e-05\n",
      "TRAIN: EPOCH 100/1000 | BATCH 70/71 | LOSS: 1.1892644540918336e-05\n",
      "VAL: EPOCH 100/1000 | BATCH 0/8 | LOSS: 1.268241430807393e-05\n",
      "VAL: EPOCH 100/1000 | BATCH 1/8 | LOSS: 1.2956731552549172e-05\n",
      "VAL: EPOCH 100/1000 | BATCH 2/8 | LOSS: 1.3407251572061796e-05\n",
      "VAL: EPOCH 100/1000 | BATCH 3/8 | LOSS: 1.2869374813817558e-05\n",
      "VAL: EPOCH 100/1000 | BATCH 4/8 | LOSS: 1.2841355601267424e-05\n",
      "VAL: EPOCH 100/1000 | BATCH 5/8 | LOSS: 1.3174181428136459e-05\n",
      "VAL: EPOCH 100/1000 | BATCH 6/8 | LOSS: 1.2868977916826094e-05\n",
      "VAL: EPOCH 100/1000 | BATCH 7/8 | LOSS: 1.2786565775968484e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 0/71 | LOSS: 1.132162560679717e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 1/71 | LOSS: 1.2217293715366395e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 2/71 | LOSS: 1.1858975691817856e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 3/71 | LOSS: 1.1981983107034466e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 4/71 | LOSS: 1.1457399887149223e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 5/71 | LOSS: 1.1927202497948505e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 6/71 | LOSS: 1.1827316484414041e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 7/71 | LOSS: 1.1656480978672334e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 8/71 | LOSS: 1.1623152810595153e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 9/71 | LOSS: 1.1787875155278017e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 10/71 | LOSS: 1.1703545830889858e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 11/71 | LOSS: 1.1798132921588452e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 12/71 | LOSS: 1.1765334280565954e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 13/71 | LOSS: 1.1869123487226066e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 14/71 | LOSS: 1.1909631818222502e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 15/71 | LOSS: 1.183245205993444e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 16/71 | LOSS: 1.209504208578865e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 17/71 | LOSS: 1.2030670707948351e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 18/71 | LOSS: 1.1985909192222416e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 19/71 | LOSS: 1.1946543645535712e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 20/71 | LOSS: 1.197849706797916e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 21/71 | LOSS: 1.192788034710578e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 22/71 | LOSS: 1.2013464267669326e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 23/71 | LOSS: 1.1964457333609365e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 24/71 | LOSS: 1.1954620022152085e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 25/71 | LOSS: 1.1900893731273334e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 26/71 | LOSS: 1.2012944285675917e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 27/71 | LOSS: 1.2185064601258741e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 28/71 | LOSS: 1.2188140662429164e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 29/71 | LOSS: 1.2332474913516004e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 30/71 | LOSS: 1.2289167223629662e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 31/71 | LOSS: 1.2390548079110886e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 32/71 | LOSS: 1.2334943676250987e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 33/71 | LOSS: 1.2362148053772912e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 34/71 | LOSS: 1.2320460830648829e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 35/71 | LOSS: 1.2301310890608067e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 36/71 | LOSS: 1.2343059933226795e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 37/71 | LOSS: 1.2303838359098576e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 38/71 | LOSS: 1.2260063858075928e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 39/71 | LOSS: 1.226327187850984e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 40/71 | LOSS: 1.2311099165278238e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 41/71 | LOSS: 1.2237335145722941e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 42/71 | LOSS: 1.2152096830906759e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 43/71 | LOSS: 1.2248459163608707e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 44/71 | LOSS: 1.224115015550827e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 45/71 | LOSS: 1.2242047155861814e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 46/71 | LOSS: 1.2222446863969264e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 47/71 | LOSS: 1.2219404898890692e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 48/71 | LOSS: 1.2252747313011372e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 49/71 | LOSS: 1.2251119442225899e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 50/71 | LOSS: 1.2208512017940508e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 51/71 | LOSS: 1.219520240388542e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 52/71 | LOSS: 1.2187586383694202e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 53/71 | LOSS: 1.2141907198518445e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 54/71 | LOSS: 1.2179058549835727e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 55/71 | LOSS: 1.2139066403246293e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 56/71 | LOSS: 1.2105582132598533e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 57/71 | LOSS: 1.2100169379695656e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 58/71 | LOSS: 1.2124317929531463e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 59/71 | LOSS: 1.2110947636756464e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 60/71 | LOSS: 1.2100136342776183e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 61/71 | LOSS: 1.2072637556200554e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 62/71 | LOSS: 1.2070605248842935e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 63/71 | LOSS: 1.2094021570874247e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 64/71 | LOSS: 1.2083532951342373e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 65/71 | LOSS: 1.2103491137553867e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 66/71 | LOSS: 1.2112837821269518e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 67/71 | LOSS: 1.2114340078743921e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 68/71 | LOSS: 1.2075103452092582e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 69/71 | LOSS: 1.2091186363250017e-05\n",
      "TRAIN: EPOCH 101/1000 | BATCH 70/71 | LOSS: 1.2044822616305064e-05\n",
      "VAL: EPOCH 101/1000 | BATCH 0/8 | LOSS: 1.2180933481431566e-05\n",
      "VAL: EPOCH 101/1000 | BATCH 1/8 | LOSS: 1.2294133739487734e-05\n",
      "VAL: EPOCH 101/1000 | BATCH 2/8 | LOSS: 1.2691870021323362e-05\n",
      "VAL: EPOCH 101/1000 | BATCH 3/8 | LOSS: 1.2206295195937855e-05\n",
      "VAL: EPOCH 101/1000 | BATCH 4/8 | LOSS: 1.2126387628086376e-05\n",
      "VAL: EPOCH 101/1000 | BATCH 5/8 | LOSS: 1.240509648899509e-05\n",
      "VAL: EPOCH 101/1000 | BATCH 6/8 | LOSS: 1.2113537455401716e-05\n",
      "VAL: EPOCH 101/1000 | BATCH 7/8 | LOSS: 1.1925519061151135e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 0/71 | LOSS: 1.1522991371748503e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 1/71 | LOSS: 1.1824849480035482e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 2/71 | LOSS: 1.1994469484003881e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 3/71 | LOSS: 1.189118347610929e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 4/71 | LOSS: 1.2927552234032191e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 5/71 | LOSS: 1.257760444180652e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 6/71 | LOSS: 1.2803171817462758e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 7/71 | LOSS: 1.2552198541015969e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 8/71 | LOSS: 1.2740693010629104e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 9/71 | LOSS: 1.2797270301234676e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 10/71 | LOSS: 1.2670256927032659e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 11/71 | LOSS: 1.2487247810592331e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 12/71 | LOSS: 1.2683570345801014e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 13/71 | LOSS: 1.2518442482749898e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 14/71 | LOSS: 1.231500082212733e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 15/71 | LOSS: 1.2454668365080579e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 16/71 | LOSS: 1.2590055495606708e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 17/71 | LOSS: 1.2599648117530807e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 18/71 | LOSS: 1.2617001199319738e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 19/71 | LOSS: 1.2532615073723719e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 20/71 | LOSS: 1.2511673395194867e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 21/71 | LOSS: 1.2457612931691322e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 22/71 | LOSS: 1.2446227708057014e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 23/71 | LOSS: 1.2405004895299498e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 24/71 | LOSS: 1.2356231272860897e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 25/71 | LOSS: 1.2304363421068187e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 26/71 | LOSS: 1.2404447429785418e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 27/71 | LOSS: 1.2335748936363547e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 28/71 | LOSS: 1.2274736789173592e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 29/71 | LOSS: 1.2316187924928576e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 30/71 | LOSS: 1.2367075860017775e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 31/71 | LOSS: 1.2429176820205612e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 32/71 | LOSS: 1.2375271498140963e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 33/71 | LOSS: 1.2370206792077037e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 34/71 | LOSS: 1.2389533341255238e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 35/71 | LOSS: 1.234905855401141e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 36/71 | LOSS: 1.2348795698434932e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 37/71 | LOSS: 1.2338957683249119e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 38/71 | LOSS: 1.2275551363507895e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 39/71 | LOSS: 1.2260099924787938e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 40/71 | LOSS: 1.2304084654116617e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 41/71 | LOSS: 1.2230916949885709e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 42/71 | LOSS: 1.2202134797028753e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 43/71 | LOSS: 1.2214455058380157e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 44/71 | LOSS: 1.227734125374506e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 45/71 | LOSS: 1.2331830238627331e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 46/71 | LOSS: 1.2327730106103155e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 47/71 | LOSS: 1.2313596926105674e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 48/71 | LOSS: 1.2309628726477373e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 49/71 | LOSS: 1.2278922586119734e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 50/71 | LOSS: 1.2276177264592501e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 51/71 | LOSS: 1.2268429722658206e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 52/71 | LOSS: 1.223799360027528e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 53/71 | LOSS: 1.2221503715813418e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 54/71 | LOSS: 1.2213372114770622e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 55/71 | LOSS: 1.218917886295198e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 56/71 | LOSS: 1.2185411450397959e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 57/71 | LOSS: 1.2175508251449832e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 58/71 | LOSS: 1.2110079990372345e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 59/71 | LOSS: 1.2094409915638002e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 60/71 | LOSS: 1.2141212403653074e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 61/71 | LOSS: 1.212235986625479e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 62/71 | LOSS: 1.2147848951465835e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 63/71 | LOSS: 1.2168688712677067e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 64/71 | LOSS: 1.2175062721116074e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 65/71 | LOSS: 1.2170981032492515e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 66/71 | LOSS: 1.2153695241800933e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 67/71 | LOSS: 1.2198291388438734e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 68/71 | LOSS: 1.2224625435550664e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 69/71 | LOSS: 1.216464761455427e-05\n",
      "TRAIN: EPOCH 102/1000 | BATCH 70/71 | LOSS: 1.2176151947539732e-05\n",
      "VAL: EPOCH 102/1000 | BATCH 0/8 | LOSS: 1.1031164831365459e-05\n",
      "VAL: EPOCH 102/1000 | BATCH 1/8 | LOSS: 1.133929799834732e-05\n",
      "VAL: EPOCH 102/1000 | BATCH 2/8 | LOSS: 1.211918151966529e-05\n",
      "VAL: EPOCH 102/1000 | BATCH 3/8 | LOSS: 1.2097340913896915e-05\n",
      "VAL: EPOCH 102/1000 | BATCH 4/8 | LOSS: 1.1926311708521098e-05\n",
      "VAL: EPOCH 102/1000 | BATCH 5/8 | LOSS: 1.204760868252682e-05\n",
      "VAL: EPOCH 102/1000 | BATCH 6/8 | LOSS: 1.177072317659622e-05\n",
      "VAL: EPOCH 102/1000 | BATCH 7/8 | LOSS: 1.1548908787517576e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 0/71 | LOSS: 1.2937330211570952e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 1/71 | LOSS: 1.1325315881549614e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 2/71 | LOSS: 1.1204738560384916e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 3/71 | LOSS: 1.1671771289911703e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 4/71 | LOSS: 1.1545276902324985e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 5/71 | LOSS: 1.1823921643857224e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 6/71 | LOSS: 1.1865515002032875e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 7/71 | LOSS: 1.169775737253076e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 8/71 | LOSS: 1.184784074818405e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 9/71 | LOSS: 1.202212906719069e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 10/71 | LOSS: 1.2026167413833635e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 11/71 | LOSS: 1.1986705885647098e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 12/71 | LOSS: 1.208693769373125e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 13/71 | LOSS: 1.1853592176131705e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 14/71 | LOSS: 1.194887918245513e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 15/71 | LOSS: 1.2135645306443621e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 16/71 | LOSS: 1.214590819847003e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 17/71 | LOSS: 1.2056379192573433e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 18/71 | LOSS: 1.2167552181620666e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 19/71 | LOSS: 1.2166326678197948e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 20/71 | LOSS: 1.2176616283319336e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 21/71 | LOSS: 1.2150776084044166e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 22/71 | LOSS: 1.2071787513496416e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 23/71 | LOSS: 1.2018420534332108e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 24/71 | LOSS: 1.2028418568661437e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 25/71 | LOSS: 1.2009468229315155e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 26/71 | LOSS: 1.192025603005378e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 27/71 | LOSS: 1.1843786166666956e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 28/71 | LOSS: 1.1826631540817948e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 29/71 | LOSS: 1.1691309949431645e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 30/71 | LOSS: 1.1701573414264263e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 31/71 | LOSS: 1.164563636280036e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 32/71 | LOSS: 1.1687177544500269e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 33/71 | LOSS: 1.1618983321301022e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 34/71 | LOSS: 1.1569831024514445e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 35/71 | LOSS: 1.1597561852896535e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 36/71 | LOSS: 1.165939478154099e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 37/71 | LOSS: 1.178911514599397e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 38/71 | LOSS: 1.1804550992495093e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 39/71 | LOSS: 1.1829374443550478e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 40/71 | LOSS: 1.1885416603124723e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 41/71 | LOSS: 1.1876121420971772e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 42/71 | LOSS: 1.1923967830689224e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 43/71 | LOSS: 1.1917563328593546e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 44/71 | LOSS: 1.1929197029934989e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 45/71 | LOSS: 1.1918122649860933e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 46/71 | LOSS: 1.1837525968811416e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 47/71 | LOSS: 1.1883425296825104e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 48/71 | LOSS: 1.1877449465754005e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 49/71 | LOSS: 1.1844379605463473e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 50/71 | LOSS: 1.1867463845370438e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 51/71 | LOSS: 1.183389828768738e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 52/71 | LOSS: 1.1870595390407615e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 53/71 | LOSS: 1.1835605181221143e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 54/71 | LOSS: 1.1837921920232475e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 55/71 | LOSS: 1.1874937367143243e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 56/71 | LOSS: 1.1862585811366188e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 57/71 | LOSS: 1.1825906312878942e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 58/71 | LOSS: 1.1917650966606126e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 59/71 | LOSS: 1.1942942849903678e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 60/71 | LOSS: 1.1998034812572993e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 61/71 | LOSS: 1.203693361070867e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 62/71 | LOSS: 1.2083254919811484e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 63/71 | LOSS: 1.2170716260584413e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 64/71 | LOSS: 1.224182357803399e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 65/71 | LOSS: 1.2265584356293513e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 66/71 | LOSS: 1.2362203037877977e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 67/71 | LOSS: 1.2409787881123894e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 68/71 | LOSS: 1.240606936493271e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 69/71 | LOSS: 1.2458646454303692e-05\n",
      "TRAIN: EPOCH 103/1000 | BATCH 70/71 | LOSS: 1.2526035263849927e-05\n",
      "VAL: EPOCH 103/1000 | BATCH 0/8 | LOSS: 1.1912674381164834e-05\n",
      "VAL: EPOCH 103/1000 | BATCH 1/8 | LOSS: 1.2049391443724744e-05\n",
      "VAL: EPOCH 103/1000 | BATCH 2/8 | LOSS: 1.2261626276692065e-05\n",
      "VAL: EPOCH 103/1000 | BATCH 3/8 | LOSS: 1.1741207799786935e-05\n",
      "VAL: EPOCH 103/1000 | BATCH 4/8 | LOSS: 1.1636305862339213e-05\n",
      "VAL: EPOCH 103/1000 | BATCH 5/8 | LOSS: 1.18225502774294e-05\n",
      "VAL: EPOCH 103/1000 | BATCH 6/8 | LOSS: 1.1588179014714634e-05\n",
      "VAL: EPOCH 103/1000 | BATCH 7/8 | LOSS: 1.1405025588828721e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 0/71 | LOSS: 1.190337661682861e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 1/71 | LOSS: 1.1664679277600953e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 2/71 | LOSS: 1.3106766042862242e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 3/71 | LOSS: 1.3472174714479479e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 4/71 | LOSS: 1.3065321036265232e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 5/71 | LOSS: 1.2908223046300312e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 6/71 | LOSS: 1.2893226604709135e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 7/71 | LOSS: 1.317904229836131e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 8/71 | LOSS: 1.3109327382052368e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 9/71 | LOSS: 1.3288746959005949e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 10/71 | LOSS: 1.3216086038234856e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 11/71 | LOSS: 1.2903308819052958e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 12/71 | LOSS: 1.2764566236000974e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 13/71 | LOSS: 1.2718705420411425e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 14/71 | LOSS: 1.2401889277195248e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 15/71 | LOSS: 1.2386782486828452e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 16/71 | LOSS: 1.2384635096275493e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 17/71 | LOSS: 1.2404991593939485e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 18/71 | LOSS: 1.2573360675703857e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 19/71 | LOSS: 1.2541396381493542e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 20/71 | LOSS: 1.2453597870868785e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 21/71 | LOSS: 1.244633230354256e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 22/71 | LOSS: 1.2400127636046028e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 23/71 | LOSS: 1.252232800652564e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 24/71 | LOSS: 1.2363181049295236e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 25/71 | LOSS: 1.2340946900971736e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 26/71 | LOSS: 1.2227643680860098e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 27/71 | LOSS: 1.224775759770377e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 28/71 | LOSS: 1.2242494135691607e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 29/71 | LOSS: 1.2129650682860908e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 30/71 | LOSS: 1.2126770226102728e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 31/71 | LOSS: 1.2205598210357493e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 32/71 | LOSS: 1.2252015315453084e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 33/71 | LOSS: 1.2277895380333683e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 34/71 | LOSS: 1.2429550432508611e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 35/71 | LOSS: 1.2520548630669105e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 36/71 | LOSS: 1.2464505596506422e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 37/71 | LOSS: 1.2507910493073532e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 38/71 | LOSS: 1.2492641895406474e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 39/71 | LOSS: 1.2430616834535613e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 40/71 | LOSS: 1.2418386354622227e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 41/71 | LOSS: 1.2360726508458833e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 42/71 | LOSS: 1.2318345736360264e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 43/71 | LOSS: 1.2357672941024331e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 44/71 | LOSS: 1.2352534880240757e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 45/71 | LOSS: 1.2346420510806164e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 46/71 | LOSS: 1.241946452826093e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 47/71 | LOSS: 1.2377187014559846e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 48/71 | LOSS: 1.2393938996698421e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 49/71 | LOSS: 1.2348282434686552e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 50/71 | LOSS: 1.2375282138094132e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 51/71 | LOSS: 1.233816685551294e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 52/71 | LOSS: 1.2302623941169893e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 53/71 | LOSS: 1.2318957824727814e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 54/71 | LOSS: 1.2336801129012284e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 55/71 | LOSS: 1.2362658448442484e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 56/71 | LOSS: 1.2335789889268773e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 57/71 | LOSS: 1.2283609333374851e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 58/71 | LOSS: 1.2318155097738658e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 59/71 | LOSS: 1.2267701216236067e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 60/71 | LOSS: 1.2257914395228058e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 61/71 | LOSS: 1.2256772753326298e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 62/71 | LOSS: 1.2284543117857538e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 63/71 | LOSS: 1.2249802693986567e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 64/71 | LOSS: 1.2305541951298857e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 65/71 | LOSS: 1.2282169874114599e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 66/71 | LOSS: 1.2305454586553492e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 67/71 | LOSS: 1.233190887968008e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 68/71 | LOSS: 1.2304582976840733e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 69/71 | LOSS: 1.232530058976928e-05\n",
      "TRAIN: EPOCH 104/1000 | BATCH 70/71 | LOSS: 1.2287269309011426e-05\n",
      "VAL: EPOCH 104/1000 | BATCH 0/8 | LOSS: 1.2438560588634573e-05\n",
      "VAL: EPOCH 104/1000 | BATCH 1/8 | LOSS: 1.3117258276906796e-05\n",
      "VAL: EPOCH 104/1000 | BATCH 2/8 | LOSS: 1.3663628730379665e-05\n",
      "VAL: EPOCH 104/1000 | BATCH 3/8 | LOSS: 1.353172046947293e-05\n",
      "VAL: EPOCH 104/1000 | BATCH 4/8 | LOSS: 1.2893367238575592e-05\n",
      "VAL: EPOCH 104/1000 | BATCH 5/8 | LOSS: 1.2568149941216689e-05\n",
      "VAL: EPOCH 104/1000 | BATCH 6/8 | LOSS: 1.231708587770949e-05\n",
      "VAL: EPOCH 104/1000 | BATCH 7/8 | LOSS: 1.1885924664056802e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 0/71 | LOSS: 1.1315556548652239e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 1/71 | LOSS: 1.2784833415935282e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 2/71 | LOSS: 1.2909593654815884e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 3/71 | LOSS: 1.3172130138627836e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 4/71 | LOSS: 1.2995034012419637e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 5/71 | LOSS: 1.2566158754149607e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 6/71 | LOSS: 1.2629533167845303e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 7/71 | LOSS: 1.2197119531265344e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 8/71 | LOSS: 1.1829571829164504e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 9/71 | LOSS: 1.1679493945848663e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 10/71 | LOSS: 1.1775855289835652e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 11/71 | LOSS: 1.168995792492448e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 12/71 | LOSS: 1.1810237433215101e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 13/71 | LOSS: 1.1761050050803793e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 14/71 | LOSS: 1.1660003171224768e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 15/71 | LOSS: 1.1452504850240075e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 16/71 | LOSS: 1.1455869860262336e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 17/71 | LOSS: 1.1462145771980027e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 18/71 | LOSS: 1.133359431773179e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 19/71 | LOSS: 1.1352729870850453e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 20/71 | LOSS: 1.1442228415468153e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 21/71 | LOSS: 1.1352446769954192e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 22/71 | LOSS: 1.139648326858625e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 23/71 | LOSS: 1.148377536234572e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 24/71 | LOSS: 1.1551217721716966e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 25/71 | LOSS: 1.1584829560608621e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 26/71 | LOSS: 1.150173358365902e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 27/71 | LOSS: 1.1513096491658611e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 28/71 | LOSS: 1.1493631596937936e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 29/71 | LOSS: 1.1510714951630992e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 30/71 | LOSS: 1.143435920605571e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 31/71 | LOSS: 1.1427040988110093e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 32/71 | LOSS: 1.1382162263158314e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 33/71 | LOSS: 1.1373368525870747e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 34/71 | LOSS: 1.132542443623866e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 35/71 | LOSS: 1.1394231604653112e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 36/71 | LOSS: 1.1334308977451213e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 37/71 | LOSS: 1.1319103601923225e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 38/71 | LOSS: 1.1271613565884018e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 39/71 | LOSS: 1.1218814029234636e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 40/71 | LOSS: 1.1236184502372563e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 41/71 | LOSS: 1.1258571335236497e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 42/71 | LOSS: 1.125828008340715e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 43/71 | LOSS: 1.1232874302632725e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 44/71 | LOSS: 1.1296704886060777e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 45/71 | LOSS: 1.1311142316117184e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 46/71 | LOSS: 1.1408065957050682e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 47/71 | LOSS: 1.1399433276437776e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 48/71 | LOSS: 1.1449863598741382e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 49/71 | LOSS: 1.1433810213929973e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 50/71 | LOSS: 1.144190430161817e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 51/71 | LOSS: 1.1476084042582303e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 52/71 | LOSS: 1.1491799169853726e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 53/71 | LOSS: 1.1470311821742345e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 54/71 | LOSS: 1.1459275529804555e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 55/71 | LOSS: 1.147758382558095e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 56/71 | LOSS: 1.1480761985008589e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 57/71 | LOSS: 1.147501957191331e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 58/71 | LOSS: 1.1515342351739784e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 59/71 | LOSS: 1.1524783409792387e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 60/71 | LOSS: 1.1542192608982585e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 61/71 | LOSS: 1.1490587615797619e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 62/71 | LOSS: 1.1533792372026096e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 63/71 | LOSS: 1.1542519487761638e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 64/71 | LOSS: 1.1494254207578846e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 65/71 | LOSS: 1.1500982826405423e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 66/71 | LOSS: 1.1483709779746115e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 67/71 | LOSS: 1.150286144526463e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 68/71 | LOSS: 1.147158064154412e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 69/71 | LOSS: 1.1458652690115352e-05\n",
      "TRAIN: EPOCH 105/1000 | BATCH 70/71 | LOSS: 1.1509416574074455e-05\n",
      "VAL: EPOCH 105/1000 | BATCH 0/8 | LOSS: 1.141836037277244e-05\n",
      "VAL: EPOCH 105/1000 | BATCH 1/8 | LOSS: 1.1594166153372498e-05\n",
      "VAL: EPOCH 105/1000 | BATCH 2/8 | LOSS: 1.2118464534675391e-05\n",
      "VAL: EPOCH 105/1000 | BATCH 3/8 | LOSS: 1.1993844509561313e-05\n",
      "VAL: EPOCH 105/1000 | BATCH 4/8 | LOSS: 1.1747959069907665e-05\n",
      "VAL: EPOCH 105/1000 | BATCH 5/8 | LOSS: 1.1781210863167265e-05\n",
      "VAL: EPOCH 105/1000 | BATCH 6/8 | LOSS: 1.1593370800255798e-05\n",
      "VAL: EPOCH 105/1000 | BATCH 7/8 | LOSS: 1.1340760352140933e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 0/71 | LOSS: 1.0483735422894824e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 1/71 | LOSS: 1.0653143817762611e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 2/71 | LOSS: 1.082685139408568e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 3/71 | LOSS: 1.0836107776412973e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 4/71 | LOSS: 1.067387784132734e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 5/71 | LOSS: 1.0939269486698322e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 6/71 | LOSS: 1.1695234726565623e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 7/71 | LOSS: 1.208642493111256e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 8/71 | LOSS: 1.1901894847849488e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 9/71 | LOSS: 1.207904024340678e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 10/71 | LOSS: 1.1955386898162859e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 11/71 | LOSS: 1.1804183259300771e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 12/71 | LOSS: 1.185039043495915e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 13/71 | LOSS: 1.1952963047536156e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 14/71 | LOSS: 1.1842167441500352e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 15/71 | LOSS: 1.2040314004480024e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 16/71 | LOSS: 1.2122038938019506e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 17/71 | LOSS: 1.220257455416787e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 18/71 | LOSS: 1.2386843321253668e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 19/71 | LOSS: 1.2227749675730592e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 20/71 | LOSS: 1.2375004102761436e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 21/71 | LOSS: 1.2480281467022989e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 22/71 | LOSS: 1.2424117694140675e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 23/71 | LOSS: 1.242423267437213e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 24/71 | LOSS: 1.2439697929949034e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 25/71 | LOSS: 1.2424878185321666e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 26/71 | LOSS: 1.2558255043081266e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 27/71 | LOSS: 1.2570127799855462e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 28/71 | LOSS: 1.2666372044648221e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 29/71 | LOSS: 1.274760995026251e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 30/71 | LOSS: 1.2718860694627849e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 31/71 | LOSS: 1.2696578238546863e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 32/71 | LOSS: 1.2711976496299911e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 33/71 | LOSS: 1.2754038487079978e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 34/71 | LOSS: 1.2692122072621715e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 35/71 | LOSS: 1.270668561422402e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 36/71 | LOSS: 1.2665466660896405e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 37/71 | LOSS: 1.2666777054523424e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 38/71 | LOSS: 1.2584467069059312e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 39/71 | LOSS: 1.249947820269881e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 40/71 | LOSS: 1.2474139954819538e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 41/71 | LOSS: 1.2402342842203021e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 42/71 | LOSS: 1.2337810784269688e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 43/71 | LOSS: 1.2340115455861616e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 44/71 | LOSS: 1.2320670612098183e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 45/71 | LOSS: 1.226963999927292e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 46/71 | LOSS: 1.222748169118162e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 47/71 | LOSS: 1.2222384536168343e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 48/71 | LOSS: 1.2202873017950155e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 49/71 | LOSS: 1.2277712994546163e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 50/71 | LOSS: 1.2309214231636896e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 51/71 | LOSS: 1.2287971615548417e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 52/71 | LOSS: 1.229505459429314e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 53/71 | LOSS: 1.2305111930025439e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 54/71 | LOSS: 1.2254474561566234e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 55/71 | LOSS: 1.2316108950471971e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 56/71 | LOSS: 1.2295431141657837e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 57/71 | LOSS: 1.2327651504541602e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 58/71 | LOSS: 1.230281588381701e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 59/71 | LOSS: 1.2239777091357002e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 60/71 | LOSS: 1.2235659581716898e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 61/71 | LOSS: 1.2219552254757747e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 62/71 | LOSS: 1.2172891859206305e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 63/71 | LOSS: 1.2164380009949127e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 64/71 | LOSS: 1.2160342339484487e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 65/71 | LOSS: 1.2126396554914677e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 66/71 | LOSS: 1.21158024617827e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 67/71 | LOSS: 1.2069045436981147e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 68/71 | LOSS: 1.2071373324336795e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 69/71 | LOSS: 1.2068966082421996e-05\n",
      "TRAIN: EPOCH 106/1000 | BATCH 70/71 | LOSS: 1.21031403645068e-05\n",
      "VAL: EPOCH 106/1000 | BATCH 0/8 | LOSS: 1.1260830433457159e-05\n",
      "VAL: EPOCH 106/1000 | BATCH 1/8 | LOSS: 1.1984223419858608e-05\n",
      "VAL: EPOCH 106/1000 | BATCH 2/8 | LOSS: 1.2460838964519402e-05\n",
      "VAL: EPOCH 106/1000 | BATCH 3/8 | LOSS: 1.2268970749573782e-05\n",
      "VAL: EPOCH 106/1000 | BATCH 4/8 | LOSS: 1.1777884901675862e-05\n",
      "VAL: EPOCH 106/1000 | BATCH 5/8 | LOSS: 1.1575847565836739e-05\n",
      "VAL: EPOCH 106/1000 | BATCH 6/8 | LOSS: 1.13739366237756e-05\n",
      "VAL: EPOCH 106/1000 | BATCH 7/8 | LOSS: 1.1017261044798943e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 0/71 | LOSS: 1.1605072359088808e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 1/71 | LOSS: 1.1792193163273623e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 2/71 | LOSS: 1.1505147692029519e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 3/71 | LOSS: 1.196682092086121e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 4/71 | LOSS: 1.1602072663663422e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 5/71 | LOSS: 1.180733488581609e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 6/71 | LOSS: 1.1554689990589395e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 7/71 | LOSS: 1.1494840123305039e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 8/71 | LOSS: 1.1570303791409566e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 9/71 | LOSS: 1.158794439106714e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 10/71 | LOSS: 1.1497684847034344e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 11/71 | LOSS: 1.149147647083737e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 12/71 | LOSS: 1.137186406525031e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 13/71 | LOSS: 1.1281605241362871e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 14/71 | LOSS: 1.1209438647104737e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 15/71 | LOSS: 1.1228004154872906e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 16/71 | LOSS: 1.1182873844090328e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 17/71 | LOSS: 1.1177956316209424e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 18/71 | LOSS: 1.1284746649739405e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 19/71 | LOSS: 1.1187609061380499e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 20/71 | LOSS: 1.1179264046880971e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 21/71 | LOSS: 1.1247091317107499e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 22/71 | LOSS: 1.1280400189128198e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 23/71 | LOSS: 1.1236217574150942e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 24/71 | LOSS: 1.1300538062641863e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 25/71 | LOSS: 1.1265287132398673e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 26/71 | LOSS: 1.1299685091351555e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 27/71 | LOSS: 1.124682041466128e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 28/71 | LOSS: 1.1210468356098161e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 29/71 | LOSS: 1.1120506981872797e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 30/71 | LOSS: 1.1172401466483866e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 31/71 | LOSS: 1.1221926115467795e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 32/71 | LOSS: 1.1221163952017598e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 33/71 | LOSS: 1.1330298040026237e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 34/71 | LOSS: 1.1390775632337733e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 35/71 | LOSS: 1.14112131339223e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 36/71 | LOSS: 1.1358065716489659e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 37/71 | LOSS: 1.1390049693497866e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 38/71 | LOSS: 1.1379315392099297e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 39/71 | LOSS: 1.140390247655887e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 40/71 | LOSS: 1.1389541391619743e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 41/71 | LOSS: 1.1402791837359213e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 42/71 | LOSS: 1.1393846169846447e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 43/71 | LOSS: 1.1387678837722474e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 44/71 | LOSS: 1.138054255231206e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 45/71 | LOSS: 1.135030112456424e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 46/71 | LOSS: 1.1362234100159456e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 47/71 | LOSS: 1.1324066709524535e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 48/71 | LOSS: 1.1283256716691243e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 49/71 | LOSS: 1.1296895718260202e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 50/71 | LOSS: 1.1341681480795765e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 51/71 | LOSS: 1.1333831620364575e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 52/71 | LOSS: 1.1354848264193794e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 53/71 | LOSS: 1.1395528855683989e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 54/71 | LOSS: 1.1402203976600008e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 55/71 | LOSS: 1.1425949131600127e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 56/71 | LOSS: 1.1419068869145121e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 57/71 | LOSS: 1.144887835772383e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 58/71 | LOSS: 1.1462360863195006e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 59/71 | LOSS: 1.1460752981899228e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 60/71 | LOSS: 1.1478065494347753e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 61/71 | LOSS: 1.1494889144336e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 62/71 | LOSS: 1.1493164507440285e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 63/71 | LOSS: 1.1495739542510819e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 64/71 | LOSS: 1.1507482696078324e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 65/71 | LOSS: 1.1547997294683503e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 66/71 | LOSS: 1.1559995928381347e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 67/71 | LOSS: 1.1562469273271477e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 68/71 | LOSS: 1.1592528937689021e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 69/71 | LOSS: 1.1577854106040572e-05\n",
      "TRAIN: EPOCH 107/1000 | BATCH 70/71 | LOSS: 1.1642608473599646e-05\n",
      "VAL: EPOCH 107/1000 | BATCH 0/8 | LOSS: 1.4034959349373821e-05\n",
      "VAL: EPOCH 107/1000 | BATCH 1/8 | LOSS: 1.369253823213512e-05\n",
      "VAL: EPOCH 107/1000 | BATCH 2/8 | LOSS: 1.4030489182914607e-05\n",
      "VAL: EPOCH 107/1000 | BATCH 3/8 | LOSS: 1.3464377161653829e-05\n",
      "VAL: EPOCH 107/1000 | BATCH 4/8 | LOSS: 1.3461454000207595e-05\n",
      "VAL: EPOCH 107/1000 | BATCH 5/8 | LOSS: 1.3749917646540174e-05\n",
      "VAL: EPOCH 107/1000 | BATCH 6/8 | LOSS: 1.3543655898372111e-05\n",
      "VAL: EPOCH 107/1000 | BATCH 7/8 | LOSS: 1.3531802210309252e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 0/71 | LOSS: 1.3502347428584471e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 1/71 | LOSS: 1.4954279322410002e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 2/71 | LOSS: 1.3026157224279208e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 3/71 | LOSS: 1.3438445876090555e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 4/71 | LOSS: 1.3197151565691456e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 5/71 | LOSS: 1.3179863951033136e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 6/71 | LOSS: 1.3354207112570293e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 7/71 | LOSS: 1.3296487622938002e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 8/71 | LOSS: 1.3237287728568643e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 9/71 | LOSS: 1.2893508937850129e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 10/71 | LOSS: 1.3022458998338235e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 11/71 | LOSS: 1.2879449514002772e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 12/71 | LOSS: 1.275476167323247e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 13/71 | LOSS: 1.275181531517384e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 14/71 | LOSS: 1.257420917681884e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 15/71 | LOSS: 1.2540998227450473e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 16/71 | LOSS: 1.2488651383832535e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 17/71 | LOSS: 1.2475831984678129e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 18/71 | LOSS: 1.2496651642753645e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 19/71 | LOSS: 1.2519144092948409e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 20/71 | LOSS: 1.247318080762246e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 21/71 | LOSS: 1.2462503615428101e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 22/71 | LOSS: 1.243144644199081e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 23/71 | LOSS: 1.2387999314948198e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 24/71 | LOSS: 1.2315611420490314e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 25/71 | LOSS: 1.2215084867560878e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 26/71 | LOSS: 1.2293087180565905e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 27/71 | LOSS: 1.2213415435066313e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 28/71 | LOSS: 1.2210557668305259e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 29/71 | LOSS: 1.2239753838609127e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 30/71 | LOSS: 1.2143196023140674e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 31/71 | LOSS: 1.2245175696534716e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 32/71 | LOSS: 1.2156469782701496e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 33/71 | LOSS: 1.2118069154145333e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 34/71 | LOSS: 1.2121750556356606e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 35/71 | LOSS: 1.2080413827991126e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 36/71 | LOSS: 1.2013921543650605e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 37/71 | LOSS: 1.1982304481774153e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 38/71 | LOSS: 1.1938354360515801e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 39/71 | LOSS: 1.1895209536305628e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 40/71 | LOSS: 1.1956901888458183e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 41/71 | LOSS: 1.1951176388503795e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 42/71 | LOSS: 1.1921189218620109e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 43/71 | LOSS: 1.1884293206259661e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 44/71 | LOSS: 1.1837037810538378e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 45/71 | LOSS: 1.1795533223251013e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 46/71 | LOSS: 1.1729584041452828e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 47/71 | LOSS: 1.1738231933122734e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 48/71 | LOSS: 1.1747682594654997e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 49/71 | LOSS: 1.1743306531570852e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 50/71 | LOSS: 1.1702574809925294e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 51/71 | LOSS: 1.1689493843592712e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 52/71 | LOSS: 1.1713209013972994e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 53/71 | LOSS: 1.1735389509542276e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 54/71 | LOSS: 1.1751441277076745e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 55/71 | LOSS: 1.1731407750760678e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 56/71 | LOSS: 1.1700180485811934e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 57/71 | LOSS: 1.1730126124316955e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 58/71 | LOSS: 1.1742208637045578e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 59/71 | LOSS: 1.1721936592342294e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 60/71 | LOSS: 1.1772840972283671e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 61/71 | LOSS: 1.1815905978554867e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 62/71 | LOSS: 1.1842513202513117e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 63/71 | LOSS: 1.1829010119868144e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 64/71 | LOSS: 1.1788674699066457e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 65/71 | LOSS: 1.1799572431217795e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 66/71 | LOSS: 1.1855369509126533e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 67/71 | LOSS: 1.1856687318440355e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 68/71 | LOSS: 1.1863615446086245e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 69/71 | LOSS: 1.1910079254968358e-05\n",
      "TRAIN: EPOCH 108/1000 | BATCH 70/71 | LOSS: 1.1931430313959194e-05\n",
      "VAL: EPOCH 108/1000 | BATCH 0/8 | LOSS: 1.1080985132139176e-05\n",
      "VAL: EPOCH 108/1000 | BATCH 1/8 | LOSS: 1.1438288311182987e-05\n",
      "VAL: EPOCH 108/1000 | BATCH 2/8 | LOSS: 1.1824889952549711e-05\n",
      "VAL: EPOCH 108/1000 | BATCH 3/8 | LOSS: 1.140976746683009e-05\n",
      "VAL: EPOCH 108/1000 | BATCH 4/8 | LOSS: 1.1039733362849801e-05\n",
      "VAL: EPOCH 108/1000 | BATCH 5/8 | LOSS: 1.0984515332287023e-05\n",
      "VAL: EPOCH 108/1000 | BATCH 6/8 | LOSS: 1.079877503278632e-05\n",
      "VAL: EPOCH 108/1000 | BATCH 7/8 | LOSS: 1.052231232279155e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 0/71 | LOSS: 7.910850399639457e-06\n",
      "TRAIN: EPOCH 109/1000 | BATCH 1/71 | LOSS: 9.276425316784298e-06\n",
      "TRAIN: EPOCH 109/1000 | BATCH 2/71 | LOSS: 1.0338238098483998e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 3/71 | LOSS: 1.0309997151125572e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 4/71 | LOSS: 1.0600769928714727e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 5/71 | LOSS: 1.0972578062743802e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 6/71 | LOSS: 1.0729200766945723e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 7/71 | LOSS: 1.048909518885921e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 8/71 | LOSS: 1.0364365354891763e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 9/71 | LOSS: 1.0623000707710162e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 10/71 | LOSS: 1.0516365033584986e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 11/71 | LOSS: 1.0571918816519124e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 12/71 | LOSS: 1.0478509046571651e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 13/71 | LOSS: 1.061461333457763e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 14/71 | LOSS: 1.0672656086777958e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 15/71 | LOSS: 1.058229668160493e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 16/71 | LOSS: 1.0774415065551533e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 17/71 | LOSS: 1.0784053756651701e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 18/71 | LOSS: 1.090397884246685e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 19/71 | LOSS: 1.092519501071365e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 20/71 | LOSS: 1.1138601664805763e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 21/71 | LOSS: 1.1129442803146297e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 22/71 | LOSS: 1.1288825589872431e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 23/71 | LOSS: 1.1175826064876068e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 24/71 | LOSS: 1.1206545786990319e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 25/71 | LOSS: 1.1159267824740919e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 26/71 | LOSS: 1.1114761238988851e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 27/71 | LOSS: 1.1082834557133278e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 28/71 | LOSS: 1.109536236166103e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 29/71 | LOSS: 1.1164493541097424e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 30/71 | LOSS: 1.1099550530132312e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 31/71 | LOSS: 1.1099341008957708e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 32/71 | LOSS: 1.1049208599918833e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 33/71 | LOSS: 1.1078173402518801e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 34/71 | LOSS: 1.1062779282968093e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 35/71 | LOSS: 1.1145974793988797e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 36/71 | LOSS: 1.1163803326387613e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 37/71 | LOSS: 1.1166062952389965e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 38/71 | LOSS: 1.127092904285886e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 39/71 | LOSS: 1.1327830634400016e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 40/71 | LOSS: 1.1343697202036999e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 41/71 | LOSS: 1.136431999492786e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 42/71 | LOSS: 1.1336149644955376e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 43/71 | LOSS: 1.1312295369366025e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 44/71 | LOSS: 1.1291281589365099e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 45/71 | LOSS: 1.1210358104068765e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 46/71 | LOSS: 1.1208437810753512e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 47/71 | LOSS: 1.1188001053596963e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 48/71 | LOSS: 1.1187675988125824e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 49/71 | LOSS: 1.1169377667101798e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 50/71 | LOSS: 1.1132704012019459e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 51/71 | LOSS: 1.1124217647962863e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 52/71 | LOSS: 1.1105782693946066e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 53/71 | LOSS: 1.1138395300939144e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 54/71 | LOSS: 1.1054996461594287e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 55/71 | LOSS: 1.1023201141467456e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 56/71 | LOSS: 1.1008321986369801e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 57/71 | LOSS: 1.1081466858552725e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 58/71 | LOSS: 1.1061907671473355e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 59/71 | LOSS: 1.109639848285345e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 60/71 | LOSS: 1.1070586051722726e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 61/71 | LOSS: 1.110498622056771e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 62/71 | LOSS: 1.1143459196607601e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 63/71 | LOSS: 1.116677447754455e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 64/71 | LOSS: 1.113546781757927e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 65/71 | LOSS: 1.1180330402842657e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 66/71 | LOSS: 1.1174139088515362e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 67/71 | LOSS: 1.1195082666905792e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 68/71 | LOSS: 1.1201134766715636e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 69/71 | LOSS: 1.1167916097422545e-05\n",
      "TRAIN: EPOCH 109/1000 | BATCH 70/71 | LOSS: 1.115322310025976e-05\n",
      "VAL: EPOCH 109/1000 | BATCH 0/8 | LOSS: 1.1496587831061333e-05\n",
      "VAL: EPOCH 109/1000 | BATCH 1/8 | LOSS: 1.2237639566592406e-05\n",
      "VAL: EPOCH 109/1000 | BATCH 2/8 | LOSS: 1.2500909482090114e-05\n",
      "VAL: EPOCH 109/1000 | BATCH 3/8 | LOSS: 1.2244076515344204e-05\n",
      "VAL: EPOCH 109/1000 | BATCH 4/8 | LOSS: 1.174947446997976e-05\n",
      "VAL: EPOCH 109/1000 | BATCH 5/8 | LOSS: 1.145687580598557e-05\n",
      "VAL: EPOCH 109/1000 | BATCH 6/8 | LOSS: 1.1349789539443528e-05\n",
      "VAL: EPOCH 109/1000 | BATCH 7/8 | LOSS: 1.0965003411911312e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 0/71 | LOSS: 9.864999810815789e-06\n",
      "TRAIN: EPOCH 110/1000 | BATCH 1/71 | LOSS: 1.2091226381016895e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 2/71 | LOSS: 1.0836127633713962e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 3/71 | LOSS: 1.0427806273582974e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 4/71 | LOSS: 1.04083543192246e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 5/71 | LOSS: 1.0527104829331316e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 6/71 | LOSS: 1.0332674849321069e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 7/71 | LOSS: 1.05268205743414e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 8/71 | LOSS: 1.0630173973267018e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 9/71 | LOSS: 1.0399495931778801e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 10/71 | LOSS: 1.040867399834414e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 11/71 | LOSS: 1.0236742355118622e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 12/71 | LOSS: 1.033877365094108e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 13/71 | LOSS: 1.0225674226863443e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 14/71 | LOSS: 1.0350281930489777e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 15/71 | LOSS: 1.0299401935753849e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 16/71 | LOSS: 1.0269155823420186e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 17/71 | LOSS: 1.0356785777629637e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 18/71 | LOSS: 1.0318199632533115e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 19/71 | LOSS: 1.031346719173598e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 20/71 | LOSS: 1.0238729287049778e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 21/71 | LOSS: 1.0503804084940136e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 22/71 | LOSS: 1.0587982812822979e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 23/71 | LOSS: 1.0539959513759337e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 24/71 | LOSS: 1.0571468919806648e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 25/71 | LOSS: 1.0647349357056486e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 26/71 | LOSS: 1.0615928490383172e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 27/71 | LOSS: 1.0738832250873592e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 28/71 | LOSS: 1.0779116439215582e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 29/71 | LOSS: 1.0770629099473202e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 30/71 | LOSS: 1.082420714092516e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 31/71 | LOSS: 1.0941866662506072e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 32/71 | LOSS: 1.0907602148963083e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 33/71 | LOSS: 1.0878412039721083e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 34/71 | LOSS: 1.0872187340282835e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 35/71 | LOSS: 1.0840170186080892e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 36/71 | LOSS: 1.0862442084888588e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 37/71 | LOSS: 1.0824499632259463e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 38/71 | LOSS: 1.083995219885718e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 39/71 | LOSS: 1.0842569008673308e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 40/71 | LOSS: 1.0877688354656396e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 41/71 | LOSS: 1.0877851092941238e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 42/71 | LOSS: 1.1012769899152349e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 43/71 | LOSS: 1.1024167261811355e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 44/71 | LOSS: 1.1064241132569603e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 45/71 | LOSS: 1.1107178658636732e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 46/71 | LOSS: 1.1122790230264728e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 47/71 | LOSS: 1.1147331539026103e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 48/71 | LOSS: 1.1131218751852059e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 49/71 | LOSS: 1.115960218157852e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 50/71 | LOSS: 1.120869964164312e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 51/71 | LOSS: 1.1210144678513574e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 52/71 | LOSS: 1.120802103544537e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 53/71 | LOSS: 1.1183177590552347e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 54/71 | LOSS: 1.1178832821976604e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 55/71 | LOSS: 1.1153382323365285e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 56/71 | LOSS: 1.1150880294607895e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 57/71 | LOSS: 1.114972657636952e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 58/71 | LOSS: 1.1147197841699195e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 59/71 | LOSS: 1.1150254006982626e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 60/71 | LOSS: 1.1122561131816808e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 61/71 | LOSS: 1.1204180076850705e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 62/71 | LOSS: 1.1152099605602589e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 63/71 | LOSS: 1.1137733139321426e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 64/71 | LOSS: 1.1180890536567089e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 65/71 | LOSS: 1.1177915370574214e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 66/71 | LOSS: 1.1195677911724596e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 67/71 | LOSS: 1.128031607656928e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 68/71 | LOSS: 1.1300250925937755e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 69/71 | LOSS: 1.1359034436152017e-05\n",
      "TRAIN: EPOCH 110/1000 | BATCH 70/71 | LOSS: 1.1322498465515181e-05\n",
      "VAL: EPOCH 110/1000 | BATCH 0/8 | LOSS: 1.3656159353558905e-05\n",
      "VAL: EPOCH 110/1000 | BATCH 1/8 | LOSS: 1.4478008779406082e-05\n",
      "VAL: EPOCH 110/1000 | BATCH 2/8 | LOSS: 1.3960270128639726e-05\n",
      "VAL: EPOCH 110/1000 | BATCH 3/8 | LOSS: 1.305117166339187e-05\n",
      "VAL: EPOCH 110/1000 | BATCH 4/8 | LOSS: 1.2818595496355557e-05\n",
      "VAL: EPOCH 110/1000 | BATCH 5/8 | LOSS: 1.2646738772067087e-05\n",
      "VAL: EPOCH 110/1000 | BATCH 6/8 | LOSS: 1.2566770757465357e-05\n",
      "VAL: EPOCH 110/1000 | BATCH 7/8 | LOSS: 1.2289316373426118e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 0/71 | LOSS: 1.3057392607151996e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 1/71 | LOSS: 1.0955589004879585e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 2/71 | LOSS: 1.0354800300168185e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 3/71 | LOSS: 1.0085067742693354e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 4/71 | LOSS: 1.0502039185666944e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 5/71 | LOSS: 1.0582573698532846e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 6/71 | LOSS: 1.0790008803139375e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 7/71 | LOSS: 1.0737124398474407e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 8/71 | LOSS: 1.0794409920183372e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 9/71 | LOSS: 1.0616854888212402e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 10/71 | LOSS: 1.0718025970907712e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 11/71 | LOSS: 1.074474827570763e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 12/71 | LOSS: 1.069682604373576e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 13/71 | LOSS: 1.0911275889416824e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 14/71 | LOSS: 1.0862578831923504e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 15/71 | LOSS: 1.0758447274383798e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 16/71 | LOSS: 1.0999641103005748e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 17/71 | LOSS: 1.111276535286581e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 18/71 | LOSS: 1.1268526242546565e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 19/71 | LOSS: 1.1457789514679462e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 20/71 | LOSS: 1.149790564557493e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 21/71 | LOSS: 1.1502349314634392e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 22/71 | LOSS: 1.1703224869131152e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 23/71 | LOSS: 1.1625264960457571e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 24/71 | LOSS: 1.1742756978492252e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 25/71 | LOSS: 1.1723831168917688e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 26/71 | LOSS: 1.1713568508437473e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 27/71 | LOSS: 1.1736557228557234e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 28/71 | LOSS: 1.1731829639255262e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 29/71 | LOSS: 1.163088651689274e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 30/71 | LOSS: 1.1590858159780187e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 31/71 | LOSS: 1.1578334550677027e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 32/71 | LOSS: 1.1557395957810733e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 33/71 | LOSS: 1.149564920577926e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 34/71 | LOSS: 1.1444957973643406e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 35/71 | LOSS: 1.1348579593888846e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 36/71 | LOSS: 1.1287147080528902e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 37/71 | LOSS: 1.1320961149115311e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 38/71 | LOSS: 1.1264463235140563e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 39/71 | LOSS: 1.1303537348794634e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 40/71 | LOSS: 1.132889919085201e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 41/71 | LOSS: 1.1351485833680878e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 42/71 | LOSS: 1.136034950113119e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 43/71 | LOSS: 1.133204052001185e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 44/71 | LOSS: 1.132416107591578e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 45/71 | LOSS: 1.1263778600076204e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 46/71 | LOSS: 1.1273321066023237e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 47/71 | LOSS: 1.1297818692431369e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 48/71 | LOSS: 1.1278480050517532e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 49/71 | LOSS: 1.1230320033064346e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 50/71 | LOSS: 1.122394484149945e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 51/71 | LOSS: 1.1250159087257298e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 52/71 | LOSS: 1.1215416926238246e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 53/71 | LOSS: 1.1205489668268624e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 54/71 | LOSS: 1.1168614334813108e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 55/71 | LOSS: 1.1185643497161177e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 56/71 | LOSS: 1.1177773398741378e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 57/71 | LOSS: 1.118952709946135e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 58/71 | LOSS: 1.1249649845521455e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 59/71 | LOSS: 1.1229049717561186e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 60/71 | LOSS: 1.1236181814981182e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 61/71 | LOSS: 1.1267083033500615e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 62/71 | LOSS: 1.1283269284629367e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 63/71 | LOSS: 1.131424122036151e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 64/71 | LOSS: 1.1304844534046983e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 65/71 | LOSS: 1.1305923856001975e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 66/71 | LOSS: 1.134648463543706e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 67/71 | LOSS: 1.1388712637734202e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 68/71 | LOSS: 1.1456286433647199e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 69/71 | LOSS: 1.1465657085604367e-05\n",
      "TRAIN: EPOCH 111/1000 | BATCH 70/71 | LOSS: 1.1548090876370314e-05\n",
      "VAL: EPOCH 111/1000 | BATCH 0/8 | LOSS: 1.5426303434651345e-05\n",
      "VAL: EPOCH 111/1000 | BATCH 1/8 | LOSS: 1.653240451560123e-05\n",
      "VAL: EPOCH 111/1000 | BATCH 2/8 | LOSS: 1.6794418115750886e-05\n",
      "VAL: EPOCH 111/1000 | BATCH 3/8 | LOSS: 1.657125312704011e-05\n",
      "VAL: EPOCH 111/1000 | BATCH 4/8 | LOSS: 1.5885315951891245e-05\n",
      "VAL: EPOCH 111/1000 | BATCH 5/8 | LOSS: 1.527917963054885e-05\n",
      "VAL: EPOCH 111/1000 | BATCH 6/8 | LOSS: 1.5225395535318447e-05\n",
      "VAL: EPOCH 111/1000 | BATCH 7/8 | LOSS: 1.4674563090011361e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 0/71 | LOSS: 1.0451252819621004e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 1/71 | LOSS: 1.444538702344289e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 2/71 | LOSS: 1.51619363653784e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 3/71 | LOSS: 1.6681441593391355e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 4/71 | LOSS: 1.7361538630211727e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 5/71 | LOSS: 1.6451812371087726e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 6/71 | LOSS: 1.6877774084735264e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 7/71 | LOSS: 1.713040069262206e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 8/71 | LOSS: 1.6842015409363536e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 9/71 | LOSS: 1.7252831821679137e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 10/71 | LOSS: 1.6858819104858082e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 11/71 | LOSS: 1.683791075871947e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 12/71 | LOSS: 1.6501906569688938e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 13/71 | LOSS: 1.6526865562939618e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 14/71 | LOSS: 1.644518233661074e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 15/71 | LOSS: 1.640085986309714e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 16/71 | LOSS: 1.6294713478761396e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 17/71 | LOSS: 1.6073618250326643e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 18/71 | LOSS: 1.6069581234708158e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 19/71 | LOSS: 1.5990730389603414e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 20/71 | LOSS: 1.608103706357291e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 21/71 | LOSS: 1.6030533929138485e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 22/71 | LOSS: 1.578196719018034e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 23/71 | LOSS: 1.5609234537805605e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 24/71 | LOSS: 1.5651441826776137e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 25/71 | LOSS: 1.5619714339174188e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 26/71 | LOSS: 1.548864805011329e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 27/71 | LOSS: 1.540944163025415e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 28/71 | LOSS: 1.5440745551915887e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 29/71 | LOSS: 1.541738665764569e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 30/71 | LOSS: 1.5311864738196752e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 31/71 | LOSS: 1.533324839897432e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 32/71 | LOSS: 1.5371447592439257e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 33/71 | LOSS: 1.5317573423303923e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 34/71 | LOSS: 1.5293579287702284e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 35/71 | LOSS: 1.527602156177939e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 36/71 | LOSS: 1.5354525768654294e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 37/71 | LOSS: 1.537319181615634e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 38/71 | LOSS: 1.5300519944270116e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 39/71 | LOSS: 1.5374404756585137e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 40/71 | LOSS: 1.52635112033572e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 41/71 | LOSS: 1.5253751782784958e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 42/71 | LOSS: 1.5192713510978771e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 43/71 | LOSS: 1.5136086403799709e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 44/71 | LOSS: 1.5115234095396268e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 45/71 | LOSS: 1.4970760513160316e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 46/71 | LOSS: 1.5058917384483217e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 47/71 | LOSS: 1.4999198109914383e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 48/71 | LOSS: 1.491326189092693e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 49/71 | LOSS: 1.4875960223434959e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 50/71 | LOSS: 1.479917106432064e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 51/71 | LOSS: 1.4747640114784679e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 52/71 | LOSS: 1.4701232808051227e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 53/71 | LOSS: 1.4616070462512585e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 54/71 | LOSS: 1.4512850132384549e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 55/71 | LOSS: 1.4464936482129684e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 56/71 | LOSS: 1.4378326341102365e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 57/71 | LOSS: 1.4333863732488305e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 58/71 | LOSS: 1.4260220129888355e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 59/71 | LOSS: 1.4213118007925611e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 60/71 | LOSS: 1.4155304314521882e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 61/71 | LOSS: 1.4084812993396848e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 62/71 | LOSS: 1.4020352604607719e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 63/71 | LOSS: 1.4020312377738264e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 64/71 | LOSS: 1.3975753357114556e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 65/71 | LOSS: 1.3930843344882526e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 66/71 | LOSS: 1.3937146805027855e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 67/71 | LOSS: 1.3871168448531535e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 68/71 | LOSS: 1.3825175538722364e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 69/71 | LOSS: 1.3754187745819633e-05\n",
      "TRAIN: EPOCH 112/1000 | BATCH 70/71 | LOSS: 1.3720017436049013e-05\n",
      "VAL: EPOCH 112/1000 | BATCH 0/8 | LOSS: 1.0597153959679417e-05\n",
      "VAL: EPOCH 112/1000 | BATCH 1/8 | LOSS: 1.1186050869582687e-05\n",
      "VAL: EPOCH 112/1000 | BATCH 2/8 | LOSS: 1.1701030113423863e-05\n",
      "VAL: EPOCH 112/1000 | BATCH 3/8 | LOSS: 1.1560244047359447e-05\n",
      "VAL: EPOCH 112/1000 | BATCH 4/8 | LOSS: 1.113380931201391e-05\n",
      "VAL: EPOCH 112/1000 | BATCH 5/8 | LOSS: 1.0910795632905016e-05\n",
      "VAL: EPOCH 112/1000 | BATCH 6/8 | LOSS: 1.0748010806439976e-05\n",
      "VAL: EPOCH 112/1000 | BATCH 7/8 | LOSS: 1.0455079859639227e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 0/71 | LOSS: 1.060678914655e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 1/71 | LOSS: 9.74990052782232e-06\n",
      "TRAIN: EPOCH 113/1000 | BATCH 2/71 | LOSS: 9.568680373680158e-06\n",
      "TRAIN: EPOCH 113/1000 | BATCH 3/71 | LOSS: 9.842268354987027e-06\n",
      "TRAIN: EPOCH 113/1000 | BATCH 4/71 | LOSS: 9.85369370027911e-06\n",
      "TRAIN: EPOCH 113/1000 | BATCH 5/71 | LOSS: 1.0294472910269784e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 6/71 | LOSS: 1.0202693504522489e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 7/71 | LOSS: 1.0441836593599874e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 8/71 | LOSS: 1.0844619888909316e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 9/71 | LOSS: 1.0863665738725104e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 10/71 | LOSS: 1.0938717754139692e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 11/71 | LOSS: 1.095763104785874e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 12/71 | LOSS: 1.0857506682581376e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 13/71 | LOSS: 1.0962382995265735e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 14/71 | LOSS: 1.0896614306451131e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 15/71 | LOSS: 1.0884982827974454e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 16/71 | LOSS: 1.0994904401099228e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 17/71 | LOSS: 1.096899985794961e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 18/71 | LOSS: 1.0875408693927488e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 19/71 | LOSS: 1.0726958043960621e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 20/71 | LOSS: 1.0773345084522762e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 21/71 | LOSS: 1.0703410473731559e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 22/71 | LOSS: 1.0895447738458524e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 23/71 | LOSS: 1.0892659626430637e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 24/71 | LOSS: 1.0925913084065542e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 25/71 | LOSS: 1.0871970516745932e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 26/71 | LOSS: 1.0928382161418321e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 27/71 | LOSS: 1.1082382051037193e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 28/71 | LOSS: 1.1057523305022062e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 29/71 | LOSS: 1.093853443308035e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 30/71 | LOSS: 1.0831167034461209e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 31/71 | LOSS: 1.0730589295349091e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 32/71 | LOSS: 1.0731484301438535e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 33/71 | LOSS: 1.0753657815097112e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 34/71 | LOSS: 1.0812589750587774e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 35/71 | LOSS: 1.0773461541349308e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 36/71 | LOSS: 1.086337403427712e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 37/71 | LOSS: 1.0876744921149925e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 38/71 | LOSS: 1.0848195147562552e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 39/71 | LOSS: 1.0846292013866332e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 40/71 | LOSS: 1.0840170755439364e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 41/71 | LOSS: 1.0777731761533818e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 42/71 | LOSS: 1.074458204616243e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 43/71 | LOSS: 1.0699352318707414e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 44/71 | LOSS: 1.0671635765094026e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 45/71 | LOSS: 1.0740529205577332e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 46/71 | LOSS: 1.0801534206117874e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 47/71 | LOSS: 1.0797713220730051e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 48/71 | LOSS: 1.074288399408664e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 49/71 | LOSS: 1.0777116176541313e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 50/71 | LOSS: 1.082055336619127e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 51/71 | LOSS: 1.0855919570601076e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 52/71 | LOSS: 1.0858769552935385e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 53/71 | LOSS: 1.0942660579800119e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 54/71 | LOSS: 1.0951276081786173e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 55/71 | LOSS: 1.0957222968954738e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 56/71 | LOSS: 1.0926624167551749e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 57/71 | LOSS: 1.1008426279909929e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 58/71 | LOSS: 1.1008670352859584e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 59/71 | LOSS: 1.1038826149463906e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 60/71 | LOSS: 1.1047532403668423e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 61/71 | LOSS: 1.1086178794299016e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 62/71 | LOSS: 1.1141489173337866e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 63/71 | LOSS: 1.1095180646236713e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 64/71 | LOSS: 1.1125588998066423e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 65/71 | LOSS: 1.1084995487015641e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 66/71 | LOSS: 1.1086267674198016e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 67/71 | LOSS: 1.1169469503864307e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 68/71 | LOSS: 1.1148152324217929e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 69/71 | LOSS: 1.1122300949344727e-05\n",
      "TRAIN: EPOCH 113/1000 | BATCH 70/71 | LOSS: 1.1116759459339296e-05\n",
      "VAL: EPOCH 113/1000 | BATCH 0/8 | LOSS: 1.356147367914673e-05\n",
      "VAL: EPOCH 113/1000 | BATCH 1/8 | LOSS: 1.3985218174639158e-05\n",
      "VAL: EPOCH 113/1000 | BATCH 2/8 | LOSS: 1.3953758752904832e-05\n",
      "VAL: EPOCH 113/1000 | BATCH 3/8 | LOSS: 1.3297090617925278e-05\n",
      "VAL: EPOCH 113/1000 | BATCH 4/8 | LOSS: 1.3119365030433983e-05\n",
      "VAL: EPOCH 113/1000 | BATCH 5/8 | LOSS: 1.2957254966750043e-05\n",
      "VAL: EPOCH 113/1000 | BATCH 6/8 | LOSS: 1.2926852832606528e-05\n",
      "VAL: EPOCH 113/1000 | BATCH 7/8 | LOSS: 1.2680857366831333e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 0/71 | LOSS: 1.2255930414539762e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 1/71 | LOSS: 1.1788076790253399e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 2/71 | LOSS: 1.1749832386461398e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 3/71 | LOSS: 1.1512721812323434e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 4/71 | LOSS: 1.1531946984177922e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 5/71 | LOSS: 1.1713897492882097e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 6/71 | LOSS: 1.1325910626120666e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 7/71 | LOSS: 1.1764645364564785e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 8/71 | LOSS: 1.1545727425578258e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 9/71 | LOSS: 1.1478314263513312e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 10/71 | LOSS: 1.1314538741399618e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 11/71 | LOSS: 1.1458673877010975e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 12/71 | LOSS: 1.1460608010775804e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 13/71 | LOSS: 1.1537887920504935e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 14/71 | LOSS: 1.1416187589929905e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 15/71 | LOSS: 1.1410977606374217e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 16/71 | LOSS: 1.1439020507917603e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 17/71 | LOSS: 1.1411656184160771e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 18/71 | LOSS: 1.134171083992298e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 19/71 | LOSS: 1.1434677480792743e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 20/71 | LOSS: 1.1462569567638206e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 21/71 | LOSS: 1.1415100082310595e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 22/71 | LOSS: 1.1450852505969005e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 23/71 | LOSS: 1.1384998742869357e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 24/71 | LOSS: 1.1380878859199584e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 25/71 | LOSS: 1.1306694991967211e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 26/71 | LOSS: 1.1349923523893165e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 27/71 | LOSS: 1.1255022627665312e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 28/71 | LOSS: 1.1302893089405502e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 29/71 | LOSS: 1.1272697308110462e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 30/71 | LOSS: 1.1256148882826129e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 31/71 | LOSS: 1.1248919520312484e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 32/71 | LOSS: 1.1224949985537961e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 33/71 | LOSS: 1.1193671472189153e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 34/71 | LOSS: 1.1142417601409502e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 35/71 | LOSS: 1.113249236873849e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 36/71 | LOSS: 1.1134718349510237e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 37/71 | LOSS: 1.1052627135206039e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 38/71 | LOSS: 1.1015581199339627e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 39/71 | LOSS: 1.0989908810188354e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 40/71 | LOSS: 1.1004209027749135e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 41/71 | LOSS: 1.1002267218005035e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 42/71 | LOSS: 1.1020549653378196e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 43/71 | LOSS: 1.1058092964462544e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 44/71 | LOSS: 1.1145971822972772e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 45/71 | LOSS: 1.1115883867403127e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 46/71 | LOSS: 1.1157021017812014e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 47/71 | LOSS: 1.115705932382601e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 48/71 | LOSS: 1.1154148858028692e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 49/71 | LOSS: 1.1169405552209354e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 50/71 | LOSS: 1.1130993823148766e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 51/71 | LOSS: 1.1146327254279571e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 52/71 | LOSS: 1.1159354042594611e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 53/71 | LOSS: 1.1134548205720623e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 54/71 | LOSS: 1.1109326442237943e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 55/71 | LOSS: 1.1091296430874667e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 56/71 | LOSS: 1.110857875462164e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 57/71 | LOSS: 1.1130017935948703e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 58/71 | LOSS: 1.1138039738752769e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 59/71 | LOSS: 1.1095399865250025e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 60/71 | LOSS: 1.1146362980905935e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 61/71 | LOSS: 1.113377689879628e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 62/71 | LOSS: 1.1110677752142129e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 63/71 | LOSS: 1.1120792279939451e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 64/71 | LOSS: 1.1123215195906456e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 65/71 | LOSS: 1.1127529129832206e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 66/71 | LOSS: 1.116498565555849e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 67/71 | LOSS: 1.1177646175543914e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 68/71 | LOSS: 1.1195052560440777e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 69/71 | LOSS: 1.1210479767344492e-05\n",
      "TRAIN: EPOCH 114/1000 | BATCH 70/71 | LOSS: 1.120555521403833e-05\n",
      "VAL: EPOCH 114/1000 | BATCH 0/8 | LOSS: 1.0584697520243935e-05\n",
      "VAL: EPOCH 114/1000 | BATCH 1/8 | LOSS: 1.079761659639189e-05\n",
      "VAL: EPOCH 114/1000 | BATCH 2/8 | LOSS: 1.1142034054500982e-05\n",
      "VAL: EPOCH 114/1000 | BATCH 3/8 | LOSS: 1.0832295856744167e-05\n",
      "VAL: EPOCH 114/1000 | BATCH 4/8 | LOSS: 1.0783172911033034e-05\n",
      "VAL: EPOCH 114/1000 | BATCH 5/8 | LOSS: 1.0835189186764183e-05\n",
      "VAL: EPOCH 114/1000 | BATCH 6/8 | LOSS: 1.0718140012094019e-05\n",
      "VAL: EPOCH 114/1000 | BATCH 7/8 | LOSS: 1.059076089404698e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 0/71 | LOSS: 1.3934196431364398e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 1/71 | LOSS: 1.3334068171388935e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 2/71 | LOSS: 1.2350282910726188e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 3/71 | LOSS: 1.206430465572339e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 4/71 | LOSS: 1.1798991727118846e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 5/71 | LOSS: 1.1255333599062093e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 6/71 | LOSS: 1.1246790433818075e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 7/71 | LOSS: 1.1544195558599313e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 8/71 | LOSS: 1.1323672070931127e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 9/71 | LOSS: 1.1280058970442042e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 10/71 | LOSS: 1.1211332589895887e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 11/71 | LOSS: 1.1160400390508585e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 12/71 | LOSS: 1.0937155960933664e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 13/71 | LOSS: 1.0914181724988989e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 14/71 | LOSS: 1.109230888687307e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 15/71 | LOSS: 1.0899898484240111e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 16/71 | LOSS: 1.0972101389437311e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 17/71 | LOSS: 1.0893057151406538e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 18/71 | LOSS: 1.1033552972979746e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 19/71 | LOSS: 1.1187846212123986e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 20/71 | LOSS: 1.1103492614070309e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 21/71 | LOSS: 1.1111829487411093e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 22/71 | LOSS: 1.0969902810440702e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 23/71 | LOSS: 1.0842683915749754e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 24/71 | LOSS: 1.0837534355232491e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 25/71 | LOSS: 1.0778031887289566e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 26/71 | LOSS: 1.0787298513740231e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 27/71 | LOSS: 1.0665041096607248e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 28/71 | LOSS: 1.0848865007699651e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 29/71 | LOSS: 1.0862141289180727e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 30/71 | LOSS: 1.086727475159287e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 31/71 | LOSS: 1.0871408150592288e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 32/71 | LOSS: 1.0954884918178184e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 33/71 | LOSS: 1.0924794731395463e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 34/71 | LOSS: 1.0850795102409652e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 35/71 | LOSS: 1.0891494399503346e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 36/71 | LOSS: 1.0949663646897534e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 37/71 | LOSS: 1.0919917749082455e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 38/71 | LOSS: 1.088501487454498e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 39/71 | LOSS: 1.0920826150595531e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 40/71 | LOSS: 1.088140334104537e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 41/71 | LOSS: 1.0926252181369428e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 42/71 | LOSS: 1.0967313406835337e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 43/71 | LOSS: 1.0995826301041234e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 44/71 | LOSS: 1.1095974868011479e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 45/71 | LOSS: 1.10762448160961e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 46/71 | LOSS: 1.1115702954099262e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 47/71 | LOSS: 1.11100886310093e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 48/71 | LOSS: 1.111529035351479e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 49/71 | LOSS: 1.1127442930956022e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 50/71 | LOSS: 1.1147494170276617e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 51/71 | LOSS: 1.1176354178132897e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 52/71 | LOSS: 1.114599058811744e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 53/71 | LOSS: 1.1157914867807347e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 54/71 | LOSS: 1.1103854913843945e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 55/71 | LOSS: 1.1085296145146068e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 56/71 | LOSS: 1.1145852073368906e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 57/71 | LOSS: 1.1126324125143583e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 58/71 | LOSS: 1.1209980700294767e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 59/71 | LOSS: 1.123778916583736e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 60/71 | LOSS: 1.128940901581387e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 61/71 | LOSS: 1.1287847376978231e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 62/71 | LOSS: 1.132093682336124e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 63/71 | LOSS: 1.139816176731756e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 64/71 | LOSS: 1.1403509072876589e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 65/71 | LOSS: 1.138837945180774e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 66/71 | LOSS: 1.140666024605422e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 67/71 | LOSS: 1.1415736607619584e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 68/71 | LOSS: 1.1413751468827957e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 69/71 | LOSS: 1.1392476679767632e-05\n",
      "TRAIN: EPOCH 115/1000 | BATCH 70/71 | LOSS: 1.1422931768553918e-05\n",
      "VAL: EPOCH 115/1000 | BATCH 0/8 | LOSS: 1.0706620741984807e-05\n",
      "VAL: EPOCH 115/1000 | BATCH 1/8 | LOSS: 1.1685904610203579e-05\n",
      "VAL: EPOCH 115/1000 | BATCH 2/8 | LOSS: 1.2151901197891371e-05\n",
      "VAL: EPOCH 115/1000 | BATCH 3/8 | LOSS: 1.2169089586677728e-05\n",
      "VAL: EPOCH 115/1000 | BATCH 4/8 | LOSS: 1.2023212184431031e-05\n",
      "VAL: EPOCH 115/1000 | BATCH 5/8 | LOSS: 1.188632541015977e-05\n",
      "VAL: EPOCH 115/1000 | BATCH 6/8 | LOSS: 1.1837927169316182e-05\n",
      "VAL: EPOCH 115/1000 | BATCH 7/8 | LOSS: 1.1559297377061739e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 0/71 | LOSS: 1.1881812497449573e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 1/71 | LOSS: 1.2011174931103596e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 2/71 | LOSS: 1.1242902777060712e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 3/71 | LOSS: 1.0776241879284498e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 4/71 | LOSS: 1.1178827662661206e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 5/71 | LOSS: 1.1142457575867107e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 6/71 | LOSS: 1.1273389775721756e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 7/71 | LOSS: 1.0877252179852803e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 8/71 | LOSS: 1.0702366428934814e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 9/71 | LOSS: 1.0567543995421147e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 10/71 | LOSS: 1.0411099620713768e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 11/71 | LOSS: 1.0647183216860867e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 12/71 | LOSS: 1.071276409885076e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 13/71 | LOSS: 1.0701036444515921e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 14/71 | LOSS: 1.0517651511084598e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 15/71 | LOSS: 1.0816661301760178e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 16/71 | LOSS: 1.087306415737497e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 17/71 | LOSS: 1.0942410275068445e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 18/71 | LOSS: 1.121703845503936e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 19/71 | LOSS: 1.1135977183585056e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 20/71 | LOSS: 1.1367842619490278e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 21/71 | LOSS: 1.1652184515365992e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 22/71 | LOSS: 1.1651452120492214e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 23/71 | LOSS: 1.1862761804574498e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 24/71 | LOSS: 1.1966749225393869e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 25/71 | LOSS: 1.2136217614729852e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 26/71 | LOSS: 1.2386880943840541e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 27/71 | LOSS: 1.2310456765719988e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 28/71 | LOSS: 1.23320349240234e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 29/71 | LOSS: 1.2469429384509567e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 30/71 | LOSS: 1.2442298201119317e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 31/71 | LOSS: 1.2552700354717672e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 32/71 | LOSS: 1.2652826240589617e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 33/71 | LOSS: 1.276641416873591e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 34/71 | LOSS: 1.2864188933495565e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 35/71 | LOSS: 1.2773625737989076e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 36/71 | LOSS: 1.298204583813581e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 37/71 | LOSS: 1.298472812107464e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 38/71 | LOSS: 1.2953521409377945e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 39/71 | LOSS: 1.2981926442989788e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 40/71 | LOSS: 1.2987063356132882e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 41/71 | LOSS: 1.2906615904198371e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 42/71 | LOSS: 1.2894462743979813e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 43/71 | LOSS: 1.2859607192116197e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 44/71 | LOSS: 1.2806438664685831e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 45/71 | LOSS: 1.2800055312412871e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 46/71 | LOSS: 1.2799729971602767e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 47/71 | LOSS: 1.2758934284799276e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 48/71 | LOSS: 1.274572941281581e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 49/71 | LOSS: 1.2710439914371819e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 50/71 | LOSS: 1.2716956203518089e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 51/71 | LOSS: 1.2644074331547017e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 52/71 | LOSS: 1.2560839937612159e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 53/71 | LOSS: 1.2516929507034581e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 54/71 | LOSS: 1.2442255270302254e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 55/71 | LOSS: 1.2387466686699814e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 56/71 | LOSS: 1.2382016489266933e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 57/71 | LOSS: 1.234039371348476e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 58/71 | LOSS: 1.2335435296675205e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 59/71 | LOSS: 1.2303294473288892e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 60/71 | LOSS: 1.2273354573675324e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 61/71 | LOSS: 1.2292962324981119e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 62/71 | LOSS: 1.2291831389741255e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 63/71 | LOSS: 1.2250193165641576e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 64/71 | LOSS: 1.2274452721892606e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 65/71 | LOSS: 1.2230667421549283e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 66/71 | LOSS: 1.2220674395407445e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 67/71 | LOSS: 1.2225714900684098e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 68/71 | LOSS: 1.2246349836894534e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 69/71 | LOSS: 1.2285252731609424e-05\n",
      "TRAIN: EPOCH 116/1000 | BATCH 70/71 | LOSS: 1.2243672590813449e-05\n",
      "VAL: EPOCH 116/1000 | BATCH 0/8 | LOSS: 1.1424170224927366e-05\n",
      "VAL: EPOCH 116/1000 | BATCH 1/8 | LOSS: 1.1603823622863274e-05\n",
      "VAL: EPOCH 116/1000 | BATCH 2/8 | LOSS: 1.2054718051028127e-05\n",
      "VAL: EPOCH 116/1000 | BATCH 3/8 | LOSS: 1.1352220326443785e-05\n",
      "VAL: EPOCH 116/1000 | BATCH 4/8 | LOSS: 1.1447033830336296e-05\n",
      "VAL: EPOCH 116/1000 | BATCH 5/8 | LOSS: 1.1533769035546962e-05\n",
      "VAL: EPOCH 116/1000 | BATCH 6/8 | LOSS: 1.122130756162473e-05\n",
      "VAL: EPOCH 116/1000 | BATCH 7/8 | LOSS: 1.1234543876526004e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 0/71 | LOSS: 1.2142490959377028e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 1/71 | LOSS: 1.2434034033503849e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 2/71 | LOSS: 1.1485966448769128e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 3/71 | LOSS: 1.1535127441675286e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 4/71 | LOSS: 1.1676813483063597e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 5/71 | LOSS: 1.163975700061807e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 6/71 | LOSS: 1.1549099846368855e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 7/71 | LOSS: 1.1772244306484936e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 8/71 | LOSS: 1.174494092184533e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 9/71 | LOSS: 1.1521522264956729e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 10/71 | LOSS: 1.1367450083939316e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 11/71 | LOSS: 1.135211543138818e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 12/71 | LOSS: 1.1402684601937206e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 13/71 | LOSS: 1.1392979753769136e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 14/71 | LOSS: 1.1192052867651607e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 15/71 | LOSS: 1.1094882211182266e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 16/71 | LOSS: 1.1245232813320507e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 17/71 | LOSS: 1.1215753147553187e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 18/71 | LOSS: 1.125918774242133e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 19/71 | LOSS: 1.1227088953091879e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 20/71 | LOSS: 1.1207051925122782e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 21/71 | LOSS: 1.1131391106160168e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 22/71 | LOSS: 1.1033368260225119e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 23/71 | LOSS: 1.1011502162242929e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 24/71 | LOSS: 1.1009595909854398e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 25/71 | LOSS: 1.1093907792551014e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 26/71 | LOSS: 1.1019536739493358e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 27/71 | LOSS: 1.0912315929090255e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 28/71 | LOSS: 1.093772971216822e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 29/71 | LOSS: 1.094976411574559e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 30/71 | LOSS: 1.0907232213104444e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 31/71 | LOSS: 1.0895672318156358e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 32/71 | LOSS: 1.083819670660005e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 33/71 | LOSS: 1.07649698293244e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 34/71 | LOSS: 1.0726796894492248e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 35/71 | LOSS: 1.0733852049169298e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 36/71 | LOSS: 1.0710354660192467e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 37/71 | LOSS: 1.064198915245422e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 38/71 | LOSS: 1.0627667926690685e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 39/71 | LOSS: 1.0604454382701079e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 40/71 | LOSS: 1.0541330284024475e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 41/71 | LOSS: 1.0474203875110972e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 42/71 | LOSS: 1.0524341209824089e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 43/71 | LOSS: 1.0491207438628242e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 44/71 | LOSS: 1.0461209300653234e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 45/71 | LOSS: 1.0420958203568539e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 46/71 | LOSS: 1.0385176229898748e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 47/71 | LOSS: 1.0381320767767951e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 48/71 | LOSS: 1.0399337172982217e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 49/71 | LOSS: 1.0424397023598431e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 50/71 | LOSS: 1.0434879578836034e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 51/71 | LOSS: 1.039305378141692e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 52/71 | LOSS: 1.0404391173753263e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 53/71 | LOSS: 1.043546517057284e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 54/71 | LOSS: 1.0494440655086444e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 55/71 | LOSS: 1.0604071413971334e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 56/71 | LOSS: 1.0554787558360482e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 57/71 | LOSS: 1.0575143058868608e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 58/71 | LOSS: 1.0623733402807977e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 59/71 | LOSS: 1.0654961746089006e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 60/71 | LOSS: 1.0619290926532828e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 61/71 | LOSS: 1.0623659891866883e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 62/71 | LOSS: 1.0627181548024676e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 63/71 | LOSS: 1.0612180531666127e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 64/71 | LOSS: 1.0627887478711693e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 65/71 | LOSS: 1.0608463071548613e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 66/71 | LOSS: 1.061319444624363e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 67/71 | LOSS: 1.05716931503754e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 68/71 | LOSS: 1.0533196038285615e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 69/71 | LOSS: 1.0550109189872663e-05\n",
      "TRAIN: EPOCH 117/1000 | BATCH 70/71 | LOSS: 1.0546181367547906e-05\n",
      "VAL: EPOCH 117/1000 | BATCH 0/8 | LOSS: 1.0522932825551834e-05\n",
      "VAL: EPOCH 117/1000 | BATCH 1/8 | LOSS: 1.0923264653683873e-05\n",
      "VAL: EPOCH 117/1000 | BATCH 2/8 | LOSS: 1.120108360434339e-05\n",
      "VAL: EPOCH 117/1000 | BATCH 3/8 | LOSS: 1.086677525563573e-05\n",
      "VAL: EPOCH 117/1000 | BATCH 4/8 | LOSS: 1.0632847806846258e-05\n",
      "VAL: EPOCH 117/1000 | BATCH 5/8 | LOSS: 1.0464609507228792e-05\n",
      "VAL: EPOCH 117/1000 | BATCH 6/8 | LOSS: 1.0436981158688599e-05\n",
      "VAL: EPOCH 117/1000 | BATCH 7/8 | LOSS: 1.0189183512920863e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 0/71 | LOSS: 1.0723742889240384e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 1/71 | LOSS: 1.0941352229565382e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 2/71 | LOSS: 1.124142484817033e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 3/71 | LOSS: 1.0979249964293558e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 4/71 | LOSS: 1.0308545188308927e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 5/71 | LOSS: 1.0647423020297234e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 6/71 | LOSS: 1.0879994403824509e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 7/71 | LOSS: 1.0982141077420238e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 8/71 | LOSS: 1.0889902366923505e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 9/71 | LOSS: 1.0809579771375866e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 10/71 | LOSS: 1.0882709292755779e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 11/71 | LOSS: 1.0859500700159211e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 12/71 | LOSS: 1.0758424405839132e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 13/71 | LOSS: 1.079557370888194e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 14/71 | LOSS: 1.0633683662793677e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 15/71 | LOSS: 1.0610031864644043e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 16/71 | LOSS: 1.0572674669032602e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 17/71 | LOSS: 1.055162710549161e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 18/71 | LOSS: 1.0562707398333404e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 19/71 | LOSS: 1.0527260224080237e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 20/71 | LOSS: 1.0530832588258255e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 21/71 | LOSS: 1.0388525652160752e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 22/71 | LOSS: 1.0301286851166008e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 23/71 | LOSS: 1.0311159239032955e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 24/71 | LOSS: 1.0364344761910616e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 25/71 | LOSS: 1.0298618602833953e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 26/71 | LOSS: 1.0250398173849573e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 27/71 | LOSS: 1.027050781691027e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 28/71 | LOSS: 1.0243240357622513e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 29/71 | LOSS: 1.0223497899157034e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 30/71 | LOSS: 1.0223410534367318e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 31/71 | LOSS: 1.0169957349148717e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 32/71 | LOSS: 1.0174967332149567e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 33/71 | LOSS: 1.011900732972957e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 34/71 | LOSS: 1.0129584539494578e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 35/71 | LOSS: 1.016758279901195e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 36/71 | LOSS: 1.0181570765731473e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 37/71 | LOSS: 1.0203339008129612e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 38/71 | LOSS: 1.0196570131553857e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 39/71 | LOSS: 1.0189564579832223e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 40/71 | LOSS: 1.0220756150108462e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 41/71 | LOSS: 1.0194995313993007e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 42/71 | LOSS: 1.0177417385251787e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 43/71 | LOSS: 1.012698307725011e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 44/71 | LOSS: 1.0164842822430526e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 45/71 | LOSS: 1.016077847096945e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 46/71 | LOSS: 1.0111842042765892e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 47/71 | LOSS: 1.0092413380865159e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 48/71 | LOSS: 1.0085633156060275e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 49/71 | LOSS: 1.0065650194519548e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 50/71 | LOSS: 1.0035281998259997e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 51/71 | LOSS: 1.00516459899434e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 52/71 | LOSS: 1.0067802088328017e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 53/71 | LOSS: 1.0077278358477526e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 54/71 | LOSS: 1.0055646329088963e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 55/71 | LOSS: 1.0092324950099802e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 56/71 | LOSS: 1.0098770002386709e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 57/71 | LOSS: 1.0082810314119782e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 58/71 | LOSS: 1.0113251700188859e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 59/71 | LOSS: 1.0122606765132029e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 60/71 | LOSS: 1.0101122773105954e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 61/71 | LOSS: 1.0091333515740718e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 62/71 | LOSS: 1.0099153689860407e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 63/71 | LOSS: 1.0109944660996462e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 64/71 | LOSS: 1.0129080629457559e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 65/71 | LOSS: 1.0143588076861112e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 66/71 | LOSS: 1.0154801657336066e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 67/71 | LOSS: 1.0111830687599328e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 68/71 | LOSS: 1.0134003833377061e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 69/71 | LOSS: 1.0131276478984675e-05\n",
      "TRAIN: EPOCH 118/1000 | BATCH 70/71 | LOSS: 1.009720472851973e-05\n",
      "VAL: EPOCH 118/1000 | BATCH 0/8 | LOSS: 1.1237358194193803e-05\n",
      "VAL: EPOCH 118/1000 | BATCH 1/8 | LOSS: 1.1842344065371435e-05\n",
      "VAL: EPOCH 118/1000 | BATCH 2/8 | LOSS: 1.1794329111580737e-05\n",
      "VAL: EPOCH 118/1000 | BATCH 3/8 | LOSS: 1.1240181265748106e-05\n",
      "VAL: EPOCH 118/1000 | BATCH 4/8 | LOSS: 1.0956133701256477e-05\n",
      "VAL: EPOCH 118/1000 | BATCH 5/8 | LOSS: 1.066399787911602e-05\n",
      "VAL: EPOCH 118/1000 | BATCH 6/8 | LOSS: 1.0553686349469768e-05\n",
      "VAL: EPOCH 118/1000 | BATCH 7/8 | LOSS: 1.0279627076670295e-05\n",
      "TRAIN: EPOCH 119/1000 | BATCH 0/71 | LOSS: 1.0174988346989267e-05\n",
      "TRAIN: EPOCH 119/1000 | BATCH 1/71 | LOSS: 1.0669250514183659e-05\n",
      "TRAIN: EPOCH 119/1000 | BATCH 2/71 | LOSS: 1.0558933960661912e-05\n",
      "TRAIN: EPOCH 119/1000 | BATCH 3/71 | LOSS: 1.0266124263580423e-05\n",
      "TRAIN: EPOCH 119/1000 | BATCH 4/71 | LOSS: 9.955044515663757e-06\n",
      "TRAIN: EPOCH 119/1000 | BATCH 5/71 | LOSS: 1.0016787806913877e-05\n",
      "TRAIN: EPOCH 119/1000 | BATCH 6/71 | LOSS: 9.760254995074189e-06\n",
      "TRAIN: EPOCH 119/1000 | BATCH 7/71 | LOSS: 9.864048138297221e-06\n",
      "TRAIN: EPOCH 119/1000 | BATCH 8/71 | LOSS: 9.748042531201565e-06\n",
      "TRAIN: EPOCH 119/1000 | BATCH 9/71 | LOSS: 9.957463316823122e-06\n",
      "TRAIN: EPOCH 119/1000 | BATCH 10/71 | LOSS: 9.927524843078572e-06\n",
      "TRAIN: EPOCH 119/1000 | BATCH 11/71 | LOSS: 9.956972538323802e-06\n",
      "TRAIN: EPOCH 119/1000 | BATCH 12/71 | LOSS: 1.001589757484348e-05\n",
      "TRAIN: EPOCH 119/1000 | BATCH 13/71 | LOSS: 9.954596862371545e-06\n",
      "TRAIN: EPOCH 119/1000 | BATCH 14/71 | LOSS: 1.0034449345160585e-05\n",
      "TRAIN: EPOCH 119/1000 | BATCH 15/71 | LOSS: 9.986264785766252e-06\n",
      "TRAIN: EPOCH 119/1000 | BATCH 16/71 | LOSS: 9.806290001827566e-06\n",
      "TRAIN: EPOCH 119/1000 | BATCH 17/71 | LOSS: 9.710436441107757e-06\n",
      "TRAIN: EPOCH 119/1000 | BATCH 18/71 | LOSS: 9.614452114635097e-06\n",
      "TRAIN: EPOCH 119/1000 | BATCH 19/71 | LOSS: 9.656527527113213e-06\n",
      "TRAIN: EPOCH 119/1000 | BATCH 20/71 | LOSS: 9.729694803744288e-06\n",
      "TRAIN: EPOCH 119/1000 | BATCH 21/71 | LOSS: 9.877459640186068e-06\n",
      "TRAIN: EPOCH 119/1000 | BATCH 22/71 | LOSS: 9.859096597052565e-06\n",
      "TRAIN: EPOCH 119/1000 | BATCH 23/71 | LOSS: 9.852647546419272e-06\n",
      "TRAIN: EPOCH 119/1000 | BATCH 24/71 | LOSS: 9.903802747430745e-06\n",
      "TRAIN: EPOCH 119/1000 | BATCH 25/71 | LOSS: 9.890454363742789e-06\n",
      "TRAIN: EPOCH 119/1000 | BATCH 26/71 | LOSS: 9.839146379683236e-06\n",
      "TRAIN: EPOCH 119/1000 | BATCH 27/71 | LOSS: 9.855323794129487e-06\n",
      "TRAIN: EPOCH 119/1000 | BATCH 28/71 | LOSS: 9.88783374160793e-06\n",
      "TRAIN: EPOCH 119/1000 | BATCH 29/71 | LOSS: 9.860122493895082e-06\n",
      "TRAIN: EPOCH 119/1000 | BATCH 30/71 | LOSS: 9.799302079632217e-06\n",
      "TRAIN: EPOCH 119/1000 | BATCH 31/71 | LOSS: 9.79078595264582e-06\n",
      "TRAIN: EPOCH 119/1000 | BATCH 32/71 | LOSS: 9.75197031727999e-06\n",
      "TRAIN: EPOCH 119/1000 | BATCH 33/71 | LOSS: 9.714187880864997e-06\n",
      "TRAIN: EPOCH 119/1000 | BATCH 34/71 | LOSS: 9.742546067822592e-06\n",
      "TRAIN: EPOCH 119/1000 | BATCH 35/71 | LOSS: 9.787568311973397e-06\n",
      "TRAIN: EPOCH 119/1000 | BATCH 36/71 | LOSS: 9.785801700451663e-06\n",
      "TRAIN: EPOCH 119/1000 | BATCH 37/71 | LOSS: 9.719679957542163e-06\n",
      "TRAIN: EPOCH 119/1000 | BATCH 38/71 | LOSS: 9.688339210137761e-06\n",
      "TRAIN: EPOCH 119/1000 | BATCH 39/71 | LOSS: 9.684525866759942e-06\n",
      "TRAIN: EPOCH 119/1000 | BATCH 40/71 | LOSS: 9.732925740891226e-06\n",
      "TRAIN: EPOCH 119/1000 | BATCH 41/71 | LOSS: 9.73920257473808e-06\n",
      "TRAIN: EPOCH 119/1000 | BATCH 42/71 | LOSS: 9.751861250342065e-06\n",
      "TRAIN: EPOCH 119/1000 | BATCH 43/71 | LOSS: 9.738422084575921e-06\n",
      "TRAIN: EPOCH 119/1000 | BATCH 44/71 | LOSS: 9.865863305296646e-06\n",
      "TRAIN: EPOCH 119/1000 | BATCH 45/71 | LOSS: 9.948441362686697e-06\n",
      "TRAIN: EPOCH 119/1000 | BATCH 46/71 | LOSS: 1.0026140327991403e-05\n",
      "TRAIN: EPOCH 119/1000 | BATCH 47/71 | LOSS: 1.0047586329164915e-05\n",
      "TRAIN: EPOCH 119/1000 | BATCH 48/71 | LOSS: 1.0052693160476486e-05\n",
      "TRAIN: EPOCH 119/1000 | BATCH 49/71 | LOSS: 1.0167777654714882e-05\n",
      "TRAIN: EPOCH 119/1000 | BATCH 50/71 | LOSS: 1.0160426623484704e-05\n",
      "TRAIN: EPOCH 119/1000 | BATCH 51/71 | LOSS: 1.0180786952892398e-05\n",
      "TRAIN: EPOCH 119/1000 | BATCH 52/71 | LOSS: 1.0189416965624573e-05\n",
      "TRAIN: EPOCH 119/1000 | BATCH 53/71 | LOSS: 1.0167394335709375e-05\n",
      "TRAIN: EPOCH 119/1000 | BATCH 54/71 | LOSS: 1.0198856249520429e-05\n",
      "TRAIN: EPOCH 119/1000 | BATCH 55/71 | LOSS: 1.0174600691113613e-05\n",
      "TRAIN: EPOCH 119/1000 | BATCH 56/71 | LOSS: 1.0156779257828786e-05\n",
      "TRAIN: EPOCH 119/1000 | BATCH 57/71 | LOSS: 1.0148303960610566e-05\n",
      "TRAIN: EPOCH 119/1000 | BATCH 58/71 | LOSS: 1.0145525821071252e-05\n",
      "TRAIN: EPOCH 119/1000 | BATCH 59/71 | LOSS: 1.0227036379243751e-05\n",
      "TRAIN: EPOCH 119/1000 | BATCH 60/71 | LOSS: 1.024554299486927e-05\n",
      "TRAIN: EPOCH 119/1000 | BATCH 61/71 | LOSS: 1.0282361956577065e-05\n",
      "TRAIN: EPOCH 119/1000 | BATCH 62/71 | LOSS: 1.0304252591306558e-05\n",
      "TRAIN: EPOCH 119/1000 | BATCH 63/71 | LOSS: 1.0299803093971605e-05\n",
      "TRAIN: EPOCH 119/1000 | BATCH 64/71 | LOSS: 1.0286520353894538e-05\n",
      "TRAIN: EPOCH 119/1000 | BATCH 65/71 | LOSS: 1.0280851683725786e-05\n",
      "TRAIN: EPOCH 119/1000 | BATCH 66/71 | LOSS: 1.025334146486703e-05\n",
      "TRAIN: EPOCH 119/1000 | BATCH 67/71 | LOSS: 1.0251088082548127e-05\n",
      "TRAIN: EPOCH 119/1000 | BATCH 68/71 | LOSS: 1.028613631047926e-05\n",
      "TRAIN: EPOCH 119/1000 | BATCH 69/71 | LOSS: 1.0293768874752069e-05\n",
      "TRAIN: EPOCH 119/1000 | BATCH 70/71 | LOSS: 1.0260055051718077e-05\n",
      "VAL: EPOCH 119/1000 | BATCH 0/8 | LOSS: 1.1370337233529426e-05\n",
      "VAL: EPOCH 119/1000 | BATCH 1/8 | LOSS: 1.1357711628079414e-05\n",
      "VAL: EPOCH 119/1000 | BATCH 2/8 | LOSS: 1.1708093249277832e-05\n",
      "VAL: EPOCH 119/1000 | BATCH 3/8 | LOSS: 1.109672530219541e-05\n",
      "VAL: EPOCH 119/1000 | BATCH 4/8 | LOSS: 1.104524708352983e-05\n",
      "VAL: EPOCH 119/1000 | BATCH 5/8 | LOSS: 1.088513772629085e-05\n",
      "VAL: EPOCH 119/1000 | BATCH 6/8 | LOSS: 1.079142735501851e-05\n",
      "VAL: EPOCH 119/1000 | BATCH 7/8 | LOSS: 1.0580829552964133e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 0/71 | LOSS: 1.1390327927074395e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 1/71 | LOSS: 1.091053081836435e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 2/71 | LOSS: 1.1676037199019143e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 3/71 | LOSS: 1.1008267392753623e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 4/71 | LOSS: 1.0672960343072191e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 5/71 | LOSS: 1.0703537858110698e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 6/71 | LOSS: 1.0806316952636865e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 7/71 | LOSS: 1.0670562915038317e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 8/71 | LOSS: 1.0636447566664881e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 9/71 | LOSS: 1.042665335262427e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 10/71 | LOSS: 1.0300838601489721e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 11/71 | LOSS: 1.0356738130212761e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 12/71 | LOSS: 1.0235052356550183e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 13/71 | LOSS: 1.041329174508324e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 14/71 | LOSS: 1.0336409650335554e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 15/71 | LOSS: 1.0231185058273695e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 16/71 | LOSS: 1.0339629625377711e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 17/71 | LOSS: 1.0286835580094097e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 18/71 | LOSS: 1.0363525019018102e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 19/71 | LOSS: 1.0409529613752965e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 20/71 | LOSS: 1.0468185662050797e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 21/71 | LOSS: 1.0493298346277284e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 22/71 | LOSS: 1.0668734988860745e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 23/71 | LOSS: 1.0684415921483984e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 24/71 | LOSS: 1.0764472681330518e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 25/71 | LOSS: 1.0840874613047792e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 26/71 | LOSS: 1.090169411731444e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 27/71 | LOSS: 1.0986179112088784e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 28/71 | LOSS: 1.0951637893319049e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 29/71 | LOSS: 1.1012254890374607e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 30/71 | LOSS: 1.0963274484487145e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 31/71 | LOSS: 1.101327083574688e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 32/71 | LOSS: 1.1088310367829928e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 33/71 | LOSS: 1.1036341300710132e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 34/71 | LOSS: 1.1048171005053779e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 35/71 | LOSS: 1.1080473617160655e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 36/71 | LOSS: 1.0992138389131444e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 37/71 | LOSS: 1.107168145608739e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 38/71 | LOSS: 1.112189506667887e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 39/71 | LOSS: 1.111583155761764e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 40/71 | LOSS: 1.107653286719763e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 41/71 | LOSS: 1.1035909925080237e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 42/71 | LOSS: 1.1116776963611956e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 43/71 | LOSS: 1.1081979168507546e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 44/71 | LOSS: 1.1086017669488987e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 45/71 | LOSS: 1.1098023131363215e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 46/71 | LOSS: 1.1096094664374486e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 47/71 | LOSS: 1.1036945807063603e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 48/71 | LOSS: 1.1050225464491724e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 49/71 | LOSS: 1.1001852617482655e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 50/71 | LOSS: 1.1007209044085661e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 51/71 | LOSS: 1.0993773003065144e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 52/71 | LOSS: 1.1012389222742058e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 53/71 | LOSS: 1.103223480135461e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 54/71 | LOSS: 1.1053625249504958e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 55/71 | LOSS: 1.1131131194425897e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 56/71 | LOSS: 1.1112833067671632e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 57/71 | LOSS: 1.107528595608089e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 58/71 | LOSS: 1.106989125450343e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 59/71 | LOSS: 1.101962587502688e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 60/71 | LOSS: 1.0983388716847345e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 61/71 | LOSS: 1.0971347218137159e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 62/71 | LOSS: 1.1017032077703219e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 63/71 | LOSS: 1.100669702225332e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 64/71 | LOSS: 1.096267972640747e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 65/71 | LOSS: 1.097959420803818e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 66/71 | LOSS: 1.0985949057924958e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 67/71 | LOSS: 1.100461099667882e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 68/71 | LOSS: 1.096774815158679e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 69/71 | LOSS: 1.0996462125539048e-05\n",
      "TRAIN: EPOCH 120/1000 | BATCH 70/71 | LOSS: 1.0943610050162533e-05\n",
      "VAL: EPOCH 120/1000 | BATCH 0/8 | LOSS: 1.147879083873704e-05\n",
      "VAL: EPOCH 120/1000 | BATCH 1/8 | LOSS: 1.1534754776221234e-05\n",
      "VAL: EPOCH 120/1000 | BATCH 2/8 | LOSS: 1.192091076518409e-05\n",
      "VAL: EPOCH 120/1000 | BATCH 3/8 | LOSS: 1.1092060503870016e-05\n",
      "VAL: EPOCH 120/1000 | BATCH 4/8 | LOSS: 1.1055422692152207e-05\n",
      "VAL: EPOCH 120/1000 | BATCH 5/8 | LOSS: 1.1026085455038507e-05\n",
      "VAL: EPOCH 120/1000 | BATCH 6/8 | LOSS: 1.0732043977311281e-05\n",
      "VAL: EPOCH 120/1000 | BATCH 7/8 | LOSS: 1.066018751316733e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 0/71 | LOSS: 9.685290933703072e-06\n",
      "TRAIN: EPOCH 121/1000 | BATCH 1/71 | LOSS: 1.0017638032877585e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 2/71 | LOSS: 1.0273039151798002e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 3/71 | LOSS: 1.1019144949386828e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 4/71 | LOSS: 1.1230700692976825e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 5/71 | LOSS: 1.1107964231390119e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 6/71 | LOSS: 1.1180364448851573e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 7/71 | LOSS: 1.131698957124172e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 8/71 | LOSS: 1.101028192351805e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 9/71 | LOSS: 1.1140197511849691e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 10/71 | LOSS: 1.085716419104508e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 11/71 | LOSS: 1.0658749033609638e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 12/71 | LOSS: 1.078537969498519e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 13/71 | LOSS: 1.0827867234703653e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 14/71 | LOSS: 1.0853549429157284e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 15/71 | LOSS: 1.0995970626481721e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 16/71 | LOSS: 1.095417032689404e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 17/71 | LOSS: 1.1066437309636967e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 18/71 | LOSS: 1.1209178266749644e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 19/71 | LOSS: 1.1455581125119352e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 20/71 | LOSS: 1.1519663617253259e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 21/71 | LOSS: 1.1377609553164802e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 22/71 | LOSS: 1.1376165512837874e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 23/71 | LOSS: 1.1396493770613839e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 24/71 | LOSS: 1.1357961448084097e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 25/71 | LOSS: 1.1283891546134748e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 26/71 | LOSS: 1.1246756541907046e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 27/71 | LOSS: 1.1244676865577016e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 28/71 | LOSS: 1.116663823832541e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 29/71 | LOSS: 1.1195093096224203e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 30/71 | LOSS: 1.1261633869341289e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 31/71 | LOSS: 1.1216475371611523e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 32/71 | LOSS: 1.1131031786851853e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 33/71 | LOSS: 1.1154296905314208e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 34/71 | LOSS: 1.1146423068047236e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 35/71 | LOSS: 1.1148643680927409e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 36/71 | LOSS: 1.1145862216024497e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 37/71 | LOSS: 1.1085152795873765e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 38/71 | LOSS: 1.0995129333679917e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 39/71 | LOSS: 1.10545887423541e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 40/71 | LOSS: 1.1046731332299248e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 41/71 | LOSS: 1.103845610963444e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 42/71 | LOSS: 1.1065076874771431e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 43/71 | LOSS: 1.1018464430311393e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 44/71 | LOSS: 1.1058363523059395e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 45/71 | LOSS: 1.1005224727657902e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 46/71 | LOSS: 1.1005634066950346e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 47/71 | LOSS: 1.1003525315800289e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 48/71 | LOSS: 1.0982092348689437e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 49/71 | LOSS: 1.1013097127943183e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 50/71 | LOSS: 1.095317001647131e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 51/71 | LOSS: 1.0879125284191105e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 52/71 | LOSS: 1.0852335298347137e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 53/71 | LOSS: 1.0839023111152022e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 54/71 | LOSS: 1.08155409841195e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 55/71 | LOSS: 1.0779272315630806e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 56/71 | LOSS: 1.0811661141225046e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 57/71 | LOSS: 1.076213311534957e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 58/71 | LOSS: 1.0835977263923269e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 59/71 | LOSS: 1.0800304653457716e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 60/71 | LOSS: 1.0802369198230377e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 61/71 | LOSS: 1.0881613962415177e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 62/71 | LOSS: 1.0909474021611949e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 63/71 | LOSS: 1.0922447913230826e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 64/71 | LOSS: 1.0941151497088587e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 65/71 | LOSS: 1.0926495950557044e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 66/71 | LOSS: 1.0933098347954028e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 67/71 | LOSS: 1.0911521779716542e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 68/71 | LOSS: 1.0889175652097776e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 69/71 | LOSS: 1.0868119742164189e-05\n",
      "TRAIN: EPOCH 121/1000 | BATCH 70/71 | LOSS: 1.0866412492145122e-05\n",
      "VAL: EPOCH 121/1000 | BATCH 0/8 | LOSS: 1.0455372830620036e-05\n",
      "VAL: EPOCH 121/1000 | BATCH 1/8 | LOSS: 1.0669851690181531e-05\n",
      "VAL: EPOCH 121/1000 | BATCH 2/8 | LOSS: 1.1004983510550423e-05\n",
      "VAL: EPOCH 121/1000 | BATCH 3/8 | LOSS: 1.0601199392112903e-05\n",
      "VAL: EPOCH 121/1000 | BATCH 4/8 | LOSS: 1.0348786236136221e-05\n",
      "VAL: EPOCH 121/1000 | BATCH 5/8 | LOSS: 1.0152011479173476e-05\n",
      "VAL: EPOCH 121/1000 | BATCH 6/8 | LOSS: 1.0058680865248399e-05\n",
      "VAL: EPOCH 121/1000 | BATCH 7/8 | LOSS: 9.77324577888794e-06\n",
      "TRAIN: EPOCH 122/1000 | BATCH 0/71 | LOSS: 1.1919928510906175e-05\n",
      "TRAIN: EPOCH 122/1000 | BATCH 1/71 | LOSS: 1.0411351468064822e-05\n",
      "TRAIN: EPOCH 122/1000 | BATCH 2/71 | LOSS: 9.569652926681252e-06\n",
      "TRAIN: EPOCH 122/1000 | BATCH 3/71 | LOSS: 9.452350241190288e-06\n",
      "TRAIN: EPOCH 122/1000 | BATCH 4/71 | LOSS: 9.759724707691931e-06\n",
      "TRAIN: EPOCH 122/1000 | BATCH 5/71 | LOSS: 9.562361507657139e-06\n",
      "TRAIN: EPOCH 122/1000 | BATCH 6/71 | LOSS: 9.737696278274857e-06\n",
      "TRAIN: EPOCH 122/1000 | BATCH 7/71 | LOSS: 9.930482292475062e-06\n",
      "TRAIN: EPOCH 122/1000 | BATCH 8/71 | LOSS: 9.855569967637874e-06\n",
      "TRAIN: EPOCH 122/1000 | BATCH 9/71 | LOSS: 9.517469152342528e-06\n",
      "TRAIN: EPOCH 122/1000 | BATCH 10/71 | LOSS: 9.435814056566663e-06\n",
      "TRAIN: EPOCH 122/1000 | BATCH 11/71 | LOSS: 9.71352483247756e-06\n",
      "TRAIN: EPOCH 122/1000 | BATCH 12/71 | LOSS: 9.823748209433128e-06\n",
      "TRAIN: EPOCH 122/1000 | BATCH 13/71 | LOSS: 9.810174820553844e-06\n",
      "TRAIN: EPOCH 122/1000 | BATCH 14/71 | LOSS: 9.771264315835044e-06\n",
      "TRAIN: EPOCH 122/1000 | BATCH 15/71 | LOSS: 9.750650121986837e-06\n",
      "TRAIN: EPOCH 122/1000 | BATCH 16/71 | LOSS: 9.942048456971807e-06\n",
      "TRAIN: EPOCH 122/1000 | BATCH 17/71 | LOSS: 9.828916972765e-06\n",
      "TRAIN: EPOCH 122/1000 | BATCH 18/71 | LOSS: 9.949182879752602e-06\n",
      "TRAIN: EPOCH 122/1000 | BATCH 19/71 | LOSS: 9.90924850157171e-06\n",
      "TRAIN: EPOCH 122/1000 | BATCH 20/71 | LOSS: 9.813543069217953e-06\n",
      "TRAIN: EPOCH 122/1000 | BATCH 21/71 | LOSS: 9.814475519802231e-06\n",
      "TRAIN: EPOCH 122/1000 | BATCH 22/71 | LOSS: 9.757747885368703e-06\n",
      "TRAIN: EPOCH 122/1000 | BATCH 23/71 | LOSS: 9.689517545060275e-06\n",
      "TRAIN: EPOCH 122/1000 | BATCH 24/71 | LOSS: 9.681783049018121e-06\n",
      "TRAIN: EPOCH 122/1000 | BATCH 25/71 | LOSS: 9.725514351325379e-06\n",
      "TRAIN: EPOCH 122/1000 | BATCH 26/71 | LOSS: 9.98083915301659e-06\n",
      "TRAIN: EPOCH 122/1000 | BATCH 27/71 | LOSS: 9.957613298995835e-06\n",
      "TRAIN: EPOCH 122/1000 | BATCH 28/71 | LOSS: 1.0009472511040352e-05\n",
      "TRAIN: EPOCH 122/1000 | BATCH 29/71 | LOSS: 9.971031704480993e-06\n",
      "TRAIN: EPOCH 122/1000 | BATCH 30/71 | LOSS: 9.987189925243822e-06\n",
      "TRAIN: EPOCH 122/1000 | BATCH 31/71 | LOSS: 9.93988518871447e-06\n",
      "TRAIN: EPOCH 122/1000 | BATCH 32/71 | LOSS: 9.93452693254574e-06\n",
      "TRAIN: EPOCH 122/1000 | BATCH 33/71 | LOSS: 9.977786944278597e-06\n",
      "TRAIN: EPOCH 122/1000 | BATCH 34/71 | LOSS: 9.949131188997334e-06\n",
      "TRAIN: EPOCH 122/1000 | BATCH 35/71 | LOSS: 9.976252714396753e-06\n",
      "TRAIN: EPOCH 122/1000 | BATCH 36/71 | LOSS: 9.976566001864125e-06\n",
      "TRAIN: EPOCH 122/1000 | BATCH 37/71 | LOSS: 9.947330023638451e-06\n",
      "TRAIN: EPOCH 122/1000 | BATCH 38/71 | LOSS: 1.0026806823868985e-05\n",
      "TRAIN: EPOCH 122/1000 | BATCH 39/71 | LOSS: 1.0026143604591197e-05\n",
      "TRAIN: EPOCH 122/1000 | BATCH 40/71 | LOSS: 1.0050962839336645e-05\n",
      "TRAIN: EPOCH 122/1000 | BATCH 41/71 | LOSS: 1.0063711085552183e-05\n",
      "TRAIN: EPOCH 122/1000 | BATCH 42/71 | LOSS: 1.0068344022858033e-05\n",
      "TRAIN: EPOCH 122/1000 | BATCH 43/71 | LOSS: 1.0101886532125222e-05\n",
      "TRAIN: EPOCH 122/1000 | BATCH 44/71 | LOSS: 1.0054599968296113e-05\n",
      "TRAIN: EPOCH 122/1000 | BATCH 45/71 | LOSS: 1.0027170051414387e-05\n",
      "TRAIN: EPOCH 122/1000 | BATCH 46/71 | LOSS: 1.0112818907803926e-05\n",
      "TRAIN: EPOCH 122/1000 | BATCH 47/71 | LOSS: 1.0089546454613204e-05\n",
      "TRAIN: EPOCH 122/1000 | BATCH 48/71 | LOSS: 1.0088042992200436e-05\n",
      "TRAIN: EPOCH 122/1000 | BATCH 49/71 | LOSS: 1.007076742098434e-05\n",
      "TRAIN: EPOCH 122/1000 | BATCH 50/71 | LOSS: 1.00846347563407e-05\n",
      "TRAIN: EPOCH 122/1000 | BATCH 51/71 | LOSS: 1.0112202289747074e-05\n",
      "TRAIN: EPOCH 122/1000 | BATCH 52/71 | LOSS: 1.0085851225188168e-05\n",
      "TRAIN: EPOCH 122/1000 | BATCH 53/71 | LOSS: 1.0057108890274927e-05\n",
      "TRAIN: EPOCH 122/1000 | BATCH 54/71 | LOSS: 1.0048688124119176e-05\n",
      "TRAIN: EPOCH 122/1000 | BATCH 55/71 | LOSS: 1.001322043846033e-05\n",
      "TRAIN: EPOCH 122/1000 | BATCH 56/71 | LOSS: 9.963780844133763e-06\n",
      "TRAIN: EPOCH 122/1000 | BATCH 57/71 | LOSS: 9.9729095225108e-06\n",
      "TRAIN: EPOCH 122/1000 | BATCH 58/71 | LOSS: 9.957507954514782e-06\n",
      "TRAIN: EPOCH 122/1000 | BATCH 59/71 | LOSS: 9.998461185508253e-06\n",
      "TRAIN: EPOCH 122/1000 | BATCH 60/71 | LOSS: 9.965251920817298e-06\n",
      "TRAIN: EPOCH 122/1000 | BATCH 61/71 | LOSS: 9.981963235223966e-06\n",
      "TRAIN: EPOCH 122/1000 | BATCH 62/71 | LOSS: 9.974304674191415e-06\n",
      "TRAIN: EPOCH 122/1000 | BATCH 63/71 | LOSS: 9.981020042459932e-06\n",
      "TRAIN: EPOCH 122/1000 | BATCH 64/71 | LOSS: 9.995260048666611e-06\n",
      "TRAIN: EPOCH 122/1000 | BATCH 65/71 | LOSS: 1.0008057457029777e-05\n",
      "TRAIN: EPOCH 122/1000 | BATCH 66/71 | LOSS: 1.0023124261025979e-05\n",
      "TRAIN: EPOCH 122/1000 | BATCH 67/71 | LOSS: 1.0030282496967737e-05\n",
      "TRAIN: EPOCH 122/1000 | BATCH 68/71 | LOSS: 1.0030009869534178e-05\n",
      "TRAIN: EPOCH 122/1000 | BATCH 69/71 | LOSS: 1.0043282340380496e-05\n",
      "TRAIN: EPOCH 122/1000 | BATCH 70/71 | LOSS: 1.0033822484690742e-05\n",
      "VAL: EPOCH 122/1000 | BATCH 0/8 | LOSS: 8.981310202216264e-06\n",
      "VAL: EPOCH 122/1000 | BATCH 1/8 | LOSS: 9.376138223160524e-06\n",
      "VAL: EPOCH 122/1000 | BATCH 2/8 | LOSS: 9.939751180354506e-06\n",
      "VAL: EPOCH 122/1000 | BATCH 3/8 | LOSS: 9.412744475412183e-06\n",
      "VAL: EPOCH 122/1000 | BATCH 4/8 | LOSS: 9.388107355334795e-06\n",
      "VAL: EPOCH 122/1000 | BATCH 5/8 | LOSS: 9.297680965876983e-06\n",
      "VAL: EPOCH 122/1000 | BATCH 6/8 | LOSS: 9.06244947275679e-06\n",
      "VAL: EPOCH 122/1000 | BATCH 7/8 | LOSS: 8.9992516905113e-06\n",
      "TRAIN: EPOCH 123/1000 | BATCH 0/71 | LOSS: 9.209262316289824e-06\n",
      "TRAIN: EPOCH 123/1000 | BATCH 1/71 | LOSS: 9.981790299207205e-06\n",
      "TRAIN: EPOCH 123/1000 | BATCH 2/71 | LOSS: 9.492971912550274e-06\n",
      "TRAIN: EPOCH 123/1000 | BATCH 3/71 | LOSS: 9.185000180877978e-06\n",
      "TRAIN: EPOCH 123/1000 | BATCH 4/71 | LOSS: 9.392421634402127e-06\n",
      "TRAIN: EPOCH 123/1000 | BATCH 5/71 | LOSS: 9.181254730113627e-06\n",
      "TRAIN: EPOCH 123/1000 | BATCH 6/71 | LOSS: 9.582986811957589e-06\n",
      "TRAIN: EPOCH 123/1000 | BATCH 7/71 | LOSS: 9.983399422708317e-06\n",
      "TRAIN: EPOCH 123/1000 | BATCH 8/71 | LOSS: 9.937122134336581e-06\n",
      "TRAIN: EPOCH 123/1000 | BATCH 9/71 | LOSS: 9.977010176953626e-06\n",
      "TRAIN: EPOCH 123/1000 | BATCH 10/71 | LOSS: 1.035639293563277e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 11/71 | LOSS: 1.06800320433346e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 12/71 | LOSS: 1.0685477368847038e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 13/71 | LOSS: 1.0871192963739823e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 14/71 | LOSS: 1.1179144530615304e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 15/71 | LOSS: 1.118478581929594e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 16/71 | LOSS: 1.1316706524453098e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 17/71 | LOSS: 1.1588150149085171e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 18/71 | LOSS: 1.15622520560895e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 19/71 | LOSS: 1.1695896955643547e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 20/71 | LOSS: 1.1719320051876517e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 21/71 | LOSS: 1.1783354239014443e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 22/71 | LOSS: 1.162989061096754e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 23/71 | LOSS: 1.1725844728971424e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 24/71 | LOSS: 1.1723295210686047e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 25/71 | LOSS: 1.1680174373815624e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 26/71 | LOSS: 1.16368908291204e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 27/71 | LOSS: 1.1725452265766631e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 28/71 | LOSS: 1.1638980799159917e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 29/71 | LOSS: 1.1587196089143012e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 30/71 | LOSS: 1.1477076030484656e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 31/71 | LOSS: 1.1426690718963073e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 32/71 | LOSS: 1.1465046499682957e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 33/71 | LOSS: 1.1395198003703948e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 34/71 | LOSS: 1.145791406348248e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 35/71 | LOSS: 1.1433149539799262e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 36/71 | LOSS: 1.1348829257012522e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 37/71 | LOSS: 1.1251290025508129e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 38/71 | LOSS: 1.1238862731634305e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 39/71 | LOSS: 1.1206754265913332e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 40/71 | LOSS: 1.1169168163444449e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 41/71 | LOSS: 1.1149974051520639e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 42/71 | LOSS: 1.1122789844595774e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 43/71 | LOSS: 1.1140893196749278e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 44/71 | LOSS: 1.107738781785075e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 45/71 | LOSS: 1.1059218610645747e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 46/71 | LOSS: 1.1029525167225771e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 47/71 | LOSS: 1.1022424075690651e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 48/71 | LOSS: 1.0999135453430984e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 49/71 | LOSS: 1.0933348148682853e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 50/71 | LOSS: 1.087681140037888e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 51/71 | LOSS: 1.0850202897927375e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 52/71 | LOSS: 1.0878265575744115e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 53/71 | LOSS: 1.0934658668567729e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 54/71 | LOSS: 1.0906062032683456e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 55/71 | LOSS: 1.0888956840712386e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 56/71 | LOSS: 1.0933583102438174e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 57/71 | LOSS: 1.0892865545962384e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 58/71 | LOSS: 1.0851435775802494e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 59/71 | LOSS: 1.0805777355926693e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 60/71 | LOSS: 1.0763873895663638e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 61/71 | LOSS: 1.0765129152259174e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 62/71 | LOSS: 1.0761904276377107e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 63/71 | LOSS: 1.076983392067632e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 64/71 | LOSS: 1.0738661834553708e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 65/71 | LOSS: 1.0772138631926447e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 66/71 | LOSS: 1.0751585440589428e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 67/71 | LOSS: 1.0734543633696054e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 68/71 | LOSS: 1.0740092967761734e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 69/71 | LOSS: 1.0739251774793956e-05\n",
      "TRAIN: EPOCH 123/1000 | BATCH 70/71 | LOSS: 1.0740241738000203e-05\n",
      "VAL: EPOCH 123/1000 | BATCH 0/8 | LOSS: 9.087185389944352e-06\n",
      "VAL: EPOCH 123/1000 | BATCH 1/8 | LOSS: 9.6220787781931e-06\n",
      "VAL: EPOCH 123/1000 | BATCH 2/8 | LOSS: 1.0212450433755293e-05\n",
      "VAL: EPOCH 123/1000 | BATCH 3/8 | LOSS: 9.91124625215889e-06\n",
      "VAL: EPOCH 123/1000 | BATCH 4/8 | LOSS: 9.840086568146944e-06\n",
      "VAL: EPOCH 123/1000 | BATCH 5/8 | LOSS: 9.639007809407e-06\n",
      "VAL: EPOCH 123/1000 | BATCH 6/8 | LOSS: 9.367918989612787e-06\n",
      "VAL: EPOCH 123/1000 | BATCH 7/8 | LOSS: 9.18852936138137e-06\n",
      "TRAIN: EPOCH 124/1000 | BATCH 0/71 | LOSS: 9.448363925912417e-06\n",
      "TRAIN: EPOCH 124/1000 | BATCH 1/71 | LOSS: 9.709222467790823e-06\n",
      "TRAIN: EPOCH 124/1000 | BATCH 2/71 | LOSS: 8.553009441432854e-06\n",
      "TRAIN: EPOCH 124/1000 | BATCH 3/71 | LOSS: 9.244143257092219e-06\n",
      "TRAIN: EPOCH 124/1000 | BATCH 4/71 | LOSS: 9.604445949662477e-06\n",
      "TRAIN: EPOCH 124/1000 | BATCH 5/71 | LOSS: 9.977162638582135e-06\n",
      "TRAIN: EPOCH 124/1000 | BATCH 6/71 | LOSS: 1.0510087252311808e-05\n",
      "TRAIN: EPOCH 124/1000 | BATCH 7/71 | LOSS: 1.0256896189275722e-05\n",
      "TRAIN: EPOCH 124/1000 | BATCH 8/71 | LOSS: 1.038715415941422e-05\n",
      "TRAIN: EPOCH 124/1000 | BATCH 9/71 | LOSS: 1.050087203111616e-05\n",
      "TRAIN: EPOCH 124/1000 | BATCH 10/71 | LOSS: 1.0206827400080245e-05\n",
      "TRAIN: EPOCH 124/1000 | BATCH 11/71 | LOSS: 1.0241653361238908e-05\n",
      "TRAIN: EPOCH 124/1000 | BATCH 12/71 | LOSS: 1.0298069670310584e-05\n",
      "TRAIN: EPOCH 124/1000 | BATCH 13/71 | LOSS: 1.0296633360797256e-05\n",
      "TRAIN: EPOCH 124/1000 | BATCH 14/71 | LOSS: 1.0155398285860429e-05\n",
      "TRAIN: EPOCH 124/1000 | BATCH 15/71 | LOSS: 1.0136172903685292e-05\n",
      "TRAIN: EPOCH 124/1000 | BATCH 16/71 | LOSS: 1.017869344115538e-05\n",
      "TRAIN: EPOCH 124/1000 | BATCH 17/71 | LOSS: 1.0042141209649142e-05\n",
      "TRAIN: EPOCH 124/1000 | BATCH 18/71 | LOSS: 1.0014627796887285e-05\n",
      "TRAIN: EPOCH 124/1000 | BATCH 19/71 | LOSS: 9.860043428489007e-06\n",
      "TRAIN: EPOCH 124/1000 | BATCH 20/71 | LOSS: 9.763270836195997e-06\n",
      "TRAIN: EPOCH 124/1000 | BATCH 21/71 | LOSS: 9.772854065720987e-06\n",
      "TRAIN: EPOCH 124/1000 | BATCH 22/71 | LOSS: 9.71174769960192e-06\n",
      "TRAIN: EPOCH 124/1000 | BATCH 23/71 | LOSS: 9.789659126605935e-06\n",
      "TRAIN: EPOCH 124/1000 | BATCH 24/71 | LOSS: 9.778613093658351e-06\n",
      "TRAIN: EPOCH 124/1000 | BATCH 25/71 | LOSS: 9.806681786218318e-06\n",
      "TRAIN: EPOCH 124/1000 | BATCH 26/71 | LOSS: 9.875239137963271e-06\n",
      "TRAIN: EPOCH 124/1000 | BATCH 27/71 | LOSS: 9.871329504156684e-06\n",
      "TRAIN: EPOCH 124/1000 | BATCH 28/71 | LOSS: 9.929022655404848e-06\n",
      "TRAIN: EPOCH 124/1000 | BATCH 29/71 | LOSS: 9.925937289760138e-06\n",
      "TRAIN: EPOCH 124/1000 | BATCH 30/71 | LOSS: 9.915255583520423e-06\n",
      "TRAIN: EPOCH 124/1000 | BATCH 31/71 | LOSS: 9.962689631493049e-06\n",
      "TRAIN: EPOCH 124/1000 | BATCH 32/71 | LOSS: 9.972003874391897e-06\n",
      "TRAIN: EPOCH 124/1000 | BATCH 33/71 | LOSS: 9.983583662260036e-06\n",
      "TRAIN: EPOCH 124/1000 | BATCH 34/71 | LOSS: 9.990899629234004e-06\n",
      "TRAIN: EPOCH 124/1000 | BATCH 35/71 | LOSS: 1.00947356713732e-05\n",
      "TRAIN: EPOCH 124/1000 | BATCH 36/71 | LOSS: 1.0153435952717601e-05\n",
      "TRAIN: EPOCH 124/1000 | BATCH 37/71 | LOSS: 1.0161268643347093e-05\n",
      "TRAIN: EPOCH 124/1000 | BATCH 38/71 | LOSS: 1.0217310400315537e-05\n",
      "TRAIN: EPOCH 124/1000 | BATCH 39/71 | LOSS: 1.0186998338213016e-05\n",
      "TRAIN: EPOCH 124/1000 | BATCH 40/71 | LOSS: 1.0183678103025958e-05\n",
      "TRAIN: EPOCH 124/1000 | BATCH 41/71 | LOSS: 1.0166610298342893e-05\n",
      "TRAIN: EPOCH 124/1000 | BATCH 42/71 | LOSS: 1.0154920536754002e-05\n",
      "TRAIN: EPOCH 124/1000 | BATCH 43/71 | LOSS: 1.0115608099361204e-05\n",
      "TRAIN: EPOCH 124/1000 | BATCH 44/71 | LOSS: 1.0129865262974313e-05\n",
      "TRAIN: EPOCH 124/1000 | BATCH 45/71 | LOSS: 1.014846028461445e-05\n",
      "TRAIN: EPOCH 124/1000 | BATCH 46/71 | LOSS: 1.0093510702953358e-05\n",
      "TRAIN: EPOCH 124/1000 | BATCH 47/71 | LOSS: 1.0088672316517963e-05\n",
      "TRAIN: EPOCH 124/1000 | BATCH 48/71 | LOSS: 1.0061834157623911e-05\n",
      "TRAIN: EPOCH 124/1000 | BATCH 49/71 | LOSS: 1.0038031668955227e-05\n",
      "TRAIN: EPOCH 124/1000 | BATCH 50/71 | LOSS: 9.99786714280216e-06\n",
      "TRAIN: EPOCH 124/1000 | BATCH 51/71 | LOSS: 9.961291687851861e-06\n",
      "TRAIN: EPOCH 124/1000 | BATCH 52/71 | LOSS: 9.929654811029016e-06\n",
      "TRAIN: EPOCH 124/1000 | BATCH 53/71 | LOSS: 9.899832076809145e-06\n",
      "TRAIN: EPOCH 124/1000 | BATCH 54/71 | LOSS: 9.84940439097541e-06\n",
      "TRAIN: EPOCH 124/1000 | BATCH 55/71 | LOSS: 9.822591258463425e-06\n",
      "TRAIN: EPOCH 124/1000 | BATCH 56/71 | LOSS: 9.82644323493444e-06\n",
      "TRAIN: EPOCH 124/1000 | BATCH 57/71 | LOSS: 9.802924398235487e-06\n",
      "TRAIN: EPOCH 124/1000 | BATCH 58/71 | LOSS: 9.840297888470143e-06\n",
      "TRAIN: EPOCH 124/1000 | BATCH 59/71 | LOSS: 9.876546088586717e-06\n",
      "TRAIN: EPOCH 124/1000 | BATCH 60/71 | LOSS: 9.874828198216627e-06\n",
      "TRAIN: EPOCH 124/1000 | BATCH 61/71 | LOSS: 9.909538851887543e-06\n",
      "TRAIN: EPOCH 124/1000 | BATCH 62/71 | LOSS: 9.888672586442674e-06\n",
      "TRAIN: EPOCH 124/1000 | BATCH 63/71 | LOSS: 9.949513568585644e-06\n",
      "TRAIN: EPOCH 124/1000 | BATCH 64/71 | LOSS: 9.998690578728342e-06\n",
      "TRAIN: EPOCH 124/1000 | BATCH 65/71 | LOSS: 1.0045331719359666e-05\n",
      "TRAIN: EPOCH 124/1000 | BATCH 66/71 | LOSS: 1.0026182145107913e-05\n",
      "TRAIN: EPOCH 124/1000 | BATCH 67/71 | LOSS: 1.0030297583879848e-05\n",
      "TRAIN: EPOCH 124/1000 | BATCH 68/71 | LOSS: 1.0034334095046464e-05\n",
      "TRAIN: EPOCH 124/1000 | BATCH 69/71 | LOSS: 1.004762516458868e-05\n",
      "TRAIN: EPOCH 124/1000 | BATCH 70/71 | LOSS: 1.0059098789958664e-05\n",
      "VAL: EPOCH 124/1000 | BATCH 0/8 | LOSS: 1.123977835959522e-05\n",
      "VAL: EPOCH 124/1000 | BATCH 1/8 | LOSS: 1.1218335657758871e-05\n",
      "VAL: EPOCH 124/1000 | BATCH 2/8 | LOSS: 1.206471764210922e-05\n",
      "VAL: EPOCH 124/1000 | BATCH 3/8 | LOSS: 1.1433315648901043e-05\n",
      "VAL: EPOCH 124/1000 | BATCH 4/8 | LOSS: 1.1773892401834018e-05\n",
      "VAL: EPOCH 124/1000 | BATCH 5/8 | LOSS: 1.1749142686312553e-05\n",
      "VAL: EPOCH 124/1000 | BATCH 6/8 | LOSS: 1.1197197376791987e-05\n",
      "VAL: EPOCH 124/1000 | BATCH 7/8 | LOSS: 1.1331857763252629e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 0/71 | LOSS: 1.2168209650553763e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 1/71 | LOSS: 1.2753726878145244e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 2/71 | LOSS: 1.2385610110262254e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 3/71 | LOSS: 1.1407305919419741e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 4/71 | LOSS: 1.1818058555945755e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 5/71 | LOSS: 1.2225707602434946e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 6/71 | LOSS: 1.2040599098587076e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 7/71 | LOSS: 1.1931574590562377e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 8/71 | LOSS: 1.1653110656576851e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 9/71 | LOSS: 1.1488258041936206e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 10/71 | LOSS: 1.1247352548790248e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 11/71 | LOSS: 1.1202553802528806e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 12/71 | LOSS: 1.1024491029204979e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 13/71 | LOSS: 1.1029849896918417e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 14/71 | LOSS: 1.0790921472410749e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 15/71 | LOSS: 1.0567176019549152e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 16/71 | LOSS: 1.0645737641907009e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 17/71 | LOSS: 1.0428109413219823e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 18/71 | LOSS: 1.0405402511088668e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 19/71 | LOSS: 1.0265197579428786e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 20/71 | LOSS: 1.01653172567326e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 21/71 | LOSS: 1.0209073603949616e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 22/71 | LOSS: 1.031325155054219e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 23/71 | LOSS: 1.0356313813038772e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 24/71 | LOSS: 1.035563116602134e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 25/71 | LOSS: 1.0277813019526478e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 26/71 | LOSS: 1.0175873588498993e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 27/71 | LOSS: 1.0318645893546221e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 28/71 | LOSS: 1.033797882977022e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 29/71 | LOSS: 1.0375068086432293e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 30/71 | LOSS: 1.0351519097551524e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 31/71 | LOSS: 1.0299332302565745e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 32/71 | LOSS: 1.0296297466883322e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 33/71 | LOSS: 1.0207879179634961e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 34/71 | LOSS: 1.0201106566195709e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 35/71 | LOSS: 1.029275906855926e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 36/71 | LOSS: 1.0256046963961027e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 37/71 | LOSS: 1.0222896315482152e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 38/71 | LOSS: 1.0242509979900951e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 39/71 | LOSS: 1.0226652238998214e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 40/71 | LOSS: 1.0206053580410166e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 41/71 | LOSS: 1.0171203333205389e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 42/71 | LOSS: 1.0133012374565222e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 43/71 | LOSS: 1.014592122704595e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 44/71 | LOSS: 1.0099543699955877e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 45/71 | LOSS: 1.0105755573813044e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 46/71 | LOSS: 1.0112259974932308e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 47/71 | LOSS: 1.0050608362159133e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 48/71 | LOSS: 1.0075135953823575e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 49/71 | LOSS: 1.0020742165579576e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 50/71 | LOSS: 1.0051589202596327e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 51/71 | LOSS: 1.0051484939420054e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 52/71 | LOSS: 1.0005635637221616e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 53/71 | LOSS: 1.0009809211960938e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 54/71 | LOSS: 1.0056890832030065e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 55/71 | LOSS: 1.0091993861546403e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 56/71 | LOSS: 1.0109260864211186e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 57/71 | LOSS: 1.0109042023032089e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 58/71 | LOSS: 1.0126189647700688e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 59/71 | LOSS: 1.0102475842662291e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 60/71 | LOSS: 1.0120267636578274e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 61/71 | LOSS: 1.0058543855320163e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 62/71 | LOSS: 1.0030200767701784e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 63/71 | LOSS: 1.0040186928961248e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 64/71 | LOSS: 1.00296011483946e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 65/71 | LOSS: 1.0003475209128792e-05\n",
      "TRAIN: EPOCH 125/1000 | BATCH 66/71 | LOSS: 9.993741724741343e-06\n",
      "TRAIN: EPOCH 125/1000 | BATCH 67/71 | LOSS: 9.963700683650077e-06\n",
      "TRAIN: EPOCH 125/1000 | BATCH 68/71 | LOSS: 9.937844042460842e-06\n",
      "TRAIN: EPOCH 125/1000 | BATCH 69/71 | LOSS: 9.935801394281692e-06\n",
      "TRAIN: EPOCH 125/1000 | BATCH 70/71 | LOSS: 9.915407433805407e-06\n",
      "VAL: EPOCH 125/1000 | BATCH 0/8 | LOSS: 1.1303503924864344e-05\n",
      "VAL: EPOCH 125/1000 | BATCH 1/8 | LOSS: 1.2093605164409382e-05\n",
      "VAL: EPOCH 125/1000 | BATCH 2/8 | LOSS: 1.3379795746004675e-05\n",
      "VAL: EPOCH 125/1000 | BATCH 3/8 | LOSS: 1.365075263493054e-05\n",
      "VAL: EPOCH 125/1000 | BATCH 4/8 | LOSS: 1.3546839363698382e-05\n",
      "VAL: EPOCH 125/1000 | BATCH 5/8 | LOSS: 1.34499594726852e-05\n",
      "VAL: EPOCH 125/1000 | BATCH 6/8 | LOSS: 1.3128871614753734e-05\n",
      "VAL: EPOCH 125/1000 | BATCH 7/8 | LOSS: 1.2967883662895474e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 0/71 | LOSS: 1.37340794026386e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 1/71 | LOSS: 1.2754691852023825e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 2/71 | LOSS: 1.1937299556545137e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 3/71 | LOSS: 1.2225217460581916e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 4/71 | LOSS: 1.1526210619194898e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 5/71 | LOSS: 1.214884787259507e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 6/71 | LOSS: 1.1652761453920643e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 7/71 | LOSS: 1.1338390663695463e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 8/71 | LOSS: 1.2030225055544482e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 9/71 | LOSS: 1.1876640928676353e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 10/71 | LOSS: 1.200935168361122e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 11/71 | LOSS: 1.2157354603914428e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 12/71 | LOSS: 1.1870517622670517e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 13/71 | LOSS: 1.1950460183080785e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 14/71 | LOSS: 1.1799562950424539e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 15/71 | LOSS: 1.173826007061507e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 16/71 | LOSS: 1.1720452622445437e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 17/71 | LOSS: 1.1615484810237023e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 18/71 | LOSS: 1.1743916442547312e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 19/71 | LOSS: 1.1662495262498851e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 20/71 | LOSS: 1.1674716222928727e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 21/71 | LOSS: 1.1780150089180097e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 22/71 | LOSS: 1.1662920571878836e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 23/71 | LOSS: 1.1873832211980092e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 24/71 | LOSS: 1.1837935744551942e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 25/71 | LOSS: 1.1881551857256385e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 26/71 | LOSS: 1.1862602150358725e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 27/71 | LOSS: 1.1737030847923182e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 28/71 | LOSS: 1.170877586164038e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 29/71 | LOSS: 1.1818729914618113e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 30/71 | LOSS: 1.1697202969498317e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 31/71 | LOSS: 1.173334612758481e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 32/71 | LOSS: 1.1780325787020212e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 33/71 | LOSS: 1.1748412923813757e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 34/71 | LOSS: 1.1754890017203121e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 35/71 | LOSS: 1.1786675688401576e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 36/71 | LOSS: 1.1780876259247117e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 37/71 | LOSS: 1.1833110779422751e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 38/71 | LOSS: 1.1781010564449482e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 39/71 | LOSS: 1.1787425978582178e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 40/71 | LOSS: 1.1723745312959115e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 41/71 | LOSS: 1.1644725894993393e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 42/71 | LOSS: 1.161842450274826e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 43/71 | LOSS: 1.1539007718139063e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 44/71 | LOSS: 1.1446943679200357e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 45/71 | LOSS: 1.149835299221867e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 46/71 | LOSS: 1.1530455565739527e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 47/71 | LOSS: 1.1535078774234838e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 48/71 | LOSS: 1.1569993102038636e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 49/71 | LOSS: 1.1480752764327917e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 50/71 | LOSS: 1.1515096190908248e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 51/71 | LOSS: 1.1472838370816764e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 52/71 | LOSS: 1.1395832635176737e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 53/71 | LOSS: 1.1338853137540692e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 54/71 | LOSS: 1.1301083527955185e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 55/71 | LOSS: 1.1282038420696544e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 56/71 | LOSS: 1.1309193053564645e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 57/71 | LOSS: 1.1245565552787317e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 58/71 | LOSS: 1.1253511328797716e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 59/71 | LOSS: 1.1235236600744732e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 60/71 | LOSS: 1.1182879387008859e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 61/71 | LOSS: 1.118839120482012e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 62/71 | LOSS: 1.1153427669977111e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 63/71 | LOSS: 1.1160576306679104e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 64/71 | LOSS: 1.1144248366270823e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 65/71 | LOSS: 1.1088851069320244e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 66/71 | LOSS: 1.1129672059981733e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 67/71 | LOSS: 1.1094458781211442e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 68/71 | LOSS: 1.1100827505551802e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 69/71 | LOSS: 1.107320325510435e-05\n",
      "TRAIN: EPOCH 126/1000 | BATCH 70/71 | LOSS: 1.1102397795030798e-05\n",
      "VAL: EPOCH 126/1000 | BATCH 0/8 | LOSS: 1.0058615771413315e-05\n",
      "VAL: EPOCH 126/1000 | BATCH 1/8 | LOSS: 1.0629035841702716e-05\n",
      "VAL: EPOCH 126/1000 | BATCH 2/8 | LOSS: 1.0681591144627115e-05\n",
      "VAL: EPOCH 126/1000 | BATCH 3/8 | LOSS: 1.000751649371523e-05\n",
      "VAL: EPOCH 126/1000 | BATCH 4/8 | LOSS: 1.0144754742213991e-05\n",
      "VAL: EPOCH 126/1000 | BATCH 5/8 | LOSS: 1.0030410976469284e-05\n",
      "VAL: EPOCH 126/1000 | BATCH 6/8 | LOSS: 9.851114035492564e-06\n",
      "VAL: EPOCH 126/1000 | BATCH 7/8 | LOSS: 9.743004738993477e-06\n",
      "TRAIN: EPOCH 127/1000 | BATCH 0/71 | LOSS: 1.3124994438840076e-05\n",
      "TRAIN: EPOCH 127/1000 | BATCH 1/71 | LOSS: 1.1379980605852325e-05\n",
      "TRAIN: EPOCH 127/1000 | BATCH 2/71 | LOSS: 1.073722766401867e-05\n",
      "TRAIN: EPOCH 127/1000 | BATCH 3/71 | LOSS: 9.990251328417799e-06\n",
      "TRAIN: EPOCH 127/1000 | BATCH 4/71 | LOSS: 9.998945643019397e-06\n",
      "TRAIN: EPOCH 127/1000 | BATCH 5/71 | LOSS: 9.932737611961784e-06\n",
      "TRAIN: EPOCH 127/1000 | BATCH 6/71 | LOSS: 9.713705237248047e-06\n",
      "TRAIN: EPOCH 127/1000 | BATCH 7/71 | LOSS: 9.923654943122528e-06\n",
      "TRAIN: EPOCH 127/1000 | BATCH 8/71 | LOSS: 9.96224753180286e-06\n",
      "TRAIN: EPOCH 127/1000 | BATCH 9/71 | LOSS: 9.996712378779193e-06\n",
      "TRAIN: EPOCH 127/1000 | BATCH 10/71 | LOSS: 1.022215928969672e-05\n",
      "TRAIN: EPOCH 127/1000 | BATCH 11/71 | LOSS: 1.0298908440139106e-05\n",
      "TRAIN: EPOCH 127/1000 | BATCH 12/71 | LOSS: 1.0122129494378056e-05\n",
      "TRAIN: EPOCH 127/1000 | BATCH 13/71 | LOSS: 1.0110090832833002e-05\n",
      "TRAIN: EPOCH 127/1000 | BATCH 14/71 | LOSS: 1.0105471786422033e-05\n",
      "TRAIN: EPOCH 127/1000 | BATCH 15/71 | LOSS: 1.002893020540796e-05\n",
      "TRAIN: EPOCH 127/1000 | BATCH 16/71 | LOSS: 9.878010400023092e-06\n",
      "TRAIN: EPOCH 127/1000 | BATCH 17/71 | LOSS: 9.789288368564383e-06\n",
      "TRAIN: EPOCH 127/1000 | BATCH 18/71 | LOSS: 9.760942702996545e-06\n",
      "TRAIN: EPOCH 127/1000 | BATCH 19/71 | LOSS: 9.659030320108286e-06\n",
      "TRAIN: EPOCH 127/1000 | BATCH 20/71 | LOSS: 9.71453712173527e-06\n",
      "TRAIN: EPOCH 127/1000 | BATCH 21/71 | LOSS: 9.683886673883535e-06\n",
      "TRAIN: EPOCH 127/1000 | BATCH 22/71 | LOSS: 9.647638881174119e-06\n",
      "TRAIN: EPOCH 127/1000 | BATCH 23/71 | LOSS: 9.600019780009461e-06\n",
      "TRAIN: EPOCH 127/1000 | BATCH 24/71 | LOSS: 9.606431012798566e-06\n",
      "TRAIN: EPOCH 127/1000 | BATCH 25/71 | LOSS: 9.891718421455993e-06\n",
      "TRAIN: EPOCH 127/1000 | BATCH 26/71 | LOSS: 9.867339435408616e-06\n",
      "TRAIN: EPOCH 127/1000 | BATCH 27/71 | LOSS: 1.0008872647761433e-05\n",
      "TRAIN: EPOCH 127/1000 | BATCH 28/71 | LOSS: 1.0091946525539368e-05\n",
      "TRAIN: EPOCH 127/1000 | BATCH 29/71 | LOSS: 1.0196094869267351e-05\n",
      "TRAIN: EPOCH 127/1000 | BATCH 30/71 | LOSS: 1.0234266805890648e-05\n",
      "TRAIN: EPOCH 127/1000 | BATCH 31/71 | LOSS: 1.022060342847908e-05\n",
      "TRAIN: EPOCH 127/1000 | BATCH 32/71 | LOSS: 1.0198047344932672e-05\n",
      "TRAIN: EPOCH 127/1000 | BATCH 33/71 | LOSS: 1.017933771115221e-05\n",
      "TRAIN: EPOCH 127/1000 | BATCH 34/71 | LOSS: 1.0145402796167348e-05\n",
      "TRAIN: EPOCH 127/1000 | BATCH 35/71 | LOSS: 1.0076830701816814e-05\n",
      "TRAIN: EPOCH 127/1000 | BATCH 36/71 | LOSS: 1.0112250271737827e-05\n",
      "TRAIN: EPOCH 127/1000 | BATCH 37/71 | LOSS: 1.0146323850341632e-05\n",
      "TRAIN: EPOCH 127/1000 | BATCH 38/71 | LOSS: 1.0110725083788379e-05\n",
      "TRAIN: EPOCH 127/1000 | BATCH 39/71 | LOSS: 1.0119516809936613e-05\n",
      "TRAIN: EPOCH 127/1000 | BATCH 40/71 | LOSS: 1.0072273719898231e-05\n",
      "TRAIN: EPOCH 127/1000 | BATCH 41/71 | LOSS: 1.0034300430561416e-05\n",
      "TRAIN: EPOCH 127/1000 | BATCH 42/71 | LOSS: 9.974952506528881e-06\n",
      "TRAIN: EPOCH 127/1000 | BATCH 43/71 | LOSS: 9.986180998566851e-06\n",
      "TRAIN: EPOCH 127/1000 | BATCH 44/71 | LOSS: 1.0019853218384217e-05\n",
      "TRAIN: EPOCH 127/1000 | BATCH 45/71 | LOSS: 1.0067743944645095e-05\n",
      "TRAIN: EPOCH 127/1000 | BATCH 46/71 | LOSS: 1.005975533924821e-05\n",
      "TRAIN: EPOCH 127/1000 | BATCH 47/71 | LOSS: 1.0050364522840027e-05\n",
      "TRAIN: EPOCH 127/1000 | BATCH 48/71 | LOSS: 1.0076363474693106e-05\n",
      "TRAIN: EPOCH 127/1000 | BATCH 49/71 | LOSS: 1.011293657029455e-05\n",
      "TRAIN: EPOCH 127/1000 | BATCH 50/71 | LOSS: 1.005390655051337e-05\n",
      "TRAIN: EPOCH 127/1000 | BATCH 51/71 | LOSS: 1.0036366242834796e-05\n",
      "TRAIN: EPOCH 127/1000 | BATCH 52/71 | LOSS: 1.0133253900281523e-05\n",
      "TRAIN: EPOCH 127/1000 | BATCH 53/71 | LOSS: 1.0100533843258205e-05\n",
      "TRAIN: EPOCH 127/1000 | BATCH 54/71 | LOSS: 1.0087020861895077e-05\n",
      "TRAIN: EPOCH 127/1000 | BATCH 55/71 | LOSS: 1.014265636253445e-05\n",
      "TRAIN: EPOCH 127/1000 | BATCH 56/71 | LOSS: 1.0140451003711573e-05\n",
      "TRAIN: EPOCH 127/1000 | BATCH 57/71 | LOSS: 1.0193729865242293e-05\n",
      "TRAIN: EPOCH 127/1000 | BATCH 58/71 | LOSS: 1.0168033256022835e-05\n",
      "TRAIN: EPOCH 127/1000 | BATCH 59/71 | LOSS: 1.0188263119440915e-05\n",
      "TRAIN: EPOCH 127/1000 | BATCH 60/71 | LOSS: 1.020333517965914e-05\n",
      "TRAIN: EPOCH 127/1000 | BATCH 61/71 | LOSS: 1.0225079823876807e-05\n",
      "TRAIN: EPOCH 127/1000 | BATCH 62/71 | LOSS: 1.0182319004213372e-05\n",
      "TRAIN: EPOCH 127/1000 | BATCH 63/71 | LOSS: 1.0164297108872233e-05\n",
      "TRAIN: EPOCH 127/1000 | BATCH 64/71 | LOSS: 1.0155674919168142e-05\n",
      "TRAIN: EPOCH 127/1000 | BATCH 65/71 | LOSS: 1.0198111243825583e-05\n",
      "TRAIN: EPOCH 127/1000 | BATCH 66/71 | LOSS: 1.0178583932483618e-05\n",
      "TRAIN: EPOCH 127/1000 | BATCH 67/71 | LOSS: 1.015250371197266e-05\n",
      "TRAIN: EPOCH 127/1000 | BATCH 68/71 | LOSS: 1.0142333796492482e-05\n",
      "TRAIN: EPOCH 127/1000 | BATCH 69/71 | LOSS: 1.010733114006663e-05\n",
      "TRAIN: EPOCH 127/1000 | BATCH 70/71 | LOSS: 1.0082306481551864e-05\n",
      "VAL: EPOCH 127/1000 | BATCH 0/8 | LOSS: 8.679375241626985e-06\n",
      "VAL: EPOCH 127/1000 | BATCH 1/8 | LOSS: 9.203035006066784e-06\n",
      "VAL: EPOCH 127/1000 | BATCH 2/8 | LOSS: 9.467266257464265e-06\n",
      "VAL: EPOCH 127/1000 | BATCH 3/8 | LOSS: 8.98032942586724e-06\n",
      "VAL: EPOCH 127/1000 | BATCH 4/8 | LOSS: 9.054334714164725e-06\n",
      "VAL: EPOCH 127/1000 | BATCH 5/8 | LOSS: 8.983611148020524e-06\n",
      "VAL: EPOCH 127/1000 | BATCH 6/8 | LOSS: 8.634890361593404e-06\n",
      "VAL: EPOCH 127/1000 | BATCH 7/8 | LOSS: 8.62213499885911e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 0/71 | LOSS: 7.3101887210214045e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 1/71 | LOSS: 7.1663278049527435e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 2/71 | LOSS: 7.875324172346154e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 3/71 | LOSS: 8.263683753284568e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 4/71 | LOSS: 8.133773462759564e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 5/71 | LOSS: 8.268781812148518e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 6/71 | LOSS: 8.403756315341785e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 7/71 | LOSS: 8.436998825800401e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 8/71 | LOSS: 8.524412047942556e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 9/71 | LOSS: 8.581815882280352e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 10/71 | LOSS: 9.165717074210988e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 11/71 | LOSS: 9.10338527167672e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 12/71 | LOSS: 9.224040660718367e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 13/71 | LOSS: 9.281331423543243e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 14/71 | LOSS: 9.392778489806611e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 15/71 | LOSS: 9.474386303054416e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 16/71 | LOSS: 9.524066653659033e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 17/71 | LOSS: 9.52442175932649e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 18/71 | LOSS: 9.42273422161203e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 19/71 | LOSS: 9.401812394571608e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 20/71 | LOSS: 9.419085125998772e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 21/71 | LOSS: 9.395631753828969e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 22/71 | LOSS: 9.344968969779313e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 23/71 | LOSS: 9.388599210069515e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 24/71 | LOSS: 9.395332890562713e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 25/71 | LOSS: 9.378339060416552e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 26/71 | LOSS: 9.394054224550794e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 27/71 | LOSS: 9.397781208722985e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 28/71 | LOSS: 9.423218185035913e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 29/71 | LOSS: 9.3862528350049e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 30/71 | LOSS: 9.37228070406784e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 31/71 | LOSS: 9.383733129197935e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 32/71 | LOSS: 9.382135128059113e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 33/71 | LOSS: 9.380678848709281e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 34/71 | LOSS: 9.411327742522449e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 35/71 | LOSS: 9.369147013179221e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 36/71 | LOSS: 9.358991249157682e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 37/71 | LOSS: 9.299779848285988e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 38/71 | LOSS: 9.259934790278725e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 39/71 | LOSS: 9.247659977518197e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 40/71 | LOSS: 9.24914139631721e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 41/71 | LOSS: 9.227898295821866e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 42/71 | LOSS: 9.322480848844184e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 43/71 | LOSS: 9.294631271669376e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 44/71 | LOSS: 9.290344961401489e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 45/71 | LOSS: 9.310093960464101e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 46/71 | LOSS: 9.295882959354748e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 47/71 | LOSS: 9.316938455109872e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 48/71 | LOSS: 9.33360327192587e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 49/71 | LOSS: 9.311875601270003e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 50/71 | LOSS: 9.30110835207754e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 51/71 | LOSS: 9.296585141363232e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 52/71 | LOSS: 9.288843600343e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 53/71 | LOSS: 9.282856404348018e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 54/71 | LOSS: 9.272253720899408e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 55/71 | LOSS: 9.287709628681893e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 56/71 | LOSS: 9.272550302915062e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 57/71 | LOSS: 9.280690235378831e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 58/71 | LOSS: 9.224755281061565e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 59/71 | LOSS: 9.265760278746408e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 60/71 | LOSS: 9.289272712208674e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 61/71 | LOSS: 9.308322909595106e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 62/71 | LOSS: 9.341004558322426e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 63/71 | LOSS: 9.332042182563782e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 64/71 | LOSS: 9.381575546285604e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 65/71 | LOSS: 9.375549187538898e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 66/71 | LOSS: 9.382423265935304e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 67/71 | LOSS: 9.421522731024785e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 68/71 | LOSS: 9.403738408434657e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 69/71 | LOSS: 9.405244483267389e-06\n",
      "TRAIN: EPOCH 128/1000 | BATCH 70/71 | LOSS: 9.40226786503483e-06\n",
      "VAL: EPOCH 128/1000 | BATCH 0/8 | LOSS: 1.0235311492579058e-05\n",
      "VAL: EPOCH 128/1000 | BATCH 1/8 | LOSS: 1.1094065484940074e-05\n",
      "VAL: EPOCH 128/1000 | BATCH 2/8 | LOSS: 1.1944953560790358e-05\n",
      "VAL: EPOCH 128/1000 | BATCH 3/8 | LOSS: 1.2044684581269394e-05\n",
      "VAL: EPOCH 128/1000 | BATCH 4/8 | LOSS: 1.1755244304367807e-05\n",
      "VAL: EPOCH 128/1000 | BATCH 5/8 | LOSS: 1.1525210690403279e-05\n",
      "VAL: EPOCH 128/1000 | BATCH 6/8 | LOSS: 1.1299255675112363e-05\n",
      "VAL: EPOCH 128/1000 | BATCH 7/8 | LOSS: 1.1038224215553782e-05\n",
      "TRAIN: EPOCH 129/1000 | BATCH 0/71 | LOSS: 1.3100890100758988e-05\n",
      "TRAIN: EPOCH 129/1000 | BATCH 1/71 | LOSS: 1.095679726859089e-05\n",
      "TRAIN: EPOCH 129/1000 | BATCH 2/71 | LOSS: 1.0836705162849588e-05\n",
      "TRAIN: EPOCH 129/1000 | BATCH 3/71 | LOSS: 1.0407891977592953e-05\n",
      "TRAIN: EPOCH 129/1000 | BATCH 4/71 | LOSS: 1.0461647798365448e-05\n",
      "TRAIN: EPOCH 129/1000 | BATCH 5/71 | LOSS: 1.0198299605690408e-05\n",
      "TRAIN: EPOCH 129/1000 | BATCH 6/71 | LOSS: 1.0286262425194894e-05\n",
      "TRAIN: EPOCH 129/1000 | BATCH 7/71 | LOSS: 1.0806966884047142e-05\n",
      "TRAIN: EPOCH 129/1000 | BATCH 8/71 | LOSS: 1.0447770465462883e-05\n",
      "TRAIN: EPOCH 129/1000 | BATCH 9/71 | LOSS: 1.0691008628782584e-05\n",
      "TRAIN: EPOCH 129/1000 | BATCH 10/71 | LOSS: 1.071558926923899e-05\n",
      "TRAIN: EPOCH 129/1000 | BATCH 11/71 | LOSS: 1.0664136652849265e-05\n",
      "TRAIN: EPOCH 129/1000 | BATCH 12/71 | LOSS: 1.0646082605839743e-05\n",
      "TRAIN: EPOCH 129/1000 | BATCH 13/71 | LOSS: 1.0610164216424372e-05\n",
      "TRAIN: EPOCH 129/1000 | BATCH 14/71 | LOSS: 1.0508855787823752e-05\n",
      "TRAIN: EPOCH 129/1000 | BATCH 15/71 | LOSS: 1.059707398098908e-05\n",
      "TRAIN: EPOCH 129/1000 | BATCH 16/71 | LOSS: 1.0560618648014497e-05\n",
      "TRAIN: EPOCH 129/1000 | BATCH 17/71 | LOSS: 1.0642914478214354e-05\n",
      "TRAIN: EPOCH 129/1000 | BATCH 18/71 | LOSS: 1.0533468842990451e-05\n",
      "TRAIN: EPOCH 129/1000 | BATCH 19/71 | LOSS: 1.0384047072875546e-05\n",
      "TRAIN: EPOCH 129/1000 | BATCH 20/71 | LOSS: 1.0517472869001462e-05\n",
      "TRAIN: EPOCH 129/1000 | BATCH 21/71 | LOSS: 1.0424953044011174e-05\n",
      "TRAIN: EPOCH 129/1000 | BATCH 22/71 | LOSS: 1.0357581192258056e-05\n",
      "TRAIN: EPOCH 129/1000 | BATCH 23/71 | LOSS: 1.034068660980362e-05\n",
      "TRAIN: EPOCH 129/1000 | BATCH 24/71 | LOSS: 1.0230238149233629e-05\n",
      "TRAIN: EPOCH 129/1000 | BATCH 25/71 | LOSS: 1.0313589287901637e-05\n",
      "TRAIN: EPOCH 129/1000 | BATCH 26/71 | LOSS: 1.0297475657555171e-05\n",
      "TRAIN: EPOCH 129/1000 | BATCH 27/71 | LOSS: 1.0305660388049936e-05\n",
      "TRAIN: EPOCH 129/1000 | BATCH 28/71 | LOSS: 1.0349357674793533e-05\n",
      "TRAIN: EPOCH 129/1000 | BATCH 29/71 | LOSS: 1.0383289676004399e-05\n",
      "TRAIN: EPOCH 129/1000 | BATCH 30/71 | LOSS: 1.0359220904055907e-05\n",
      "TRAIN: EPOCH 129/1000 | BATCH 31/71 | LOSS: 1.0342295098553222e-05\n",
      "TRAIN: EPOCH 129/1000 | BATCH 32/71 | LOSS: 1.0344632893534392e-05\n",
      "TRAIN: EPOCH 129/1000 | BATCH 33/71 | LOSS: 1.029807112097521e-05\n",
      "TRAIN: EPOCH 129/1000 | BATCH 34/71 | LOSS: 1.0283587575291417e-05\n",
      "TRAIN: EPOCH 129/1000 | BATCH 35/71 | LOSS: 1.0313713371740758e-05\n",
      "TRAIN: EPOCH 129/1000 | BATCH 36/71 | LOSS: 1.0277531564006003e-05\n",
      "TRAIN: EPOCH 129/1000 | BATCH 37/71 | LOSS: 1.0281607333844908e-05\n",
      "TRAIN: EPOCH 129/1000 | BATCH 38/71 | LOSS: 1.0261444283689623e-05\n",
      "TRAIN: EPOCH 129/1000 | BATCH 39/71 | LOSS: 1.0231657483927847e-05\n",
      "TRAIN: EPOCH 129/1000 | BATCH 40/71 | LOSS: 1.0246504865701753e-05\n",
      "TRAIN: EPOCH 129/1000 | BATCH 41/71 | LOSS: 1.0220581208115964e-05\n",
      "TRAIN: EPOCH 129/1000 | BATCH 42/71 | LOSS: 1.0191869055128082e-05\n",
      "TRAIN: EPOCH 129/1000 | BATCH 43/71 | LOSS: 1.016747346098286e-05\n",
      "TRAIN: EPOCH 129/1000 | BATCH 44/71 | LOSS: 1.0167917551168809e-05\n",
      "TRAIN: EPOCH 129/1000 | BATCH 45/71 | LOSS: 1.0202554538251286e-05\n",
      "TRAIN: EPOCH 129/1000 | BATCH 46/71 | LOSS: 1.0185072592031112e-05\n",
      "TRAIN: EPOCH 129/1000 | BATCH 47/71 | LOSS: 1.0172425258285026e-05\n",
      "TRAIN: EPOCH 129/1000 | BATCH 48/71 | LOSS: 1.0164274555085731e-05\n",
      "TRAIN: EPOCH 129/1000 | BATCH 49/71 | LOSS: 1.0131156905117678e-05\n",
      "TRAIN: EPOCH 129/1000 | BATCH 50/71 | LOSS: 1.0120006521117152e-05\n",
      "TRAIN: EPOCH 129/1000 | BATCH 51/71 | LOSS: 1.0082347041763849e-05\n",
      "TRAIN: EPOCH 129/1000 | BATCH 52/71 | LOSS: 1.0044505681846959e-05\n",
      "TRAIN: EPOCH 129/1000 | BATCH 53/71 | LOSS: 1.0010371153367925e-05\n",
      "TRAIN: EPOCH 129/1000 | BATCH 54/71 | LOSS: 9.961971673791678e-06\n",
      "TRAIN: EPOCH 129/1000 | BATCH 55/71 | LOSS: 9.945426313606731e-06\n",
      "TRAIN: EPOCH 129/1000 | BATCH 56/71 | LOSS: 9.899519418946798e-06\n",
      "TRAIN: EPOCH 129/1000 | BATCH 57/71 | LOSS: 9.890818083821895e-06\n",
      "TRAIN: EPOCH 129/1000 | BATCH 58/71 | LOSS: 9.854940051778704e-06\n",
      "TRAIN: EPOCH 129/1000 | BATCH 59/71 | LOSS: 9.814676900532504e-06\n",
      "TRAIN: EPOCH 129/1000 | BATCH 60/71 | LOSS: 9.860047649439008e-06\n",
      "TRAIN: EPOCH 129/1000 | BATCH 61/71 | LOSS: 9.8307267131942e-06\n",
      "TRAIN: EPOCH 129/1000 | BATCH 62/71 | LOSS: 9.789163298606466e-06\n",
      "TRAIN: EPOCH 129/1000 | BATCH 63/71 | LOSS: 9.751785214007214e-06\n",
      "TRAIN: EPOCH 129/1000 | BATCH 64/71 | LOSS: 9.692112626698173e-06\n",
      "TRAIN: EPOCH 129/1000 | BATCH 65/71 | LOSS: 9.68067387695941e-06\n",
      "TRAIN: EPOCH 129/1000 | BATCH 66/71 | LOSS: 9.691441255878334e-06\n",
      "TRAIN: EPOCH 129/1000 | BATCH 67/71 | LOSS: 9.690753699942434e-06\n",
      "TRAIN: EPOCH 129/1000 | BATCH 68/71 | LOSS: 9.686871576179882e-06\n",
      "TRAIN: EPOCH 129/1000 | BATCH 69/71 | LOSS: 9.653981039394109e-06\n",
      "TRAIN: EPOCH 129/1000 | BATCH 70/71 | LOSS: 9.695851057470681e-06\n",
      "VAL: EPOCH 129/1000 | BATCH 0/8 | LOSS: 1.0035756531578954e-05\n",
      "VAL: EPOCH 129/1000 | BATCH 1/8 | LOSS: 9.881304777081823e-06\n",
      "VAL: EPOCH 129/1000 | BATCH 2/8 | LOSS: 1.0233218138940478e-05\n",
      "VAL: EPOCH 129/1000 | BATCH 3/8 | LOSS: 9.616602028472698e-06\n",
      "VAL: EPOCH 129/1000 | BATCH 4/8 | LOSS: 9.740094901644624e-06\n",
      "VAL: EPOCH 129/1000 | BATCH 5/8 | LOSS: 9.739622176615134e-06\n",
      "VAL: EPOCH 129/1000 | BATCH 6/8 | LOSS: 9.42888709687395e-06\n",
      "VAL: EPOCH 129/1000 | BATCH 7/8 | LOSS: 9.348746857540391e-06\n",
      "TRAIN: EPOCH 130/1000 | BATCH 0/71 | LOSS: 1.0123023457708769e-05\n",
      "TRAIN: EPOCH 130/1000 | BATCH 1/71 | LOSS: 9.994454558182042e-06\n",
      "TRAIN: EPOCH 130/1000 | BATCH 2/71 | LOSS: 9.50142445314365e-06\n",
      "TRAIN: EPOCH 130/1000 | BATCH 3/71 | LOSS: 1.0432878752908437e-05\n",
      "TRAIN: EPOCH 130/1000 | BATCH 4/71 | LOSS: 9.90111511782743e-06\n",
      "TRAIN: EPOCH 130/1000 | BATCH 5/71 | LOSS: 9.880030726587089e-06\n",
      "TRAIN: EPOCH 130/1000 | BATCH 6/71 | LOSS: 9.918730549023686e-06\n",
      "TRAIN: EPOCH 130/1000 | BATCH 7/71 | LOSS: 9.598396900400985e-06\n",
      "TRAIN: EPOCH 130/1000 | BATCH 8/71 | LOSS: 9.505939083788286e-06\n",
      "TRAIN: EPOCH 130/1000 | BATCH 9/71 | LOSS: 9.796127869776683e-06\n",
      "TRAIN: EPOCH 130/1000 | BATCH 10/71 | LOSS: 9.76265726587206e-06\n",
      "TRAIN: EPOCH 130/1000 | BATCH 11/71 | LOSS: 9.636082343907523e-06\n",
      "TRAIN: EPOCH 130/1000 | BATCH 12/71 | LOSS: 9.728732248521947e-06\n",
      "TRAIN: EPOCH 130/1000 | BATCH 13/71 | LOSS: 1.0098888521627356e-05\n",
      "TRAIN: EPOCH 130/1000 | BATCH 14/71 | LOSS: 9.974404383683577e-06\n",
      "TRAIN: EPOCH 130/1000 | BATCH 15/71 | LOSS: 1.0163394790652092e-05\n",
      "TRAIN: EPOCH 130/1000 | BATCH 16/71 | LOSS: 1.0281143177588306e-05\n",
      "TRAIN: EPOCH 130/1000 | BATCH 17/71 | LOSS: 1.028431547133045e-05\n",
      "TRAIN: EPOCH 130/1000 | BATCH 18/71 | LOSS: 1.018711669817812e-05\n",
      "TRAIN: EPOCH 130/1000 | BATCH 19/71 | LOSS: 1.0176762816627161e-05\n",
      "TRAIN: EPOCH 130/1000 | BATCH 20/71 | LOSS: 1.0280627325950523e-05\n",
      "TRAIN: EPOCH 130/1000 | BATCH 21/71 | LOSS: 1.0236515043454181e-05\n",
      "TRAIN: EPOCH 130/1000 | BATCH 22/71 | LOSS: 1.0171592926464813e-05\n",
      "TRAIN: EPOCH 130/1000 | BATCH 23/71 | LOSS: 1.0067767865014806e-05\n",
      "TRAIN: EPOCH 130/1000 | BATCH 24/71 | LOSS: 1.001698234176729e-05\n",
      "TRAIN: EPOCH 130/1000 | BATCH 25/71 | LOSS: 1.0023852120446883e-05\n",
      "TRAIN: EPOCH 130/1000 | BATCH 26/71 | LOSS: 1.0005141491462752e-05\n",
      "TRAIN: EPOCH 130/1000 | BATCH 27/71 | LOSS: 1.001093785037353e-05\n",
      "TRAIN: EPOCH 130/1000 | BATCH 28/71 | LOSS: 1.0064070199311164e-05\n",
      "TRAIN: EPOCH 130/1000 | BATCH 29/71 | LOSS: 1.0015922574287591e-05\n",
      "TRAIN: EPOCH 130/1000 | BATCH 30/71 | LOSS: 1.010567226448104e-05\n",
      "TRAIN: EPOCH 130/1000 | BATCH 31/71 | LOSS: 1.0091866869288424e-05\n",
      "TRAIN: EPOCH 130/1000 | BATCH 32/71 | LOSS: 1.0075567209269414e-05\n",
      "TRAIN: EPOCH 130/1000 | BATCH 33/71 | LOSS: 1.005817611097454e-05\n",
      "TRAIN: EPOCH 130/1000 | BATCH 34/71 | LOSS: 1.0044187599435515e-05\n",
      "TRAIN: EPOCH 130/1000 | BATCH 35/71 | LOSS: 1.0022951477771534e-05\n",
      "TRAIN: EPOCH 130/1000 | BATCH 36/71 | LOSS: 9.929910840976328e-06\n",
      "TRAIN: EPOCH 130/1000 | BATCH 37/71 | LOSS: 9.966750248598768e-06\n",
      "TRAIN: EPOCH 130/1000 | BATCH 38/71 | LOSS: 9.941620821616827e-06\n",
      "TRAIN: EPOCH 130/1000 | BATCH 39/71 | LOSS: 9.931744671121123e-06\n",
      "TRAIN: EPOCH 130/1000 | BATCH 40/71 | LOSS: 9.927783915971792e-06\n",
      "TRAIN: EPOCH 130/1000 | BATCH 41/71 | LOSS: 9.834377557299809e-06\n",
      "TRAIN: EPOCH 130/1000 | BATCH 42/71 | LOSS: 9.89230495686697e-06\n",
      "TRAIN: EPOCH 130/1000 | BATCH 43/71 | LOSS: 9.846270855136522e-06\n",
      "TRAIN: EPOCH 130/1000 | BATCH 44/71 | LOSS: 9.853395588126862e-06\n",
      "TRAIN: EPOCH 130/1000 | BATCH 45/71 | LOSS: 9.865246718855696e-06\n",
      "TRAIN: EPOCH 130/1000 | BATCH 46/71 | LOSS: 9.881013845207486e-06\n",
      "TRAIN: EPOCH 130/1000 | BATCH 47/71 | LOSS: 9.85017790829564e-06\n",
      "TRAIN: EPOCH 130/1000 | BATCH 48/71 | LOSS: 9.797593483366358e-06\n",
      "TRAIN: EPOCH 130/1000 | BATCH 49/71 | LOSS: 9.80630560661666e-06\n",
      "TRAIN: EPOCH 130/1000 | BATCH 50/71 | LOSS: 9.876665596190529e-06\n",
      "TRAIN: EPOCH 130/1000 | BATCH 51/71 | LOSS: 9.846296017293263e-06\n",
      "TRAIN: EPOCH 130/1000 | BATCH 52/71 | LOSS: 9.857217985384369e-06\n",
      "TRAIN: EPOCH 130/1000 | BATCH 53/71 | LOSS: 9.857746876891556e-06\n",
      "TRAIN: EPOCH 130/1000 | BATCH 54/71 | LOSS: 9.861940799799579e-06\n",
      "TRAIN: EPOCH 130/1000 | BATCH 55/71 | LOSS: 9.835620630838093e-06\n",
      "TRAIN: EPOCH 130/1000 | BATCH 56/71 | LOSS: 9.834786684031746e-06\n",
      "TRAIN: EPOCH 130/1000 | BATCH 57/71 | LOSS: 9.786592092245098e-06\n",
      "TRAIN: EPOCH 130/1000 | BATCH 58/71 | LOSS: 9.788657866072568e-06\n",
      "TRAIN: EPOCH 130/1000 | BATCH 59/71 | LOSS: 9.746710937482324e-06\n",
      "TRAIN: EPOCH 130/1000 | BATCH 60/71 | LOSS: 9.736924058699038e-06\n",
      "TRAIN: EPOCH 130/1000 | BATCH 61/71 | LOSS: 9.724465488973017e-06\n",
      "TRAIN: EPOCH 130/1000 | BATCH 62/71 | LOSS: 9.747500710560301e-06\n",
      "TRAIN: EPOCH 130/1000 | BATCH 63/71 | LOSS: 9.739641136263799e-06\n",
      "TRAIN: EPOCH 130/1000 | BATCH 64/71 | LOSS: 9.737886921357131e-06\n",
      "TRAIN: EPOCH 130/1000 | BATCH 65/71 | LOSS: 9.687419358008562e-06\n",
      "TRAIN: EPOCH 130/1000 | BATCH 66/71 | LOSS: 9.66328175920027e-06\n",
      "TRAIN: EPOCH 130/1000 | BATCH 67/71 | LOSS: 9.628461892883506e-06\n",
      "TRAIN: EPOCH 130/1000 | BATCH 68/71 | LOSS: 9.616646504740777e-06\n",
      "TRAIN: EPOCH 130/1000 | BATCH 69/71 | LOSS: 9.58521129307753e-06\n",
      "TRAIN: EPOCH 130/1000 | BATCH 70/71 | LOSS: 9.572234567956553e-06\n",
      "VAL: EPOCH 130/1000 | BATCH 0/8 | LOSS: 8.37409697851399e-06\n",
      "VAL: EPOCH 130/1000 | BATCH 1/8 | LOSS: 9.076818969333544e-06\n",
      "VAL: EPOCH 130/1000 | BATCH 2/8 | LOSS: 9.130225710881254e-06\n",
      "VAL: EPOCH 130/1000 | BATCH 3/8 | LOSS: 8.769278338149888e-06\n",
      "VAL: EPOCH 130/1000 | BATCH 4/8 | LOSS: 8.822803283692338e-06\n",
      "VAL: EPOCH 130/1000 | BATCH 5/8 | LOSS: 8.757920416731698e-06\n",
      "VAL: EPOCH 130/1000 | BATCH 6/8 | LOSS: 8.416427330562978e-06\n",
      "VAL: EPOCH 130/1000 | BATCH 7/8 | LOSS: 8.359874072993989e-06\n",
      "TRAIN: EPOCH 131/1000 | BATCH 0/71 | LOSS: 7.673090294701979e-06\n",
      "TRAIN: EPOCH 131/1000 | BATCH 1/71 | LOSS: 8.147913376888027e-06\n",
      "TRAIN: EPOCH 131/1000 | BATCH 2/71 | LOSS: 8.774278891602686e-06\n",
      "TRAIN: EPOCH 131/1000 | BATCH 3/71 | LOSS: 8.686981118444237e-06\n",
      "TRAIN: EPOCH 131/1000 | BATCH 4/71 | LOSS: 8.349833751708503e-06\n",
      "TRAIN: EPOCH 131/1000 | BATCH 5/71 | LOSS: 9.011945697542009e-06\n",
      "TRAIN: EPOCH 131/1000 | BATCH 6/71 | LOSS: 8.97589669485959e-06\n",
      "TRAIN: EPOCH 131/1000 | BATCH 7/71 | LOSS: 8.857187538069411e-06\n",
      "TRAIN: EPOCH 131/1000 | BATCH 8/71 | LOSS: 9.12872095189717e-06\n",
      "TRAIN: EPOCH 131/1000 | BATCH 9/71 | LOSS: 9.028773501995601e-06\n",
      "TRAIN: EPOCH 131/1000 | BATCH 10/71 | LOSS: 9.03243029494082e-06\n",
      "TRAIN: EPOCH 131/1000 | BATCH 11/71 | LOSS: 9.338800850855478e-06\n",
      "TRAIN: EPOCH 131/1000 | BATCH 12/71 | LOSS: 9.226667001572563e-06\n",
      "TRAIN: EPOCH 131/1000 | BATCH 13/71 | LOSS: 9.142187764155096e-06\n",
      "TRAIN: EPOCH 131/1000 | BATCH 14/71 | LOSS: 9.069417471134026e-06\n",
      "TRAIN: EPOCH 131/1000 | BATCH 15/71 | LOSS: 9.255939033891991e-06\n",
      "TRAIN: EPOCH 131/1000 | BATCH 16/71 | LOSS: 9.276306761474938e-06\n",
      "TRAIN: EPOCH 131/1000 | BATCH 17/71 | LOSS: 9.371091512851612e-06\n",
      "TRAIN: EPOCH 131/1000 | BATCH 18/71 | LOSS: 9.398667579692923e-06\n",
      "TRAIN: EPOCH 131/1000 | BATCH 19/71 | LOSS: 9.391945400238911e-06\n",
      "TRAIN: EPOCH 131/1000 | BATCH 20/71 | LOSS: 9.447550512829497e-06\n",
      "TRAIN: EPOCH 131/1000 | BATCH 21/71 | LOSS: 9.535030275484049e-06\n",
      "TRAIN: EPOCH 131/1000 | BATCH 22/71 | LOSS: 9.53651019501748e-06\n",
      "TRAIN: EPOCH 131/1000 | BATCH 23/71 | LOSS: 9.597229961855192e-06\n",
      "TRAIN: EPOCH 131/1000 | BATCH 24/71 | LOSS: 9.56593505179626e-06\n",
      "TRAIN: EPOCH 131/1000 | BATCH 25/71 | LOSS: 9.572634920774502e-06\n",
      "TRAIN: EPOCH 131/1000 | BATCH 26/71 | LOSS: 9.565043759115118e-06\n",
      "TRAIN: EPOCH 131/1000 | BATCH 27/71 | LOSS: 9.513336684514278e-06\n",
      "TRAIN: EPOCH 131/1000 | BATCH 28/71 | LOSS: 9.472871499446197e-06\n",
      "TRAIN: EPOCH 131/1000 | BATCH 29/71 | LOSS: 9.598096842940626e-06\n",
      "TRAIN: EPOCH 131/1000 | BATCH 30/71 | LOSS: 9.5641992394972e-06\n",
      "TRAIN: EPOCH 131/1000 | BATCH 31/71 | LOSS: 9.636998598239188e-06\n",
      "TRAIN: EPOCH 131/1000 | BATCH 32/71 | LOSS: 9.656202716238719e-06\n",
      "TRAIN: EPOCH 131/1000 | BATCH 33/71 | LOSS: 9.713416107735933e-06\n",
      "TRAIN: EPOCH 131/1000 | BATCH 34/71 | LOSS: 9.775435721946581e-06\n",
      "TRAIN: EPOCH 131/1000 | BATCH 35/71 | LOSS: 9.739527071259443e-06\n",
      "TRAIN: EPOCH 131/1000 | BATCH 36/71 | LOSS: 9.941132272640086e-06\n",
      "TRAIN: EPOCH 131/1000 | BATCH 37/71 | LOSS: 9.904841267359044e-06\n",
      "TRAIN: EPOCH 131/1000 | BATCH 38/71 | LOSS: 9.952877136152567e-06\n",
      "TRAIN: EPOCH 131/1000 | BATCH 39/71 | LOSS: 1.0097042684265034e-05\n",
      "TRAIN: EPOCH 131/1000 | BATCH 40/71 | LOSS: 1.007033757204914e-05\n",
      "TRAIN: EPOCH 131/1000 | BATCH 41/71 | LOSS: 1.016290929371843e-05\n",
      "TRAIN: EPOCH 131/1000 | BATCH 42/71 | LOSS: 1.0200428087680553e-05\n",
      "TRAIN: EPOCH 131/1000 | BATCH 43/71 | LOSS: 1.0152639674743114e-05\n",
      "TRAIN: EPOCH 131/1000 | BATCH 44/71 | LOSS: 1.0333710866284996e-05\n",
      "TRAIN: EPOCH 131/1000 | BATCH 45/71 | LOSS: 1.0306344027254534e-05\n",
      "TRAIN: EPOCH 131/1000 | BATCH 46/71 | LOSS: 1.0386779717435352e-05\n",
      "TRAIN: EPOCH 131/1000 | BATCH 47/71 | LOSS: 1.0549614188448686e-05\n",
      "TRAIN: EPOCH 131/1000 | BATCH 48/71 | LOSS: 1.0520483806895031e-05\n",
      "TRAIN: EPOCH 131/1000 | BATCH 49/71 | LOSS: 1.0803741961353808e-05\n",
      "TRAIN: EPOCH 131/1000 | BATCH 50/71 | LOSS: 1.093488899463338e-05\n",
      "TRAIN: EPOCH 131/1000 | BATCH 51/71 | LOSS: 1.090255293197865e-05\n",
      "TRAIN: EPOCH 131/1000 | BATCH 52/71 | LOSS: 1.1286099894312188e-05\n",
      "TRAIN: EPOCH 131/1000 | BATCH 53/71 | LOSS: 1.1213636492323497e-05\n",
      "TRAIN: EPOCH 131/1000 | BATCH 54/71 | LOSS: 1.1321989197974538e-05\n",
      "TRAIN: EPOCH 131/1000 | BATCH 55/71 | LOSS: 1.140243857175197e-05\n",
      "TRAIN: EPOCH 131/1000 | BATCH 56/71 | LOSS: 1.1339169400583304e-05\n",
      "TRAIN: EPOCH 131/1000 | BATCH 57/71 | LOSS: 1.1344156887546354e-05\n",
      "TRAIN: EPOCH 131/1000 | BATCH 58/71 | LOSS: 1.1396559854139182e-05\n",
      "TRAIN: EPOCH 131/1000 | BATCH 59/71 | LOSS: 1.1418694437755524e-05\n",
      "TRAIN: EPOCH 131/1000 | BATCH 60/71 | LOSS: 1.1496483261535254e-05\n",
      "TRAIN: EPOCH 131/1000 | BATCH 61/71 | LOSS: 1.1492548038961925e-05\n",
      "TRAIN: EPOCH 131/1000 | BATCH 62/71 | LOSS: 1.1502450022470883e-05\n",
      "TRAIN: EPOCH 131/1000 | BATCH 63/71 | LOSS: 1.1615072104120827e-05\n",
      "TRAIN: EPOCH 131/1000 | BATCH 64/71 | LOSS: 1.1553829377729786e-05\n",
      "TRAIN: EPOCH 131/1000 | BATCH 65/71 | LOSS: 1.1598965674045647e-05\n",
      "TRAIN: EPOCH 131/1000 | BATCH 66/71 | LOSS: 1.1566691757334004e-05\n",
      "TRAIN: EPOCH 131/1000 | BATCH 67/71 | LOSS: 1.1553935076549619e-05\n",
      "TRAIN: EPOCH 131/1000 | BATCH 68/71 | LOSS: 1.1521739162307695e-05\n",
      "TRAIN: EPOCH 131/1000 | BATCH 69/71 | LOSS: 1.1472952577865466e-05\n",
      "TRAIN: EPOCH 131/1000 | BATCH 70/71 | LOSS: 1.1467033948039745e-05\n",
      "VAL: EPOCH 131/1000 | BATCH 0/8 | LOSS: 1.218559373228345e-05\n",
      "VAL: EPOCH 131/1000 | BATCH 1/8 | LOSS: 1.172585689346306e-05\n",
      "VAL: EPOCH 131/1000 | BATCH 2/8 | LOSS: 1.2871909954507524e-05\n",
      "VAL: EPOCH 131/1000 | BATCH 3/8 | LOSS: 1.2506411167123588e-05\n",
      "VAL: EPOCH 131/1000 | BATCH 4/8 | LOSS: 1.2855977729486767e-05\n",
      "VAL: EPOCH 131/1000 | BATCH 5/8 | LOSS: 1.293223582858142e-05\n",
      "VAL: EPOCH 131/1000 | BATCH 6/8 | LOSS: 1.2375185048897816e-05\n",
      "VAL: EPOCH 131/1000 | BATCH 7/8 | LOSS: 1.2515768162302265e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 0/71 | LOSS: 1.4763720173505135e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 1/71 | LOSS: 1.3755023246631026e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 2/71 | LOSS: 1.2157668910125116e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 3/71 | LOSS: 1.2098461638743174e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 4/71 | LOSS: 1.1306931628496387e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 5/71 | LOSS: 1.1320026866693903e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 6/71 | LOSS: 1.148411190245367e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 7/71 | LOSS: 1.1326857247695443e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 8/71 | LOSS: 1.157741988006617e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 9/71 | LOSS: 1.1587841072469018e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 10/71 | LOSS: 1.1555138288796032e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 11/71 | LOSS: 1.2190873803774593e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 12/71 | LOSS: 1.2075054165996755e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 13/71 | LOSS: 1.203032089896234e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 14/71 | LOSS: 1.2433524049508076e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 15/71 | LOSS: 1.2177715802863531e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 16/71 | LOSS: 1.2519802005365764e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 17/71 | LOSS: 1.2370538469339306e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 18/71 | LOSS: 1.2313196796305037e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 19/71 | LOSS: 1.2255332330823876e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 20/71 | LOSS: 1.2035906235853742e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 21/71 | LOSS: 1.1986386942391453e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 22/71 | LOSS: 1.1895143238096194e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 23/71 | LOSS: 1.1746851934428074e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 24/71 | LOSS: 1.1760639827116393e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 25/71 | LOSS: 1.1787097089556762e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 26/71 | LOSS: 1.1713789380910047e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 27/71 | LOSS: 1.1760564088295464e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 28/71 | LOSS: 1.1630501517331516e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 29/71 | LOSS: 1.1574517278252946e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 30/71 | LOSS: 1.1499067876420374e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 31/71 | LOSS: 1.1365829493570345e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 32/71 | LOSS: 1.1271689028503854e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 33/71 | LOSS: 1.1231471141988604e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 34/71 | LOSS: 1.1219156710597287e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 35/71 | LOSS: 1.1137268681219817e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 36/71 | LOSS: 1.0997247733718501e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 37/71 | LOSS: 1.0965041957732816e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 38/71 | LOSS: 1.0904176447067397e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 39/71 | LOSS: 1.0860120755751269e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 40/71 | LOSS: 1.0820761062637846e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 41/71 | LOSS: 1.0788151896468618e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 42/71 | LOSS: 1.0799001376103978e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 43/71 | LOSS: 1.0767450517464154e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 44/71 | LOSS: 1.0702069044378327e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 45/71 | LOSS: 1.0719222661029324e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 46/71 | LOSS: 1.0683021502429371e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 47/71 | LOSS: 1.0624565713139114e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 48/71 | LOSS: 1.0628726195878995e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 49/71 | LOSS: 1.0576931017567404e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 50/71 | LOSS: 1.0631367188704876e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 51/71 | LOSS: 1.062637857802866e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 52/71 | LOSS: 1.0608550374459844e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 53/71 | LOSS: 1.060245803172519e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 54/71 | LOSS: 1.0584005047514274e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 55/71 | LOSS: 1.0592348676904553e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 56/71 | LOSS: 1.057610930537952e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 57/71 | LOSS: 1.059178220171377e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 58/71 | LOSS: 1.058656021163866e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 59/71 | LOSS: 1.0556724782873062e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 60/71 | LOSS: 1.0533389837798838e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 61/71 | LOSS: 1.0505400804240414e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 62/71 | LOSS: 1.0448181470449542e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 63/71 | LOSS: 1.0428924611005641e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 64/71 | LOSS: 1.0453773001455165e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 65/71 | LOSS: 1.0431935276177352e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 66/71 | LOSS: 1.0419664344660862e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 67/71 | LOSS: 1.0383163632521995e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 68/71 | LOSS: 1.036272226264228e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 69/71 | LOSS: 1.0362243691426037e-05\n",
      "TRAIN: EPOCH 132/1000 | BATCH 70/71 | LOSS: 1.0339123600459342e-05\n",
      "VAL: EPOCH 132/1000 | BATCH 0/8 | LOSS: 9.084566045203246e-06\n",
      "VAL: EPOCH 132/1000 | BATCH 1/8 | LOSS: 1.0124826985702384e-05\n",
      "VAL: EPOCH 132/1000 | BATCH 2/8 | LOSS: 1.0079937055706978e-05\n",
      "VAL: EPOCH 132/1000 | BATCH 3/8 | LOSS: 1.0059107580673299e-05\n",
      "VAL: EPOCH 132/1000 | BATCH 4/8 | LOSS: 9.990386934077833e-06\n",
      "VAL: EPOCH 132/1000 | BATCH 5/8 | LOSS: 9.883892289508367e-06\n",
      "VAL: EPOCH 132/1000 | BATCH 6/8 | LOSS: 9.405904165760148e-06\n",
      "VAL: EPOCH 132/1000 | BATCH 7/8 | LOSS: 9.376518391945865e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 0/71 | LOSS: 7.961192750371993e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 1/71 | LOSS: 1.0170436326006893e-05\n",
      "TRAIN: EPOCH 133/1000 | BATCH 2/71 | LOSS: 9.700004435823454e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 3/71 | LOSS: 9.408595815330045e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 4/71 | LOSS: 9.522488289803732e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 5/71 | LOSS: 9.733373341684151e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 6/71 | LOSS: 9.36905001123835e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 7/71 | LOSS: 9.350429195364995e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 8/71 | LOSS: 9.215083789765938e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 9/71 | LOSS: 9.36088508751709e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 10/71 | LOSS: 9.497041487934025e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 11/71 | LOSS: 9.136017676307043e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 12/71 | LOSS: 9.144916895400876e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 13/71 | LOSS: 9.147605006936438e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 14/71 | LOSS: 9.260489908532085e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 15/71 | LOSS: 9.24235189359024e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 16/71 | LOSS: 9.277870370116194e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 17/71 | LOSS: 9.236854970772078e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 18/71 | LOSS: 9.382866712832383e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 19/71 | LOSS: 9.434702633370762e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 20/71 | LOSS: 9.528456411360475e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 21/71 | LOSS: 9.620550082962092e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 22/71 | LOSS: 9.554052232250916e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 23/71 | LOSS: 9.577572692857453e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 24/71 | LOSS: 9.588211541995406e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 25/71 | LOSS: 9.661376365189566e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 26/71 | LOSS: 9.63355655080199e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 27/71 | LOSS: 9.680691781405975e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 28/71 | LOSS: 9.67122032934326e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 29/71 | LOSS: 9.71165721542396e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 30/71 | LOSS: 9.691398425333787e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 31/71 | LOSS: 9.63460311709241e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 32/71 | LOSS: 9.705459088956669e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 33/71 | LOSS: 9.71036947435898e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 34/71 | LOSS: 9.753969087406794e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 35/71 | LOSS: 9.832091437702831e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 36/71 | LOSS: 9.868272787276215e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 37/71 | LOSS: 9.864715593721485e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 38/71 | LOSS: 9.85540937574861e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 39/71 | LOSS: 9.81967491497926e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 40/71 | LOSS: 9.804984296263173e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 41/71 | LOSS: 9.806953476938707e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 42/71 | LOSS: 9.79638996288359e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 43/71 | LOSS: 9.76539986781559e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 44/71 | LOSS: 9.84419869685856e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 45/71 | LOSS: 9.827051144384313e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 46/71 | LOSS: 9.745387847323535e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 47/71 | LOSS: 9.73306038076771e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 48/71 | LOSS: 9.763247566743116e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 49/71 | LOSS: 9.79880274826428e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 50/71 | LOSS: 9.839910579138828e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 51/71 | LOSS: 9.805320395081742e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 52/71 | LOSS: 9.843130993551381e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 53/71 | LOSS: 9.955051576037443e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 54/71 | LOSS: 9.924494968965354e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 55/71 | LOSS: 9.970611050188641e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 56/71 | LOSS: 9.971431242312273e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 57/71 | LOSS: 9.982087311951106e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 58/71 | LOSS: 1.0009437496823227e-05\n",
      "TRAIN: EPOCH 133/1000 | BATCH 59/71 | LOSS: 9.994765423471109e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 60/71 | LOSS: 9.997202976622055e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 61/71 | LOSS: 1.0021139719502835e-05\n",
      "TRAIN: EPOCH 133/1000 | BATCH 62/71 | LOSS: 9.991136952492756e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 63/71 | LOSS: 9.979499637324807e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 64/71 | LOSS: 9.945274863499574e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 65/71 | LOSS: 9.926093601582883e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 66/71 | LOSS: 9.913637306942577e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 67/71 | LOSS: 9.879198514414276e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 68/71 | LOSS: 9.853861024896395e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 69/71 | LOSS: 9.835353421294712e-06\n",
      "TRAIN: EPOCH 133/1000 | BATCH 70/71 | LOSS: 9.815637619961398e-06\n",
      "VAL: EPOCH 133/1000 | BATCH 0/8 | LOSS: 1.136343416874297e-05\n",
      "VAL: EPOCH 133/1000 | BATCH 1/8 | LOSS: 1.2016530945402337e-05\n",
      "VAL: EPOCH 133/1000 | BATCH 2/8 | LOSS: 1.1513338904478587e-05\n",
      "VAL: EPOCH 133/1000 | BATCH 3/8 | LOSS: 1.0708575473472592e-05\n",
      "VAL: EPOCH 133/1000 | BATCH 4/8 | LOSS: 1.0927032235485967e-05\n",
      "VAL: EPOCH 133/1000 | BATCH 5/8 | LOSS: 1.083947669637079e-05\n",
      "VAL: EPOCH 133/1000 | BATCH 6/8 | LOSS: 1.0631555726701794e-05\n",
      "VAL: EPOCH 133/1000 | BATCH 7/8 | LOSS: 1.0497330777070601e-05\n",
      "TRAIN: EPOCH 134/1000 | BATCH 0/71 | LOSS: 8.500321200699545e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 1/71 | LOSS: 8.60821091919206e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 2/71 | LOSS: 8.416636167870214e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 3/71 | LOSS: 8.216974038077751e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 4/71 | LOSS: 8.259610331151634e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 5/71 | LOSS: 8.110231343986621e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 6/71 | LOSS: 7.785052860396848e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 7/71 | LOSS: 8.046057132560236e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 8/71 | LOSS: 8.18632371293562e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 9/71 | LOSS: 8.142506794683868e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 10/71 | LOSS: 8.020239378807178e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 11/71 | LOSS: 8.196645846207199e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 12/71 | LOSS: 8.060919544032704e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 13/71 | LOSS: 8.2121335773471e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 14/71 | LOSS: 8.217206959670875e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 15/71 | LOSS: 8.05181366558827e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 16/71 | LOSS: 7.92572657647885e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 17/71 | LOSS: 8.005627933016513e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 18/71 | LOSS: 8.021456395861032e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 19/71 | LOSS: 8.153165435942355e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 20/71 | LOSS: 8.249150812930783e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 21/71 | LOSS: 8.353493946990717e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 22/71 | LOSS: 8.473877128183275e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 23/71 | LOSS: 8.538270852416948e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 24/71 | LOSS: 8.49095982630388e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 25/71 | LOSS: 8.643726774747707e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 26/71 | LOSS: 8.587740854569279e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 27/71 | LOSS: 8.751995567633588e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 28/71 | LOSS: 8.737869205200194e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 29/71 | LOSS: 8.725435903519003e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 30/71 | LOSS: 8.658868131122642e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 31/71 | LOSS: 8.663478922699142e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 32/71 | LOSS: 8.762159984943343e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 33/71 | LOSS: 8.84176011716002e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 34/71 | LOSS: 8.883943103553195e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 35/71 | LOSS: 8.860184000999046e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 36/71 | LOSS: 8.897080963809433e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 37/71 | LOSS: 8.858339134875629e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 38/71 | LOSS: 8.793372158740964e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 39/71 | LOSS: 8.821528547287017e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 40/71 | LOSS: 8.802442308234919e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 41/71 | LOSS: 8.863753821840095e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 42/71 | LOSS: 8.865187917467136e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 43/71 | LOSS: 8.813448217055834e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 44/71 | LOSS: 8.781865037437658e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 45/71 | LOSS: 8.784322240582854e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 46/71 | LOSS: 8.791459520657651e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 47/71 | LOSS: 8.74487478578582e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 48/71 | LOSS: 8.76348023518162e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 49/71 | LOSS: 8.774453553996865e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 50/71 | LOSS: 8.770498362876014e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 51/71 | LOSS: 8.759741892670438e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 52/71 | LOSS: 8.775464034700252e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 53/71 | LOSS: 8.795590213560865e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 54/71 | LOSS: 8.878137776429702e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 55/71 | LOSS: 8.922906001121841e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 56/71 | LOSS: 9.019463225724808e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 57/71 | LOSS: 9.020565961503147e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 58/71 | LOSS: 8.998463819530039e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 59/71 | LOSS: 9.03140786097841e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 60/71 | LOSS: 9.051581244301681e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 61/71 | LOSS: 9.07716228157956e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 62/71 | LOSS: 9.04857229432396e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 63/71 | LOSS: 9.013791704148844e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 64/71 | LOSS: 9.01777234153437e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 65/71 | LOSS: 9.00737293732439e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 66/71 | LOSS: 9.019823739517553e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 67/71 | LOSS: 8.989015702140664e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 68/71 | LOSS: 8.972860264886707e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 69/71 | LOSS: 8.953914032515188e-06\n",
      "TRAIN: EPOCH 134/1000 | BATCH 70/71 | LOSS: 8.912125894581344e-06\n",
      "VAL: EPOCH 134/1000 | BATCH 0/8 | LOSS: 8.86923044163268e-06\n",
      "VAL: EPOCH 134/1000 | BATCH 1/8 | LOSS: 9.624533959140535e-06\n",
      "VAL: EPOCH 134/1000 | BATCH 2/8 | LOSS: 9.305290708046718e-06\n",
      "VAL: EPOCH 134/1000 | BATCH 3/8 | LOSS: 8.960647846834036e-06\n",
      "VAL: EPOCH 134/1000 | BATCH 4/8 | LOSS: 9.010433495859616e-06\n",
      "VAL: EPOCH 134/1000 | BATCH 5/8 | LOSS: 9.055565290812714e-06\n",
      "VAL: EPOCH 134/1000 | BATCH 6/8 | LOSS: 8.746952257102489e-06\n",
      "VAL: EPOCH 134/1000 | BATCH 7/8 | LOSS: 8.66083985329169e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 0/71 | LOSS: 8.708470886631403e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 1/71 | LOSS: 7.901598110038321e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 2/71 | LOSS: 7.176635790528962e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 3/71 | LOSS: 7.32761134258908e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 4/71 | LOSS: 7.2865989750425795e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 5/71 | LOSS: 7.496569272310201e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 6/71 | LOSS: 7.561876048254947e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 7/71 | LOSS: 7.742392710952117e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 8/71 | LOSS: 8.032277492020512e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 9/71 | LOSS: 8.25373213046987e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 10/71 | LOSS: 8.364986611426998e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 11/71 | LOSS: 8.363114413138343e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 12/71 | LOSS: 8.535035179263035e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 13/71 | LOSS: 8.590255642307706e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 14/71 | LOSS: 8.542796088780354e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 15/71 | LOSS: 8.53480551654684e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 16/71 | LOSS: 8.656615644962157e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 17/71 | LOSS: 8.623262475440344e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 18/71 | LOSS: 8.688599695903432e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 19/71 | LOSS: 8.67804826611973e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 20/71 | LOSS: 8.71184097593955e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 21/71 | LOSS: 8.743371195536879e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 22/71 | LOSS: 8.776184052333955e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 23/71 | LOSS: 8.738285278771704e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 24/71 | LOSS: 8.871134177752537e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 25/71 | LOSS: 8.934851201467189e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 26/71 | LOSS: 8.86256157173193e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 27/71 | LOSS: 8.774233021670494e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 28/71 | LOSS: 8.931005971856488e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 29/71 | LOSS: 8.90463426609737e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 30/71 | LOSS: 8.906216466599407e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 31/71 | LOSS: 8.951088503295068e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 32/71 | LOSS: 9.025166519043552e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 33/71 | LOSS: 8.967224790895297e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 34/71 | LOSS: 9.039837394604028e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 35/71 | LOSS: 9.061875706114128e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 36/71 | LOSS: 9.065923456325214e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 37/71 | LOSS: 9.056065125964538e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 38/71 | LOSS: 9.039357360635246e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 39/71 | LOSS: 8.975428556823317e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 40/71 | LOSS: 9.00512992391269e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 41/71 | LOSS: 8.996198836203326e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 42/71 | LOSS: 9.030035058923775e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 43/71 | LOSS: 9.03428710072066e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 44/71 | LOSS: 9.015785882285046e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 45/71 | LOSS: 8.999980141395454e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 46/71 | LOSS: 9.026845863774453e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 47/71 | LOSS: 9.043030189559431e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 48/71 | LOSS: 9.10270442286024e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 49/71 | LOSS: 9.180220704365638e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 50/71 | LOSS: 9.176110048500442e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 51/71 | LOSS: 9.16699831612697e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 52/71 | LOSS: 9.153205584605633e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 53/71 | LOSS: 9.168909021658086e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 54/71 | LOSS: 9.197527114420982e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 55/71 | LOSS: 9.211222334215563e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 56/71 | LOSS: 9.176409875771354e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 57/71 | LOSS: 9.177908889890613e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 58/71 | LOSS: 9.133857166504593e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 59/71 | LOSS: 9.122295834155618e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 60/71 | LOSS: 9.102269516125623e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 61/71 | LOSS: 9.131969136433327e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 62/71 | LOSS: 9.122657985923098e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 63/71 | LOSS: 9.12847075795753e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 64/71 | LOSS: 9.124343558561612e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 65/71 | LOSS: 9.123981333852333e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 66/71 | LOSS: 9.13838898464855e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 67/71 | LOSS: 9.17275238353956e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 68/71 | LOSS: 9.165291549320303e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 69/71 | LOSS: 9.168097072428542e-06\n",
      "TRAIN: EPOCH 135/1000 | BATCH 70/71 | LOSS: 9.230663411946014e-06\n",
      "VAL: EPOCH 135/1000 | BATCH 0/8 | LOSS: 9.450399375054985e-06\n",
      "VAL: EPOCH 135/1000 | BATCH 1/8 | LOSS: 1.0615274732117541e-05\n",
      "VAL: EPOCH 135/1000 | BATCH 2/8 | LOSS: 1.0104236935148947e-05\n",
      "VAL: EPOCH 135/1000 | BATCH 3/8 | LOSS: 9.922511253535049e-06\n",
      "VAL: EPOCH 135/1000 | BATCH 4/8 | LOSS: 1.011834610835649e-05\n",
      "VAL: EPOCH 135/1000 | BATCH 5/8 | LOSS: 1.0183036465605255e-05\n",
      "VAL: EPOCH 135/1000 | BATCH 6/8 | LOSS: 9.909199304404734e-06\n",
      "VAL: EPOCH 135/1000 | BATCH 7/8 | LOSS: 9.788480724637338e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 0/71 | LOSS: 8.953828000812791e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 1/71 | LOSS: 8.457040621578926e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 2/71 | LOSS: 8.659500963403843e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 3/71 | LOSS: 8.712491080586915e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 4/71 | LOSS: 9.072030843526591e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 5/71 | LOSS: 8.918331180514846e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 6/71 | LOSS: 8.544914375566545e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 7/71 | LOSS: 8.475978461319755e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 8/71 | LOSS: 8.262276019700545e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 9/71 | LOSS: 8.419026471528923e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 10/71 | LOSS: 8.644429478656754e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 11/71 | LOSS: 8.711627515367582e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 12/71 | LOSS: 8.695888237315767e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 13/71 | LOSS: 8.713278670516697e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 14/71 | LOSS: 8.696289357127778e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 15/71 | LOSS: 8.70753098070054e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 16/71 | LOSS: 8.708744912035078e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 17/71 | LOSS: 8.683317850631687e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 18/71 | LOSS: 8.81151213673691e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 19/71 | LOSS: 8.920702839532169e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 20/71 | LOSS: 9.095534204832456e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 21/71 | LOSS: 9.090664827288657e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 22/71 | LOSS: 9.03799635758016e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 23/71 | LOSS: 9.075444722839165e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 24/71 | LOSS: 9.067188148037531e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 25/71 | LOSS: 9.006613691571356e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 26/71 | LOSS: 9.04033423567954e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 27/71 | LOSS: 9.02224631512841e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 28/71 | LOSS: 8.968704621330634e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 29/71 | LOSS: 9.003873052885562e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 30/71 | LOSS: 8.984919209216323e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 31/71 | LOSS: 9.059430666980006e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 32/71 | LOSS: 9.06075686571333e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 33/71 | LOSS: 9.07069760186834e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 34/71 | LOSS: 9.054924560457169e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 35/71 | LOSS: 9.052043916805511e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 36/71 | LOSS: 9.092923773022375e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 37/71 | LOSS: 9.068200370165869e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 38/71 | LOSS: 9.108394491564608e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 39/71 | LOSS: 9.09760009335514e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 40/71 | LOSS: 9.101875714738371e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 41/71 | LOSS: 9.03991549637881e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 42/71 | LOSS: 9.0149386524836e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 43/71 | LOSS: 9.033463160531854e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 44/71 | LOSS: 9.040301311971335e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 45/71 | LOSS: 8.9900181691242e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 46/71 | LOSS: 8.993197481802472e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 47/71 | LOSS: 8.974948750998616e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 48/71 | LOSS: 8.955164410471465e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 49/71 | LOSS: 8.908979953048402e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 50/71 | LOSS: 8.866428261706674e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 51/71 | LOSS: 8.848032734931621e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 52/71 | LOSS: 8.891119568085381e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 53/71 | LOSS: 8.850211644231524e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 54/71 | LOSS: 8.842468775004487e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 55/71 | LOSS: 8.834288386034522e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 56/71 | LOSS: 8.84098021184368e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 57/71 | LOSS: 8.865409983690339e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 58/71 | LOSS: 8.826029289146822e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 59/71 | LOSS: 8.860769200206657e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 60/71 | LOSS: 8.860449076460324e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 61/71 | LOSS: 8.857274370975868e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 62/71 | LOSS: 8.847064222324646e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 63/71 | LOSS: 8.88443715041376e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 64/71 | LOSS: 8.899864867719141e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 65/71 | LOSS: 8.881732038858129e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 66/71 | LOSS: 8.94116030069009e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 67/71 | LOSS: 8.924581205813892e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 68/71 | LOSS: 8.90463531728871e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 69/71 | LOSS: 8.917818805847284e-06\n",
      "TRAIN: EPOCH 136/1000 | BATCH 70/71 | LOSS: 8.889476094724082e-06\n",
      "VAL: EPOCH 136/1000 | BATCH 0/8 | LOSS: 1.0265010132570751e-05\n",
      "VAL: EPOCH 136/1000 | BATCH 1/8 | LOSS: 1.1584093044803012e-05\n",
      "VAL: EPOCH 136/1000 | BATCH 2/8 | LOSS: 1.0617526944164032e-05\n",
      "VAL: EPOCH 136/1000 | BATCH 3/8 | LOSS: 1.0265031733069918e-05\n",
      "VAL: EPOCH 136/1000 | BATCH 4/8 | LOSS: 1.0354992264183238e-05\n",
      "VAL: EPOCH 136/1000 | BATCH 5/8 | LOSS: 1.0380686944699846e-05\n",
      "VAL: EPOCH 136/1000 | BATCH 6/8 | LOSS: 1.0120007833133318e-05\n",
      "VAL: EPOCH 136/1000 | BATCH 7/8 | LOSS: 1.0047761293208168e-05\n",
      "TRAIN: EPOCH 137/1000 | BATCH 0/71 | LOSS: 8.194532711058855e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 1/71 | LOSS: 8.359527782886289e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 2/71 | LOSS: 9.120778486249037e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 3/71 | LOSS: 8.556732382203336e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 4/71 | LOSS: 8.254392014350742e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 5/71 | LOSS: 8.18749746637574e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 6/71 | LOSS: 8.03027329051734e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 7/71 | LOSS: 8.232252298512321e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 8/71 | LOSS: 8.155851749810003e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 9/71 | LOSS: 8.036275266931625e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 10/71 | LOSS: 7.926135698322799e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 11/71 | LOSS: 8.012031685211696e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 12/71 | LOSS: 8.14771619144057e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 13/71 | LOSS: 8.155407223447192e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 14/71 | LOSS: 8.049029444615978e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 15/71 | LOSS: 7.982218733104673e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 16/71 | LOSS: 8.141118301222485e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 17/71 | LOSS: 8.245995155044107e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 18/71 | LOSS: 8.313722949089042e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 19/71 | LOSS: 8.427338798355777e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 20/71 | LOSS: 8.407004185885722e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 21/71 | LOSS: 8.59044195584085e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 22/71 | LOSS: 8.539047287384797e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 23/71 | LOSS: 8.52863972037691e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 24/71 | LOSS: 8.5444217256736e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 25/71 | LOSS: 8.612424433242548e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 26/71 | LOSS: 8.662746848939503e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 27/71 | LOSS: 8.748845890035487e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 28/71 | LOSS: 8.782929336152377e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 29/71 | LOSS: 8.743952002987499e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 30/71 | LOSS: 8.740723798485217e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 31/71 | LOSS: 8.709896945902074e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 32/71 | LOSS: 8.644161150159286e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 33/71 | LOSS: 8.573658546725602e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 34/71 | LOSS: 8.571148542354682e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 35/71 | LOSS: 8.489583150448096e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 36/71 | LOSS: 8.465351166287783e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 37/71 | LOSS: 8.423868286244166e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 38/71 | LOSS: 8.504631156168985e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 39/71 | LOSS: 8.538808992852865e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 40/71 | LOSS: 8.5644872054151e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 41/71 | LOSS: 8.579108063193936e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 42/71 | LOSS: 8.583193448430393e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 43/71 | LOSS: 8.539783576369784e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 44/71 | LOSS: 8.556267898206391e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 45/71 | LOSS: 8.565408437710214e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 46/71 | LOSS: 8.582133616189512e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 47/71 | LOSS: 8.568461983789652e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 48/71 | LOSS: 8.584853127772197e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 49/71 | LOSS: 8.602836678619497e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 50/71 | LOSS: 8.615087590798936e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 51/71 | LOSS: 8.647890214118748e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 52/71 | LOSS: 8.65241956901473e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 53/71 | LOSS: 8.754554511580392e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 54/71 | LOSS: 8.826182635278779e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 55/71 | LOSS: 8.860562989606738e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 56/71 | LOSS: 8.930881768608009e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 57/71 | LOSS: 8.913126482090366e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 58/71 | LOSS: 8.929102554926215e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 59/71 | LOSS: 8.9140861746273e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 60/71 | LOSS: 8.963431072083577e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 61/71 | LOSS: 8.943198457517053e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 62/71 | LOSS: 8.927050715548888e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 63/71 | LOSS: 8.910010549811886e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 64/71 | LOSS: 8.912500827416526e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 65/71 | LOSS: 8.903401852545688e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 66/71 | LOSS: 8.88182541611561e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 67/71 | LOSS: 8.874389664015325e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 68/71 | LOSS: 8.88775572172523e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 69/71 | LOSS: 8.890907383829472e-06\n",
      "TRAIN: EPOCH 137/1000 | BATCH 70/71 | LOSS: 8.90456010278234e-06\n",
      "VAL: EPOCH 137/1000 | BATCH 0/8 | LOSS: 9.170025805360638e-06\n",
      "VAL: EPOCH 137/1000 | BATCH 1/8 | LOSS: 9.57740257945261e-06\n",
      "VAL: EPOCH 137/1000 | BATCH 2/8 | LOSS: 9.173554341638615e-06\n",
      "VAL: EPOCH 137/1000 | BATCH 3/8 | LOSS: 8.648206630823552e-06\n",
      "VAL: EPOCH 137/1000 | BATCH 4/8 | LOSS: 8.99773931450909e-06\n",
      "VAL: EPOCH 137/1000 | BATCH 5/8 | LOSS: 9.035199885450615e-06\n",
      "VAL: EPOCH 137/1000 | BATCH 6/8 | LOSS: 8.746519987263518e-06\n",
      "VAL: EPOCH 137/1000 | BATCH 7/8 | LOSS: 8.730543640922406e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 0/71 | LOSS: 7.611135970364558e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 1/71 | LOSS: 7.641399633939727e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 2/71 | LOSS: 8.049145890254294e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 3/71 | LOSS: 8.173947321665764e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 4/71 | LOSS: 8.092257030511973e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 5/71 | LOSS: 7.936420843179803e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 6/71 | LOSS: 7.682449709786202e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 7/71 | LOSS: 7.749970848180965e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 8/71 | LOSS: 7.68419204177917e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 9/71 | LOSS: 7.713962759225979e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 10/71 | LOSS: 8.193749512098623e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 11/71 | LOSS: 8.301586793398505e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 12/71 | LOSS: 8.294351748470664e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 13/71 | LOSS: 8.272171109118582e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 14/71 | LOSS: 8.274324469918308e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 15/71 | LOSS: 8.229014298422044e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 16/71 | LOSS: 8.130560966973836e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 17/71 | LOSS: 8.170470891855075e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 18/71 | LOSS: 8.178841510112092e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 19/71 | LOSS: 8.15912787857087e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 20/71 | LOSS: 8.104276730591664e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 21/71 | LOSS: 8.139426406408099e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 22/71 | LOSS: 8.097289913206149e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 23/71 | LOSS: 8.105927861379314e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 24/71 | LOSS: 8.032977584662149e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 25/71 | LOSS: 8.054474147474348e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 26/71 | LOSS: 7.984844379044026e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 27/71 | LOSS: 8.003577233596712e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 28/71 | LOSS: 7.946991853055681e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 29/71 | LOSS: 7.926440124113773e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 30/71 | LOSS: 7.906700331307459e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 31/71 | LOSS: 7.979415030945347e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 32/71 | LOSS: 8.005612843672596e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 33/71 | LOSS: 8.0522512395392e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 34/71 | LOSS: 8.103142876539745e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 35/71 | LOSS: 8.136572672709815e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 36/71 | LOSS: 8.087344323595627e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 37/71 | LOSS: 8.086608539470993e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 38/71 | LOSS: 8.03372044034544e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 39/71 | LOSS: 8.056585795657156e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 40/71 | LOSS: 8.03648725461865e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 41/71 | LOSS: 8.046443294266023e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 42/71 | LOSS: 8.117514722090524e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 43/71 | LOSS: 8.109161099541366e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 44/71 | LOSS: 8.11785055500675e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 45/71 | LOSS: 8.103189123215524e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 46/71 | LOSS: 8.223338817457417e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 47/71 | LOSS: 8.20854665069722e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 48/71 | LOSS: 8.220359474335613e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 49/71 | LOSS: 8.211240865421133e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 50/71 | LOSS: 8.228536907885355e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 51/71 | LOSS: 8.253034147506136e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 52/71 | LOSS: 8.24307248619317e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 53/71 | LOSS: 8.191161121565662e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 54/71 | LOSS: 8.200977481465088e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 55/71 | LOSS: 8.204723737240524e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 56/71 | LOSS: 8.151398129771942e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 57/71 | LOSS: 8.15726366454328e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 58/71 | LOSS: 8.118314380519253e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 59/71 | LOSS: 8.144426321147572e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 60/71 | LOSS: 8.121491705945819e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 61/71 | LOSS: 8.141296634236031e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 62/71 | LOSS: 8.125621604001878e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 63/71 | LOSS: 8.140215165042264e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 64/71 | LOSS: 8.156851345120231e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 65/71 | LOSS: 8.184949978606186e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 66/71 | LOSS: 8.183403516246038e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 67/71 | LOSS: 8.175943649161235e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 68/71 | LOSS: 8.19296064946684e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 69/71 | LOSS: 8.189722906796045e-06\n",
      "TRAIN: EPOCH 138/1000 | BATCH 70/71 | LOSS: 8.181666320088996e-06\n",
      "VAL: EPOCH 138/1000 | BATCH 0/8 | LOSS: 9.333032721769996e-06\n",
      "VAL: EPOCH 138/1000 | BATCH 1/8 | LOSS: 8.755374437896535e-06\n",
      "VAL: EPOCH 138/1000 | BATCH 2/8 | LOSS: 8.953578192934705e-06\n",
      "VAL: EPOCH 138/1000 | BATCH 3/8 | LOSS: 8.69509835865756e-06\n",
      "VAL: EPOCH 138/1000 | BATCH 4/8 | LOSS: 8.943457396526356e-06\n",
      "VAL: EPOCH 138/1000 | BATCH 5/8 | LOSS: 9.002668927375149e-06\n",
      "VAL: EPOCH 138/1000 | BATCH 6/8 | LOSS: 8.525525117875077e-06\n",
      "VAL: EPOCH 138/1000 | BATCH 7/8 | LOSS: 8.503926892444724e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 0/71 | LOSS: 8.73893077368848e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 1/71 | LOSS: 7.756569402772584e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 2/71 | LOSS: 8.14975661948362e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 3/71 | LOSS: 8.509478334417508e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 4/71 | LOSS: 8.523564520146466e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 5/71 | LOSS: 8.67405037752178e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 6/71 | LOSS: 8.505549236100965e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 7/71 | LOSS: 8.601289891885244e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 8/71 | LOSS: 8.535781590681938e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 9/71 | LOSS: 8.639403858978767e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 10/71 | LOSS: 8.86595857578372e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 11/71 | LOSS: 8.879714187060017e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 12/71 | LOSS: 8.684479815621251e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 13/71 | LOSS: 8.654892878569496e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 14/71 | LOSS: 8.577742361618827e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 15/71 | LOSS: 8.829753596728551e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 16/71 | LOSS: 8.963020424262676e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 17/71 | LOSS: 9.005691127741658e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 18/71 | LOSS: 9.151534342029327e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 19/71 | LOSS: 9.063075322046642e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 20/71 | LOSS: 9.07916949342637e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 21/71 | LOSS: 9.01402179145159e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 22/71 | LOSS: 8.95243532716489e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 23/71 | LOSS: 8.912844540039563e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 24/71 | LOSS: 8.90144601726206e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 25/71 | LOSS: 8.843991473930566e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 26/71 | LOSS: 8.854012742597627e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 27/71 | LOSS: 8.793108967048674e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 28/71 | LOSS: 8.702120308514387e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 29/71 | LOSS: 8.633264269519713e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 30/71 | LOSS: 8.615915908927902e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 31/71 | LOSS: 8.594843833975574e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 32/71 | LOSS: 8.64125995852219e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 33/71 | LOSS: 8.680852060398833e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 34/71 | LOSS: 8.70568722218325e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 35/71 | LOSS: 8.707021594192258e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 36/71 | LOSS: 8.710871350411105e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 37/71 | LOSS: 8.69038138405717e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 38/71 | LOSS: 8.705470568551951e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 39/71 | LOSS: 8.698087901848339e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 40/71 | LOSS: 8.701342122970723e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 41/71 | LOSS: 8.683299072453382e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 42/71 | LOSS: 8.679284556544803e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 43/71 | LOSS: 8.667739755517026e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 44/71 | LOSS: 8.635662965793421e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 45/71 | LOSS: 8.631964449349375e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 46/71 | LOSS: 8.648247011420764e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 47/71 | LOSS: 8.643978067842303e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 48/71 | LOSS: 8.633357909423707e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 49/71 | LOSS: 8.647909799037735e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 50/71 | LOSS: 8.645448325100291e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 51/71 | LOSS: 8.633330130928349e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 52/71 | LOSS: 8.594053999228702e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 53/71 | LOSS: 8.583025831925355e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 54/71 | LOSS: 8.55905484720345e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 55/71 | LOSS: 8.48705864850022e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 56/71 | LOSS: 8.503755636187985e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 57/71 | LOSS: 8.501880584249037e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 58/71 | LOSS: 8.457778491547441e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 59/71 | LOSS: 8.441868991819017e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 60/71 | LOSS: 8.465291743885275e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 61/71 | LOSS: 8.44601690768022e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 62/71 | LOSS: 8.441353021760098e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 63/71 | LOSS: 8.431168915024045e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 64/71 | LOSS: 8.417724571840013e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 65/71 | LOSS: 8.375461682204111e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 66/71 | LOSS: 8.402873777752698e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 67/71 | LOSS: 8.417763203450544e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 68/71 | LOSS: 8.425008336812228e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 69/71 | LOSS: 8.443371065628265e-06\n",
      "TRAIN: EPOCH 139/1000 | BATCH 70/71 | LOSS: 8.420203793205692e-06\n",
      "VAL: EPOCH 139/1000 | BATCH 0/8 | LOSS: 9.532660442346241e-06\n",
      "VAL: EPOCH 139/1000 | BATCH 1/8 | LOSS: 1.0551761533861281e-05\n",
      "VAL: EPOCH 139/1000 | BATCH 2/8 | LOSS: 1.02934042539952e-05\n",
      "VAL: EPOCH 139/1000 | BATCH 3/8 | LOSS: 1.0562257330093416e-05\n",
      "VAL: EPOCH 139/1000 | BATCH 4/8 | LOSS: 1.0702304462029132e-05\n",
      "VAL: EPOCH 139/1000 | BATCH 5/8 | LOSS: 1.0735334853961831e-05\n",
      "VAL: EPOCH 139/1000 | BATCH 6/8 | LOSS: 1.0457664887196319e-05\n",
      "VAL: EPOCH 139/1000 | BATCH 7/8 | LOSS: 1.0282461744282045e-05\n",
      "TRAIN: EPOCH 140/1000 | BATCH 0/71 | LOSS: 1.0620587090670597e-05\n",
      "TRAIN: EPOCH 140/1000 | BATCH 1/71 | LOSS: 9.771639270184096e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 2/71 | LOSS: 9.29897699582701e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 3/71 | LOSS: 8.957559657574166e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 4/71 | LOSS: 9.633868830860593e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 5/71 | LOSS: 9.52730897552101e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 6/71 | LOSS: 9.223615896709297e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 7/71 | LOSS: 9.43928836250052e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 8/71 | LOSS: 9.407542519410425e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 9/71 | LOSS: 9.108932817980531e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 10/71 | LOSS: 9.113075438191034e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 11/71 | LOSS: 8.994634943822652e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 12/71 | LOSS: 8.948899833810105e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 13/71 | LOSS: 8.822426382591533e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 14/71 | LOSS: 8.683392873839087e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 15/71 | LOSS: 8.691996328025198e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 16/71 | LOSS: 8.612808414909523e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 17/71 | LOSS: 8.597750012187236e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 18/71 | LOSS: 8.547496499407939e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 19/71 | LOSS: 8.523864516973845e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 20/71 | LOSS: 8.51530958676622e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 21/71 | LOSS: 8.395925595488569e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 22/71 | LOSS: 8.384712758846074e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 23/71 | LOSS: 8.286913195358162e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 24/71 | LOSS: 8.308832620969042e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 25/71 | LOSS: 8.281512096940647e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 26/71 | LOSS: 8.311627189633092e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 27/71 | LOSS: 8.469511093218379e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 28/71 | LOSS: 8.460601622636174e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 29/71 | LOSS: 8.393463637427583e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 30/71 | LOSS: 8.387075731955139e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 31/71 | LOSS: 8.3456376387403e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 32/71 | LOSS: 8.339917423489334e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 33/71 | LOSS: 8.298331147884978e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 34/71 | LOSS: 8.260634552113646e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 35/71 | LOSS: 8.184138727705835e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 36/71 | LOSS: 8.204999200667796e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 37/71 | LOSS: 8.175315817999562e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 38/71 | LOSS: 8.222787971298647e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 39/71 | LOSS: 8.258587820364483e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 40/71 | LOSS: 8.284615377328365e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 41/71 | LOSS: 8.288355935465439e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 42/71 | LOSS: 8.272245331893426e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 43/71 | LOSS: 8.222601805259314e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 44/71 | LOSS: 8.227316498555916e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 45/71 | LOSS: 8.176738724021403e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 46/71 | LOSS: 8.168872498456349e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 47/71 | LOSS: 8.162137741389111e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 48/71 | LOSS: 8.14218187120527e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 49/71 | LOSS: 8.15069342024799e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 50/71 | LOSS: 8.137117567778606e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 51/71 | LOSS: 8.159919117973073e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 52/71 | LOSS: 8.156220152365145e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 53/71 | LOSS: 8.128242724423115e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 54/71 | LOSS: 8.11240509144475e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 55/71 | LOSS: 8.097056122226474e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 56/71 | LOSS: 8.113429861336236e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 57/71 | LOSS: 8.11006632296806e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 58/71 | LOSS: 8.099156514355731e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 59/71 | LOSS: 8.10477433030125e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 60/71 | LOSS: 8.097205431650004e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 61/71 | LOSS: 8.105165600300684e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 62/71 | LOSS: 8.110818011378537e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 63/71 | LOSS: 8.0864022180549e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 64/71 | LOSS: 8.147922646737872e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 65/71 | LOSS: 8.125934445243646e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 66/71 | LOSS: 8.129353583212628e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 67/71 | LOSS: 8.136842352006865e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 68/71 | LOSS: 8.19145420330418e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 69/71 | LOSS: 8.18136051878225e-06\n",
      "TRAIN: EPOCH 140/1000 | BATCH 70/71 | LOSS: 8.174558651019101e-06\n",
      "VAL: EPOCH 140/1000 | BATCH 0/8 | LOSS: 1.1710779290297069e-05\n",
      "VAL: EPOCH 140/1000 | BATCH 1/8 | LOSS: 1.1082967830589041e-05\n",
      "VAL: EPOCH 140/1000 | BATCH 2/8 | LOSS: 1.0879753366073905e-05\n",
      "VAL: EPOCH 140/1000 | BATCH 3/8 | LOSS: 9.951150218512339e-06\n",
      "VAL: EPOCH 140/1000 | BATCH 4/8 | LOSS: 1.0498145911697065e-05\n",
      "VAL: EPOCH 140/1000 | BATCH 5/8 | LOSS: 1.0432005410621059e-05\n",
      "VAL: EPOCH 140/1000 | BATCH 6/8 | LOSS: 1.0170207588089397e-05\n",
      "VAL: EPOCH 140/1000 | BATCH 7/8 | LOSS: 1.0231181192921213e-05\n",
      "TRAIN: EPOCH 141/1000 | BATCH 0/71 | LOSS: 9.507060894975439e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 1/71 | LOSS: 9.04313401406398e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 2/71 | LOSS: 8.070011214537468e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 3/71 | LOSS: 7.98961775672069e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 4/71 | LOSS: 8.293226437672275e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 5/71 | LOSS: 8.180813741394862e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 6/71 | LOSS: 8.08796604750179e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 7/71 | LOSS: 8.257727472482657e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 8/71 | LOSS: 8.194102317954983e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 9/71 | LOSS: 8.074912193478667e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 10/71 | LOSS: 8.234469822293084e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 11/71 | LOSS: 8.215146522161376e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 12/71 | LOSS: 8.16433559800946e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 13/71 | LOSS: 8.199545456071583e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 14/71 | LOSS: 8.21582434582524e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 15/71 | LOSS: 8.228428043821623e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 16/71 | LOSS: 8.31327275484321e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 17/71 | LOSS: 8.309175629821968e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 18/71 | LOSS: 8.493899019269616e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 19/71 | LOSS: 8.60160539559729e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 20/71 | LOSS: 8.565749513287474e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 21/71 | LOSS: 8.749961745541606e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 22/71 | LOSS: 8.79037341529392e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 23/71 | LOSS: 8.912371868063929e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 24/71 | LOSS: 8.961075727711432e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 25/71 | LOSS: 8.964829213744872e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 26/71 | LOSS: 9.096308396557881e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 27/71 | LOSS: 9.084731248419433e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 28/71 | LOSS: 9.105501546933926e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 29/71 | LOSS: 9.073244837054516e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 30/71 | LOSS: 9.080124923897411e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 31/71 | LOSS: 9.04390648770459e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 32/71 | LOSS: 9.000141041972462e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 33/71 | LOSS: 9.01460987948025e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 34/71 | LOSS: 8.999755196522789e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 35/71 | LOSS: 8.974044956428568e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 36/71 | LOSS: 8.989851021151235e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 37/71 | LOSS: 9.012522361062062e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 38/71 | LOSS: 9.005319952848367e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 39/71 | LOSS: 8.996752512757666e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 40/71 | LOSS: 9.037102910685751e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 41/71 | LOSS: 9.031844326403058e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 42/71 | LOSS: 9.019441909744232e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 43/71 | LOSS: 9.028728362119926e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 44/71 | LOSS: 9.02327742046004e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 45/71 | LOSS: 9.009823909012914e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 46/71 | LOSS: 9.006641816318115e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 47/71 | LOSS: 9.015876097843526e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 48/71 | LOSS: 8.993924522507528e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 49/71 | LOSS: 8.960571876741597e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 50/71 | LOSS: 8.921743675968815e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 51/71 | LOSS: 8.881774131343544e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 52/71 | LOSS: 8.838593554894444e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 53/71 | LOSS: 8.78803555476932e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 54/71 | LOSS: 8.767199952589263e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 55/71 | LOSS: 8.734957840975507e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 56/71 | LOSS: 8.709554908579631e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 57/71 | LOSS: 8.698901606764813e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 58/71 | LOSS: 8.690776112262492e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 59/71 | LOSS: 8.66558869650665e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 60/71 | LOSS: 8.673964195443913e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 61/71 | LOSS: 8.691747274086435e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 62/71 | LOSS: 8.706095069924014e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 63/71 | LOSS: 8.718154560938274e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 64/71 | LOSS: 8.743428882623378e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 65/71 | LOSS: 8.716594892934099e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 66/71 | LOSS: 8.691286900523379e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 67/71 | LOSS: 8.674260290236442e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 68/71 | LOSS: 8.672814786130903e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 69/71 | LOSS: 8.643767192292476e-06\n",
      "TRAIN: EPOCH 141/1000 | BATCH 70/71 | LOSS: 8.625865927864601e-06\n",
      "VAL: EPOCH 141/1000 | BATCH 0/8 | LOSS: 1.045349745254498e-05\n",
      "VAL: EPOCH 141/1000 | BATCH 1/8 | LOSS: 1.0352354365750216e-05\n",
      "VAL: EPOCH 141/1000 | BATCH 2/8 | LOSS: 9.735115478785398e-06\n",
      "VAL: EPOCH 141/1000 | BATCH 3/8 | LOSS: 9.081904181584832e-06\n",
      "VAL: EPOCH 141/1000 | BATCH 4/8 | LOSS: 9.341852455690969e-06\n",
      "VAL: EPOCH 141/1000 | BATCH 5/8 | LOSS: 9.471481613824531e-06\n",
      "VAL: EPOCH 141/1000 | BATCH 6/8 | LOSS: 9.293424487363414e-06\n",
      "VAL: EPOCH 141/1000 | BATCH 7/8 | LOSS: 9.212123245561088e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 0/71 | LOSS: 8.15183921076823e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 1/71 | LOSS: 7.5889290656050434e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 2/71 | LOSS: 7.2650429198498996e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 3/71 | LOSS: 7.63404057124717e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 4/71 | LOSS: 7.491147971450118e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 5/71 | LOSS: 7.5062215879976675e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 6/71 | LOSS: 7.670939274768379e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 7/71 | LOSS: 7.813357854047354e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 8/71 | LOSS: 7.661588369956007e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 9/71 | LOSS: 7.656466959815588e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 10/71 | LOSS: 7.700836869678989e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 11/71 | LOSS: 7.84547974793289e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 12/71 | LOSS: 8.001888545550173e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 13/71 | LOSS: 8.090793244264205e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 14/71 | LOSS: 7.996821265502755e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 15/71 | LOSS: 8.167555421323414e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 16/71 | LOSS: 8.117554818240665e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 17/71 | LOSS: 8.068741155713926e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 18/71 | LOSS: 8.02715222610124e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 19/71 | LOSS: 7.949421888042708e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 20/71 | LOSS: 7.951982730619853e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 21/71 | LOSS: 7.987316456075694e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 22/71 | LOSS: 7.980977384052139e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 23/71 | LOSS: 8.052476308269737e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 24/71 | LOSS: 8.063278655754403e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 25/71 | LOSS: 8.022380353185536e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 26/71 | LOSS: 7.946415499410014e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 27/71 | LOSS: 7.967175195387557e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 28/71 | LOSS: 7.951069980255601e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 29/71 | LOSS: 7.912702615916108e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 30/71 | LOSS: 7.850455873384846e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 31/71 | LOSS: 7.88326363476699e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 32/71 | LOSS: 7.927647609242491e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 33/71 | LOSS: 7.908998641676804e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 34/71 | LOSS: 7.865013880551227e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 35/71 | LOSS: 7.870909093475751e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 36/71 | LOSS: 7.871435173001373e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 37/71 | LOSS: 7.847850455570887e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 38/71 | LOSS: 7.923386116696443e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 39/71 | LOSS: 7.93881957861231e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 40/71 | LOSS: 7.96343810423689e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 41/71 | LOSS: 7.980137860656777e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 42/71 | LOSS: 7.959063919389697e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 43/71 | LOSS: 7.975503236097707e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 44/71 | LOSS: 7.948227105063981e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 45/71 | LOSS: 7.9550538885087e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 46/71 | LOSS: 8.004472168435545e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 47/71 | LOSS: 8.002817869131226e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 48/71 | LOSS: 7.997824347828044e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 49/71 | LOSS: 7.974858990564825e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 50/71 | LOSS: 8.005987174507099e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 51/71 | LOSS: 7.966146182088078e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 52/71 | LOSS: 7.950678897237972e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 53/71 | LOSS: 7.974341205799309e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 54/71 | LOSS: 8.030867427971151e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 55/71 | LOSS: 8.022353621787001e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 56/71 | LOSS: 8.064652304216125e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 57/71 | LOSS: 8.066772289598232e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 58/71 | LOSS: 8.074635612286066e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 59/71 | LOSS: 8.062944160277159e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 60/71 | LOSS: 8.07939742847592e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 61/71 | LOSS: 8.033135578267731e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 62/71 | LOSS: 8.046451721528437e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 63/71 | LOSS: 8.043406815261278e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 64/71 | LOSS: 8.033130689000245e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 65/71 | LOSS: 8.070429830144294e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 66/71 | LOSS: 8.065570936970058e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 67/71 | LOSS: 8.05818480972341e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 68/71 | LOSS: 8.051949974459063e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 69/71 | LOSS: 8.022128213269752e-06\n",
      "TRAIN: EPOCH 142/1000 | BATCH 70/71 | LOSS: 8.108570391029811e-06\n",
      "VAL: EPOCH 142/1000 | BATCH 0/8 | LOSS: 8.236506801040377e-06\n",
      "VAL: EPOCH 142/1000 | BATCH 1/8 | LOSS: 8.359258117707213e-06\n",
      "VAL: EPOCH 142/1000 | BATCH 2/8 | LOSS: 8.139097796326192e-06\n",
      "VAL: EPOCH 142/1000 | BATCH 3/8 | LOSS: 8.271808383142343e-06\n",
      "VAL: EPOCH 142/1000 | BATCH 4/8 | LOSS: 8.313100988743826e-06\n",
      "VAL: EPOCH 142/1000 | BATCH 5/8 | LOSS: 8.238861179658366e-06\n",
      "VAL: EPOCH 142/1000 | BATCH 6/8 | LOSS: 7.843883785036659e-06\n",
      "VAL: EPOCH 142/1000 | BATCH 7/8 | LOSS: 7.845980803722341e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 0/71 | LOSS: 7.806233043083921e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 1/71 | LOSS: 8.856761724018725e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 2/71 | LOSS: 8.059379524638643e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 3/71 | LOSS: 7.889911898928403e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 4/71 | LOSS: 8.4328964476299e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 5/71 | LOSS: 8.50872819076661e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 6/71 | LOSS: 8.649076530023844e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 7/71 | LOSS: 8.585568082253303e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 8/71 | LOSS: 8.45897001353377e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 9/71 | LOSS: 8.249896200140939e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 10/71 | LOSS: 8.507632297881752e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 11/71 | LOSS: 8.258176156535532e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 12/71 | LOSS: 8.080780354738146e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 13/71 | LOSS: 7.982302609629447e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 14/71 | LOSS: 8.05162596104007e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 15/71 | LOSS: 8.060293765765891e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 16/71 | LOSS: 7.96357300500591e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 17/71 | LOSS: 8.07211017672671e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 18/71 | LOSS: 8.037109900645814e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 19/71 | LOSS: 8.043655270739691e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 20/71 | LOSS: 8.030776674180137e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 21/71 | LOSS: 8.080553140065273e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 22/71 | LOSS: 8.092698724636728e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 23/71 | LOSS: 8.046431370682209e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 24/71 | LOSS: 8.08225880973623e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 25/71 | LOSS: 8.08674756696676e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 26/71 | LOSS: 8.140908078160094e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 27/71 | LOSS: 8.158218682703072e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 28/71 | LOSS: 8.194148104566607e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 29/71 | LOSS: 8.14506943243032e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 30/71 | LOSS: 8.110700095600013e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 31/71 | LOSS: 8.083484601684177e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 32/71 | LOSS: 8.06651915450912e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 33/71 | LOSS: 8.042199557594894e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 34/71 | LOSS: 8.045981862129079e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 35/71 | LOSS: 8.051726897267346e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 36/71 | LOSS: 8.057038731085658e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 37/71 | LOSS: 8.074201476024187e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 38/71 | LOSS: 8.13322960824828e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 39/71 | LOSS: 8.080824591161217e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 40/71 | LOSS: 8.182193396794724e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 41/71 | LOSS: 8.303621966444466e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 42/71 | LOSS: 8.28813610792696e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 43/71 | LOSS: 8.301613413343809e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 44/71 | LOSS: 8.430329338201166e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 45/71 | LOSS: 8.419570606391694e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 46/71 | LOSS: 8.404020515259386e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 47/71 | LOSS: 8.436577047632454e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 48/71 | LOSS: 8.46843496609446e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 49/71 | LOSS: 8.434178880634136e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 50/71 | LOSS: 8.407192108636224e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 51/71 | LOSS: 8.433444349975616e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 52/71 | LOSS: 8.42512839160506e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 53/71 | LOSS: 8.37651835657501e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 54/71 | LOSS: 8.402740720405497e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 55/71 | LOSS: 8.418586535948894e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 56/71 | LOSS: 8.374097345503079e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 57/71 | LOSS: 8.380311743982521e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 58/71 | LOSS: 8.453280747359051e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 59/71 | LOSS: 8.477707524434663e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 60/71 | LOSS: 8.490413627620656e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 61/71 | LOSS: 8.491671994101302e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 62/71 | LOSS: 8.494232465708924e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 63/71 | LOSS: 8.51035045457138e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 64/71 | LOSS: 8.492415662868343e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 65/71 | LOSS: 8.536335647504805e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 66/71 | LOSS: 8.52139734223471e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 67/71 | LOSS: 8.558394684394657e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 68/71 | LOSS: 8.542024287622105e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 69/71 | LOSS: 8.539141955614988e-06\n",
      "TRAIN: EPOCH 143/1000 | BATCH 70/71 | LOSS: 8.493908500869867e-06\n",
      "VAL: EPOCH 143/1000 | BATCH 0/8 | LOSS: 8.567070835852064e-06\n",
      "VAL: EPOCH 143/1000 | BATCH 1/8 | LOSS: 8.940497082221555e-06\n",
      "VAL: EPOCH 143/1000 | BATCH 2/8 | LOSS: 8.322957910422701e-06\n",
      "VAL: EPOCH 143/1000 | BATCH 3/8 | LOSS: 8.38637788547203e-06\n",
      "VAL: EPOCH 143/1000 | BATCH 4/8 | LOSS: 8.462420373689383e-06\n",
      "VAL: EPOCH 143/1000 | BATCH 5/8 | LOSS: 8.33458731600937e-06\n",
      "VAL: EPOCH 143/1000 | BATCH 6/8 | LOSS: 8.007348956847896e-06\n",
      "VAL: EPOCH 143/1000 | BATCH 7/8 | LOSS: 7.966852876961639e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 0/71 | LOSS: 8.042531590035651e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 1/71 | LOSS: 9.079375104192877e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 2/71 | LOSS: 8.61653643369209e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 3/71 | LOSS: 8.333627761203388e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 4/71 | LOSS: 8.639870520710246e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 5/71 | LOSS: 8.385362662011175e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 6/71 | LOSS: 8.107516155827657e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 7/71 | LOSS: 7.909999567345949e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 8/71 | LOSS: 7.84964302308961e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 9/71 | LOSS: 8.028936554183019e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 10/71 | LOSS: 8.03946245882385e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 11/71 | LOSS: 8.080805021866885e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 12/71 | LOSS: 7.954310044274629e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 13/71 | LOSS: 8.102234914986184e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 14/71 | LOSS: 7.984325899694037e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 15/71 | LOSS: 7.976307841772723e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 16/71 | LOSS: 7.962458847246393e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 17/71 | LOSS: 7.993799524936347e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 18/71 | LOSS: 8.098371620833764e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 19/71 | LOSS: 8.138070802488073e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 20/71 | LOSS: 8.145217634591972e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 21/71 | LOSS: 8.213105440352757e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 22/71 | LOSS: 8.257999287825838e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 23/71 | LOSS: 8.22649676971802e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 24/71 | LOSS: 8.412228362431052e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 25/71 | LOSS: 8.420853698855633e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 26/71 | LOSS: 8.62978445352551e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 27/71 | LOSS: 8.576888002867886e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 28/71 | LOSS: 8.69745588653163e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 29/71 | LOSS: 8.659924302871029e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 30/71 | LOSS: 8.622567381075164e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 31/71 | LOSS: 8.712770011243265e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 32/71 | LOSS: 8.688922924083078e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 33/71 | LOSS: 8.723709844857427e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 34/71 | LOSS: 8.772508252669858e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 35/71 | LOSS: 8.73444034570841e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 36/71 | LOSS: 8.994225690666763e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 37/71 | LOSS: 8.939793683806018e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 38/71 | LOSS: 9.133437113352837e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 39/71 | LOSS: 9.197846497954743e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 40/71 | LOSS: 9.209979951975118e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 41/71 | LOSS: 9.324958119534477e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 42/71 | LOSS: 9.309468792750077e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 43/71 | LOSS: 9.409098125119767e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 44/71 | LOSS: 9.418599620403257e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 45/71 | LOSS: 9.362488734278367e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 46/71 | LOSS: 9.388100758595566e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 47/71 | LOSS: 9.339123058301388e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 48/71 | LOSS: 9.30678245878175e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 49/71 | LOSS: 9.355174806842115e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 50/71 | LOSS: 9.392737375296123e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 51/71 | LOSS: 9.403022606700408e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 52/71 | LOSS: 9.342086681465175e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 53/71 | LOSS: 9.320878647216617e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 54/71 | LOSS: 9.28785145581721e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 55/71 | LOSS: 9.230319611625809e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 56/71 | LOSS: 9.184012820664895e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 57/71 | LOSS: 9.162757758782265e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 58/71 | LOSS: 9.128132421472525e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 59/71 | LOSS: 9.078871069808277e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 60/71 | LOSS: 9.04662595933244e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 61/71 | LOSS: 9.016807494375708e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 62/71 | LOSS: 8.962479143288903e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 63/71 | LOSS: 8.932842135322971e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 64/71 | LOSS: 8.891422035398696e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 65/71 | LOSS: 8.87438215723898e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 66/71 | LOSS: 8.856550123967811e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 67/71 | LOSS: 8.879254444175732e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 68/71 | LOSS: 8.858818137673222e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 69/71 | LOSS: 8.848749153652794e-06\n",
      "TRAIN: EPOCH 144/1000 | BATCH 70/71 | LOSS: 8.855233138835412e-06\n",
      "VAL: EPOCH 144/1000 | BATCH 0/8 | LOSS: 8.6920535977697e-06\n",
      "VAL: EPOCH 144/1000 | BATCH 1/8 | LOSS: 9.424905329069588e-06\n",
      "VAL: EPOCH 144/1000 | BATCH 2/8 | LOSS: 8.771160689017657e-06\n",
      "VAL: EPOCH 144/1000 | BATCH 3/8 | LOSS: 8.770868248575425e-06\n",
      "VAL: EPOCH 144/1000 | BATCH 4/8 | LOSS: 8.949263337854063e-06\n",
      "VAL: EPOCH 144/1000 | BATCH 5/8 | LOSS: 8.927163359354987e-06\n",
      "VAL: EPOCH 144/1000 | BATCH 6/8 | LOSS: 8.690877945939843e-06\n",
      "VAL: EPOCH 144/1000 | BATCH 7/8 | LOSS: 8.565221776279941e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 0/71 | LOSS: 1.0150150046683848e-05\n",
      "TRAIN: EPOCH 145/1000 | BATCH 1/71 | LOSS: 1.07365249277791e-05\n",
      "TRAIN: EPOCH 145/1000 | BATCH 2/71 | LOSS: 1.0931525745642526e-05\n",
      "TRAIN: EPOCH 145/1000 | BATCH 3/71 | LOSS: 1.0525517609494273e-05\n",
      "TRAIN: EPOCH 145/1000 | BATCH 4/71 | LOSS: 1.0537580055824947e-05\n",
      "TRAIN: EPOCH 145/1000 | BATCH 5/71 | LOSS: 1.0998132286961967e-05\n",
      "TRAIN: EPOCH 145/1000 | BATCH 6/71 | LOSS: 1.0700710975340502e-05\n",
      "TRAIN: EPOCH 145/1000 | BATCH 7/71 | LOSS: 1.016120637586937e-05\n",
      "TRAIN: EPOCH 145/1000 | BATCH 8/71 | LOSS: 1.0066990348099756e-05\n",
      "TRAIN: EPOCH 145/1000 | BATCH 9/71 | LOSS: 1.036001817737997e-05\n",
      "TRAIN: EPOCH 145/1000 | BATCH 10/71 | LOSS: 1.0009584953381404e-05\n",
      "TRAIN: EPOCH 145/1000 | BATCH 11/71 | LOSS: 9.758766092697138e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 12/71 | LOSS: 9.766596436887854e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 13/71 | LOSS: 9.673074763278627e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 14/71 | LOSS: 9.591413345333422e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 15/71 | LOSS: 9.424331238960804e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 16/71 | LOSS: 9.448240368382198e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 17/71 | LOSS: 9.436158532278367e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 18/71 | LOSS: 9.262842204885888e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 19/71 | LOSS: 9.299850148636324e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 20/71 | LOSS: 9.19914464247995e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 21/71 | LOSS: 9.165963030516815e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 22/71 | LOSS: 9.121584278783184e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 23/71 | LOSS: 9.03087754977605e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 24/71 | LOSS: 8.996446304081474e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 25/71 | LOSS: 8.889428115183433e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 26/71 | LOSS: 8.795086917719649e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 27/71 | LOSS: 8.785601916575356e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 28/71 | LOSS: 8.777270146499083e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 29/71 | LOSS: 8.779860127106077e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 30/71 | LOSS: 8.725170382897505e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 31/71 | LOSS: 8.684123642410668e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 32/71 | LOSS: 8.669981249995269e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 33/71 | LOSS: 8.629538770572467e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 34/71 | LOSS: 8.56631218734297e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 35/71 | LOSS: 8.54603632029062e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 36/71 | LOSS: 8.51512194876213e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 37/71 | LOSS: 8.490143284832716e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 38/71 | LOSS: 8.483560962374144e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 39/71 | LOSS: 8.48591723752179e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 40/71 | LOSS: 8.454395012405456e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 41/71 | LOSS: 8.405003178441326e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 42/71 | LOSS: 8.432918960796426e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 43/71 | LOSS: 8.412581740396325e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 44/71 | LOSS: 8.416275512798973e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 45/71 | LOSS: 8.387372763931838e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 46/71 | LOSS: 8.334931011477044e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 47/71 | LOSS: 8.341060218223598e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 48/71 | LOSS: 8.319996372940332e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 49/71 | LOSS: 8.288753815577365e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 50/71 | LOSS: 8.263484482549579e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 51/71 | LOSS: 8.240633449601704e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 52/71 | LOSS: 8.291717393766925e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 53/71 | LOSS: 8.280952367082742e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 54/71 | LOSS: 8.268081125028071e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 55/71 | LOSS: 8.305390687317932e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 56/71 | LOSS: 8.301761952903339e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 57/71 | LOSS: 8.296555475242023e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 58/71 | LOSS: 8.282907640670477e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 59/71 | LOSS: 8.264332503434465e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 60/71 | LOSS: 8.292690940610644e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 61/71 | LOSS: 8.266083942808403e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 62/71 | LOSS: 8.271303295981535e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 63/71 | LOSS: 8.235047623372793e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 64/71 | LOSS: 8.238772415307511e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 65/71 | LOSS: 8.222462252943483e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 66/71 | LOSS: 8.25660360003813e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 67/71 | LOSS: 8.24180759697329e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 68/71 | LOSS: 8.261058841183683e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 69/71 | LOSS: 8.228489208964415e-06\n",
      "TRAIN: EPOCH 145/1000 | BATCH 70/71 | LOSS: 8.193258323239796e-06\n",
      "VAL: EPOCH 145/1000 | BATCH 0/8 | LOSS: 9.155255611403845e-06\n",
      "VAL: EPOCH 145/1000 | BATCH 1/8 | LOSS: 8.578381311963312e-06\n",
      "VAL: EPOCH 145/1000 | BATCH 2/8 | LOSS: 8.200209170657521e-06\n",
      "VAL: EPOCH 145/1000 | BATCH 3/8 | LOSS: 7.828526577213779e-06\n",
      "VAL: EPOCH 145/1000 | BATCH 4/8 | LOSS: 7.997852662811056e-06\n",
      "VAL: EPOCH 145/1000 | BATCH 5/8 | LOSS: 7.885001044390568e-06\n",
      "VAL: EPOCH 145/1000 | BATCH 6/8 | LOSS: 7.538256340922089e-06\n",
      "VAL: EPOCH 145/1000 | BATCH 7/8 | LOSS: 7.565899466044357e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 0/71 | LOSS: 7.2873945100582205e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 1/71 | LOSS: 7.497403657907853e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 2/71 | LOSS: 8.325049141906979e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 3/71 | LOSS: 8.228317028624588e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 4/71 | LOSS: 8.111524402920623e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 5/71 | LOSS: 8.35409097514154e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 6/71 | LOSS: 8.228832094963373e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 7/71 | LOSS: 8.13483200090559e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 8/71 | LOSS: 8.102795719524794e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 9/71 | LOSS: 7.967767487571109e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 10/71 | LOSS: 7.782900907345306e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 11/71 | LOSS: 8.131072528764586e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 12/71 | LOSS: 8.084707727763229e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 13/71 | LOSS: 8.243288513248054e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 14/71 | LOSS: 8.184703165170504e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 15/71 | LOSS: 8.120051433024855e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 16/71 | LOSS: 8.289411171597963e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 17/71 | LOSS: 8.258975438164069e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 18/71 | LOSS: 8.336207931853623e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 19/71 | LOSS: 8.42106248910568e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 20/71 | LOSS: 8.3997406796871e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 21/71 | LOSS: 8.355571108867563e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 22/71 | LOSS: 8.3593993464171e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 23/71 | LOSS: 8.426219740916471e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 24/71 | LOSS: 8.391973969992251e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 25/71 | LOSS: 8.347369710183837e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 26/71 | LOSS: 8.384899703944231e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 27/71 | LOSS: 8.54032082640645e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 28/71 | LOSS: 8.68658560585604e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 29/71 | LOSS: 8.601009600776403e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 30/71 | LOSS: 8.850841162887762e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 31/71 | LOSS: 8.912336511457397e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 32/71 | LOSS: 8.879762197356092e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 33/71 | LOSS: 8.862319378642818e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 34/71 | LOSS: 8.961072074141287e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 35/71 | LOSS: 9.003601689983043e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 36/71 | LOSS: 8.954249748254566e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 37/71 | LOSS: 8.967291550059025e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 38/71 | LOSS: 8.918421990062761e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 39/71 | LOSS: 8.917200227642752e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 40/71 | LOSS: 8.89471196542883e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 41/71 | LOSS: 8.857092812572123e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 42/71 | LOSS: 8.882541055225924e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 43/71 | LOSS: 8.78333651144203e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 44/71 | LOSS: 8.816616001341673e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 45/71 | LOSS: 8.831209124368995e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 46/71 | LOSS: 8.846289840724414e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 47/71 | LOSS: 8.856577087120362e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 48/71 | LOSS: 8.83969763593513e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 49/71 | LOSS: 8.846252649163944e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 50/71 | LOSS: 8.835424550317625e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 51/71 | LOSS: 8.7836803004393e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 52/71 | LOSS: 8.755758287561377e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 53/71 | LOSS: 8.743446254879086e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 54/71 | LOSS: 8.735659825602356e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 55/71 | LOSS: 8.706173283761537e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 56/71 | LOSS: 8.711928362652288e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 57/71 | LOSS: 8.707736838161885e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 58/71 | LOSS: 8.671682881883452e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 59/71 | LOSS: 8.678946862043328e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 60/71 | LOSS: 8.6716528488471e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 61/71 | LOSS: 8.691359029868298e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 62/71 | LOSS: 8.66324977785495e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 63/71 | LOSS: 8.651770109224799e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 64/71 | LOSS: 8.642923778377455e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 65/71 | LOSS: 8.614757276022415e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 66/71 | LOSS: 8.593864651575403e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 67/71 | LOSS: 8.613711603335533e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 68/71 | LOSS: 8.603339316270675e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 69/71 | LOSS: 8.604860620055531e-06\n",
      "TRAIN: EPOCH 146/1000 | BATCH 70/71 | LOSS: 8.576106678523963e-06\n",
      "VAL: EPOCH 146/1000 | BATCH 0/8 | LOSS: 8.669363523949869e-06\n",
      "VAL: EPOCH 146/1000 | BATCH 1/8 | LOSS: 8.955772500485182e-06\n",
      "VAL: EPOCH 146/1000 | BATCH 2/8 | LOSS: 8.101223556877812e-06\n",
      "VAL: EPOCH 146/1000 | BATCH 3/8 | LOSS: 8.034597271944222e-06\n",
      "VAL: EPOCH 146/1000 | BATCH 4/8 | LOSS: 8.13441229183809e-06\n",
      "VAL: EPOCH 146/1000 | BATCH 5/8 | LOSS: 8.07044011708058e-06\n",
      "VAL: EPOCH 146/1000 | BATCH 6/8 | LOSS: 7.834000630932028e-06\n",
      "VAL: EPOCH 146/1000 | BATCH 7/8 | LOSS: 7.719896927937953e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 0/71 | LOSS: 7.911321517894976e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 1/71 | LOSS: 7.178115538408747e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 2/71 | LOSS: 7.5467963445892865e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 3/71 | LOSS: 7.739126658634632e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 4/71 | LOSS: 8.010348938114476e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 5/71 | LOSS: 7.808197627430976e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 6/71 | LOSS: 7.889736025390448e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 7/71 | LOSS: 7.807943973148213e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 8/71 | LOSS: 7.785480799308667e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 9/71 | LOSS: 7.885544619057328e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 10/71 | LOSS: 7.849967832126739e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 11/71 | LOSS: 7.874748007452581e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 12/71 | LOSS: 7.864491355856164e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 13/71 | LOSS: 7.92945489333111e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 14/71 | LOSS: 7.943297290087987e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 15/71 | LOSS: 8.136923611345992e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 16/71 | LOSS: 8.048987076429458e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 17/71 | LOSS: 8.043880724370763e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 18/71 | LOSS: 7.910786208460769e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 19/71 | LOSS: 8.053708097577327e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 20/71 | LOSS: 8.02435650777105e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 21/71 | LOSS: 8.085091766671129e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 22/71 | LOSS: 8.236236562484207e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 23/71 | LOSS: 8.162435449321492e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 24/71 | LOSS: 8.372679676540429e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 25/71 | LOSS: 8.351435606248113e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 26/71 | LOSS: 8.417863463600278e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 27/71 | LOSS: 8.42624745343658e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 28/71 | LOSS: 8.414625269857613e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 29/71 | LOSS: 8.577261345029305e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 30/71 | LOSS: 8.524849642027778e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 31/71 | LOSS: 8.591200085561468e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 32/71 | LOSS: 8.620074078697309e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 33/71 | LOSS: 8.675407504115891e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 34/71 | LOSS: 8.680754984067919e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 35/71 | LOSS: 8.6424586874677e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 36/71 | LOSS: 8.719260455797445e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 37/71 | LOSS: 8.649701600772555e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 38/71 | LOSS: 8.663037929998096e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 39/71 | LOSS: 8.628073021554884e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 40/71 | LOSS: 8.598148857655782e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 41/71 | LOSS: 8.621029781438882e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 42/71 | LOSS: 8.621824563729312e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 43/71 | LOSS: 8.622572160923234e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 44/71 | LOSS: 8.592379021542406e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 45/71 | LOSS: 8.57516646568041e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 46/71 | LOSS: 8.59472965338624e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 47/71 | LOSS: 8.604092045061407e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 48/71 | LOSS: 8.590241605463457e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 49/71 | LOSS: 8.63857372678467e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 50/71 | LOSS: 8.649885357419193e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 51/71 | LOSS: 8.637580129708602e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 52/71 | LOSS: 8.619984105022905e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 53/71 | LOSS: 8.623963232686813e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 54/71 | LOSS: 8.652959513710811e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 55/71 | LOSS: 8.62886265297545e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 56/71 | LOSS: 8.608712210333211e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 57/71 | LOSS: 8.626872025568696e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 58/71 | LOSS: 8.62997888870561e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 59/71 | LOSS: 8.617938283350668e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 60/71 | LOSS: 8.609909810565726e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 61/71 | LOSS: 8.634023801747507e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 62/71 | LOSS: 8.650835716541643e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 63/71 | LOSS: 8.6805980004101e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 64/71 | LOSS: 8.646110137096212e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 65/71 | LOSS: 8.705752475868918e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 66/71 | LOSS: 8.695275910348987e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 67/71 | LOSS: 8.692878576338818e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 68/71 | LOSS: 8.729240186727585e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 69/71 | LOSS: 8.712800016447935e-06\n",
      "TRAIN: EPOCH 147/1000 | BATCH 70/71 | LOSS: 8.7292407609401e-06\n",
      "VAL: EPOCH 147/1000 | BATCH 0/8 | LOSS: 8.810264262137935e-06\n",
      "VAL: EPOCH 147/1000 | BATCH 1/8 | LOSS: 8.110298267638427e-06\n",
      "VAL: EPOCH 147/1000 | BATCH 2/8 | LOSS: 8.20426657810458e-06\n",
      "VAL: EPOCH 147/1000 | BATCH 3/8 | LOSS: 8.14894963241386e-06\n",
      "VAL: EPOCH 147/1000 | BATCH 4/8 | LOSS: 8.287659420602722e-06\n",
      "VAL: EPOCH 147/1000 | BATCH 5/8 | LOSS: 8.13749791935455e-06\n",
      "VAL: EPOCH 147/1000 | BATCH 6/8 | LOSS: 7.718328236348628e-06\n",
      "VAL: EPOCH 147/1000 | BATCH 7/8 | LOSS: 7.770402078222105e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 0/71 | LOSS: 8.08836193755269e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 1/71 | LOSS: 8.394838005187921e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 2/71 | LOSS: 9.23481275094673e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 3/71 | LOSS: 8.918465255192132e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 4/71 | LOSS: 9.380822484672536e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 5/71 | LOSS: 9.113837374267556e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 6/71 | LOSS: 8.693049884251585e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 7/71 | LOSS: 8.968992460722802e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 8/71 | LOSS: 8.840487781627518e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 9/71 | LOSS: 8.841372073220555e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 10/71 | LOSS: 8.70023111267735e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 11/71 | LOSS: 8.531612706974556e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 12/71 | LOSS: 8.865387161886947e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 13/71 | LOSS: 8.77371612010133e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 14/71 | LOSS: 8.63041268530651e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 15/71 | LOSS: 8.668202468697928e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 16/71 | LOSS: 8.579061801249148e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 17/71 | LOSS: 8.856498753731204e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 18/71 | LOSS: 8.78280293124446e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 19/71 | LOSS: 8.870991996445809e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 20/71 | LOSS: 8.801005606073886e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 21/71 | LOSS: 8.72464552644074e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 22/71 | LOSS: 8.762502563078431e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 23/71 | LOSS: 8.658799041919943e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 24/71 | LOSS: 8.612055662524653e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 25/71 | LOSS: 8.583951879560258e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 26/71 | LOSS: 8.58882906497995e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 27/71 | LOSS: 8.51869430919448e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 28/71 | LOSS: 8.456466667698335e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 29/71 | LOSS: 8.420138465226046e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 30/71 | LOSS: 8.388225377265987e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 31/71 | LOSS: 8.317722091533142e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 32/71 | LOSS: 8.345931746589486e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 33/71 | LOSS: 8.296704075238427e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 34/71 | LOSS: 8.270469217157889e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 35/71 | LOSS: 8.252654828942872e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 36/71 | LOSS: 8.24255049165741e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 37/71 | LOSS: 8.163139443097048e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 38/71 | LOSS: 8.167314939409829e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 39/71 | LOSS: 8.185100591617811e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 40/71 | LOSS: 8.163033027546878e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 41/71 | LOSS: 8.14867366032212e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 42/71 | LOSS: 8.200373602364358e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 43/71 | LOSS: 8.221826078624872e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 44/71 | LOSS: 8.262100962181445e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 45/71 | LOSS: 8.29339572429371e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 46/71 | LOSS: 8.263440896429373e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 47/71 | LOSS: 8.258547334586789e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 48/71 | LOSS: 8.245087651537651e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 49/71 | LOSS: 8.269828094853438e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 50/71 | LOSS: 8.215780834528714e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 51/71 | LOSS: 8.218218323026331e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 52/71 | LOSS: 8.197437336831464e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 53/71 | LOSS: 8.166132063167794e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 54/71 | LOSS: 8.1496795246494e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 55/71 | LOSS: 8.159310206273013e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 56/71 | LOSS: 8.139820046811732e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 57/71 | LOSS: 8.112075929916344e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 58/71 | LOSS: 8.127480429982948e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 59/71 | LOSS: 8.106734662760573e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 60/71 | LOSS: 8.09505363427948e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 61/71 | LOSS: 8.086939077099575e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 62/71 | LOSS: 8.1014533197814e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 63/71 | LOSS: 8.057350896706339e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 64/71 | LOSS: 8.033422049126015e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 65/71 | LOSS: 8.051802317574833e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 66/71 | LOSS: 8.071831762751586e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 67/71 | LOSS: 8.078754690894343e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 68/71 | LOSS: 8.087756306348888e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 69/71 | LOSS: 8.083340721246454e-06\n",
      "TRAIN: EPOCH 148/1000 | BATCH 70/71 | LOSS: 8.086921310754866e-06\n",
      "VAL: EPOCH 148/1000 | BATCH 0/8 | LOSS: 9.534125638310798e-06\n",
      "VAL: EPOCH 148/1000 | BATCH 1/8 | LOSS: 8.908316885936074e-06\n",
      "VAL: EPOCH 148/1000 | BATCH 2/8 | LOSS: 8.508094348750698e-06\n",
      "VAL: EPOCH 148/1000 | BATCH 3/8 | LOSS: 8.134224458444805e-06\n",
      "VAL: EPOCH 148/1000 | BATCH 4/8 | LOSS: 8.43126499603386e-06\n",
      "VAL: EPOCH 148/1000 | BATCH 5/8 | LOSS: 8.284817264817926e-06\n",
      "VAL: EPOCH 148/1000 | BATCH 6/8 | LOSS: 7.980761113555803e-06\n",
      "VAL: EPOCH 148/1000 | BATCH 7/8 | LOSS: 8.089134951205779e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 0/71 | LOSS: 8.809001883491874e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 1/71 | LOSS: 9.833525382418884e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 2/71 | LOSS: 9.718200393156925e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 3/71 | LOSS: 9.286307204092736e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 4/71 | LOSS: 9.204037087329198e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 5/71 | LOSS: 9.15822283786838e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 6/71 | LOSS: 9.557698571630421e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 7/71 | LOSS: 9.357272801935324e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 8/71 | LOSS: 9.293566108681262e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 9/71 | LOSS: 9.17477236725972e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 10/71 | LOSS: 8.885393320021897e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 11/71 | LOSS: 8.972737305157352e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 12/71 | LOSS: 8.840655377403331e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 13/71 | LOSS: 8.637614850418426e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 14/71 | LOSS: 8.416472095026014e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 15/71 | LOSS: 8.393978532694746e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 16/71 | LOSS: 8.342068159596666e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 17/71 | LOSS: 8.260760852054875e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 18/71 | LOSS: 8.251027601976316e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 19/71 | LOSS: 8.185778347069572e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 20/71 | LOSS: 8.339912212596529e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 21/71 | LOSS: 8.297005148382265e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 22/71 | LOSS: 8.376281307984913e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 23/71 | LOSS: 8.308694191327959e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 24/71 | LOSS: 8.234725537477062e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 25/71 | LOSS: 8.166970718985585e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 26/71 | LOSS: 8.15582680607568e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 27/71 | LOSS: 8.122353035235261e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 28/71 | LOSS: 8.134080636754615e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 29/71 | LOSS: 8.102259683558561e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 30/71 | LOSS: 8.109915905797313e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 31/71 | LOSS: 8.133929156883823e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 32/71 | LOSS: 8.142430267594825e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 33/71 | LOSS: 8.121874142328606e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 34/71 | LOSS: 8.101225531780593e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 35/71 | LOSS: 8.081000866392666e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 36/71 | LOSS: 8.057549068227573e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 37/71 | LOSS: 8.012986606780415e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 38/71 | LOSS: 8.021187405131805e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 39/71 | LOSS: 7.974834011292841e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 40/71 | LOSS: 7.98206603088524e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 41/71 | LOSS: 8.015345775631777e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 42/71 | LOSS: 8.002920335562775e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 43/71 | LOSS: 8.18877181659032e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 44/71 | LOSS: 8.1497841266456e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 45/71 | LOSS: 8.167260158069952e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 46/71 | LOSS: 8.243807731056957e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 47/71 | LOSS: 8.245402180288389e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 48/71 | LOSS: 8.33018112662474e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 49/71 | LOSS: 8.291206240755855e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 50/71 | LOSS: 8.358620811586483e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 51/71 | LOSS: 8.399040806440238e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 52/71 | LOSS: 8.389786354149143e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 53/71 | LOSS: 8.458728887961587e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 54/71 | LOSS: 8.43029065435985e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 55/71 | LOSS: 8.499810503183522e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 56/71 | LOSS: 8.53436816677251e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 57/71 | LOSS: 8.519909066496344e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 58/71 | LOSS: 8.564715845765885e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 59/71 | LOSS: 8.549402461236847e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 60/71 | LOSS: 8.634405648841553e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 61/71 | LOSS: 8.614658503164162e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 62/71 | LOSS: 8.608160449454324e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 63/71 | LOSS: 8.658384800241947e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 64/71 | LOSS: 8.649820903463898e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 65/71 | LOSS: 8.677780497788318e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 66/71 | LOSS: 8.668088833188964e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 67/71 | LOSS: 8.714726324315587e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 68/71 | LOSS: 8.70657070175883e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 69/71 | LOSS: 8.717119427663939e-06\n",
      "TRAIN: EPOCH 149/1000 | BATCH 70/71 | LOSS: 8.715318907773726e-06\n",
      "VAL: EPOCH 149/1000 | BATCH 0/8 | LOSS: 9.99285293801222e-06\n",
      "VAL: EPOCH 149/1000 | BATCH 1/8 | LOSS: 9.731043974170461e-06\n",
      "VAL: EPOCH 149/1000 | BATCH 2/8 | LOSS: 9.336572475149296e-06\n",
      "VAL: EPOCH 149/1000 | BATCH 3/8 | LOSS: 8.914385489333654e-06\n",
      "VAL: EPOCH 149/1000 | BATCH 4/8 | LOSS: 9.337472874904051e-06\n",
      "VAL: EPOCH 149/1000 | BATCH 5/8 | LOSS: 9.312372185377171e-06\n",
      "VAL: EPOCH 149/1000 | BATCH 6/8 | LOSS: 9.158098821769403e-06\n",
      "VAL: EPOCH 149/1000 | BATCH 7/8 | LOSS: 9.135355867329054e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 0/71 | LOSS: 9.426612450624816e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 1/71 | LOSS: 9.317855983681511e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 2/71 | LOSS: 8.859060471877456e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 3/71 | LOSS: 8.516688922100002e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 4/71 | LOSS: 9.498461622570176e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 5/71 | LOSS: 9.263860950644206e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 6/71 | LOSS: 8.968330673399447e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 7/71 | LOSS: 8.799071508747147e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 8/71 | LOSS: 8.597241907813845e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 9/71 | LOSS: 8.382890655411757e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 10/71 | LOSS: 8.432456318339312e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 11/71 | LOSS: 8.511824906539308e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 12/71 | LOSS: 8.553021206563035e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 13/71 | LOSS: 8.309572389667405e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 14/71 | LOSS: 8.37594407130382e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 15/71 | LOSS: 8.461137355197934e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 16/71 | LOSS: 8.536555282369269e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 17/71 | LOSS: 8.56085921441263e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 18/71 | LOSS: 8.485261260878025e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 19/71 | LOSS: 8.545327136744164e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 20/71 | LOSS: 8.620090912223705e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 21/71 | LOSS: 8.628523573861457e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 22/71 | LOSS: 8.578015141487729e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 23/71 | LOSS: 8.491175587247804e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 24/71 | LOSS: 8.459182354272343e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 25/71 | LOSS: 8.480100404995028e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 26/71 | LOSS: 8.488004587653214e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 27/71 | LOSS: 8.439858317745217e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 28/71 | LOSS: 8.400146443505193e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 29/71 | LOSS: 8.407301114251217e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 30/71 | LOSS: 8.353208496344216e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 31/71 | LOSS: 8.276145791796807e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 32/71 | LOSS: 8.323365308928205e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 33/71 | LOSS: 8.315055551707593e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 34/71 | LOSS: 8.323566284421498e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 35/71 | LOSS: 8.32905253547425e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 36/71 | LOSS: 8.327737133013952e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 37/71 | LOSS: 8.305563501141334e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 38/71 | LOSS: 8.26833643375908e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 39/71 | LOSS: 8.254203567048535e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 40/71 | LOSS: 8.205710689320013e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 41/71 | LOSS: 8.158405838306257e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 42/71 | LOSS: 8.172808488885748e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 43/71 | LOSS: 8.180446322760623e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 44/71 | LOSS: 8.13033250071587e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 45/71 | LOSS: 8.08961665469447e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 46/71 | LOSS: 8.071571252738533e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 47/71 | LOSS: 8.068909134332595e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 48/71 | LOSS: 8.043699251304912e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 49/71 | LOSS: 7.986385571712163e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 50/71 | LOSS: 7.94417228819072e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 51/71 | LOSS: 7.930311006440696e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 52/71 | LOSS: 7.920083057883258e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 53/71 | LOSS: 7.904417770987493e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 54/71 | LOSS: 7.924968295728005e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 55/71 | LOSS: 7.922297031979855e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 56/71 | LOSS: 7.892722207639293e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 57/71 | LOSS: 7.83815169691887e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 58/71 | LOSS: 7.865767980485729e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 59/71 | LOSS: 7.858180849022271e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 60/71 | LOSS: 7.84978944511004e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 61/71 | LOSS: 7.847214243567226e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 62/71 | LOSS: 7.838646596942526e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 63/71 | LOSS: 7.84262917363776e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 64/71 | LOSS: 7.82335320344338e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 65/71 | LOSS: 7.829594461795889e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 66/71 | LOSS: 7.834407722893686e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 67/71 | LOSS: 7.840293526040863e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 68/71 | LOSS: 7.853087086202677e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 69/71 | LOSS: 7.829457458033824e-06\n",
      "TRAIN: EPOCH 150/1000 | BATCH 70/71 | LOSS: 7.835039635300374e-06\n",
      "VAL: EPOCH 150/1000 | BATCH 0/8 | LOSS: 9.166112249658909e-06\n",
      "VAL: EPOCH 150/1000 | BATCH 1/8 | LOSS: 8.097138788798475e-06\n",
      "VAL: EPOCH 150/1000 | BATCH 2/8 | LOSS: 7.937486619387831e-06\n",
      "VAL: EPOCH 150/1000 | BATCH 3/8 | LOSS: 7.648287351003091e-06\n",
      "VAL: EPOCH 150/1000 | BATCH 4/8 | LOSS: 7.89878076830064e-06\n",
      "VAL: EPOCH 150/1000 | BATCH 5/8 | LOSS: 7.821446994663953e-06\n",
      "VAL: EPOCH 150/1000 | BATCH 6/8 | LOSS: 7.493358485329996e-06\n",
      "VAL: EPOCH 150/1000 | BATCH 7/8 | LOSS: 7.525421153786738e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 0/71 | LOSS: 8.244349373853765e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 1/71 | LOSS: 9.289494300901424e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 2/71 | LOSS: 8.969157837176075e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 3/71 | LOSS: 8.650850759295281e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 4/71 | LOSS: 8.948065078584477e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 5/71 | LOSS: 8.653950771986274e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 6/71 | LOSS: 8.586342249015746e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 7/71 | LOSS: 8.378657867069705e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 8/71 | LOSS: 8.069108793683376e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 9/71 | LOSS: 7.89841474215791e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 10/71 | LOSS: 8.101526832020186e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 11/71 | LOSS: 7.950372567696226e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 12/71 | LOSS: 7.74344282315444e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 13/71 | LOSS: 7.821993384433362e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 14/71 | LOSS: 7.682243843494992e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 15/71 | LOSS: 7.662177097245149e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 16/71 | LOSS: 7.756427615223562e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 17/71 | LOSS: 7.770854457097206e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 18/71 | LOSS: 7.747337277626349e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 19/71 | LOSS: 7.803013522789115e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 20/71 | LOSS: 7.845340665621084e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 21/71 | LOSS: 7.944417699367147e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 22/71 | LOSS: 7.950228712655624e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 23/71 | LOSS: 8.02642792526361e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 24/71 | LOSS: 8.016356659936718e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 25/71 | LOSS: 8.053780780196226e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 26/71 | LOSS: 8.158871423802339e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 27/71 | LOSS: 8.110448009444684e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 28/71 | LOSS: 8.252611631429192e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 29/71 | LOSS: 8.220559690623001e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 30/71 | LOSS: 8.2455338586794e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 31/71 | LOSS: 8.177158989042255e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 32/71 | LOSS: 8.162457823235661e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 33/71 | LOSS: 8.277425997542142e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 34/71 | LOSS: 8.273299122915238e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 35/71 | LOSS: 8.24294278976999e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 36/71 | LOSS: 8.209840379512988e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 37/71 | LOSS: 8.189483399914216e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 38/71 | LOSS: 8.146922377468749e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 39/71 | LOSS: 8.129187801841908e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 40/71 | LOSS: 8.084293013536541e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 41/71 | LOSS: 8.040297113135845e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 42/71 | LOSS: 7.992672667463667e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 43/71 | LOSS: 7.93011725942382e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 44/71 | LOSS: 7.882717090978985e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 45/71 | LOSS: 7.840479028068923e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 46/71 | LOSS: 7.833203982683815e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 47/71 | LOSS: 7.811171627736258e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 48/71 | LOSS: 7.773901100391617e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 49/71 | LOSS: 7.779019761073868e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 50/71 | LOSS: 7.744860839938709e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 51/71 | LOSS: 7.71247131034705e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 52/71 | LOSS: 7.728453124829409e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 53/71 | LOSS: 7.700930239109396e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 54/71 | LOSS: 7.674358113779983e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 55/71 | LOSS: 7.654373429691727e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 56/71 | LOSS: 7.63485872755011e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 57/71 | LOSS: 7.67447170738737e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 58/71 | LOSS: 7.680039928700199e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 59/71 | LOSS: 7.673483416207698e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 60/71 | LOSS: 7.697062942423174e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 61/71 | LOSS: 7.70988525976962e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 62/71 | LOSS: 7.696379862676305e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 63/71 | LOSS: 7.742699871471359e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 64/71 | LOSS: 7.770545401469393e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 65/71 | LOSS: 7.725255817135665e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 66/71 | LOSS: 7.787688439327498e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 67/71 | LOSS: 7.771428954583826e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 68/71 | LOSS: 7.779775525863363e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 69/71 | LOSS: 7.799781035698418e-06\n",
      "TRAIN: EPOCH 151/1000 | BATCH 70/71 | LOSS: 7.749406513924019e-06\n",
      "VAL: EPOCH 151/1000 | BATCH 0/8 | LOSS: 8.972940122475848e-06\n",
      "VAL: EPOCH 151/1000 | BATCH 1/8 | LOSS: 9.28113058762392e-06\n",
      "VAL: EPOCH 151/1000 | BATCH 2/8 | LOSS: 8.24384854543799e-06\n",
      "VAL: EPOCH 151/1000 | BATCH 3/8 | LOSS: 8.493687573718489e-06\n",
      "VAL: EPOCH 151/1000 | BATCH 4/8 | LOSS: 8.527970385330263e-06\n",
      "VAL: EPOCH 151/1000 | BATCH 5/8 | LOSS: 8.387125338534437e-06\n",
      "VAL: EPOCH 151/1000 | BATCH 6/8 | LOSS: 8.160923243849538e-06\n",
      "VAL: EPOCH 151/1000 | BATCH 7/8 | LOSS: 8.059650838276866e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 0/71 | LOSS: 6.378302714438178e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 1/71 | LOSS: 7.428259777952917e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 2/71 | LOSS: 7.30287138139829e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 3/71 | LOSS: 7.391906137854676e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 4/71 | LOSS: 7.426127649523551e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 5/71 | LOSS: 7.231943754959502e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 6/71 | LOSS: 7.400149180674427e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 7/71 | LOSS: 7.672107415146456e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 8/71 | LOSS: 8.05738742807686e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 9/71 | LOSS: 7.894253258200479e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 10/71 | LOSS: 8.235977103057932e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 11/71 | LOSS: 8.254899853454845e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 12/71 | LOSS: 8.170791086460948e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 13/71 | LOSS: 8.27802728053939e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 14/71 | LOSS: 8.173690336358656e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 15/71 | LOSS: 8.243303994959206e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 16/71 | LOSS: 8.150070190073593e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 17/71 | LOSS: 8.145938712308029e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 18/71 | LOSS: 8.270205685939322e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 19/71 | LOSS: 8.316507091876701e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 20/71 | LOSS: 8.28854566255662e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 21/71 | LOSS: 8.333963292402173e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 22/71 | LOSS: 8.242240176553858e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 23/71 | LOSS: 8.251127193640665e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 24/71 | LOSS: 8.287213568110019e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 25/71 | LOSS: 8.387772630577763e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 26/71 | LOSS: 8.361525909061095e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 27/71 | LOSS: 8.2660412772384e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 28/71 | LOSS: 8.240096796161478e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 29/71 | LOSS: 8.256880179639363e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 30/71 | LOSS: 8.305424436853613e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 31/71 | LOSS: 8.217530790943783e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 32/71 | LOSS: 8.15689761359615e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 33/71 | LOSS: 8.136841776885216e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 34/71 | LOSS: 8.125069897297571e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 35/71 | LOSS: 8.085739687481288e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 36/71 | LOSS: 8.069459282351827e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 37/71 | LOSS: 8.056400343105596e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 38/71 | LOSS: 8.010864337101782e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 39/71 | LOSS: 7.949477708280029e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 40/71 | LOSS: 7.929338020092961e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 41/71 | LOSS: 7.943493214735922e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 42/71 | LOSS: 7.933890935609545e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 43/71 | LOSS: 7.924498969590456e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 44/71 | LOSS: 7.937958909880965e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 45/71 | LOSS: 7.956389145360648e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 46/71 | LOSS: 7.943254051032783e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 47/71 | LOSS: 7.969830392084987e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 48/71 | LOSS: 7.94750245144574e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 49/71 | LOSS: 7.948673128339578e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 50/71 | LOSS: 7.924411812721291e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 51/71 | LOSS: 7.94979378217529e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 52/71 | LOSS: 7.942730994641952e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 53/71 | LOSS: 7.995514646676451e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 54/71 | LOSS: 8.008908339847536e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 55/71 | LOSS: 8.031790952957505e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 56/71 | LOSS: 8.01162593090435e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 57/71 | LOSS: 8.00354166977772e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 58/71 | LOSS: 7.97045661676162e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 59/71 | LOSS: 7.989720779733034e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 60/71 | LOSS: 7.960085038009356e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 61/71 | LOSS: 7.953497222314904e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 62/71 | LOSS: 7.946351464246012e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 63/71 | LOSS: 7.92989251152676e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 64/71 | LOSS: 7.945468349554666e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 65/71 | LOSS: 7.907421198097202e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 66/71 | LOSS: 7.948616508765011e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 67/71 | LOSS: 7.948444209325638e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 68/71 | LOSS: 7.938858578964373e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 69/71 | LOSS: 7.948824207103047e-06\n",
      "TRAIN: EPOCH 152/1000 | BATCH 70/71 | LOSS: 7.956564857035836e-06\n",
      "VAL: EPOCH 152/1000 | BATCH 0/8 | LOSS: 8.091767085716128e-06\n",
      "VAL: EPOCH 152/1000 | BATCH 1/8 | LOSS: 7.656953584955772e-06\n",
      "VAL: EPOCH 152/1000 | BATCH 2/8 | LOSS: 7.059298998986681e-06\n",
      "VAL: EPOCH 152/1000 | BATCH 3/8 | LOSS: 7.179043450378231e-06\n",
      "VAL: EPOCH 152/1000 | BATCH 4/8 | LOSS: 7.225678018585313e-06\n",
      "VAL: EPOCH 152/1000 | BATCH 5/8 | LOSS: 7.096640653495949e-06\n",
      "VAL: EPOCH 152/1000 | BATCH 6/8 | LOSS: 6.8061099877273335e-06\n",
      "VAL: EPOCH 152/1000 | BATCH 7/8 | LOSS: 6.7465758206708415e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 0/71 | LOSS: 7.14235648047179e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 1/71 | LOSS: 7.724741863057716e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 2/71 | LOSS: 8.331437735857131e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 3/71 | LOSS: 8.024437761378067e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 4/71 | LOSS: 7.635735892108642e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 5/71 | LOSS: 7.328433033156519e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 6/71 | LOSS: 7.288541447841064e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 7/71 | LOSS: 7.391795804778667e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 8/71 | LOSS: 7.176866650600762e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 9/71 | LOSS: 7.232178631966235e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 10/71 | LOSS: 7.138283845878587e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 11/71 | LOSS: 7.2051914230542025e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 12/71 | LOSS: 7.14868964188589e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 13/71 | LOSS: 7.150826767039169e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 14/71 | LOSS: 7.2378231379843784e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 15/71 | LOSS: 7.148195322770334e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 16/71 | LOSS: 7.2075838889919826e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 17/71 | LOSS: 7.231343816884974e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 18/71 | LOSS: 7.195106120075491e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 19/71 | LOSS: 7.2048605943564326e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 20/71 | LOSS: 7.232520450391651e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 21/71 | LOSS: 7.18564762213315e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 22/71 | LOSS: 7.13788858774933e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 23/71 | LOSS: 7.121960341767893e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 24/71 | LOSS: 7.0707323902752254e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 25/71 | LOSS: 7.062827272910419e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 26/71 | LOSS: 7.0706335093375486e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 27/71 | LOSS: 7.100491026384199e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 28/71 | LOSS: 7.248525952459674e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 29/71 | LOSS: 7.213638112564998e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 30/71 | LOSS: 7.277624673963967e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 31/71 | LOSS: 7.31794834507582e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 32/71 | LOSS: 7.317106613327544e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 33/71 | LOSS: 7.332975054155481e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 34/71 | LOSS: 7.28358496157203e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 35/71 | LOSS: 7.347786890932184e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 36/71 | LOSS: 7.366799628790751e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 37/71 | LOSS: 7.411287613553884e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 38/71 | LOSS: 7.4203654455628475e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 39/71 | LOSS: 7.368677972863224e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 40/71 | LOSS: 7.414299355931268e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 41/71 | LOSS: 7.405783727725586e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 42/71 | LOSS: 7.41575419407698e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 43/71 | LOSS: 7.433655117562342e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 44/71 | LOSS: 7.466059923899593e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 45/71 | LOSS: 7.4817173556410754e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 46/71 | LOSS: 7.492073922200708e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 47/71 | LOSS: 7.519491125170437e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 48/71 | LOSS: 7.493803200397489e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 49/71 | LOSS: 7.507424434152199e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 50/71 | LOSS: 7.497351361962501e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 51/71 | LOSS: 7.521857346896118e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 52/71 | LOSS: 7.539617966032446e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 53/71 | LOSS: 7.530056058789745e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 54/71 | LOSS: 7.544355197420704e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 55/71 | LOSS: 7.551839782341371e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 56/71 | LOSS: 7.539801285211549e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 57/71 | LOSS: 7.533602346766516e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 58/71 | LOSS: 7.501734324836496e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 59/71 | LOSS: 7.497908366834357e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 60/71 | LOSS: 7.482990619538497e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 61/71 | LOSS: 7.484611226191778e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 62/71 | LOSS: 7.480297224699623e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 63/71 | LOSS: 7.476080838841881e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 64/71 | LOSS: 7.464719027876317e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 65/71 | LOSS: 7.443133826558496e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 66/71 | LOSS: 7.459714563979173e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 67/71 | LOSS: 7.458493207516713e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 68/71 | LOSS: 7.4632540167510965e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 69/71 | LOSS: 7.457309201137312e-06\n",
      "TRAIN: EPOCH 153/1000 | BATCH 70/71 | LOSS: 7.434356133748685e-06\n",
      "VAL: EPOCH 153/1000 | BATCH 0/8 | LOSS: 1.1722319868567865e-05\n",
      "VAL: EPOCH 153/1000 | BATCH 1/8 | LOSS: 1.18590755846526e-05\n",
      "VAL: EPOCH 153/1000 | BATCH 2/8 | LOSS: 1.0940777732078763e-05\n",
      "VAL: EPOCH 153/1000 | BATCH 3/8 | LOSS: 1.0324591812604922e-05\n",
      "VAL: EPOCH 153/1000 | BATCH 4/8 | LOSS: 1.0841895527846645e-05\n",
      "VAL: EPOCH 153/1000 | BATCH 5/8 | LOSS: 1.06050712626408e-05\n",
      "VAL: EPOCH 153/1000 | BATCH 6/8 | LOSS: 1.057843227109905e-05\n",
      "VAL: EPOCH 153/1000 | BATCH 7/8 | LOSS: 1.0561971976130735e-05\n",
      "TRAIN: EPOCH 154/1000 | BATCH 0/71 | LOSS: 8.92986008693697e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 1/71 | LOSS: 8.6081540757732e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 2/71 | LOSS: 8.094070987378169e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 3/71 | LOSS: 8.1864828871403e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 4/71 | LOSS: 8.13948581708246e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 5/71 | LOSS: 8.613438012616825e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 6/71 | LOSS: 8.526579157270525e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 7/71 | LOSS: 8.570223656079179e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 8/71 | LOSS: 8.979916401585797e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 9/71 | LOSS: 8.820532048048336e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 10/71 | LOSS: 8.674055565775648e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 11/71 | LOSS: 8.841533106836627e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 12/71 | LOSS: 8.910010889779256e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 13/71 | LOSS: 8.811116263781774e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 14/71 | LOSS: 8.612425608589546e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 15/71 | LOSS: 8.795929687721582e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 16/71 | LOSS: 8.686469648048788e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 17/71 | LOSS: 8.687416109549102e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 18/71 | LOSS: 8.688650771211159e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 19/71 | LOSS: 8.691146535966255e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 20/71 | LOSS: 8.720116251684626e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 21/71 | LOSS: 8.644344626935146e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 22/71 | LOSS: 8.541923129403427e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 23/71 | LOSS: 8.517453674509548e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 24/71 | LOSS: 8.524983986717416e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 25/71 | LOSS: 8.515295733629431e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 26/71 | LOSS: 8.431290653889948e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 27/71 | LOSS: 8.385778707114306e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 28/71 | LOSS: 8.342759287941214e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 29/71 | LOSS: 8.338163797816379e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 30/71 | LOSS: 8.350255029654363e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 31/71 | LOSS: 8.336582126844405e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 32/71 | LOSS: 8.311767528952755e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 33/71 | LOSS: 8.299043937607648e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 34/71 | LOSS: 8.297746418455582e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 35/71 | LOSS: 8.273644665577092e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 36/71 | LOSS: 8.199077542116897e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 37/71 | LOSS: 8.186607290458356e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 38/71 | LOSS: 8.190118944931339e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 39/71 | LOSS: 8.134162374062726e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 40/71 | LOSS: 8.114165921586214e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 41/71 | LOSS: 8.086197329335154e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 42/71 | LOSS: 8.041254384510847e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 43/71 | LOSS: 8.037756102070984e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 44/71 | LOSS: 8.003241560396014e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 45/71 | LOSS: 7.984601661123831e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 46/71 | LOSS: 7.949950157397209e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 47/71 | LOSS: 7.92810848793124e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 48/71 | LOSS: 7.951956247000322e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 49/71 | LOSS: 7.92897819337668e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 50/71 | LOSS: 7.916473627048225e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 51/71 | LOSS: 7.904486735172282e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 52/71 | LOSS: 7.890310671599471e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 53/71 | LOSS: 7.875736476610959e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 54/71 | LOSS: 7.847336267084095e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 55/71 | LOSS: 7.844457286410034e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 56/71 | LOSS: 7.845154310384634e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 57/71 | LOSS: 7.816112013736937e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 58/71 | LOSS: 7.779742136952306e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 59/71 | LOSS: 7.769303185038249e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 60/71 | LOSS: 7.76958770492471e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 61/71 | LOSS: 7.756157827081492e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 62/71 | LOSS: 7.717100635047591e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 63/71 | LOSS: 7.745835340244867e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 64/71 | LOSS: 7.753237843224689e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 65/71 | LOSS: 7.76439705160473e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 66/71 | LOSS: 7.727303373437973e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 67/71 | LOSS: 7.730421002099106e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 68/71 | LOSS: 7.705083207574848e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 69/71 | LOSS: 7.69601629794384e-06\n",
      "TRAIN: EPOCH 154/1000 | BATCH 70/71 | LOSS: 7.65905738070371e-06\n",
      "VAL: EPOCH 154/1000 | BATCH 0/8 | LOSS: 7.747932613710873e-06\n",
      "VAL: EPOCH 154/1000 | BATCH 1/8 | LOSS: 7.828218258509878e-06\n",
      "VAL: EPOCH 154/1000 | BATCH 2/8 | LOSS: 7.084632215992315e-06\n",
      "VAL: EPOCH 154/1000 | BATCH 3/8 | LOSS: 7.310342994060193e-06\n",
      "VAL: EPOCH 154/1000 | BATCH 4/8 | LOSS: 7.505556186515605e-06\n",
      "VAL: EPOCH 154/1000 | BATCH 5/8 | LOSS: 7.326541738924182e-06\n",
      "VAL: EPOCH 154/1000 | BATCH 6/8 | LOSS: 7.083206323191657e-06\n",
      "VAL: EPOCH 154/1000 | BATCH 7/8 | LOSS: 7.036125055037701e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 0/71 | LOSS: 6.688687790301628e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 1/71 | LOSS: 5.836986929352861e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 2/71 | LOSS: 6.4376757412295165e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 3/71 | LOSS: 6.266313221203745e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 4/71 | LOSS: 6.514233064081054e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 5/71 | LOSS: 6.891781746768781e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 6/71 | LOSS: 6.885958360466507e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 7/71 | LOSS: 6.813465063260082e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 8/71 | LOSS: 6.903840383327204e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 9/71 | LOSS: 6.891341945447493e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 10/71 | LOSS: 6.898428637214238e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 11/71 | LOSS: 6.933979496655714e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 12/71 | LOSS: 6.895158618975144e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 13/71 | LOSS: 6.895771288587379e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 14/71 | LOSS: 6.879311695229262e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 15/71 | LOSS: 6.937239845683507e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 16/71 | LOSS: 6.853744547269718e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 17/71 | LOSS: 6.849388127092324e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 18/71 | LOSS: 6.845745007778628e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 19/71 | LOSS: 6.818490214755002e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 20/71 | LOSS: 6.808141300834472e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 21/71 | LOSS: 6.784155299431983e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 22/71 | LOSS: 6.805177137088638e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 23/71 | LOSS: 6.937388813336535e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 24/71 | LOSS: 6.902209970576223e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 25/71 | LOSS: 6.874696229995327e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 26/71 | LOSS: 6.9105530773985835e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 27/71 | LOSS: 6.940480147932249e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 28/71 | LOSS: 6.9417380379083376e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 29/71 | LOSS: 6.9512174074285815e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 30/71 | LOSS: 6.932341860984677e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 31/71 | LOSS: 6.9122079935368674e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 32/71 | LOSS: 6.984355670944999e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 33/71 | LOSS: 6.9714205357526865e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 34/71 | LOSS: 6.98072542166171e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 35/71 | LOSS: 6.9677106593088765e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 36/71 | LOSS: 6.92257489808489e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 37/71 | LOSS: 6.907691768949582e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 38/71 | LOSS: 6.904079805864081e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 39/71 | LOSS: 6.913463164437417e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 40/71 | LOSS: 6.934653411895496e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 41/71 | LOSS: 6.973240585596484e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 42/71 | LOSS: 6.974271738811715e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 43/71 | LOSS: 6.973257200115768e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 44/71 | LOSS: 6.980569913947773e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 45/71 | LOSS: 7.005449817923996e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 46/71 | LOSS: 6.985484425116583e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 47/71 | LOSS: 6.966782090482108e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 48/71 | LOSS: 6.933727537234889e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 49/71 | LOSS: 6.974671969146584e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 50/71 | LOSS: 6.95863995062217e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 51/71 | LOSS: 6.987621916848114e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 52/71 | LOSS: 6.9705315839218085e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 53/71 | LOSS: 6.9715937300956014e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 54/71 | LOSS: 6.968094467910387e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 55/71 | LOSS: 6.966516463892601e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 56/71 | LOSS: 6.956571572421429e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 57/71 | LOSS: 6.956360337049961e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 58/71 | LOSS: 6.944098112075215e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 59/71 | LOSS: 6.941624678802327e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 60/71 | LOSS: 6.943315190151448e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 61/71 | LOSS: 6.927978750853424e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 62/71 | LOSS: 6.943444373425178e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 63/71 | LOSS: 6.936700202686552e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 64/71 | LOSS: 6.946840438523885e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 65/71 | LOSS: 6.93772571911254e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 66/71 | LOSS: 6.932919323342286e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 67/71 | LOSS: 6.9312302524421655e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 68/71 | LOSS: 6.925632408018683e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 69/71 | LOSS: 6.916470556461718e-06\n",
      "TRAIN: EPOCH 155/1000 | BATCH 70/71 | LOSS: 6.9290726713288545e-06\n",
      "VAL: EPOCH 155/1000 | BATCH 0/8 | LOSS: 9.485052942181937e-06\n",
      "VAL: EPOCH 155/1000 | BATCH 1/8 | LOSS: 8.903731213649735e-06\n",
      "VAL: EPOCH 155/1000 | BATCH 2/8 | LOSS: 9.26106743766771e-06\n",
      "VAL: EPOCH 155/1000 | BATCH 3/8 | LOSS: 9.564487299940083e-06\n",
      "VAL: EPOCH 155/1000 | BATCH 4/8 | LOSS: 9.84976995823672e-06\n",
      "VAL: EPOCH 155/1000 | BATCH 5/8 | LOSS: 9.704243893793318e-06\n",
      "VAL: EPOCH 155/1000 | BATCH 6/8 | LOSS: 9.340113917590185e-06\n",
      "VAL: EPOCH 155/1000 | BATCH 7/8 | LOSS: 9.343137548967206e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 0/71 | LOSS: 9.907810635922942e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 1/71 | LOSS: 9.329704425908858e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 2/71 | LOSS: 8.731396064831642e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 3/71 | LOSS: 1.0645502356965153e-05\n",
      "TRAIN: EPOCH 156/1000 | BATCH 4/71 | LOSS: 9.927478367899312e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 5/71 | LOSS: 9.958997225112398e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 6/71 | LOSS: 9.589125121627668e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 7/71 | LOSS: 9.309960830705677e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 8/71 | LOSS: 9.16392102428492e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 9/71 | LOSS: 9.045588876688271e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 10/71 | LOSS: 9.03419802191572e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 11/71 | LOSS: 8.924083545025496e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 12/71 | LOSS: 8.664170239241615e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 13/71 | LOSS: 8.685709027694039e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 14/71 | LOSS: 8.703547367379845e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 15/71 | LOSS: 8.760010643982241e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 16/71 | LOSS: 8.85307278675849e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 17/71 | LOSS: 8.758606782066636e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 18/71 | LOSS: 8.716191587154754e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 19/71 | LOSS: 8.571881244279211e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 20/71 | LOSS: 8.591493540388045e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 21/71 | LOSS: 8.559963672186397e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 22/71 | LOSS: 8.507228298897289e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 23/71 | LOSS: 8.440653933424377e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 24/71 | LOSS: 8.365978119400098e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 25/71 | LOSS: 8.364768973478931e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 26/71 | LOSS: 8.380043945416455e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 27/71 | LOSS: 8.344927388342122e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 28/71 | LOSS: 8.297025660481424e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 29/71 | LOSS: 8.238422575838436e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 30/71 | LOSS: 8.220782503133617e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 31/71 | LOSS: 8.22951675161221e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 32/71 | LOSS: 8.171958763176173e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 33/71 | LOSS: 8.203522838812205e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 34/71 | LOSS: 8.145664768172927e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 35/71 | LOSS: 8.12997497076948e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 36/71 | LOSS: 8.0475273215278e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 37/71 | LOSS: 7.99612708808701e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 38/71 | LOSS: 7.962210801428553e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 39/71 | LOSS: 7.929552043606237e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 40/71 | LOSS: 7.887296461132525e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 41/71 | LOSS: 7.85276418875063e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 42/71 | LOSS: 7.843686003176371e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 43/71 | LOSS: 7.824807126284709e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 44/71 | LOSS: 7.791690081325618e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 45/71 | LOSS: 7.75805961972725e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 46/71 | LOSS: 7.72459789198152e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 47/71 | LOSS: 7.652860584054602e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 48/71 | LOSS: 7.63931677506569e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 49/71 | LOSS: 7.644618444828665e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 50/71 | LOSS: 7.620849222197042e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 51/71 | LOSS: 7.570971967232674e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 52/71 | LOSS: 7.563185221751721e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 53/71 | LOSS: 7.563948639909978e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 54/71 | LOSS: 7.527735512915322e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 55/71 | LOSS: 7.5220204637714365e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 56/71 | LOSS: 7.496647413063329e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 57/71 | LOSS: 7.474916824791982e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 58/71 | LOSS: 7.484411934594884e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 59/71 | LOSS: 7.477771350750117e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 60/71 | LOSS: 7.45587661439175e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 61/71 | LOSS: 7.449007464475148e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 62/71 | LOSS: 7.468653786908983e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 63/71 | LOSS: 7.463152122966221e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 64/71 | LOSS: 7.482372690831723e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 65/71 | LOSS: 7.465758237616903e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 66/71 | LOSS: 7.46029202524206e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 67/71 | LOSS: 7.440831087906725e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 68/71 | LOSS: 7.449224929165422e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 69/71 | LOSS: 7.4434373280902425e-06\n",
      "TRAIN: EPOCH 156/1000 | BATCH 70/71 | LOSS: 7.422025768212568e-06\n",
      "VAL: EPOCH 156/1000 | BATCH 0/8 | LOSS: 7.637728231202345e-06\n",
      "VAL: EPOCH 156/1000 | BATCH 1/8 | LOSS: 7.314720278372988e-06\n",
      "VAL: EPOCH 156/1000 | BATCH 2/8 | LOSS: 7.061434340963994e-06\n",
      "VAL: EPOCH 156/1000 | BATCH 3/8 | LOSS: 7.195773946477857e-06\n",
      "VAL: EPOCH 156/1000 | BATCH 4/8 | LOSS: 7.361471125477692e-06\n",
      "VAL: EPOCH 156/1000 | BATCH 5/8 | LOSS: 7.327726886311818e-06\n",
      "VAL: EPOCH 156/1000 | BATCH 6/8 | LOSS: 7.03338504795933e-06\n",
      "VAL: EPOCH 156/1000 | BATCH 7/8 | LOSS: 7.003660527971078e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 0/71 | LOSS: 7.75741136749275e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 1/71 | LOSS: 7.435516408804688e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 2/71 | LOSS: 7.891479223568846e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 3/71 | LOSS: 7.510975478908222e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 4/71 | LOSS: 7.54078091631527e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 5/71 | LOSS: 7.403262846613264e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 6/71 | LOSS: 7.256160383154306e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 7/71 | LOSS: 7.107700184860732e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 8/71 | LOSS: 6.875479913206719e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 9/71 | LOSS: 6.940825460333144e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 10/71 | LOSS: 6.933171830909983e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 11/71 | LOSS: 6.82115296513075e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 12/71 | LOSS: 6.821654481497647e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 13/71 | LOSS: 6.901750923913953e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 14/71 | LOSS: 6.903500009987814e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 15/71 | LOSS: 7.04257541883635e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 16/71 | LOSS: 7.012692228325998e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 17/71 | LOSS: 7.007385066673224e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 18/71 | LOSS: 7.055030269647862e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 19/71 | LOSS: 7.159645087995159e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 20/71 | LOSS: 7.263375166422477e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 21/71 | LOSS: 7.324358627804693e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 22/71 | LOSS: 7.336200190405615e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 23/71 | LOSS: 7.280425336375629e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 24/71 | LOSS: 7.229448037833208e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 25/71 | LOSS: 7.220445957920254e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 26/71 | LOSS: 7.201433820430086e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 27/71 | LOSS: 7.1734588280898085e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 28/71 | LOSS: 7.1461950185415656e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 29/71 | LOSS: 7.085012612151331e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 30/71 | LOSS: 7.0634729977868e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 31/71 | LOSS: 7.0466383732537e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 32/71 | LOSS: 6.987320416969438e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 33/71 | LOSS: 7.018840399135061e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 34/71 | LOSS: 6.974038712672024e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 35/71 | LOSS: 6.941464941216206e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 36/71 | LOSS: 6.900017745686676e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 37/71 | LOSS: 6.904565811725517e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 38/71 | LOSS: 6.905764598158361e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 39/71 | LOSS: 6.877159978557756e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 40/71 | LOSS: 6.901456810915189e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 41/71 | LOSS: 6.874255318290948e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 42/71 | LOSS: 6.841188340246824e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 43/71 | LOSS: 6.878991691585477e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 44/71 | LOSS: 6.938111568565041e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 45/71 | LOSS: 6.960287336092417e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 46/71 | LOSS: 6.906594187076986e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 47/71 | LOSS: 6.872096179222353e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 48/71 | LOSS: 6.877011020726653e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 49/71 | LOSS: 6.882504239911214e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 50/71 | LOSS: 6.887427861882173e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 51/71 | LOSS: 6.909249175525754e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 52/71 | LOSS: 6.8649874007843326e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 53/71 | LOSS: 6.862201981103108e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 54/71 | LOSS: 6.8732817866971756e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 55/71 | LOSS: 6.935945756530211e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 56/71 | LOSS: 6.924193042466772e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 57/71 | LOSS: 6.907375203307561e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 58/71 | LOSS: 6.908874075957949e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 59/71 | LOSS: 6.917308564879932e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 60/71 | LOSS: 6.902080057160242e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 61/71 | LOSS: 6.890965631746146e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 62/71 | LOSS: 6.882585578765314e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 63/71 | LOSS: 6.893414841613321e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 64/71 | LOSS: 6.879794070230743e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 65/71 | LOSS: 6.870097388056103e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 66/71 | LOSS: 6.864765360497491e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 67/71 | LOSS: 6.8670304258108335e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 68/71 | LOSS: 6.8542559031060515e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 69/71 | LOSS: 6.849527004045999e-06\n",
      "TRAIN: EPOCH 157/1000 | BATCH 70/71 | LOSS: 6.842736978105164e-06\n",
      "VAL: EPOCH 157/1000 | BATCH 0/8 | LOSS: 8.133257324516308e-06\n",
      "VAL: EPOCH 157/1000 | BATCH 1/8 | LOSS: 7.82812981015013e-06\n",
      "VAL: EPOCH 157/1000 | BATCH 2/8 | LOSS: 6.992859349945017e-06\n",
      "VAL: EPOCH 157/1000 | BATCH 3/8 | LOSS: 6.918135682099091e-06\n",
      "VAL: EPOCH 157/1000 | BATCH 4/8 | LOSS: 7.091572206263663e-06\n",
      "VAL: EPOCH 157/1000 | BATCH 5/8 | LOSS: 6.8993128934380366e-06\n",
      "VAL: EPOCH 157/1000 | BATCH 6/8 | LOSS: 6.651674279315298e-06\n",
      "VAL: EPOCH 157/1000 | BATCH 7/8 | LOSS: 6.672226220416633e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 0/71 | LOSS: 5.996175332256826e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 1/71 | LOSS: 5.557821395996143e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 2/71 | LOSS: 5.614355207702222e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 3/71 | LOSS: 6.07677986863564e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 4/71 | LOSS: 6.57675418551662e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 5/71 | LOSS: 7.0779503857920645e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 6/71 | LOSS: 7.102143626980251e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 7/71 | LOSS: 7.0399010496657866e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 8/71 | LOSS: 7.123741094498352e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 9/71 | LOSS: 7.0378297550632855e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 10/71 | LOSS: 7.114362109529214e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 11/71 | LOSS: 7.311482666712739e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 12/71 | LOSS: 7.4380620669846676e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 13/71 | LOSS: 7.377063379213464e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 14/71 | LOSS: 7.496337730117375e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 15/71 | LOSS: 7.550115213916797e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 16/71 | LOSS: 7.5649189271002e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 17/71 | LOSS: 7.659407426924898e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 18/71 | LOSS: 7.659743579623816e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 19/71 | LOSS: 7.693098609706794e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 20/71 | LOSS: 7.668992869488042e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 21/71 | LOSS: 7.740477795695453e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 22/71 | LOSS: 7.91949635284887e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 23/71 | LOSS: 7.84075369134977e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 24/71 | LOSS: 7.808318769093602e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 25/71 | LOSS: 7.74511569873609e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 26/71 | LOSS: 7.692702672136e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 27/71 | LOSS: 7.665099246878526e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 28/71 | LOSS: 7.648900636609298e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 29/71 | LOSS: 7.638595813356611e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 30/71 | LOSS: 7.666931181912303e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 31/71 | LOSS: 7.6067843650662326e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 32/71 | LOSS: 7.577274600683295e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 33/71 | LOSS: 7.602564584671841e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 34/71 | LOSS: 7.621480199304642e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 35/71 | LOSS: 7.633520744497622e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 36/71 | LOSS: 7.593163408691416e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 37/71 | LOSS: 7.735396761027483e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 38/71 | LOSS: 7.751330206072173e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 39/71 | LOSS: 7.866096029829351e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 40/71 | LOSS: 7.822152816160958e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 41/71 | LOSS: 7.80826808078841e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 42/71 | LOSS: 7.834345164748049e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 43/71 | LOSS: 7.846850642388084e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 44/71 | LOSS: 7.842858515990277e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 45/71 | LOSS: 7.89754429443628e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 46/71 | LOSS: 8.03148300969661e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 47/71 | LOSS: 7.983932079014266e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 48/71 | LOSS: 7.98661688591261e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 49/71 | LOSS: 7.985253696460858e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 50/71 | LOSS: 7.999852864575623e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 51/71 | LOSS: 7.961021660426362e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 52/71 | LOSS: 7.945584705871539e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 53/71 | LOSS: 7.92231194450492e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 54/71 | LOSS: 7.872933524181876e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 55/71 | LOSS: 7.833402256502658e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 56/71 | LOSS: 7.801807082854028e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 57/71 | LOSS: 7.808423270975693e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 58/71 | LOSS: 7.79603184991313e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 59/71 | LOSS: 7.819229593527173e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 60/71 | LOSS: 7.824215848157736e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 61/71 | LOSS: 7.782059634747941e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 62/71 | LOSS: 7.778115653468015e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 63/71 | LOSS: 7.770495095371643e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 64/71 | LOSS: 7.766719381134653e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 65/71 | LOSS: 7.80838082171227e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 66/71 | LOSS: 7.781849931302191e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 67/71 | LOSS: 7.805119667757348e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 68/71 | LOSS: 7.756899005629668e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 69/71 | LOSS: 7.739036352307137e-06\n",
      "TRAIN: EPOCH 158/1000 | BATCH 70/71 | LOSS: 7.789199308961184e-06\n",
      "VAL: EPOCH 158/1000 | BATCH 0/8 | LOSS: 8.391992196266074e-06\n",
      "VAL: EPOCH 158/1000 | BATCH 1/8 | LOSS: 7.819120810381719e-06\n",
      "VAL: EPOCH 158/1000 | BATCH 2/8 | LOSS: 7.170721649648233e-06\n",
      "VAL: EPOCH 158/1000 | BATCH 3/8 | LOSS: 7.070866672620468e-06\n",
      "VAL: EPOCH 158/1000 | BATCH 4/8 | LOSS: 7.095027103787288e-06\n",
      "VAL: EPOCH 158/1000 | BATCH 5/8 | LOSS: 7.020178069675846e-06\n",
      "VAL: EPOCH 158/1000 | BATCH 6/8 | LOSS: 6.788157211953408e-06\n",
      "VAL: EPOCH 158/1000 | BATCH 7/8 | LOSS: 6.780257933769462e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 0/71 | LOSS: 6.3248162405216135e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 1/71 | LOSS: 7.175915925472509e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 2/71 | LOSS: 7.223622257394406e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 3/71 | LOSS: 7.037399882392492e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 4/71 | LOSS: 7.013490903773345e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 5/71 | LOSS: 6.883698536815548e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 6/71 | LOSS: 6.970149700334462e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 7/71 | LOSS: 6.942921288555226e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 8/71 | LOSS: 6.846795006115119e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 9/71 | LOSS: 6.757410437785438e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 10/71 | LOSS: 6.76756289149952e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 11/71 | LOSS: 6.904224619574961e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 12/71 | LOSS: 6.912954946668693e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 13/71 | LOSS: 7.008429650211058e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 14/71 | LOSS: 7.0529406912100965e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 15/71 | LOSS: 7.084343394581083e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 16/71 | LOSS: 6.98865524635253e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 17/71 | LOSS: 6.959497618986967e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 18/71 | LOSS: 7.024630433175173e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 19/71 | LOSS: 7.028832510513894e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 20/71 | LOSS: 7.131332083753521e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 21/71 | LOSS: 7.117111325979667e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 22/71 | LOSS: 7.091732542270936e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 23/71 | LOSS: 7.045548462277414e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 24/71 | LOSS: 6.996224983595311e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 25/71 | LOSS: 6.967065543834066e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 26/71 | LOSS: 6.929878542939184e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 27/71 | LOSS: 6.880876932362818e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 28/71 | LOSS: 6.862423184617816e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 29/71 | LOSS: 6.899035436921016e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 30/71 | LOSS: 6.933693531473475e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 31/71 | LOSS: 6.918449855675135e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 32/71 | LOSS: 6.878706974958098e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 33/71 | LOSS: 6.880944163319381e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 34/71 | LOSS: 6.873958461385752e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 35/71 | LOSS: 6.907719251911557e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 36/71 | LOSS: 6.880508162548476e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 37/71 | LOSS: 6.852778202109651e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 38/71 | LOSS: 6.878068000869229e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 39/71 | LOSS: 6.879807278892258e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 40/71 | LOSS: 6.90440027960349e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 41/71 | LOSS: 6.948388697007128e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 42/71 | LOSS: 6.93625023877546e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 43/71 | LOSS: 6.9334681608200874e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 44/71 | LOSS: 6.98978148850276e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 45/71 | LOSS: 6.998300299844983e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 46/71 | LOSS: 6.9952610577642425e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 47/71 | LOSS: 7.058709854845802e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 48/71 | LOSS: 7.086841870492505e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 49/71 | LOSS: 7.103353473212337e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 50/71 | LOSS: 7.071461832132556e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 51/71 | LOSS: 7.075665245373295e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 52/71 | LOSS: 7.08376537316e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 53/71 | LOSS: 7.0707842328206255e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 54/71 | LOSS: 7.062877549205124e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 55/71 | LOSS: 7.050895516970611e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 56/71 | LOSS: 7.081866897351574e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 57/71 | LOSS: 7.072044182082052e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 58/71 | LOSS: 7.094951951777626e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 59/71 | LOSS: 7.07988724570896e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 60/71 | LOSS: 7.066179545081915e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 61/71 | LOSS: 7.0833753969993506e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 62/71 | LOSS: 7.061773813470537e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 63/71 | LOSS: 7.068165892576417e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 64/71 | LOSS: 7.063900438179889e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 65/71 | LOSS: 7.083337929240467e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 66/71 | LOSS: 7.099185895911894e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 67/71 | LOSS: 7.087329396092494e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 68/71 | LOSS: 7.0941434479618035e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 69/71 | LOSS: 7.1128059386994985e-06\n",
      "TRAIN: EPOCH 159/1000 | BATCH 70/71 | LOSS: 7.137874689133113e-06\n",
      "VAL: EPOCH 159/1000 | BATCH 0/8 | LOSS: 8.212929969886318e-06\n",
      "VAL: EPOCH 159/1000 | BATCH 1/8 | LOSS: 8.528259513695957e-06\n",
      "VAL: EPOCH 159/1000 | BATCH 2/8 | LOSS: 7.592189831484575e-06\n",
      "VAL: EPOCH 159/1000 | BATCH 3/8 | LOSS: 8.001180276551167e-06\n",
      "VAL: EPOCH 159/1000 | BATCH 4/8 | LOSS: 8.096281453617848e-06\n",
      "VAL: EPOCH 159/1000 | BATCH 5/8 | LOSS: 7.90060471445031e-06\n",
      "VAL: EPOCH 159/1000 | BATCH 6/8 | LOSS: 7.676142104173778e-06\n",
      "VAL: EPOCH 159/1000 | BATCH 7/8 | LOSS: 7.557568665106373e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 0/71 | LOSS: 8.055025318753906e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 1/71 | LOSS: 7.502503194700694e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 2/71 | LOSS: 7.548516805400141e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 3/71 | LOSS: 7.160441100495518e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 4/71 | LOSS: 6.961126655369299e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 5/71 | LOSS: 6.948151925219766e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 6/71 | LOSS: 7.135505516738963e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 7/71 | LOSS: 7.112760044947208e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 8/71 | LOSS: 7.286853967040467e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 9/71 | LOSS: 7.406334589177277e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 10/71 | LOSS: 7.231445124489255e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 11/71 | LOSS: 7.437822053664907e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 12/71 | LOSS: 7.318726042845251e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 13/71 | LOSS: 7.290587096317072e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 14/71 | LOSS: 7.2172533691627905e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 15/71 | LOSS: 7.1268491410592105e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 16/71 | LOSS: 7.153210042494774e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 17/71 | LOSS: 7.076153679008712e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 18/71 | LOSS: 6.947332789614417e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 19/71 | LOSS: 7.000717164373782e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 20/71 | LOSS: 7.000436696779659e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 21/71 | LOSS: 6.938700649202061e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 22/71 | LOSS: 6.957631878480163e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 23/71 | LOSS: 6.9792935732948536e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 24/71 | LOSS: 6.926721125637414e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 25/71 | LOSS: 6.928584805511771e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 26/71 | LOSS: 6.8831747605010475e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 27/71 | LOSS: 6.86167010956394e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 28/71 | LOSS: 6.841452761426776e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 29/71 | LOSS: 6.863505556490661e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 30/71 | LOSS: 6.857224933252926e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 31/71 | LOSS: 6.908799520033426e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 32/71 | LOSS: 6.946775901076652e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 33/71 | LOSS: 6.914070635342368e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 34/71 | LOSS: 6.889048069881807e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 35/71 | LOSS: 6.9793646212524945e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 36/71 | LOSS: 6.994148001516855e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 37/71 | LOSS: 6.998896220753331e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 38/71 | LOSS: 6.950359152259084e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 39/71 | LOSS: 6.919114650827396e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 40/71 | LOSS: 6.973363369757951e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 41/71 | LOSS: 6.982244886308936e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 42/71 | LOSS: 7.029882792351621e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 43/71 | LOSS: 7.030599848886513e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 44/71 | LOSS: 7.086674395395676e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 45/71 | LOSS: 7.126340249116133e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 46/71 | LOSS: 7.143746314157281e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 47/71 | LOSS: 7.1796845020344335e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 48/71 | LOSS: 7.207886214093043e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 49/71 | LOSS: 7.238489279188798e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 50/71 | LOSS: 7.285274397658855e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 51/71 | LOSS: 7.283863440604294e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 52/71 | LOSS: 7.321191461878183e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 53/71 | LOSS: 7.295783394443721e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 54/71 | LOSS: 7.379501361852321e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 55/71 | LOSS: 7.361961893691062e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 56/71 | LOSS: 7.38658560581634e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 57/71 | LOSS: 7.3816006220270006e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 58/71 | LOSS: 7.381388005049272e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 59/71 | LOSS: 7.381002243770733e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 60/71 | LOSS: 7.389323974300661e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 61/71 | LOSS: 7.392470369297605e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 62/71 | LOSS: 7.36264368358192e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 63/71 | LOSS: 7.322437646450908e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 64/71 | LOSS: 7.3359770515078424e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 65/71 | LOSS: 7.307750827363414e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 66/71 | LOSS: 7.316980583112384e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 67/71 | LOSS: 7.322795273894906e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 68/71 | LOSS: 7.3071188007420656e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 69/71 | LOSS: 7.295924219111579e-06\n",
      "TRAIN: EPOCH 160/1000 | BATCH 70/71 | LOSS: 7.299609094356846e-06\n",
      "VAL: EPOCH 160/1000 | BATCH 0/8 | LOSS: 7.894030204624869e-06\n",
      "VAL: EPOCH 160/1000 | BATCH 1/8 | LOSS: 7.753089903417276e-06\n",
      "VAL: EPOCH 160/1000 | BATCH 2/8 | LOSS: 7.3902228905353695e-06\n",
      "VAL: EPOCH 160/1000 | BATCH 3/8 | LOSS: 7.60938041821646e-06\n",
      "VAL: EPOCH 160/1000 | BATCH 4/8 | LOSS: 7.816501420165877e-06\n",
      "VAL: EPOCH 160/1000 | BATCH 5/8 | LOSS: 7.606267899973318e-06\n",
      "VAL: EPOCH 160/1000 | BATCH 6/8 | LOSS: 7.3597920423448415e-06\n",
      "VAL: EPOCH 160/1000 | BATCH 7/8 | LOSS: 7.3090456567115325e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 0/71 | LOSS: 7.841713340894785e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 1/71 | LOSS: 7.332143468374852e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 2/71 | LOSS: 7.222380797126486e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 3/71 | LOSS: 7.223315037663269e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 4/71 | LOSS: 7.25576328477473e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 5/71 | LOSS: 7.0709046061286545e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 6/71 | LOSS: 6.86115358153724e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 7/71 | LOSS: 7.026037678770081e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 8/71 | LOSS: 6.855011431778419e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 9/71 | LOSS: 6.809298247389961e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 10/71 | LOSS: 6.822746358507588e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 11/71 | LOSS: 6.886297721090766e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 12/71 | LOSS: 6.85050298880714e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 13/71 | LOSS: 6.871380459675233e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 14/71 | LOSS: 6.7898054718777225e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 15/71 | LOSS: 6.769628328129329e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 16/71 | LOSS: 6.701340439844096e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 17/71 | LOSS: 6.722475366890042e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 18/71 | LOSS: 6.685979410816033e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 19/71 | LOSS: 6.832604776718654e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 20/71 | LOSS: 6.810042881018792e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 21/71 | LOSS: 6.807934947904538e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 22/71 | LOSS: 6.89937085395365e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 23/71 | LOSS: 6.830745519437187e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 24/71 | LOSS: 6.7636353924172e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 25/71 | LOSS: 6.798486293309207e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 26/71 | LOSS: 6.832227107359493e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 27/71 | LOSS: 6.89186237780502e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 28/71 | LOSS: 6.866357470526368e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 29/71 | LOSS: 7.013124392566775e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 30/71 | LOSS: 7.026650725874201e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 31/71 | LOSS: 6.983181194186727e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 32/71 | LOSS: 7.074293438508351e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 33/71 | LOSS: 7.091848668578143e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 34/71 | LOSS: 7.0345400802450185e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 35/71 | LOSS: 7.088807314580157e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 36/71 | LOSS: 7.072734575017789e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 37/71 | LOSS: 7.070846705621654e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 38/71 | LOSS: 7.1087448794479806e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 39/71 | LOSS: 7.1501722345601594e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 40/71 | LOSS: 7.157205090618113e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 41/71 | LOSS: 7.174535334169854e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 42/71 | LOSS: 7.207014996945576e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 43/71 | LOSS: 7.205456478748767e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 44/71 | LOSS: 7.237552967530468e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 45/71 | LOSS: 7.1944290756737645e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 46/71 | LOSS: 7.1511386290404385e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 47/71 | LOSS: 7.227546149124464e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 48/71 | LOSS: 7.2129357472384035e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 49/71 | LOSS: 7.259298317876528e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 50/71 | LOSS: 7.235752901561129e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 51/71 | LOSS: 7.238743076133967e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 52/71 | LOSS: 7.260191068780841e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 53/71 | LOSS: 7.277937862050345e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 54/71 | LOSS: 7.241356930039315e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 55/71 | LOSS: 7.245256369969866e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 56/71 | LOSS: 7.236482807664352e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 57/71 | LOSS: 7.220422103587068e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 58/71 | LOSS: 7.20191289890524e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 59/71 | LOSS: 7.201543295802064e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 60/71 | LOSS: 7.17615249100834e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 61/71 | LOSS: 7.192277889184732e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 62/71 | LOSS: 7.1812056549311996e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 63/71 | LOSS: 7.170065650541346e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 64/71 | LOSS: 7.172868187738529e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 65/71 | LOSS: 7.160685051778935e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 66/71 | LOSS: 7.147990168932801e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 67/71 | LOSS: 7.174022257191502e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 68/71 | LOSS: 7.142935189314311e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 69/71 | LOSS: 7.145097593560682e-06\n",
      "TRAIN: EPOCH 161/1000 | BATCH 70/71 | LOSS: 7.131648025804193e-06\n",
      "VAL: EPOCH 161/1000 | BATCH 0/8 | LOSS: 8.13226870377548e-06\n",
      "VAL: EPOCH 161/1000 | BATCH 1/8 | LOSS: 8.266689292213414e-06\n",
      "VAL: EPOCH 161/1000 | BATCH 2/8 | LOSS: 7.357330559898401e-06\n",
      "VAL: EPOCH 161/1000 | BATCH 3/8 | LOSS: 7.462921871592698e-06\n",
      "VAL: EPOCH 161/1000 | BATCH 4/8 | LOSS: 7.6255330895946825e-06\n",
      "VAL: EPOCH 161/1000 | BATCH 5/8 | LOSS: 7.4793639820806374e-06\n",
      "VAL: EPOCH 161/1000 | BATCH 6/8 | LOSS: 7.332491415062188e-06\n",
      "VAL: EPOCH 161/1000 | BATCH 7/8 | LOSS: 7.252594855344796e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 0/71 | LOSS: 6.5147805798915215e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 1/71 | LOSS: 6.545461246787454e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 2/71 | LOSS: 6.52314687007068e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 3/71 | LOSS: 7.311637432394491e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 4/71 | LOSS: 7.092468604241731e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 5/71 | LOSS: 6.996486793771813e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 6/71 | LOSS: 7.036861071745599e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 7/71 | LOSS: 7.017009863830026e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 8/71 | LOSS: 6.9546003942377865e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 9/71 | LOSS: 7.037094110273756e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 10/71 | LOSS: 6.905512071336323e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 11/71 | LOSS: 6.971730954319355e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 12/71 | LOSS: 6.9133820943534374e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 13/71 | LOSS: 6.93938539240792e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 14/71 | LOSS: 6.949501433458257e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 15/71 | LOSS: 6.853661375316733e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 16/71 | LOSS: 6.941312462646131e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 17/71 | LOSS: 6.911889487229119e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 18/71 | LOSS: 6.894993332121215e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 19/71 | LOSS: 6.966408568587212e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 20/71 | LOSS: 6.950705548141352e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 21/71 | LOSS: 6.912640401424142e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 22/71 | LOSS: 6.882274034148355e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 23/71 | LOSS: 6.852600658172984e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 24/71 | LOSS: 6.831095361121698e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 25/71 | LOSS: 6.801118825723489e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 26/71 | LOSS: 6.814114857223575e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 27/71 | LOSS: 6.770188877323692e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 28/71 | LOSS: 6.7447082732667635e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 29/71 | LOSS: 6.7447585024638105e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 30/71 | LOSS: 6.733408627628852e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 31/71 | LOSS: 6.759798012012652e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 32/71 | LOSS: 6.7756311154605075e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 33/71 | LOSS: 6.765502589319915e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 34/71 | LOSS: 6.7557724313311545e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 35/71 | LOSS: 6.789309433467376e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 36/71 | LOSS: 6.814383620646392e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 37/71 | LOSS: 6.8495657471526345e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 38/71 | LOSS: 6.850290796697104e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 39/71 | LOSS: 6.925298043825023e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 40/71 | LOSS: 6.9437862582229325e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 41/71 | LOSS: 7.020850262051681e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 42/71 | LOSS: 7.04853474142214e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 43/71 | LOSS: 7.067308719011965e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 44/71 | LOSS: 7.156381434469949e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 45/71 | LOSS: 7.1314616730966876e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 46/71 | LOSS: 7.219755437464746e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 47/71 | LOSS: 7.219604138223683e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 48/71 | LOSS: 7.218977084356046e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 49/71 | LOSS: 7.274358958966331e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 50/71 | LOSS: 7.279279481165349e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 51/71 | LOSS: 7.28641241319285e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 52/71 | LOSS: 7.2578189950568145e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 53/71 | LOSS: 7.283826292863253e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 54/71 | LOSS: 7.293788522391961e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 55/71 | LOSS: 7.281399437683181e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 56/71 | LOSS: 7.324778164873255e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 57/71 | LOSS: 7.33066838557565e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 58/71 | LOSS: 7.3193025516155685e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 59/71 | LOSS: 7.332127772012124e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 60/71 | LOSS: 7.306840118081843e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 61/71 | LOSS: 7.354280408054042e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 62/71 | LOSS: 7.3574081340218925e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 63/71 | LOSS: 7.389374658828274e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 64/71 | LOSS: 7.3984018364661515e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 65/71 | LOSS: 7.448807647244504e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 66/71 | LOSS: 7.469796577452008e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 67/71 | LOSS: 7.512089991103173e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 68/71 | LOSS: 7.549832896957937e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 69/71 | LOSS: 7.521082151080399e-06\n",
      "TRAIN: EPOCH 162/1000 | BATCH 70/71 | LOSS: 7.546739402962078e-06\n",
      "VAL: EPOCH 162/1000 | BATCH 0/8 | LOSS: 8.61723947309656e-06\n",
      "VAL: EPOCH 162/1000 | BATCH 1/8 | LOSS: 7.71298368817952e-06\n",
      "VAL: EPOCH 162/1000 | BATCH 2/8 | LOSS: 7.415593548406226e-06\n",
      "VAL: EPOCH 162/1000 | BATCH 3/8 | LOSS: 7.507125701522455e-06\n",
      "VAL: EPOCH 162/1000 | BATCH 4/8 | LOSS: 7.595958413730841e-06\n",
      "VAL: EPOCH 162/1000 | BATCH 5/8 | LOSS: 7.543171174499245e-06\n",
      "VAL: EPOCH 162/1000 | BATCH 6/8 | LOSS: 7.311841922533599e-06\n",
      "VAL: EPOCH 162/1000 | BATCH 7/8 | LOSS: 7.241845082717191e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 0/71 | LOSS: 6.60775413052761e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 1/71 | LOSS: 8.387520892938483e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 2/71 | LOSS: 8.884592261892976e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 3/71 | LOSS: 8.689171067999268e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 4/71 | LOSS: 8.832760522636818e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 5/71 | LOSS: 8.54392972845138e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 6/71 | LOSS: 9.30093022231761e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 7/71 | LOSS: 8.987424052975257e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 8/71 | LOSS: 8.998901749792923e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 9/71 | LOSS: 9.356035025120946e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 10/71 | LOSS: 9.248928687487602e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 11/71 | LOSS: 9.495718965505754e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 12/71 | LOSS: 9.296088662197759e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 13/71 | LOSS: 9.363752561902012e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 14/71 | LOSS: 9.329773608139173e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 15/71 | LOSS: 9.234535895075169e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 16/71 | LOSS: 9.248891916179085e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 17/71 | LOSS: 9.097479429935143e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 18/71 | LOSS: 9.062120553655978e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 19/71 | LOSS: 8.93702310804656e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 20/71 | LOSS: 8.809152101700116e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 21/71 | LOSS: 8.725143516130629e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 22/71 | LOSS: 8.578486813348718e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 23/71 | LOSS: 8.58394620687856e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 24/71 | LOSS: 8.547662000637501e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 25/71 | LOSS: 8.49703795980671e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 26/71 | LOSS: 8.532912471747733e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 27/71 | LOSS: 8.439600492238242e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 28/71 | LOSS: 8.392990931215222e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 29/71 | LOSS: 8.41051698140897e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 30/71 | LOSS: 8.37264945901904e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 31/71 | LOSS: 8.30837250020977e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 32/71 | LOSS: 8.244264405000264e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 33/71 | LOSS: 8.289890314806731e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 34/71 | LOSS: 8.231447165079382e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 35/71 | LOSS: 8.242235392370074e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 36/71 | LOSS: 8.187385274878407e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 37/71 | LOSS: 8.29778916436465e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 38/71 | LOSS: 8.26099336009998e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 39/71 | LOSS: 8.264831922133454e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 40/71 | LOSS: 8.339606210938655e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 41/71 | LOSS: 8.287115525447353e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 42/71 | LOSS: 8.337622243426415e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 43/71 | LOSS: 8.333038377631139e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 44/71 | LOSS: 8.311943070617013e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 45/71 | LOSS: 8.296738196409221e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 46/71 | LOSS: 8.262968249448719e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 47/71 | LOSS: 8.249693091026225e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 48/71 | LOSS: 8.206243930952932e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 49/71 | LOSS: 8.204395899156224e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 50/71 | LOSS: 8.171400078900465e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 51/71 | LOSS: 8.165992915984623e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 52/71 | LOSS: 8.160696874050322e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 53/71 | LOSS: 8.134450564722539e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 54/71 | LOSS: 8.112587784125935e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 55/71 | LOSS: 8.089446298851857e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 56/71 | LOSS: 8.071896948087287e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 57/71 | LOSS: 8.034789814324729e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 58/71 | LOSS: 8.018262494677062e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 59/71 | LOSS: 7.982282401523359e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 60/71 | LOSS: 7.976122284818586e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 61/71 | LOSS: 7.956532367496684e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 62/71 | LOSS: 7.933679958186409e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 63/71 | LOSS: 7.915078171549794e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 64/71 | LOSS: 7.90146405051928e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 65/71 | LOSS: 7.881704050305416e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 66/71 | LOSS: 7.857594747333474e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 67/71 | LOSS: 7.823188257936957e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 68/71 | LOSS: 7.804837443235675e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 69/71 | LOSS: 7.77691574772429e-06\n",
      "TRAIN: EPOCH 163/1000 | BATCH 70/71 | LOSS: 7.786293940946154e-06\n",
      "VAL: EPOCH 163/1000 | BATCH 0/8 | LOSS: 8.156829608196858e-06\n",
      "VAL: EPOCH 163/1000 | BATCH 1/8 | LOSS: 7.18333922122838e-06\n",
      "VAL: EPOCH 163/1000 | BATCH 2/8 | LOSS: 6.967108068541468e-06\n",
      "VAL: EPOCH 163/1000 | BATCH 3/8 | LOSS: 6.806280680393684e-06\n",
      "VAL: EPOCH 163/1000 | BATCH 4/8 | LOSS: 6.968487105041276e-06\n",
      "VAL: EPOCH 163/1000 | BATCH 5/8 | LOSS: 6.798474259994691e-06\n",
      "VAL: EPOCH 163/1000 | BATCH 6/8 | LOSS: 6.519696398754604e-06\n",
      "VAL: EPOCH 163/1000 | BATCH 7/8 | LOSS: 6.545394626300549e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 0/71 | LOSS: 5.746401711803628e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 1/71 | LOSS: 5.80061532673426e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 2/71 | LOSS: 6.057897432280394e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 3/71 | LOSS: 6.422188334909151e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 4/71 | LOSS: 6.372803727572318e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 5/71 | LOSS: 6.6904682777627995e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 6/71 | LOSS: 6.6568289055015025e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 7/71 | LOSS: 6.588516612282547e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 8/71 | LOSS: 6.763637025465465e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 9/71 | LOSS: 6.697210619677208e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 10/71 | LOSS: 6.8539493854173505e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 11/71 | LOSS: 6.687096060886688e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 12/71 | LOSS: 6.774139679831793e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 13/71 | LOSS: 6.91399765985677e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 14/71 | LOSS: 6.911808304721489e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 15/71 | LOSS: 6.960317818993644e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 16/71 | LOSS: 6.930636118340772e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 17/71 | LOSS: 6.935174572693843e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 18/71 | LOSS: 6.8744437062138645e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 19/71 | LOSS: 6.853908325865632e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 20/71 | LOSS: 6.886508691379623e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 21/71 | LOSS: 6.819061809437699e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 22/71 | LOSS: 6.832368754934894e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 23/71 | LOSS: 6.822772794142414e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 24/71 | LOSS: 6.738203082932159e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 25/71 | LOSS: 6.6547125488096544e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 26/71 | LOSS: 6.654633765450584e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 27/71 | LOSS: 6.718481878514078e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 28/71 | LOSS: 6.708978570055353e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 29/71 | LOSS: 6.7023083223224e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 30/71 | LOSS: 6.696938022111182e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 31/71 | LOSS: 6.713791805168512e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 32/71 | LOSS: 6.72333160103087e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 33/71 | LOSS: 6.662817160010009e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 34/71 | LOSS: 6.674775977444369e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 35/71 | LOSS: 6.662426560271544e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 36/71 | LOSS: 6.6402006612750826e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 37/71 | LOSS: 6.6251575031186035e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 38/71 | LOSS: 6.6144905012594954e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 39/71 | LOSS: 6.5806908992271925e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 40/71 | LOSS: 6.571100817470834e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 41/71 | LOSS: 6.575583310699412e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 42/71 | LOSS: 6.574487415914624e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 43/71 | LOSS: 6.566969742297343e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 44/71 | LOSS: 6.560205545408988e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 45/71 | LOSS: 6.5202905360543495e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 46/71 | LOSS: 6.609822408558721e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 47/71 | LOSS: 6.619588134526566e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 48/71 | LOSS: 6.654104458438993e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 49/71 | LOSS: 6.6868441717815585e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 50/71 | LOSS: 6.730678269021012e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 51/71 | LOSS: 6.818863662024692e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 52/71 | LOSS: 6.777197223476284e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 53/71 | LOSS: 6.8600514977232606e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 54/71 | LOSS: 6.8699206002003684e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 55/71 | LOSS: 6.947558826401032e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 56/71 | LOSS: 7.011060341411442e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 57/71 | LOSS: 7.013629517038273e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 58/71 | LOSS: 7.089502906961508e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 59/71 | LOSS: 7.085113535746738e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 60/71 | LOSS: 7.201627221413636e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 61/71 | LOSS: 7.21121665675685e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 62/71 | LOSS: 7.2174392381197e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 63/71 | LOSS: 7.28736584676426e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 64/71 | LOSS: 7.2848629315558355e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 65/71 | LOSS: 7.336978101055578e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 66/71 | LOSS: 7.317664794619074e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 67/71 | LOSS: 7.318519592177729e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 68/71 | LOSS: 7.297448176530821e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 69/71 | LOSS: 7.296809839577431e-06\n",
      "TRAIN: EPOCH 164/1000 | BATCH 70/71 | LOSS: 7.279828186653158e-06\n",
      "VAL: EPOCH 164/1000 | BATCH 0/8 | LOSS: 8.350595635420177e-06\n",
      "VAL: EPOCH 164/1000 | BATCH 1/8 | LOSS: 7.717217386016273e-06\n",
      "VAL: EPOCH 164/1000 | BATCH 2/8 | LOSS: 7.238095349748619e-06\n",
      "VAL: EPOCH 164/1000 | BATCH 3/8 | LOSS: 7.070366564221331e-06\n",
      "VAL: EPOCH 164/1000 | BATCH 4/8 | LOSS: 7.345645826717373e-06\n",
      "VAL: EPOCH 164/1000 | BATCH 5/8 | LOSS: 7.2731995715002995e-06\n",
      "VAL: EPOCH 164/1000 | BATCH 6/8 | LOSS: 7.160075061360008e-06\n",
      "VAL: EPOCH 164/1000 | BATCH 7/8 | LOSS: 7.1211223939826596e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 0/71 | LOSS: 5.114955911267316e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 1/71 | LOSS: 5.7016491155081894e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 2/71 | LOSS: 5.76084327500818e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 3/71 | LOSS: 6.272095220083429e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 4/71 | LOSS: 6.5451028604002205e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 5/71 | LOSS: 6.672655445072451e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 6/71 | LOSS: 6.56245103008197e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 7/71 | LOSS: 6.328598885829706e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 8/71 | LOSS: 6.550240616181529e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 9/71 | LOSS: 6.7514490638131974e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 10/71 | LOSS: 6.92268965642804e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 11/71 | LOSS: 6.979876729928947e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 12/71 | LOSS: 6.85596817745066e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 13/71 | LOSS: 6.99413661615316e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 14/71 | LOSS: 6.9827697795214284e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 15/71 | LOSS: 6.985862427200118e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 16/71 | LOSS: 6.970227679951623e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 17/71 | LOSS: 7.037058645033136e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 18/71 | LOSS: 6.99991939032579e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 19/71 | LOSS: 6.9989007670301365e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 20/71 | LOSS: 7.05940443540818e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 21/71 | LOSS: 7.030681383019493e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 22/71 | LOSS: 7.0756226823900565e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 23/71 | LOSS: 7.083127153843331e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 24/71 | LOSS: 7.1180217128130604e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 25/71 | LOSS: 7.125158320447484e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 26/71 | LOSS: 7.1415247475670185e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 27/71 | LOSS: 7.1850688365365e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 28/71 | LOSS: 7.224849453231055e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 29/71 | LOSS: 7.1651520859935165e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 30/71 | LOSS: 7.168135521022768e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 31/71 | LOSS: 7.240184316970044e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 32/71 | LOSS: 7.203879277133665e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 33/71 | LOSS: 7.191227843898506e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 34/71 | LOSS: 7.247089173948292e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 35/71 | LOSS: 7.244478246725015e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 36/71 | LOSS: 7.2096785933771985e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 37/71 | LOSS: 7.220684928573687e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 38/71 | LOSS: 7.270626424425968e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 39/71 | LOSS: 7.27430200413437e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 40/71 | LOSS: 7.239742322648143e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 41/71 | LOSS: 7.217362670936771e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 42/71 | LOSS: 7.2307945502384535e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 43/71 | LOSS: 7.231653853523312e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 44/71 | LOSS: 7.204756831116457e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 45/71 | LOSS: 7.1842490125045116e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 46/71 | LOSS: 7.218301784342521e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 47/71 | LOSS: 7.171144593106267e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 48/71 | LOSS: 7.137207376217702e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 49/71 | LOSS: 7.094323245837586e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 50/71 | LOSS: 7.062109747779437e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 51/71 | LOSS: 7.0883759935196185e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 52/71 | LOSS: 7.059561762874199e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 53/71 | LOSS: 7.082712609526762e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 54/71 | LOSS: 7.1262086666220885e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 55/71 | LOSS: 7.101718080905682e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 56/71 | LOSS: 7.103217588640594e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 57/71 | LOSS: 7.169369084671228e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 58/71 | LOSS: 7.199985548603285e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 59/71 | LOSS: 7.191045673001402e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 60/71 | LOSS: 7.208979054741546e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 61/71 | LOSS: 7.2211634922554835e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 62/71 | LOSS: 7.22382303917714e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 63/71 | LOSS: 7.2702921158906975e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 64/71 | LOSS: 7.252395642773571e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 65/71 | LOSS: 7.297621295671861e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 66/71 | LOSS: 7.288995722991609e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 67/71 | LOSS: 7.299134722416056e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 68/71 | LOSS: 7.396495372328597e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 69/71 | LOSS: 7.390334056773489e-06\n",
      "TRAIN: EPOCH 165/1000 | BATCH 70/71 | LOSS: 7.433763783692356e-06\n",
      "VAL: EPOCH 165/1000 | BATCH 0/8 | LOSS: 9.337455594504718e-06\n",
      "VAL: EPOCH 165/1000 | BATCH 1/8 | LOSS: 9.891286481433781e-06\n",
      "VAL: EPOCH 165/1000 | BATCH 2/8 | LOSS: 8.90107700494506e-06\n",
      "VAL: EPOCH 165/1000 | BATCH 3/8 | LOSS: 9.323915946879424e-06\n",
      "VAL: EPOCH 165/1000 | BATCH 4/8 | LOSS: 9.400213457411155e-06\n",
      "VAL: EPOCH 165/1000 | BATCH 5/8 | LOSS: 9.248302073198525e-06\n",
      "VAL: EPOCH 165/1000 | BATCH 6/8 | LOSS: 9.082145749874013e-06\n",
      "VAL: EPOCH 165/1000 | BATCH 7/8 | LOSS: 8.89257728431403e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 0/71 | LOSS: 7.378821919701295e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 1/71 | LOSS: 7.364359362327377e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 2/71 | LOSS: 6.98342698039293e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 3/71 | LOSS: 7.512103707085771e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 4/71 | LOSS: 7.418796030833619e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 5/71 | LOSS: 7.421311693178723e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 6/71 | LOSS: 7.699140171877974e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 7/71 | LOSS: 7.714942626080301e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 8/71 | LOSS: 7.597063106206608e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 9/71 | LOSS: 7.887826905061957e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 10/71 | LOSS: 7.848024820718406e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 11/71 | LOSS: 8.018462115918131e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 12/71 | LOSS: 8.042330521745429e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 13/71 | LOSS: 8.008803206394077e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 14/71 | LOSS: 8.141654537515327e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 15/71 | LOSS: 8.024603971534816e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 16/71 | LOSS: 7.954516524264096e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 17/71 | LOSS: 7.903273854026338e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 18/71 | LOSS: 7.985561512070585e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 19/71 | LOSS: 7.99475119492854e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 20/71 | LOSS: 7.9645818307957e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 21/71 | LOSS: 7.961074019972743e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 22/71 | LOSS: 8.074140841136282e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 23/71 | LOSS: 8.086996823900941e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 24/71 | LOSS: 8.051541844906752e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 25/71 | LOSS: 8.127040333560077e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 26/71 | LOSS: 8.08428504333952e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 27/71 | LOSS: 8.134585316708711e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 28/71 | LOSS: 8.217469916542477e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 29/71 | LOSS: 8.207547519608246e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 30/71 | LOSS: 8.245901265200377e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 31/71 | LOSS: 8.251962356098375e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 32/71 | LOSS: 8.412883096642679e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 33/71 | LOSS: 8.353003683142187e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 34/71 | LOSS: 8.387847654895658e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 35/71 | LOSS: 8.369028920343328e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 36/71 | LOSS: 8.329146874382639e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 37/71 | LOSS: 8.332871283001672e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 38/71 | LOSS: 8.317319066592087e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 39/71 | LOSS: 8.290204709737736e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 40/71 | LOSS: 8.284202965846706e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 41/71 | LOSS: 8.263829797561767e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 42/71 | LOSS: 8.261061402451961e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 43/71 | LOSS: 8.202274288619678e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 44/71 | LOSS: 8.208052006213822e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 45/71 | LOSS: 8.158050476318433e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 46/71 | LOSS: 8.151715664620922e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 47/71 | LOSS: 8.097185874097098e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 48/71 | LOSS: 8.15583430545414e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 49/71 | LOSS: 8.091107310974621e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 50/71 | LOSS: 8.041644511703293e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 51/71 | LOSS: 8.04688228037356e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 52/71 | LOSS: 8.004766976159343e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 53/71 | LOSS: 7.998701381371507e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 54/71 | LOSS: 7.97863183999487e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 55/71 | LOSS: 7.996572573379776e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 56/71 | LOSS: 7.971170122657081e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 57/71 | LOSS: 7.972017642557682e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 58/71 | LOSS: 7.943358915804697e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 59/71 | LOSS: 7.883763585899335e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 60/71 | LOSS: 7.876174915181167e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 61/71 | LOSS: 7.857105643896859e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 62/71 | LOSS: 7.821211273126427e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 63/71 | LOSS: 7.828761276584828e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 64/71 | LOSS: 7.796739380585047e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 65/71 | LOSS: 7.761781138139089e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 66/71 | LOSS: 7.731931785188022e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 67/71 | LOSS: 7.733173821875158e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 68/71 | LOSS: 7.716875419303676e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 69/71 | LOSS: 7.682679954866346e-06\n",
      "TRAIN: EPOCH 166/1000 | BATCH 70/71 | LOSS: 7.720465848097762e-06\n",
      "VAL: EPOCH 166/1000 | BATCH 0/8 | LOSS: 8.048860763665289e-06\n",
      "VAL: EPOCH 166/1000 | BATCH 1/8 | LOSS: 7.3284290920128115e-06\n",
      "VAL: EPOCH 166/1000 | BATCH 2/8 | LOSS: 6.744393128125618e-06\n",
      "VAL: EPOCH 166/1000 | BATCH 3/8 | LOSS: 6.672434210486244e-06\n",
      "VAL: EPOCH 166/1000 | BATCH 4/8 | LOSS: 6.861776819278021e-06\n",
      "VAL: EPOCH 166/1000 | BATCH 5/8 | LOSS: 6.683671017526649e-06\n",
      "VAL: EPOCH 166/1000 | BATCH 6/8 | LOSS: 6.574414328497369e-06\n",
      "VAL: EPOCH 166/1000 | BATCH 7/8 | LOSS: 6.5980013346234045e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 0/71 | LOSS: 5.8418672779225744e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 1/71 | LOSS: 6.483157449110877e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 2/71 | LOSS: 6.203108265860162e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 3/71 | LOSS: 6.504387215500174e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 4/71 | LOSS: 6.545455926243449e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 5/71 | LOSS: 6.17581129821095e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 6/71 | LOSS: 6.208167178556323e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 7/71 | LOSS: 6.320053444142104e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 8/71 | LOSS: 6.258493259439193e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 9/71 | LOSS: 6.229826612980105e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 10/71 | LOSS: 6.176173139300028e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 11/71 | LOSS: 6.292792742594126e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 12/71 | LOSS: 6.336657476752477e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 13/71 | LOSS: 6.334504210566852e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 14/71 | LOSS: 6.347831489013818e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 15/71 | LOSS: 6.5076383748419175e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 16/71 | LOSS: 6.539458407881766e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 17/71 | LOSS: 6.466837501446005e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 18/71 | LOSS: 6.5068765922516926e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 19/71 | LOSS: 6.62944348732708e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 20/71 | LOSS: 6.6035405931567466e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 21/71 | LOSS: 6.535881187697999e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 22/71 | LOSS: 6.578302082229841e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 23/71 | LOSS: 6.65040346348178e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 24/71 | LOSS: 6.623573881370248e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 25/71 | LOSS: 6.594009042041412e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 26/71 | LOSS: 6.608734734241066e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 27/71 | LOSS: 6.62772012154684e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 28/71 | LOSS: 6.662201987386777e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 29/71 | LOSS: 6.657844141955138e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 30/71 | LOSS: 6.6182378759549465e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 31/71 | LOSS: 6.60093782300919e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 32/71 | LOSS: 6.5671943048073445e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 33/71 | LOSS: 6.609413784484359e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 34/71 | LOSS: 6.5971697430151316e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 35/71 | LOSS: 6.577493688079509e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 36/71 | LOSS: 6.623524719494476e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 37/71 | LOSS: 6.645969434478167e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 38/71 | LOSS: 6.625167992378687e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 39/71 | LOSS: 6.611498076836142e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 40/71 | LOSS: 6.6591864995688215e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 41/71 | LOSS: 6.68274338706743e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 42/71 | LOSS: 6.708752946081425e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 43/71 | LOSS: 6.701725469197712e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 44/71 | LOSS: 6.750578975495753e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 45/71 | LOSS: 6.762636837226202e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 46/71 | LOSS: 6.762375854103742e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 47/71 | LOSS: 6.772223921795255e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 48/71 | LOSS: 6.752344725843001e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 49/71 | LOSS: 6.804387039665016e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 50/71 | LOSS: 6.8055422910701425e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 51/71 | LOSS: 6.832150557574306e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 52/71 | LOSS: 6.855455810508366e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 53/71 | LOSS: 6.832650266611988e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 54/71 | LOSS: 6.820883547995684e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 55/71 | LOSS: 6.856368039669698e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 56/71 | LOSS: 6.835328692607602e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 57/71 | LOSS: 6.828246451450154e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 58/71 | LOSS: 6.84906340750986e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 59/71 | LOSS: 6.838537918459527e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 60/71 | LOSS: 6.829041586704571e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 61/71 | LOSS: 6.817773979609258e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 62/71 | LOSS: 6.837515013016639e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 63/71 | LOSS: 6.810113696076314e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 64/71 | LOSS: 6.7967791017928484e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 65/71 | LOSS: 6.816309127261223e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 66/71 | LOSS: 6.813943224160675e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 67/71 | LOSS: 6.8202316871226205e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 68/71 | LOSS: 6.816159582186906e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 69/71 | LOSS: 6.816230032719821e-06\n",
      "TRAIN: EPOCH 167/1000 | BATCH 70/71 | LOSS: 6.8194589992242364e-06\n",
      "VAL: EPOCH 167/1000 | BATCH 0/8 | LOSS: 7.872677088016644e-06\n",
      "VAL: EPOCH 167/1000 | BATCH 1/8 | LOSS: 7.1539052441949025e-06\n",
      "VAL: EPOCH 167/1000 | BATCH 2/8 | LOSS: 7.047300035386191e-06\n",
      "VAL: EPOCH 167/1000 | BATCH 3/8 | LOSS: 7.003766086199903e-06\n",
      "VAL: EPOCH 167/1000 | BATCH 4/8 | LOSS: 7.114202708180528e-06\n",
      "VAL: EPOCH 167/1000 | BATCH 5/8 | LOSS: 6.942488046964475e-06\n",
      "VAL: EPOCH 167/1000 | BATCH 6/8 | LOSS: 6.725029964270236e-06\n",
      "VAL: EPOCH 167/1000 | BATCH 7/8 | LOSS: 6.715760491715628e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 0/71 | LOSS: 6.5002650444512255e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 1/71 | LOSS: 7.816079687472666e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 2/71 | LOSS: 6.873553199208497e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 3/71 | LOSS: 7.680568614887306e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 4/71 | LOSS: 7.5626758189173415e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 5/71 | LOSS: 7.739397915429436e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 6/71 | LOSS: 7.618832309422682e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 7/71 | LOSS: 7.39313890107951e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 8/71 | LOSS: 7.586595276936552e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 9/71 | LOSS: 7.425653575410252e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 10/71 | LOSS: 7.747757701343454e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 11/71 | LOSS: 7.759618749029565e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 12/71 | LOSS: 8.031744108358487e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 13/71 | LOSS: 7.912006854634715e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 14/71 | LOSS: 7.893426391092362e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 15/71 | LOSS: 8.03609131594385e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 16/71 | LOSS: 7.907082336340168e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 17/71 | LOSS: 8.015578739812352e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 18/71 | LOSS: 7.86238258928949e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 19/71 | LOSS: 7.952878900141513e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 20/71 | LOSS: 8.010439051715712e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 21/71 | LOSS: 8.25996756826829e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 22/71 | LOSS: 8.333369865083469e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 23/71 | LOSS: 8.372690463905505e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 24/71 | LOSS: 8.5155840315565e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 25/71 | LOSS: 8.437030513820131e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 26/71 | LOSS: 8.649201845400967e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 27/71 | LOSS: 8.54377422651201e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 28/71 | LOSS: 8.609031691117582e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 29/71 | LOSS: 8.70641770234215e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 30/71 | LOSS: 8.604032935240426e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 31/71 | LOSS: 8.714355431038712e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 32/71 | LOSS: 8.738798869396277e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 33/71 | LOSS: 8.841396747276817e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 34/71 | LOSS: 8.97379161740121e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 35/71 | LOSS: 8.914409211987126e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 36/71 | LOSS: 9.082878932531457e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 37/71 | LOSS: 9.063344518117925e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 38/71 | LOSS: 9.17652409836405e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 39/71 | LOSS: 9.24434066291724e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 40/71 | LOSS: 9.24370574577507e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 41/71 | LOSS: 9.376404272015429e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 42/71 | LOSS: 9.344286259282706e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 43/71 | LOSS: 9.464696259700842e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 44/71 | LOSS: 9.484340545087536e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 45/71 | LOSS: 9.450000245282274e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 46/71 | LOSS: 9.545322969410955e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 47/71 | LOSS: 9.522148995226113e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 48/71 | LOSS: 9.609189911149156e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 49/71 | LOSS: 9.555647457091254e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 50/71 | LOSS: 9.675219233958713e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 51/71 | LOSS: 9.630345551578592e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 52/71 | LOSS: 9.590912925904257e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 53/71 | LOSS: 9.591334286664346e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 54/71 | LOSS: 9.595588876849929e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 55/71 | LOSS: 9.538381050333555e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 56/71 | LOSS: 9.522988588478734e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 57/71 | LOSS: 9.551210606050203e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 58/71 | LOSS: 9.521395533461069e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 59/71 | LOSS: 9.494913729213295e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 60/71 | LOSS: 9.489894674501198e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 61/71 | LOSS: 9.474245710026157e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 62/71 | LOSS: 9.432551422154546e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 63/71 | LOSS: 9.37989673843731e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 64/71 | LOSS: 9.331579167337623e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 65/71 | LOSS: 9.297497771595689e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 66/71 | LOSS: 9.254837489457551e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 67/71 | LOSS: 9.227188496986533e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 68/71 | LOSS: 9.19074537323204e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 69/71 | LOSS: 9.151143318116998e-06\n",
      "TRAIN: EPOCH 168/1000 | BATCH 70/71 | LOSS: 9.12283935640901e-06\n",
      "VAL: EPOCH 168/1000 | BATCH 0/8 | LOSS: 1.0311739060853142e-05\n",
      "VAL: EPOCH 168/1000 | BATCH 1/8 | LOSS: 8.71325573825743e-06\n",
      "VAL: EPOCH 168/1000 | BATCH 2/8 | LOSS: 8.832251296553295e-06\n",
      "VAL: EPOCH 168/1000 | BATCH 3/8 | LOSS: 8.256425644503906e-06\n",
      "VAL: EPOCH 168/1000 | BATCH 4/8 | LOSS: 8.568941484554671e-06\n",
      "VAL: EPOCH 168/1000 | BATCH 5/8 | LOSS: 8.314007497271328e-06\n",
      "VAL: EPOCH 168/1000 | BATCH 6/8 | LOSS: 8.033872615799606e-06\n",
      "VAL: EPOCH 168/1000 | BATCH 7/8 | LOSS: 8.150848827881418e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 0/71 | LOSS: 7.214834568003425e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 1/71 | LOSS: 6.699626055706176e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 2/71 | LOSS: 6.260194671388793e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 3/71 | LOSS: 6.443372399189684e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 4/71 | LOSS: 6.691367798339343e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 5/71 | LOSS: 6.476330554505694e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 6/71 | LOSS: 6.475016886854844e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 7/71 | LOSS: 6.60843124933308e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 8/71 | LOSS: 6.6867397537053976e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 9/71 | LOSS: 6.5206265389861075e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 10/71 | LOSS: 6.521269576544662e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 11/71 | LOSS: 6.543619330538301e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 12/71 | LOSS: 6.476889666373609e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 13/71 | LOSS: 6.474726855490839e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 14/71 | LOSS: 6.55242308008989e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 15/71 | LOSS: 6.5240267304034205e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 16/71 | LOSS: 6.468200407492559e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 17/71 | LOSS: 6.443719611423957e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 18/71 | LOSS: 6.4326205622484465e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 19/71 | LOSS: 6.486834172392264e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 20/71 | LOSS: 6.418282369157255e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 21/71 | LOSS: 6.4602065570149785e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 22/71 | LOSS: 6.5246042694248585e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 23/71 | LOSS: 6.460352930541073e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 24/71 | LOSS: 6.484853656729683e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 25/71 | LOSS: 6.4674899686696544e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 26/71 | LOSS: 6.445179493581506e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 27/71 | LOSS: 6.436337620081238e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 28/71 | LOSS: 6.400524006463017e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 29/71 | LOSS: 6.373552256870122e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 30/71 | LOSS: 6.338315327617062e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 31/71 | LOSS: 6.321955368093768e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 32/71 | LOSS: 6.328593578721418e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 33/71 | LOSS: 6.357569043971393e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 34/71 | LOSS: 6.364801707344928e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 35/71 | LOSS: 6.351526078813347e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 36/71 | LOSS: 6.359718247187777e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 37/71 | LOSS: 6.361687754083266e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 38/71 | LOSS: 6.366811808570324e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 39/71 | LOSS: 6.330720316327642e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 40/71 | LOSS: 6.346283332343929e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 41/71 | LOSS: 6.3452159468267515e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 42/71 | LOSS: 6.327940566332599e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 43/71 | LOSS: 6.3525909994992124e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 44/71 | LOSS: 6.3681863644483705e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 45/71 | LOSS: 6.383217149287583e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 46/71 | LOSS: 6.394227279707457e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 47/71 | LOSS: 6.408383901164901e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 48/71 | LOSS: 6.391712285760002e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 49/71 | LOSS: 6.355196019285358e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 50/71 | LOSS: 6.347732992520939e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 51/71 | LOSS: 6.3202442631266494e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 52/71 | LOSS: 6.311031604196134e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 53/71 | LOSS: 6.324646897660892e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 54/71 | LOSS: 6.308784420318923e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 55/71 | LOSS: 6.313139554744599e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 56/71 | LOSS: 6.30264646947341e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 57/71 | LOSS: 6.2975480055872046e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 58/71 | LOSS: 6.319784915831406e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 59/71 | LOSS: 6.3699598134311e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 60/71 | LOSS: 6.389466650079322e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 61/71 | LOSS: 6.418111114168912e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 62/71 | LOSS: 6.418847851097188e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 63/71 | LOSS: 6.4859569661734895e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 64/71 | LOSS: 6.496104721848566e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 65/71 | LOSS: 6.497732742487146e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 66/71 | LOSS: 6.488566994638818e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 67/71 | LOSS: 6.488038740612597e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 68/71 | LOSS: 6.4950934532701634e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 69/71 | LOSS: 6.493685727296647e-06\n",
      "TRAIN: EPOCH 169/1000 | BATCH 70/71 | LOSS: 6.497092521974889e-06\n",
      "VAL: EPOCH 169/1000 | BATCH 0/8 | LOSS: 7.581992576888297e-06\n",
      "VAL: EPOCH 169/1000 | BATCH 1/8 | LOSS: 6.6306020016781986e-06\n",
      "VAL: EPOCH 169/1000 | BATCH 2/8 | LOSS: 6.488793739360214e-06\n",
      "VAL: EPOCH 169/1000 | BATCH 3/8 | LOSS: 6.360557449625048e-06\n",
      "VAL: EPOCH 169/1000 | BATCH 4/8 | LOSS: 6.566030697285896e-06\n",
      "VAL: EPOCH 169/1000 | BATCH 5/8 | LOSS: 6.262436954784789e-06\n",
      "VAL: EPOCH 169/1000 | BATCH 6/8 | LOSS: 6.037561044130209e-06\n",
      "VAL: EPOCH 169/1000 | BATCH 7/8 | LOSS: 6.076703868984623e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 0/71 | LOSS: 5.585135568253463e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 1/71 | LOSS: 5.862106945642154e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 2/71 | LOSS: 6.01530185425266e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 3/71 | LOSS: 6.496058176708175e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 4/71 | LOSS: 6.469864456448704e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 5/71 | LOSS: 6.464815138921646e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 6/71 | LOSS: 6.446420814297328e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 7/71 | LOSS: 6.495451714272349e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 8/71 | LOSS: 6.55379290037672e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 9/71 | LOSS: 6.568695744135766e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 10/71 | LOSS: 6.451326002702857e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 11/71 | LOSS: 6.443958758003039e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 12/71 | LOSS: 6.45174934386607e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 13/71 | LOSS: 6.555336150475861e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 14/71 | LOSS: 6.712760447650605e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 15/71 | LOSS: 6.621403485951305e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 16/71 | LOSS: 6.656287630047539e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 17/71 | LOSS: 6.771010804287572e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 18/71 | LOSS: 6.783960016994235e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 19/71 | LOSS: 6.899912182234402e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 20/71 | LOSS: 6.8652120284068155e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 21/71 | LOSS: 6.965196482931126e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 22/71 | LOSS: 6.910560564985802e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 23/71 | LOSS: 7.0223080304761725e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 24/71 | LOSS: 6.96577819326194e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 25/71 | LOSS: 7.059176438745523e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 26/71 | LOSS: 7.009761096317864e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 27/71 | LOSS: 6.9914227326860005e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 28/71 | LOSS: 6.9793276577848735e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 29/71 | LOSS: 6.9971452073029164e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 30/71 | LOSS: 7.01166281062122e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 31/71 | LOSS: 6.972967483420689e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 32/71 | LOSS: 7.016094155970319e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 33/71 | LOSS: 6.9823986717999695e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 34/71 | LOSS: 6.969366248605574e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 35/71 | LOSS: 6.998385717654148e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 36/71 | LOSS: 7.007031227486378e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 37/71 | LOSS: 7.0078737113171405e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 38/71 | LOSS: 7.0473595489882146e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 39/71 | LOSS: 7.056113645376172e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 40/71 | LOSS: 7.112094752345711e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 41/71 | LOSS: 7.124140297708523e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 42/71 | LOSS: 7.126669611035288e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 43/71 | LOSS: 7.160937818959858e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 44/71 | LOSS: 7.186196439255986e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 45/71 | LOSS: 7.353407820277346e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 46/71 | LOSS: 7.388283257627181e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 47/71 | LOSS: 7.45649393252279e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 48/71 | LOSS: 7.459258874720889e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 49/71 | LOSS: 7.488817091143573e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 50/71 | LOSS: 7.521559373522976e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 51/71 | LOSS: 7.52051915661958e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 52/71 | LOSS: 7.527041945203959e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 53/71 | LOSS: 7.53424315344428e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 54/71 | LOSS: 7.57445780188639e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 55/71 | LOSS: 7.528480336662012e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 56/71 | LOSS: 7.5281502113125095e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 57/71 | LOSS: 7.53724679396265e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 58/71 | LOSS: 7.527798307989033e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 59/71 | LOSS: 7.504059256765079e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 60/71 | LOSS: 7.471219491815675e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 61/71 | LOSS: 7.45249022040826e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 62/71 | LOSS: 7.450686682984399e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 63/71 | LOSS: 7.432250406225194e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 64/71 | LOSS: 7.427218973204547e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 65/71 | LOSS: 7.385681604346521e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 66/71 | LOSS: 7.398065314028732e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 67/71 | LOSS: 7.388408836478073e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 68/71 | LOSS: 7.369855910033896e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 69/71 | LOSS: 7.396324852899332e-06\n",
      "TRAIN: EPOCH 170/1000 | BATCH 70/71 | LOSS: 7.363700795292356e-06\n",
      "VAL: EPOCH 170/1000 | BATCH 0/8 | LOSS: 1.0545447366894223e-05\n",
      "VAL: EPOCH 170/1000 | BATCH 1/8 | LOSS: 1.0058068255602848e-05\n",
      "VAL: EPOCH 170/1000 | BATCH 2/8 | LOSS: 9.060858095229682e-06\n",
      "VAL: EPOCH 170/1000 | BATCH 3/8 | LOSS: 9.036274718710047e-06\n",
      "VAL: EPOCH 170/1000 | BATCH 4/8 | LOSS: 9.269971542380517e-06\n",
      "VAL: EPOCH 170/1000 | BATCH 5/8 | LOSS: 8.951009097775872e-06\n",
      "VAL: EPOCH 170/1000 | BATCH 6/8 | LOSS: 9.061443376724907e-06\n",
      "VAL: EPOCH 170/1000 | BATCH 7/8 | LOSS: 9.019053379688557e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 0/71 | LOSS: 8.395805707550608e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 1/71 | LOSS: 7.1786519129091175e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 2/71 | LOSS: 7.898773674241966e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 3/71 | LOSS: 7.480438284801494e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 4/71 | LOSS: 7.953820659167832e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 5/71 | LOSS: 7.346055781454197e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 6/71 | LOSS: 7.243729669426102e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 7/71 | LOSS: 7.277745510236855e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 8/71 | LOSS: 7.1420895437768195e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 9/71 | LOSS: 7.149689599827979e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 10/71 | LOSS: 6.9395329195355195e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 11/71 | LOSS: 7.018509791123506e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 12/71 | LOSS: 7.05875821110497e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 13/71 | LOSS: 7.095440358690601e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 14/71 | LOSS: 6.869202146238725e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 15/71 | LOSS: 6.8406258435516065e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 16/71 | LOSS: 6.811756681118448e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 17/71 | LOSS: 6.770722721840785e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 18/71 | LOSS: 6.707425559158621e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 19/71 | LOSS: 6.704453858219494e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 20/71 | LOSS: 6.658338233613731e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 21/71 | LOSS: 6.670920811302494e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 22/71 | LOSS: 6.60607676526643e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 23/71 | LOSS: 6.561617775939037e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 24/71 | LOSS: 6.477311071648728e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 25/71 | LOSS: 6.4505153344585915e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 26/71 | LOSS: 6.455898174964305e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 27/71 | LOSS: 6.420551439337682e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 28/71 | LOSS: 6.409893417028421e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 29/71 | LOSS: 6.399952447585141e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 30/71 | LOSS: 6.363607392432318e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 31/71 | LOSS: 6.375969078931121e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 32/71 | LOSS: 6.359669579537832e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 33/71 | LOSS: 6.338701696757412e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 34/71 | LOSS: 6.360909394840876e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 35/71 | LOSS: 6.365058905228378e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 36/71 | LOSS: 6.3908094833439806e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 37/71 | LOSS: 6.405697640251387e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 38/71 | LOSS: 6.395550150419018e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 39/71 | LOSS: 6.381041714575986e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 40/71 | LOSS: 6.337705366515765e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 41/71 | LOSS: 6.330731637371216e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 42/71 | LOSS: 6.332062692435588e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 43/71 | LOSS: 6.328340470479981e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 44/71 | LOSS: 6.367210112835488e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 45/71 | LOSS: 6.38061902938479e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 46/71 | LOSS: 6.387452685885364e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 47/71 | LOSS: 6.376545940156575e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 48/71 | LOSS: 6.41007261724646e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 49/71 | LOSS: 6.445654244089383e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 50/71 | LOSS: 6.475434243018789e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 51/71 | LOSS: 6.485896759701203e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 52/71 | LOSS: 6.531396807849029e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 53/71 | LOSS: 6.543558390182659e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 54/71 | LOSS: 6.517255199221175e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 55/71 | LOSS: 6.509827561071038e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 56/71 | LOSS: 6.534548742421433e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 57/71 | LOSS: 6.529290164413898e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 58/71 | LOSS: 6.516592231676795e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 59/71 | LOSS: 6.5391568568884395e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 60/71 | LOSS: 6.544229392125462e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 61/71 | LOSS: 6.557967862458862e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 62/71 | LOSS: 6.564480408708314e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 63/71 | LOSS: 6.576917932932247e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 64/71 | LOSS: 6.5663206302045955e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 65/71 | LOSS: 6.5430826149774175e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 66/71 | LOSS: 6.557271527567333e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 67/71 | LOSS: 6.528775650301267e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 68/71 | LOSS: 6.536600972469676e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 69/71 | LOSS: 6.543409394258301e-06\n",
      "TRAIN: EPOCH 171/1000 | BATCH 70/71 | LOSS: 6.5074822243690834e-06\n",
      "VAL: EPOCH 171/1000 | BATCH 0/8 | LOSS: 1.0050018317997456e-05\n",
      "VAL: EPOCH 171/1000 | BATCH 1/8 | LOSS: 1.0334825219615595e-05\n",
      "VAL: EPOCH 171/1000 | BATCH 2/8 | LOSS: 8.983491170511115e-06\n",
      "VAL: EPOCH 171/1000 | BATCH 3/8 | LOSS: 8.912602197597153e-06\n",
      "VAL: EPOCH 171/1000 | BATCH 4/8 | LOSS: 9.079449591808953e-06\n",
      "VAL: EPOCH 171/1000 | BATCH 5/8 | LOSS: 8.775288885469005e-06\n",
      "VAL: EPOCH 171/1000 | BATCH 6/8 | LOSS: 8.85314472855368e-06\n",
      "VAL: EPOCH 171/1000 | BATCH 7/8 | LOSS: 8.777118637226522e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 0/71 | LOSS: 8.410897862631828e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 1/71 | LOSS: 7.4539075285429135e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 2/71 | LOSS: 6.595273892647431e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 3/71 | LOSS: 6.420399358830764e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 4/71 | LOSS: 6.180631862662267e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 5/71 | LOSS: 6.232255221524004e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 6/71 | LOSS: 6.416591401960302e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 7/71 | LOSS: 6.468294202477409e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 8/71 | LOSS: 6.626927592555552e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 9/71 | LOSS: 6.561204372701468e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 10/71 | LOSS: 6.616955116359432e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 11/71 | LOSS: 6.553873542240278e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 12/71 | LOSS: 6.6071390672452535e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 13/71 | LOSS: 6.650627061130113e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 14/71 | LOSS: 6.568307677904765e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 15/71 | LOSS: 6.603921406167501e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 16/71 | LOSS: 6.548938499155852e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 17/71 | LOSS: 6.516746629535595e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 18/71 | LOSS: 6.430333925029692e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 19/71 | LOSS: 6.567222158082586e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 20/71 | LOSS: 6.558308931354466e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 21/71 | LOSS: 6.523446080047341e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 22/71 | LOSS: 6.589456461369991e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 23/71 | LOSS: 6.6515998090229305e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 24/71 | LOSS: 6.63474342218251e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 25/71 | LOSS: 6.61818580738327e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 26/71 | LOSS: 6.655734590889618e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 27/71 | LOSS: 6.648214967494173e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 28/71 | LOSS: 6.641587788180914e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 29/71 | LOSS: 6.630465107567337e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 30/71 | LOSS: 6.627854667561722e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 31/71 | LOSS: 6.57446990715016e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 32/71 | LOSS: 6.524758329319549e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 33/71 | LOSS: 6.525195818967939e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 34/71 | LOSS: 6.570687738920762e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 35/71 | LOSS: 6.593767794685037e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 36/71 | LOSS: 6.581278518530559e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 37/71 | LOSS: 6.567334761903591e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 38/71 | LOSS: 6.567796949988965e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 39/71 | LOSS: 6.534474584896088e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 40/71 | LOSS: 6.541457168634402e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 41/71 | LOSS: 6.531279225219762e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 42/71 | LOSS: 6.501183041870908e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 43/71 | LOSS: 6.470760984649877e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 44/71 | LOSS: 6.498515611181372e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 45/71 | LOSS: 6.551172913057836e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 46/71 | LOSS: 6.5417200161430225e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 47/71 | LOSS: 6.506289849994573e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 48/71 | LOSS: 6.548700750908192e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 49/71 | LOSS: 6.535936181535362e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 50/71 | LOSS: 6.531442852742746e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 51/71 | LOSS: 6.507139189055194e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 52/71 | LOSS: 6.499250280027792e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 53/71 | LOSS: 6.510560002357967e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 54/71 | LOSS: 6.524578103298237e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 55/71 | LOSS: 6.531209075027229e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 56/71 | LOSS: 6.564353545974499e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 57/71 | LOSS: 6.588773760148765e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 58/71 | LOSS: 6.593736065566437e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 59/71 | LOSS: 6.5743740075655905e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 60/71 | LOSS: 6.584373250752535e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 61/71 | LOSS: 6.568605664553264e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 62/71 | LOSS: 6.570938470729974e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 63/71 | LOSS: 6.597649431228092e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 64/71 | LOSS: 6.602141953548058e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 65/71 | LOSS: 6.603813896314629e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 66/71 | LOSS: 6.574567720887369e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 67/71 | LOSS: 6.547850001184356e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 68/71 | LOSS: 6.540156398156229e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 69/71 | LOSS: 6.556344936013505e-06\n",
      "TRAIN: EPOCH 172/1000 | BATCH 70/71 | LOSS: 6.574915005330695e-06\n",
      "VAL: EPOCH 172/1000 | BATCH 0/8 | LOSS: 7.601363904541358e-06\n",
      "VAL: EPOCH 172/1000 | BATCH 1/8 | LOSS: 7.08126731296943e-06\n",
      "VAL: EPOCH 172/1000 | BATCH 2/8 | LOSS: 7.053801709844265e-06\n",
      "VAL: EPOCH 172/1000 | BATCH 3/8 | LOSS: 7.270847163454164e-06\n",
      "VAL: EPOCH 172/1000 | BATCH 4/8 | LOSS: 7.492485201510135e-06\n",
      "VAL: EPOCH 172/1000 | BATCH 5/8 | LOSS: 7.211885000894351e-06\n",
      "VAL: EPOCH 172/1000 | BATCH 6/8 | LOSS: 7.023999972131735e-06\n",
      "VAL: EPOCH 172/1000 | BATCH 7/8 | LOSS: 6.9976034069441084e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 0/71 | LOSS: 6.900313110236311e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 1/71 | LOSS: 6.6810375756176654e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 2/71 | LOSS: 6.2501130742020905e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 3/71 | LOSS: 6.971160473767668e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 4/71 | LOSS: 6.85410759615479e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 5/71 | LOSS: 7.008612707674426e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 6/71 | LOSS: 7.121481368293254e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 7/71 | LOSS: 7.158557480124728e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 8/71 | LOSS: 6.999707137664599e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 9/71 | LOSS: 7.085733204803546e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 10/71 | LOSS: 7.156504786755911e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 11/71 | LOSS: 7.216350581984443e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 12/71 | LOSS: 7.094776160206843e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 13/71 | LOSS: 7.194608217884836e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 14/71 | LOSS: 7.131246547942283e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 15/71 | LOSS: 7.056071609667924e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 16/71 | LOSS: 7.011688948670567e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 17/71 | LOSS: 7.168533102230867e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 18/71 | LOSS: 7.194494891767758e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 19/71 | LOSS: 7.170281924118171e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 20/71 | LOSS: 7.118624342270084e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 21/71 | LOSS: 7.0977479977459135e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 22/71 | LOSS: 7.05392892047277e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 23/71 | LOSS: 7.040976280829152e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 24/71 | LOSS: 7.0805143695906735e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 25/71 | LOSS: 7.00429756342111e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 26/71 | LOSS: 6.939511506600495e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 27/71 | LOSS: 6.953403401764392e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 28/71 | LOSS: 6.923249672801266e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 29/71 | LOSS: 6.885535519055944e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 30/71 | LOSS: 6.877543118236879e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 31/71 | LOSS: 6.933756807825375e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 32/71 | LOSS: 6.923287387370223e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 33/71 | LOSS: 7.046668743522091e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 34/71 | LOSS: 7.0391524267116825e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 35/71 | LOSS: 6.993540296207357e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 36/71 | LOSS: 6.972142936983555e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 37/71 | LOSS: 6.992138316108092e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 38/71 | LOSS: 7.002777179784458e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 39/71 | LOSS: 7.022588226845982e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 40/71 | LOSS: 7.01608266864219e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 41/71 | LOSS: 7.0563596745203196e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 42/71 | LOSS: 7.023622031759492e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 43/71 | LOSS: 7.005059842413175e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 44/71 | LOSS: 7.003143966560148e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 45/71 | LOSS: 6.978153222640369e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 46/71 | LOSS: 7.001822810475169e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 47/71 | LOSS: 6.997151918615903e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 48/71 | LOSS: 7.023356022041205e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 49/71 | LOSS: 7.013671538516064e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 50/71 | LOSS: 6.997767197354537e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 51/71 | LOSS: 6.9914181302430315e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 52/71 | LOSS: 6.975030708349121e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 53/71 | LOSS: 6.988867181900315e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 54/71 | LOSS: 6.981409412467408e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 55/71 | LOSS: 6.967766921661678e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 56/71 | LOSS: 6.9569214087606635e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 57/71 | LOSS: 6.941380104700361e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 58/71 | LOSS: 6.964676439396098e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 59/71 | LOSS: 6.967636469804953e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 60/71 | LOSS: 6.9615154464626076e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 61/71 | LOSS: 6.9447533391094996e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 62/71 | LOSS: 6.915316690639075e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 63/71 | LOSS: 6.9139676170948405e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 64/71 | LOSS: 6.921822988750556e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 65/71 | LOSS: 6.917908601253382e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 66/71 | LOSS: 6.921761635553662e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 67/71 | LOSS: 6.917285003617187e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 68/71 | LOSS: 6.9331905989742335e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 69/71 | LOSS: 6.914643498540889e-06\n",
      "TRAIN: EPOCH 173/1000 | BATCH 70/71 | LOSS: 6.8972210396117285e-06\n",
      "VAL: EPOCH 173/1000 | BATCH 0/8 | LOSS: 8.392740710405633e-06\n",
      "VAL: EPOCH 173/1000 | BATCH 1/8 | LOSS: 8.587722277297871e-06\n",
      "VAL: EPOCH 173/1000 | BATCH 2/8 | LOSS: 7.498237058219577e-06\n",
      "VAL: EPOCH 173/1000 | BATCH 3/8 | LOSS: 7.685262630729994e-06\n",
      "VAL: EPOCH 173/1000 | BATCH 4/8 | LOSS: 7.784918943798403e-06\n",
      "VAL: EPOCH 173/1000 | BATCH 5/8 | LOSS: 7.479519733048316e-06\n",
      "VAL: EPOCH 173/1000 | BATCH 6/8 | LOSS: 7.459756553933923e-06\n",
      "VAL: EPOCH 173/1000 | BATCH 7/8 | LOSS: 7.353683429300872e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 0/71 | LOSS: 6.83499320075498e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 1/71 | LOSS: 6.30443605587061e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 2/71 | LOSS: 6.034854929263626e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 3/71 | LOSS: 5.949237106506189e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 4/71 | LOSS: 5.976045122224604e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 5/71 | LOSS: 5.964766614852124e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 6/71 | LOSS: 6.060413527718213e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 7/71 | LOSS: 6.1688937762482965e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 8/71 | LOSS: 6.206905962042381e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 9/71 | LOSS: 6.14109435446153e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 10/71 | LOSS: 6.194340916077966e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 11/71 | LOSS: 6.0546074109879555e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 12/71 | LOSS: 6.055786728281349e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 13/71 | LOSS: 6.040293978263175e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 14/71 | LOSS: 6.027692961652065e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 15/71 | LOSS: 5.984456237229097e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 16/71 | LOSS: 5.929858696831884e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 17/71 | LOSS: 5.859544141155008e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 18/71 | LOSS: 5.898748273275008e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 19/71 | LOSS: 5.84295507906063e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 20/71 | LOSS: 5.791557235906588e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 21/71 | LOSS: 5.815204779289409e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 22/71 | LOSS: 5.847716791955147e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 23/71 | LOSS: 5.852190118578922e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 24/71 | LOSS: 5.8791957599169106e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 25/71 | LOSS: 5.8893242742040175e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 26/71 | LOSS: 5.901057209956451e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 27/71 | LOSS: 5.8700597459652016e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 28/71 | LOSS: 5.867271942532108e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 29/71 | LOSS: 5.9121590614571085e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 30/71 | LOSS: 5.907253023084899e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 31/71 | LOSS: 6.018183484002293e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 32/71 | LOSS: 6.053668571301827e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 33/71 | LOSS: 6.039007196116418e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 34/71 | LOSS: 6.125502620437016e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 35/71 | LOSS: 6.173073403361842e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 36/71 | LOSS: 6.162274420037991e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 37/71 | LOSS: 6.239488644304832e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 38/71 | LOSS: 6.219945857875969e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 39/71 | LOSS: 6.2115410401020196e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 40/71 | LOSS: 6.243840732899915e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 41/71 | LOSS: 6.257583392999506e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 42/71 | LOSS: 6.362755166899364e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 43/71 | LOSS: 6.32454188317669e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 44/71 | LOSS: 6.379912034996475e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 45/71 | LOSS: 6.381625454472985e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 46/71 | LOSS: 6.373668896985498e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 47/71 | LOSS: 6.402244442445711e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 48/71 | LOSS: 6.466136961089618e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 49/71 | LOSS: 6.470067528425716e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 50/71 | LOSS: 6.475688018790428e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 51/71 | LOSS: 6.463245007090825e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 52/71 | LOSS: 6.496349000509275e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 53/71 | LOSS: 6.516134430125088e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 54/71 | LOSS: 6.510567932723048e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 55/71 | LOSS: 6.515174504784227e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 56/71 | LOSS: 6.509489937518977e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 57/71 | LOSS: 6.528026084385507e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 58/71 | LOSS: 6.535932775246477e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 59/71 | LOSS: 6.504935072371154e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 60/71 | LOSS: 6.477074062696956e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 61/71 | LOSS: 6.46018331222407e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 62/71 | LOSS: 6.464743653359908e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 63/71 | LOSS: 6.4586638970354215e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 64/71 | LOSS: 6.461622944278553e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 65/71 | LOSS: 6.4699553756023915e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 66/71 | LOSS: 6.454187042884348e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 67/71 | LOSS: 6.489475601804716e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 68/71 | LOSS: 6.5055143512955e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 69/71 | LOSS: 6.5372137409472735e-06\n",
      "TRAIN: EPOCH 174/1000 | BATCH 70/71 | LOSS: 6.509582663953475e-06\n",
      "VAL: EPOCH 174/1000 | BATCH 0/8 | LOSS: 9.418744411959779e-06\n",
      "VAL: EPOCH 174/1000 | BATCH 1/8 | LOSS: 9.027064152178355e-06\n",
      "VAL: EPOCH 174/1000 | BATCH 2/8 | LOSS: 8.109817827062216e-06\n",
      "VAL: EPOCH 174/1000 | BATCH 3/8 | LOSS: 8.055387297645211e-06\n",
      "VAL: EPOCH 174/1000 | BATCH 4/8 | LOSS: 8.273760795418638e-06\n",
      "VAL: EPOCH 174/1000 | BATCH 5/8 | LOSS: 8.01632207488486e-06\n",
      "VAL: EPOCH 174/1000 | BATCH 6/8 | LOSS: 8.090401284529694e-06\n",
      "VAL: EPOCH 174/1000 | BATCH 7/8 | LOSS: 8.008898305433831e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 0/71 | LOSS: 7.416764219669858e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 1/71 | LOSS: 7.49291598367563e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 2/71 | LOSS: 6.5352772556555765e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 3/71 | LOSS: 6.159194640531496e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 4/71 | LOSS: 6.138697062851861e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 5/71 | LOSS: 6.242526372564801e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 6/71 | LOSS: 6.091855409197576e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 7/71 | LOSS: 6.146064720269351e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 8/71 | LOSS: 6.231638659907428e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 9/71 | LOSS: 6.301510575212888e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 10/71 | LOSS: 6.170433400918475e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 11/71 | LOSS: 6.1350054920694674e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 12/71 | LOSS: 6.177701939472731e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 13/71 | LOSS: 6.141208132248721e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 14/71 | LOSS: 6.1076181433842675e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 15/71 | LOSS: 6.068535668646291e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 16/71 | LOSS: 6.129207470033126e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 17/71 | LOSS: 6.15982882360792e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 18/71 | LOSS: 6.163758323390012e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 19/71 | LOSS: 6.330738688120618e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 20/71 | LOSS: 6.3079490982567046e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 21/71 | LOSS: 6.2283224079609765e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 22/71 | LOSS: 6.177080293760493e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 23/71 | LOSS: 6.1071052641636925e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 24/71 | LOSS: 6.189338164404035e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 25/71 | LOSS: 6.163899745213432e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 26/71 | LOSS: 6.133960950036352e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 27/71 | LOSS: 6.1407441113000715e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 28/71 | LOSS: 6.1809253189682655e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 29/71 | LOSS: 6.192187614336338e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 30/71 | LOSS: 6.215465521475586e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 31/71 | LOSS: 6.230503885262806e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 32/71 | LOSS: 6.237594623764215e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 33/71 | LOSS: 6.197936352905612e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 34/71 | LOSS: 6.2361816682303986e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 35/71 | LOSS: 6.230822059983944e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 36/71 | LOSS: 6.202213515624297e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 37/71 | LOSS: 6.185917371974363e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 38/71 | LOSS: 6.179433582404342e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 39/71 | LOSS: 6.186239920680236e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 40/71 | LOSS: 6.1873845147712885e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 41/71 | LOSS: 6.234228099596553e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 42/71 | LOSS: 6.269137905524816e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 43/71 | LOSS: 6.239600752037817e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 44/71 | LOSS: 6.272882647964353e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 45/71 | LOSS: 6.307123276596742e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 46/71 | LOSS: 6.306006245258106e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 47/71 | LOSS: 6.294583120810178e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 48/71 | LOSS: 6.317655194778832e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 49/71 | LOSS: 6.33099976766971e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 50/71 | LOSS: 6.313920779326045e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 51/71 | LOSS: 6.336833498957835e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 52/71 | LOSS: 6.344834768036512e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 53/71 | LOSS: 6.33911976264143e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 54/71 | LOSS: 6.349969300407049e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 55/71 | LOSS: 6.354663452969232e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 56/71 | LOSS: 6.3629676285353e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 57/71 | LOSS: 6.363994277213853e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 58/71 | LOSS: 6.360963143995801e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 59/71 | LOSS: 6.3485219849705265e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 60/71 | LOSS: 6.356326251035006e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 61/71 | LOSS: 6.340047078875584e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 62/71 | LOSS: 6.3453945686988946e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 63/71 | LOSS: 6.3631812281528255e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 64/71 | LOSS: 6.3541524958474415e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 65/71 | LOSS: 6.337829917714716e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 66/71 | LOSS: 6.358508546408805e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 67/71 | LOSS: 6.3450016796196e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 68/71 | LOSS: 6.353039963500308e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 69/71 | LOSS: 6.3366777145087586e-06\n",
      "TRAIN: EPOCH 175/1000 | BATCH 70/71 | LOSS: 6.325692141563451e-06\n",
      "VAL: EPOCH 175/1000 | BATCH 0/8 | LOSS: 6.91970853949897e-06\n",
      "VAL: EPOCH 175/1000 | BATCH 1/8 | LOSS: 6.615284519284614e-06\n",
      "VAL: EPOCH 175/1000 | BATCH 2/8 | LOSS: 6.045656391506782e-06\n",
      "VAL: EPOCH 175/1000 | BATCH 3/8 | LOSS: 6.4168633571171085e-06\n",
      "VAL: EPOCH 175/1000 | BATCH 4/8 | LOSS: 6.537355511682108e-06\n",
      "VAL: EPOCH 175/1000 | BATCH 5/8 | LOSS: 6.3336556953193695e-06\n",
      "VAL: EPOCH 175/1000 | BATCH 6/8 | LOSS: 6.251500508369645e-06\n",
      "VAL: EPOCH 175/1000 | BATCH 7/8 | LOSS: 6.193208037075237e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 0/71 | LOSS: 4.201016963634174e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 1/71 | LOSS: 5.0973364977835445e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 2/71 | LOSS: 5.696122419370416e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 3/71 | LOSS: 5.542945814340783e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 4/71 | LOSS: 5.484006123879226e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 5/71 | LOSS: 5.690909423113529e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 6/71 | LOSS: 6.000244801336001e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 7/71 | LOSS: 5.899796917674394e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 8/71 | LOSS: 6.004523179904532e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 9/71 | LOSS: 5.939654010944651e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 10/71 | LOSS: 5.881968387587801e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 11/71 | LOSS: 5.966496397983671e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 12/71 | LOSS: 6.062920454916401e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 13/71 | LOSS: 6.080548571065135e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 14/71 | LOSS: 6.011003946089962e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 15/71 | LOSS: 6.0264754608851945e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 16/71 | LOSS: 6.032864830023541e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 17/71 | LOSS: 5.996224520761946e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 18/71 | LOSS: 6.0306161972939184e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 19/71 | LOSS: 6.1738204749417495e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 20/71 | LOSS: 6.178279753450104e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 21/71 | LOSS: 6.130008593820755e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 22/71 | LOSS: 6.158578680301814e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 23/71 | LOSS: 6.155889574680866e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 24/71 | LOSS: 6.101308172219433e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 25/71 | LOSS: 6.072880051550993e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 26/71 | LOSS: 6.058841639363284e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 27/71 | LOSS: 6.028583243278263e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 28/71 | LOSS: 6.019428853725348e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 29/71 | LOSS: 6.012609825726638e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 30/71 | LOSS: 5.995294530645967e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 31/71 | LOSS: 6.00692091268229e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 32/71 | LOSS: 6.02608062046894e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 33/71 | LOSS: 6.002733471177635e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 34/71 | LOSS: 6.047789832207075e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 35/71 | LOSS: 6.05567225243754e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 36/71 | LOSS: 6.09220852139343e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 37/71 | LOSS: 6.087092299965566e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 38/71 | LOSS: 6.048805283707901e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 39/71 | LOSS: 6.118864894233411e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 40/71 | LOSS: 6.153074060066279e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 41/71 | LOSS: 6.177391967520678e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 42/71 | LOSS: 6.208257736732268e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 43/71 | LOSS: 6.221954570471346e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 44/71 | LOSS: 6.249350826692535e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 45/71 | LOSS: 6.238562610119318e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 46/71 | LOSS: 6.290541294103914e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 47/71 | LOSS: 6.309950246456235e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 48/71 | LOSS: 6.2987920426501066e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 49/71 | LOSS: 6.314669753919589e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 50/71 | LOSS: 6.309938672690323e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 51/71 | LOSS: 6.312682846608088e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 52/71 | LOSS: 6.311581153499472e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 53/71 | LOSS: 6.2905624721073155e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 54/71 | LOSS: 6.33078699882555e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 55/71 | LOSS: 6.343735979693779e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 56/71 | LOSS: 6.334551926556742e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 57/71 | LOSS: 6.326790824083406e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 58/71 | LOSS: 6.352331993030483e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 59/71 | LOSS: 6.371270205818291e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 60/71 | LOSS: 6.415967432150835e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 61/71 | LOSS: 6.448541442648181e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 62/71 | LOSS: 6.467315069173873e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 63/71 | LOSS: 6.5253084855498855e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 64/71 | LOSS: 6.5300292730031424e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 65/71 | LOSS: 6.5584867818622365e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 66/71 | LOSS: 6.568119030151735e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 67/71 | LOSS: 6.577546755312035e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 68/71 | LOSS: 6.585546167986214e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 69/71 | LOSS: 6.579484754963363e-06\n",
      "TRAIN: EPOCH 176/1000 | BATCH 70/71 | LOSS: 6.568449529188744e-06\n",
      "VAL: EPOCH 176/1000 | BATCH 0/8 | LOSS: 8.94043841981329e-06\n",
      "VAL: EPOCH 176/1000 | BATCH 1/8 | LOSS: 7.533129064540844e-06\n",
      "VAL: EPOCH 176/1000 | BATCH 2/8 | LOSS: 7.840196000567326e-06\n",
      "VAL: EPOCH 176/1000 | BATCH 3/8 | LOSS: 7.624300678799045e-06\n",
      "VAL: EPOCH 176/1000 | BATCH 4/8 | LOSS: 7.793399163347203e-06\n",
      "VAL: EPOCH 176/1000 | BATCH 5/8 | LOSS: 7.669209783974415e-06\n",
      "VAL: EPOCH 176/1000 | BATCH 6/8 | LOSS: 7.473937979999132e-06\n",
      "VAL: EPOCH 176/1000 | BATCH 7/8 | LOSS: 7.6063358847022755e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 0/71 | LOSS: 6.6602133301785216e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 1/71 | LOSS: 7.739619832136668e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 2/71 | LOSS: 7.37298129630896e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 3/71 | LOSS: 7.022315912763588e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 4/71 | LOSS: 6.913349170645233e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 5/71 | LOSS: 7.0474414618123165e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 6/71 | LOSS: 6.7153249411993395e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 7/71 | LOSS: 6.496544870060461e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 8/71 | LOSS: 6.623762702575833e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 9/71 | LOSS: 6.656755522271851e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 10/71 | LOSS: 6.592649159839229e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 11/71 | LOSS: 6.56695321292015e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 12/71 | LOSS: 6.4801860949396196e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 13/71 | LOSS: 6.3999421919496465e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 14/71 | LOSS: 6.410425066860626e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 15/71 | LOSS: 6.46161132067391e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 16/71 | LOSS: 6.442690525599413e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 17/71 | LOSS: 6.400564845737083e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 18/71 | LOSS: 6.452487540579903e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 19/71 | LOSS: 6.468435572060116e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 20/71 | LOSS: 6.393601043306435e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 21/71 | LOSS: 6.400278917598453e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 22/71 | LOSS: 6.426092825321298e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 23/71 | LOSS: 6.451415780854101e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 24/71 | LOSS: 6.5490697306813675e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 25/71 | LOSS: 6.558528079599805e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 26/71 | LOSS: 6.649603198672628e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 27/71 | LOSS: 6.663319904224149e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 28/71 | LOSS: 6.642928085433408e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 29/71 | LOSS: 6.728498450077798e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 30/71 | LOSS: 6.6535874705756036e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 31/71 | LOSS: 6.785627036265396e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 32/71 | LOSS: 6.737902877472207e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 33/71 | LOSS: 6.756677213856806e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 34/71 | LOSS: 6.698224504881572e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 35/71 | LOSS: 6.669023971678851e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 36/71 | LOSS: 6.623196711459329e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 37/71 | LOSS: 6.557587108832913e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 38/71 | LOSS: 6.543111281550806e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 39/71 | LOSS: 6.5171900246241424e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 40/71 | LOSS: 6.499323928171468e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 41/71 | LOSS: 6.489700776260829e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 42/71 | LOSS: 6.504342700493506e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 43/71 | LOSS: 6.5085557811140795e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 44/71 | LOSS: 6.453188749825737e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 45/71 | LOSS: 6.461022285688871e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 46/71 | LOSS: 6.465983789623467e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 47/71 | LOSS: 6.466603357087782e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 48/71 | LOSS: 6.448462759830623e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 49/71 | LOSS: 6.48300675493374e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 50/71 | LOSS: 6.4578182401401496e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 51/71 | LOSS: 6.435210480049136e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 52/71 | LOSS: 6.433295303000739e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 53/71 | LOSS: 6.418793477509839e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 54/71 | LOSS: 6.422566994760101e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 55/71 | LOSS: 6.4038646720681366e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 56/71 | LOSS: 6.385023218108313e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 57/71 | LOSS: 6.3755183356110575e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 58/71 | LOSS: 6.358999390783029e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 59/71 | LOSS: 6.39040168456025e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 60/71 | LOSS: 6.393178215953636e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 61/71 | LOSS: 6.3717589000589214e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 62/71 | LOSS: 6.365031512114145e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 63/71 | LOSS: 6.353118344293307e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 64/71 | LOSS: 6.355603244838466e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 65/71 | LOSS: 6.335439999902244e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 66/71 | LOSS: 6.312871629410465e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 67/71 | LOSS: 6.34891787686844e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 68/71 | LOSS: 6.3313409555121325e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 69/71 | LOSS: 6.34440602230565e-06\n",
      "TRAIN: EPOCH 177/1000 | BATCH 70/71 | LOSS: 6.317812458804274e-06\n",
      "VAL: EPOCH 177/1000 | BATCH 0/8 | LOSS: 8.555538443033583e-06\n",
      "VAL: EPOCH 177/1000 | BATCH 1/8 | LOSS: 7.543541642007767e-06\n",
      "VAL: EPOCH 177/1000 | BATCH 2/8 | LOSS: 6.831436166976346e-06\n",
      "VAL: EPOCH 177/1000 | BATCH 3/8 | LOSS: 6.7811342887580395e-06\n",
      "VAL: EPOCH 177/1000 | BATCH 4/8 | LOSS: 6.842386756034102e-06\n",
      "VAL: EPOCH 177/1000 | BATCH 5/8 | LOSS: 6.540610759960448e-06\n",
      "VAL: EPOCH 177/1000 | BATCH 6/8 | LOSS: 6.507192795522444e-06\n",
      "VAL: EPOCH 177/1000 | BATCH 7/8 | LOSS: 6.532246118240437e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 0/71 | LOSS: 7.454645128746051e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 1/71 | LOSS: 6.675804797851015e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 2/71 | LOSS: 6.12472255549316e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 3/71 | LOSS: 6.366426077875076e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 4/71 | LOSS: 6.238086734811077e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 5/71 | LOSS: 6.512771885051431e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 6/71 | LOSS: 6.567797949433693e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 7/71 | LOSS: 6.739786101661593e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 8/71 | LOSS: 6.756617797994598e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 9/71 | LOSS: 6.6557673108036395e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 10/71 | LOSS: 6.861011033007261e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 11/71 | LOSS: 6.8097450593995745e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 12/71 | LOSS: 7.153625504612976e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 13/71 | LOSS: 7.010333742333127e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 14/71 | LOSS: 7.208822019795965e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 15/71 | LOSS: 7.121685968058955e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 16/71 | LOSS: 7.167724882439122e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 17/71 | LOSS: 7.199369595885703e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 18/71 | LOSS: 7.127199751266744e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 19/71 | LOSS: 7.125140291464049e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 20/71 | LOSS: 7.169365614702526e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 21/71 | LOSS: 7.131940905806418e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 22/71 | LOSS: 7.247286149599265e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 23/71 | LOSS: 7.2296579674002714e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 24/71 | LOSS: 7.1910288534127174e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 25/71 | LOSS: 7.259931478004616e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 26/71 | LOSS: 7.232577950667797e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 27/71 | LOSS: 7.309652833750338e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 28/71 | LOSS: 7.23936163294952e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 29/71 | LOSS: 7.2450057662839145e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 30/71 | LOSS: 7.178557699526502e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 31/71 | LOSS: 7.196629994155046e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 32/71 | LOSS: 7.177134262526648e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 33/71 | LOSS: 7.1734690637266096e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 34/71 | LOSS: 7.092771258092918e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 35/71 | LOSS: 7.0372568011912844e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 36/71 | LOSS: 6.989121556142817e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 37/71 | LOSS: 6.987395420908701e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 38/71 | LOSS: 6.942640340686268e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 39/71 | LOSS: 6.891401199027314e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 40/71 | LOSS: 6.867856139185123e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 41/71 | LOSS: 6.86335211174169e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 42/71 | LOSS: 6.830066129538843e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 43/71 | LOSS: 6.816026711931045e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 44/71 | LOSS: 6.7827845062614266e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 45/71 | LOSS: 6.764538699391541e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 46/71 | LOSS: 6.761570148228202e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 47/71 | LOSS: 6.746566270976473e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 48/71 | LOSS: 6.690980236661355e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 49/71 | LOSS: 6.692774604744045e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 50/71 | LOSS: 6.655141024727045e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 51/71 | LOSS: 6.623898299879399e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 52/71 | LOSS: 6.607327680474143e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 53/71 | LOSS: 6.5709139444489095e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 54/71 | LOSS: 6.569766975933982e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 55/71 | LOSS: 6.544503386456719e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 56/71 | LOSS: 6.535232674459142e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 57/71 | LOSS: 6.536552165988692e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 58/71 | LOSS: 6.521052640336921e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 59/71 | LOSS: 6.521336573920659e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 60/71 | LOSS: 6.525012730755824e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 61/71 | LOSS: 6.533941022127927e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 62/71 | LOSS: 6.5380803327628645e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 63/71 | LOSS: 6.548972017128563e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 64/71 | LOSS: 6.554036001314391e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 65/71 | LOSS: 6.5427317980665425e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 66/71 | LOSS: 6.5316236140827625e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 67/71 | LOSS: 6.559952647180848e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 68/71 | LOSS: 6.563392590591769e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 69/71 | LOSS: 6.546258990575942e-06\n",
      "TRAIN: EPOCH 178/1000 | BATCH 70/71 | LOSS: 6.558104404844721e-06\n",
      "VAL: EPOCH 178/1000 | BATCH 0/8 | LOSS: 8.537829671695363e-06\n",
      "VAL: EPOCH 178/1000 | BATCH 1/8 | LOSS: 7.139391072996659e-06\n",
      "VAL: EPOCH 178/1000 | BATCH 2/8 | LOSS: 7.145284750246598e-06\n",
      "VAL: EPOCH 178/1000 | BATCH 3/8 | LOSS: 6.940180583114852e-06\n",
      "VAL: EPOCH 178/1000 | BATCH 4/8 | LOSS: 7.104167889337987e-06\n",
      "VAL: EPOCH 178/1000 | BATCH 5/8 | LOSS: 6.860714014086018e-06\n",
      "VAL: EPOCH 178/1000 | BATCH 6/8 | LOSS: 6.720610274766971e-06\n",
      "VAL: EPOCH 178/1000 | BATCH 7/8 | LOSS: 6.798899050863838e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 0/71 | LOSS: 6.673774350929307e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 1/71 | LOSS: 5.94469634052075e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 2/71 | LOSS: 5.543160265612339e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 3/71 | LOSS: 5.7233063444073196e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 4/71 | LOSS: 6.068501625122735e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 5/71 | LOSS: 5.684926463800366e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 6/71 | LOSS: 5.723007039445552e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 7/71 | LOSS: 5.851918388088961e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 8/71 | LOSS: 5.940617383102007e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 9/71 | LOSS: 5.84392209930229e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 10/71 | LOSS: 6.007897354720626e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 11/71 | LOSS: 5.887404237607067e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 12/71 | LOSS: 5.868326717553338e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 13/71 | LOSS: 5.944831140628334e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 14/71 | LOSS: 5.877625062566949e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 15/71 | LOSS: 5.867560702199626e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 16/71 | LOSS: 5.881850564251449e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 17/71 | LOSS: 5.841667037505734e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 18/71 | LOSS: 5.791385678764384e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 19/71 | LOSS: 5.796559435111703e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 20/71 | LOSS: 5.740055409357107e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 21/71 | LOSS: 5.747609851492928e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 22/71 | LOSS: 5.761498869105708e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 23/71 | LOSS: 5.772711782962385e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 24/71 | LOSS: 5.814855358039495e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 25/71 | LOSS: 5.812500463574766e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 26/71 | LOSS: 5.902457040199941e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 27/71 | LOSS: 5.969209269096609e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 28/71 | LOSS: 6.031017086382329e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 29/71 | LOSS: 6.149462296889396e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 30/71 | LOSS: 6.14122231009758e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 31/71 | LOSS: 6.1566851314864834e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 32/71 | LOSS: 6.1403250335636335e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 33/71 | LOSS: 6.118997509260867e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 34/71 | LOSS: 6.089969117186099e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 35/71 | LOSS: 6.126046047130431e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 36/71 | LOSS: 6.126304199554127e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 37/71 | LOSS: 6.114336500966027e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 38/71 | LOSS: 6.079007807084753e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 39/71 | LOSS: 6.078078786231344e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 40/71 | LOSS: 6.09897718210804e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 41/71 | LOSS: 6.0818432151184725e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 42/71 | LOSS: 6.097151209508221e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 43/71 | LOSS: 6.125276224173087e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 44/71 | LOSS: 6.085254608478863e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 45/71 | LOSS: 6.077094281942624e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 46/71 | LOSS: 6.148432263178117e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 47/71 | LOSS: 6.176841822025381e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 48/71 | LOSS: 6.184104241045282e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 49/71 | LOSS: 6.288026861511753e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 50/71 | LOSS: 6.308541242997664e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 51/71 | LOSS: 6.3508993592292354e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 52/71 | LOSS: 6.344264489118085e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 53/71 | LOSS: 6.323617955988286e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 54/71 | LOSS: 6.377110871240306e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 55/71 | LOSS: 6.373742662292768e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 56/71 | LOSS: 6.424557644398495e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 57/71 | LOSS: 6.4289605500656264e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 58/71 | LOSS: 6.4471808754961335e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 59/71 | LOSS: 6.453149793136011e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 60/71 | LOSS: 6.452661577374083e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 61/71 | LOSS: 6.458600424711251e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 62/71 | LOSS: 6.4811486423885015e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 63/71 | LOSS: 6.497295245821988e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 64/71 | LOSS: 6.523050517269159e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 65/71 | LOSS: 6.558513343241595e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 66/71 | LOSS: 6.644153486309163e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 67/71 | LOSS: 6.630755953718856e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 68/71 | LOSS: 6.688842147369819e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 69/71 | LOSS: 6.693772417877751e-06\n",
      "TRAIN: EPOCH 179/1000 | BATCH 70/71 | LOSS: 6.7106701427407425e-06\n",
      "VAL: EPOCH 179/1000 | BATCH 0/8 | LOSS: 8.130959031404927e-06\n",
      "VAL: EPOCH 179/1000 | BATCH 1/8 | LOSS: 6.9925806656101486e-06\n",
      "VAL: EPOCH 179/1000 | BATCH 2/8 | LOSS: 6.731467237841571e-06\n",
      "VAL: EPOCH 179/1000 | BATCH 3/8 | LOSS: 6.7616614387588925e-06\n",
      "VAL: EPOCH 179/1000 | BATCH 4/8 | LOSS: 6.7643967668118424e-06\n",
      "VAL: EPOCH 179/1000 | BATCH 5/8 | LOSS: 6.699154027955956e-06\n",
      "VAL: EPOCH 179/1000 | BATCH 6/8 | LOSS: 6.628355030053561e-06\n",
      "VAL: EPOCH 179/1000 | BATCH 7/8 | LOSS: 6.638215552356996e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 0/71 | LOSS: 7.0913379204284865e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 1/71 | LOSS: 6.950279612283339e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 2/71 | LOSS: 6.8751633079955354e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 3/71 | LOSS: 6.46979310658935e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 4/71 | LOSS: 6.480739193648332e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 5/71 | LOSS: 6.622919727305998e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 6/71 | LOSS: 6.3888331039509334e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 7/71 | LOSS: 6.4447443719473085e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 8/71 | LOSS: 6.610151508034e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 9/71 | LOSS: 6.748209489160217e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 10/71 | LOSS: 6.7173210812722e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 11/71 | LOSS: 6.552007334903465e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 12/71 | LOSS: 6.647751363813698e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 13/71 | LOSS: 6.57665597308161e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 14/71 | LOSS: 6.5158521465491505e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 15/71 | LOSS: 6.533380940254574e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 16/71 | LOSS: 6.558870288298588e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 17/71 | LOSS: 6.560094005989312e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 18/71 | LOSS: 6.515679998349362e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 19/71 | LOSS: 6.523854062834289e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 20/71 | LOSS: 6.451858185062606e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 21/71 | LOSS: 6.463475570357828e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 22/71 | LOSS: 6.42903662241145e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 23/71 | LOSS: 6.36530084345092e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 24/71 | LOSS: 6.275897321756929e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 25/71 | LOSS: 6.278416584301829e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 26/71 | LOSS: 6.233695347435531e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 27/71 | LOSS: 6.274625109524225e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 28/71 | LOSS: 6.237022544469098e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 29/71 | LOSS: 6.244789483389468e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 30/71 | LOSS: 6.29883985428962e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 31/71 | LOSS: 6.290306657774636e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 32/71 | LOSS: 6.308834985468296e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 33/71 | LOSS: 6.312087394048028e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 34/71 | LOSS: 6.265653051481682e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 35/71 | LOSS: 6.258261300394628e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 36/71 | LOSS: 6.295744647964917e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 37/71 | LOSS: 6.290512074931605e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 38/71 | LOSS: 6.365818022365998e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 39/71 | LOSS: 6.309790194336529e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 40/71 | LOSS: 6.326417627772061e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 41/71 | LOSS: 6.321520296686296e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 42/71 | LOSS: 6.2986066971694356e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 43/71 | LOSS: 6.2991418252005875e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 44/71 | LOSS: 6.292772660445836e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 45/71 | LOSS: 6.289550010083274e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 46/71 | LOSS: 6.266468091703149e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 47/71 | LOSS: 6.278286794743811e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 48/71 | LOSS: 6.293156915564061e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 49/71 | LOSS: 6.286162079049973e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 50/71 | LOSS: 6.319619869292878e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 51/71 | LOSS: 6.345231563150614e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 52/71 | LOSS: 6.362520544603028e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 53/71 | LOSS: 6.340099347383969e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 54/71 | LOSS: 6.336263471563473e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 55/71 | LOSS: 6.38033538637371e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 56/71 | LOSS: 6.346428196130753e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 57/71 | LOSS: 6.337398103980753e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 58/71 | LOSS: 6.3431217791206774e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 59/71 | LOSS: 6.321627355040012e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 60/71 | LOSS: 6.324942063895192e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 61/71 | LOSS: 6.3254546617944496e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 62/71 | LOSS: 6.304562399842932e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 63/71 | LOSS: 6.2957611035585614e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 64/71 | LOSS: 6.303415767223879e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 65/71 | LOSS: 6.290855811709083e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 66/71 | LOSS: 6.278442191642035e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 67/71 | LOSS: 6.2590484851491914e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 68/71 | LOSS: 6.247546209941493e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 69/71 | LOSS: 6.224923962301026e-06\n",
      "TRAIN: EPOCH 180/1000 | BATCH 70/71 | LOSS: 6.240330952698034e-06\n",
      "VAL: EPOCH 180/1000 | BATCH 0/8 | LOSS: 7.787343747622799e-06\n",
      "VAL: EPOCH 180/1000 | BATCH 1/8 | LOSS: 6.525985099870013e-06\n",
      "VAL: EPOCH 180/1000 | BATCH 2/8 | LOSS: 6.424710591090843e-06\n",
      "VAL: EPOCH 180/1000 | BATCH 3/8 | LOSS: 6.556507173627324e-06\n",
      "VAL: EPOCH 180/1000 | BATCH 4/8 | LOSS: 6.6063817939721044e-06\n",
      "VAL: EPOCH 180/1000 | BATCH 5/8 | LOSS: 6.336037889316988e-06\n",
      "VAL: EPOCH 180/1000 | BATCH 6/8 | LOSS: 6.10172823404095e-06\n",
      "VAL: EPOCH 180/1000 | BATCH 7/8 | LOSS: 6.190187889387744e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 0/71 | LOSS: 7.000171535764821e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 1/71 | LOSS: 6.596248113055481e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 2/71 | LOSS: 5.738922178958698e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 3/71 | LOSS: 5.607594857792719e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 4/71 | LOSS: 5.322628476278624e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 5/71 | LOSS: 5.378223477237043e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 6/71 | LOSS: 5.564303916928891e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 7/71 | LOSS: 5.6068171829792846e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 8/71 | LOSS: 5.817186926530364e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 9/71 | LOSS: 5.874812904949067e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 10/71 | LOSS: 5.928670733820499e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 11/71 | LOSS: 6.094302420933673e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 12/71 | LOSS: 6.049191702354269e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 13/71 | LOSS: 6.015090613280856e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 14/71 | LOSS: 5.897815632730877e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 15/71 | LOSS: 5.965537837937518e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 16/71 | LOSS: 5.921215180184363e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 17/71 | LOSS: 5.901336281668692e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 18/71 | LOSS: 5.888870921473044e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 19/71 | LOSS: 5.89108667554683e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 20/71 | LOSS: 5.88491717020848e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 21/71 | LOSS: 5.8700570455336924e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 22/71 | LOSS: 5.885765838921424e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 23/71 | LOSS: 5.92456746062453e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 24/71 | LOSS: 5.938869235251332e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 25/71 | LOSS: 5.921569604302372e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 26/71 | LOSS: 6.128643909269832e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 27/71 | LOSS: 6.141986481062693e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 28/71 | LOSS: 6.3228807887534115e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 29/71 | LOSS: 6.298271561414973e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 30/71 | LOSS: 6.353926804172994e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 31/71 | LOSS: 6.44155208817665e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 32/71 | LOSS: 6.445763340183696e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 33/71 | LOSS: 6.632574013565318e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 34/71 | LOSS: 6.62602920523828e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 35/71 | LOSS: 6.7505407944028975e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 36/71 | LOSS: 6.76001084414295e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 37/71 | LOSS: 6.789373700437289e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 38/71 | LOSS: 6.804578205423567e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 39/71 | LOSS: 6.787809991237737e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 40/71 | LOSS: 6.848595381686719e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 41/71 | LOSS: 6.842071592299839e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 42/71 | LOSS: 6.869301625218129e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 43/71 | LOSS: 6.9085258370581535e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 44/71 | LOSS: 6.90651459080982e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 45/71 | LOSS: 6.916573778178882e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 46/71 | LOSS: 6.903950218250603e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 47/71 | LOSS: 6.990729815470331e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 48/71 | LOSS: 6.971437749004452e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 49/71 | LOSS: 7.018618243819219e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 50/71 | LOSS: 7.023692450827951e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 51/71 | LOSS: 7.023608427166787e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 52/71 | LOSS: 7.028785328760212e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 53/71 | LOSS: 7.075285515788386e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 54/71 | LOSS: 7.054004031074741e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 55/71 | LOSS: 7.059646187761766e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 56/71 | LOSS: 7.071528798724901e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 57/71 | LOSS: 7.058140085816909e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 58/71 | LOSS: 7.035134893743878e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 59/71 | LOSS: 7.044622619408377e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 60/71 | LOSS: 7.032492368259719e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 61/71 | LOSS: 7.0263627387776655e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 62/71 | LOSS: 6.99630209777434e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 63/71 | LOSS: 6.968640718696406e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 64/71 | LOSS: 6.939132981642615e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 65/71 | LOSS: 6.8962769105520705e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 66/71 | LOSS: 6.875337041845396e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 67/71 | LOSS: 6.848883318151125e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 68/71 | LOSS: 6.8105831801865175e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 69/71 | LOSS: 6.791997644281115e-06\n",
      "TRAIN: EPOCH 181/1000 | BATCH 70/71 | LOSS: 6.754005509309805e-06\n",
      "VAL: EPOCH 181/1000 | BATCH 0/8 | LOSS: 7.197911145340186e-06\n",
      "VAL: EPOCH 181/1000 | BATCH 1/8 | LOSS: 6.719005114064203e-06\n",
      "VAL: EPOCH 181/1000 | BATCH 2/8 | LOSS: 6.019870700887016e-06\n",
      "VAL: EPOCH 181/1000 | BATCH 3/8 | LOSS: 6.463665727096668e-06\n",
      "VAL: EPOCH 181/1000 | BATCH 4/8 | LOSS: 6.371789368131431e-06\n",
      "VAL: EPOCH 181/1000 | BATCH 5/8 | LOSS: 6.259449416271916e-06\n",
      "VAL: EPOCH 181/1000 | BATCH 6/8 | LOSS: 6.247280842736862e-06\n",
      "VAL: EPOCH 181/1000 | BATCH 7/8 | LOSS: 6.125412028268329e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 0/71 | LOSS: 6.2435765357804485e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 1/71 | LOSS: 5.679381501977332e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 2/71 | LOSS: 5.768335237614035e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 3/71 | LOSS: 5.568135065914248e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 4/71 | LOSS: 5.5219590649358e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 5/71 | LOSS: 5.552986294787843e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 6/71 | LOSS: 5.2857505059884195e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 7/71 | LOSS: 5.3155048362896196e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 8/71 | LOSS: 5.355477393701828e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 9/71 | LOSS: 5.313538031259668e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 10/71 | LOSS: 5.181143106628125e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 11/71 | LOSS: 5.2085779316257685e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 12/71 | LOSS: 5.226035542504038e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 13/71 | LOSS: 5.223783741062757e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 14/71 | LOSS: 5.230664646660443e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 15/71 | LOSS: 5.305548512524183e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 16/71 | LOSS: 5.344457028567215e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 17/71 | LOSS: 5.388240677146112e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 18/71 | LOSS: 5.412993471652858e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 19/71 | LOSS: 5.52326227989397e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 20/71 | LOSS: 5.571437083874896e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 21/71 | LOSS: 5.534065042990154e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 22/71 | LOSS: 5.540571810738386e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 23/71 | LOSS: 5.648058201283372e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 24/71 | LOSS: 5.69624564377591e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 25/71 | LOSS: 5.695104041828577e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 26/71 | LOSS: 5.693490729080858e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 27/71 | LOSS: 5.684791728656689e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 28/71 | LOSS: 5.6779673318017335e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 29/71 | LOSS: 5.7200956537902435e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 30/71 | LOSS: 5.716568789319999e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 31/71 | LOSS: 5.76479577318878e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 32/71 | LOSS: 5.743040026462757e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 33/71 | LOSS: 5.785809020487838e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 34/71 | LOSS: 5.843200129415241e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 35/71 | LOSS: 5.8641432537519395e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 36/71 | LOSS: 5.92519449423275e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 37/71 | LOSS: 5.900213421371422e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 38/71 | LOSS: 5.976061308898258e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 39/71 | LOSS: 5.993672289150709e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 40/71 | LOSS: 6.0197198098005794e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 41/71 | LOSS: 6.026818742351939e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 42/71 | LOSS: 6.091300958867705e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 43/71 | LOSS: 6.110097915139208e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 44/71 | LOSS: 6.0959356334125105e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 45/71 | LOSS: 6.133396294424766e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 46/71 | LOSS: 6.151405749675332e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 47/71 | LOSS: 6.161962251856797e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 48/71 | LOSS: 6.186321375675366e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 49/71 | LOSS: 6.178256080602296e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 50/71 | LOSS: 6.217758888013515e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 51/71 | LOSS: 6.204843010401908e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 52/71 | LOSS: 6.2406744754693105e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 53/71 | LOSS: 6.227437990202344e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 54/71 | LOSS: 6.218409453140339e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 55/71 | LOSS: 6.17755453699179e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 56/71 | LOSS: 6.159642780084253e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 57/71 | LOSS: 6.158681367980597e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 58/71 | LOSS: 6.140527379318235e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 59/71 | LOSS: 6.151373584846927e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 60/71 | LOSS: 6.152119172451212e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 61/71 | LOSS: 6.156097088610442e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 62/71 | LOSS: 6.135160385874481e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 63/71 | LOSS: 6.1241770197284495e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 64/71 | LOSS: 6.105493873921939e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 65/71 | LOSS: 6.0986574432966085e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 66/71 | LOSS: 6.110841437855843e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 67/71 | LOSS: 6.076058716242936e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 68/71 | LOSS: 6.071734764771174e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 69/71 | LOSS: 6.0474551543977996e-06\n",
      "TRAIN: EPOCH 182/1000 | BATCH 70/71 | LOSS: 6.053690525040507e-06\n",
      "VAL: EPOCH 182/1000 | BATCH 0/8 | LOSS: 7.58165060688043e-06\n",
      "VAL: EPOCH 182/1000 | BATCH 1/8 | LOSS: 6.397701099558617e-06\n",
      "VAL: EPOCH 182/1000 | BATCH 2/8 | LOSS: 6.288409925521894e-06\n",
      "VAL: EPOCH 182/1000 | BATCH 3/8 | LOSS: 6.4970821540555335e-06\n",
      "VAL: EPOCH 182/1000 | BATCH 4/8 | LOSS: 6.462508918048116e-06\n",
      "VAL: EPOCH 182/1000 | BATCH 5/8 | LOSS: 6.339415601056923e-06\n",
      "VAL: EPOCH 182/1000 | BATCH 6/8 | LOSS: 6.1556819283785965e-06\n",
      "VAL: EPOCH 182/1000 | BATCH 7/8 | LOSS: 6.124578533217573e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 0/71 | LOSS: 4.911591076961486e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 1/71 | LOSS: 5.394504796640831e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 2/71 | LOSS: 5.540309454469631e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 3/71 | LOSS: 5.1356773838051595e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 4/71 | LOSS: 5.203119872021489e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 5/71 | LOSS: 5.511800812503982e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 6/71 | LOSS: 5.714759091850803e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 7/71 | LOSS: 5.636228593175474e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 8/71 | LOSS: 5.823198282289215e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 9/71 | LOSS: 5.837785965923103e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 10/71 | LOSS: 5.921475969278783e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 11/71 | LOSS: 5.876622897934188e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 12/71 | LOSS: 5.888364949686757e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 13/71 | LOSS: 5.803379963253974e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 14/71 | LOSS: 5.910693319795731e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 15/71 | LOSS: 5.8727903251565294e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 16/71 | LOSS: 5.842908595856416e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 17/71 | LOSS: 5.868698079009644e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 18/71 | LOSS: 5.900915168336053e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 19/71 | LOSS: 5.8918531067320146e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 20/71 | LOSS: 5.845329074613151e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 21/71 | LOSS: 5.837322574238484e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 22/71 | LOSS: 5.8873360617524115e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 23/71 | LOSS: 5.867298947729675e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 24/71 | LOSS: 5.9396905271569265e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 25/71 | LOSS: 5.927971030289952e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 26/71 | LOSS: 5.8832269031780825e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 27/71 | LOSS: 5.909760683867457e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 28/71 | LOSS: 5.909714343880328e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 29/71 | LOSS: 5.906339280651688e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 30/71 | LOSS: 5.886798501911295e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 31/71 | LOSS: 5.91982292519333e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 32/71 | LOSS: 5.990662912429825e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 33/71 | LOSS: 6.008585949209245e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 34/71 | LOSS: 6.03250638440451e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 35/71 | LOSS: 6.032404876855759e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 36/71 | LOSS: 5.993456127455559e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 37/71 | LOSS: 6.064867672026403e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 38/71 | LOSS: 6.044893535335387e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 39/71 | LOSS: 6.080249124806869e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 40/71 | LOSS: 6.088721364926087e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 41/71 | LOSS: 6.095577007380641e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 42/71 | LOSS: 6.1013748182990954e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 43/71 | LOSS: 6.1274735013615676e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 44/71 | LOSS: 6.129521575025542e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 45/71 | LOSS: 6.141678706784581e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 46/71 | LOSS: 6.17244727726892e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 47/71 | LOSS: 6.151067897993319e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 48/71 | LOSS: 6.1714273141085275e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 49/71 | LOSS: 6.144375338408281e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 50/71 | LOSS: 6.150909333276407e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 51/71 | LOSS: 6.113111777057594e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 52/71 | LOSS: 6.108490626227752e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 53/71 | LOSS: 6.093990165296688e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 54/71 | LOSS: 6.113582243084569e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 55/71 | LOSS: 6.09201837927945e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 56/71 | LOSS: 6.101601157800025e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 57/71 | LOSS: 6.113280090270062e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 58/71 | LOSS: 6.118991662379674e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 59/71 | LOSS: 6.101824336231706e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 60/71 | LOSS: 6.089876374286088e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 61/71 | LOSS: 6.063875448049775e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 62/71 | LOSS: 6.051211671763754e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 63/71 | LOSS: 6.0412416544863845e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 64/71 | LOSS: 6.027119993982174e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 65/71 | LOSS: 6.008221789395038e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 66/71 | LOSS: 6.008575321935333e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 67/71 | LOSS: 6.009750055615287e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 68/71 | LOSS: 5.987195910837846e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 69/71 | LOSS: 5.987479822319334e-06\n",
      "TRAIN: EPOCH 183/1000 | BATCH 70/71 | LOSS: 6.012159298132272e-06\n",
      "VAL: EPOCH 183/1000 | BATCH 0/8 | LOSS: 6.568945082108257e-06\n",
      "VAL: EPOCH 183/1000 | BATCH 1/8 | LOSS: 5.64484867027204e-06\n",
      "VAL: EPOCH 183/1000 | BATCH 2/8 | LOSS: 5.4907069776769886e-06\n",
      "VAL: EPOCH 183/1000 | BATCH 3/8 | LOSS: 5.834405897076067e-06\n",
      "VAL: EPOCH 183/1000 | BATCH 4/8 | LOSS: 5.79344387006131e-06\n",
      "VAL: EPOCH 183/1000 | BATCH 5/8 | LOSS: 5.681989705408341e-06\n",
      "VAL: EPOCH 183/1000 | BATCH 6/8 | LOSS: 5.556495580094634e-06\n",
      "VAL: EPOCH 183/1000 | BATCH 7/8 | LOSS: 5.5542429890920175e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 0/71 | LOSS: 6.56802831144887e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 1/71 | LOSS: 5.731236569772591e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 2/71 | LOSS: 5.704093988849006e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 3/71 | LOSS: 6.164495744087617e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 4/71 | LOSS: 6.386201766872545e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 5/71 | LOSS: 6.350788453346468e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 6/71 | LOSS: 6.491369991376164e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 7/71 | LOSS: 6.471352662629215e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 8/71 | LOSS: 6.384389507729793e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 9/71 | LOSS: 6.608941339436569e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 10/71 | LOSS: 6.480194025987823e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 11/71 | LOSS: 6.606625106542197e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 12/71 | LOSS: 6.6087128778655406e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 13/71 | LOSS: 6.703493357105929e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 14/71 | LOSS: 6.6755133654320766e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 15/71 | LOSS: 6.518820015344318e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 16/71 | LOSS: 6.472896610135007e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 17/71 | LOSS: 6.363078733152684e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 18/71 | LOSS: 6.306884978058454e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 19/71 | LOSS: 6.366504840116249e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 20/71 | LOSS: 6.3303740327163335e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 21/71 | LOSS: 6.3807948539430406e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 22/71 | LOSS: 6.394102851829856e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 23/71 | LOSS: 6.4566233959340025e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 24/71 | LOSS: 6.449622742366046e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 25/71 | LOSS: 6.417850674831872e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 26/71 | LOSS: 6.410062319951572e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 27/71 | LOSS: 6.347053824486336e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 28/71 | LOSS: 6.318395505865016e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 29/71 | LOSS: 6.288755184868933e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 30/71 | LOSS: 6.268795503626368e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 31/71 | LOSS: 6.306664630528758e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 32/71 | LOSS: 6.274192042893088e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 33/71 | LOSS: 6.3015536397870164e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 34/71 | LOSS: 6.283762091438153e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 35/71 | LOSS: 6.321558784192247e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 36/71 | LOSS: 6.335446377724529e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 37/71 | LOSS: 6.304834821429195e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 38/71 | LOSS: 6.2730262307480025e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 39/71 | LOSS: 6.2646763581142295e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 40/71 | LOSS: 6.236183796828278e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 41/71 | LOSS: 6.180087016868388e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 42/71 | LOSS: 6.199409833856788e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 43/71 | LOSS: 6.179845296504606e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 44/71 | LOSS: 6.207767637533834e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 45/71 | LOSS: 6.166421075170587e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 46/71 | LOSS: 6.181660226386466e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 47/71 | LOSS: 6.216872539728986e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 48/71 | LOSS: 6.222728689881849e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 49/71 | LOSS: 6.206547459441935e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 50/71 | LOSS: 6.180650041856365e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 51/71 | LOSS: 6.157301807154279e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 52/71 | LOSS: 6.170361208411501e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 53/71 | LOSS: 6.175678890940617e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 54/71 | LOSS: 6.157168809295399e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 55/71 | LOSS: 6.14506581086971e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 56/71 | LOSS: 6.159741149116471e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 57/71 | LOSS: 6.146523038955036e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 58/71 | LOSS: 6.1477784953949285e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 59/71 | LOSS: 6.1648435045450844e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 60/71 | LOSS: 6.169325790890936e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 61/71 | LOSS: 6.166519406472235e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 62/71 | LOSS: 6.199567935023908e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 63/71 | LOSS: 6.19351290964687e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 64/71 | LOSS: 6.167412196261396e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 65/71 | LOSS: 6.165433942993387e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 66/71 | LOSS: 6.168896405468521e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 67/71 | LOSS: 6.13872212210588e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 68/71 | LOSS: 6.1445551089992225e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 69/71 | LOSS: 6.134802950847578e-06\n",
      "TRAIN: EPOCH 184/1000 | BATCH 70/71 | LOSS: 6.154466281656478e-06\n",
      "VAL: EPOCH 184/1000 | BATCH 0/8 | LOSS: 7.687784091103822e-06\n",
      "VAL: EPOCH 184/1000 | BATCH 1/8 | LOSS: 6.579042747034691e-06\n",
      "VAL: EPOCH 184/1000 | BATCH 2/8 | LOSS: 6.833446908179515e-06\n",
      "VAL: EPOCH 184/1000 | BATCH 3/8 | LOSS: 6.995676585574984e-06\n",
      "VAL: EPOCH 184/1000 | BATCH 4/8 | LOSS: 7.043675213935785e-06\n",
      "VAL: EPOCH 184/1000 | BATCH 5/8 | LOSS: 6.848814943320273e-06\n",
      "VAL: EPOCH 184/1000 | BATCH 6/8 | LOSS: 6.631651233744508e-06\n",
      "VAL: EPOCH 184/1000 | BATCH 7/8 | LOSS: 6.7381427584223275e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 0/71 | LOSS: 5.796599907625932e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 1/71 | LOSS: 5.487573389473255e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 2/71 | LOSS: 6.613034050436302e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 3/71 | LOSS: 6.107704280111648e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 4/71 | LOSS: 6.57505088383914e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 5/71 | LOSS: 6.416442602130701e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 6/71 | LOSS: 6.241395827341226e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 7/71 | LOSS: 6.2187734783947235e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 8/71 | LOSS: 6.349683947822389e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 9/71 | LOSS: 6.345310248434544e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 10/71 | LOSS: 6.440111560963984e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 11/71 | LOSS: 6.415714096874581e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 12/71 | LOSS: 6.3155402340971004e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 13/71 | LOSS: 6.1977737979239985e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 14/71 | LOSS: 6.242808255289371e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 15/71 | LOSS: 6.1592892564021895e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 16/71 | LOSS: 6.120829392089303e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 17/71 | LOSS: 6.225671086212969e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 18/71 | LOSS: 6.206211573474943e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 19/71 | LOSS: 6.1925546106067484e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 20/71 | LOSS: 6.185143969820014e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 21/71 | LOSS: 6.198419007639379e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 22/71 | LOSS: 6.1658366061949534e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 23/71 | LOSS: 6.095428337478855e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 24/71 | LOSS: 6.0431193014665045e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 25/71 | LOSS: 6.008586506840612e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 26/71 | LOSS: 5.994276829436852e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 27/71 | LOSS: 5.968282493995503e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 28/71 | LOSS: 5.969732322635787e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 29/71 | LOSS: 5.950222869917828e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 30/71 | LOSS: 5.9767246878642076e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 31/71 | LOSS: 5.9261516582864715e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 32/71 | LOSS: 5.893574408136456e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 33/71 | LOSS: 5.854931166194386e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 34/71 | LOSS: 5.850921804397201e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 35/71 | LOSS: 5.810557670176624e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 36/71 | LOSS: 5.7859285113187755e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 37/71 | LOSS: 5.774681157115418e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 38/71 | LOSS: 5.764483620833534e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 39/71 | LOSS: 5.7653365729493086e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 40/71 | LOSS: 5.752684423563684e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 41/71 | LOSS: 5.738936600946112e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 42/71 | LOSS: 5.73060838918494e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 43/71 | LOSS: 5.713944336672615e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 44/71 | LOSS: 5.71700851044928e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 45/71 | LOSS: 5.726898092214706e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 46/71 | LOSS: 5.7647893241819665e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 47/71 | LOSS: 5.7598030214952205e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 48/71 | LOSS: 5.787614628964232e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 49/71 | LOSS: 5.809299864267814e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 50/71 | LOSS: 5.836430577177203e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 51/71 | LOSS: 5.852627396052412e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 52/71 | LOSS: 5.850403477728284e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 53/71 | LOSS: 5.852202497812363e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 54/71 | LOSS: 5.849442021082558e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 55/71 | LOSS: 5.863427864889152e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 56/71 | LOSS: 5.849036098656493e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 57/71 | LOSS: 5.8408938128132866e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 58/71 | LOSS: 5.862190206799068e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 59/71 | LOSS: 5.833540641712413e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 60/71 | LOSS: 5.811776265515812e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 61/71 | LOSS: 5.8046557642815204e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 62/71 | LOSS: 5.787004774613288e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 63/71 | LOSS: 5.788299652920159e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 64/71 | LOSS: 5.789649807971168e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 65/71 | LOSS: 5.772946465323471e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 66/71 | LOSS: 5.765557529936013e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 67/71 | LOSS: 5.758826820484126e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 68/71 | LOSS: 5.756157057423506e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 69/71 | LOSS: 5.778237878725382e-06\n",
      "TRAIN: EPOCH 185/1000 | BATCH 70/71 | LOSS: 5.768402702479236e-06\n",
      "VAL: EPOCH 185/1000 | BATCH 0/8 | LOSS: 7.869979526731186e-06\n",
      "VAL: EPOCH 185/1000 | BATCH 1/8 | LOSS: 6.970140020712279e-06\n",
      "VAL: EPOCH 185/1000 | BATCH 2/8 | LOSS: 7.509375791414641e-06\n",
      "VAL: EPOCH 185/1000 | BATCH 3/8 | LOSS: 7.733609891147353e-06\n",
      "VAL: EPOCH 185/1000 | BATCH 4/8 | LOSS: 7.842977538530249e-06\n",
      "VAL: EPOCH 185/1000 | BATCH 5/8 | LOSS: 7.655225545022404e-06\n",
      "VAL: EPOCH 185/1000 | BATCH 6/8 | LOSS: 7.402886564835041e-06\n",
      "VAL: EPOCH 185/1000 | BATCH 7/8 | LOSS: 7.529335107392399e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 0/71 | LOSS: 7.60306784286513e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 1/71 | LOSS: 6.589765007447568e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 2/71 | LOSS: 7.218543184232355e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 3/71 | LOSS: 6.923584692231088e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 4/71 | LOSS: 6.592450608877698e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 5/71 | LOSS: 6.664132494430912e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 6/71 | LOSS: 6.838993613200728e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 7/71 | LOSS: 6.721451200064621e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 8/71 | LOSS: 6.5625726569528015e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 9/71 | LOSS: 6.597477704417543e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 10/71 | LOSS: 6.506505955754123e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 11/71 | LOSS: 6.5243808270073105e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 12/71 | LOSS: 6.4495821928945725e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 13/71 | LOSS: 6.384186982748881e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 14/71 | LOSS: 6.4060228699721245e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 15/71 | LOSS: 6.40519860439781e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 16/71 | LOSS: 6.4119624446046895e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 17/71 | LOSS: 6.4493337706355505e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 18/71 | LOSS: 6.481104647766762e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 19/71 | LOSS: 6.61693813981401e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 20/71 | LOSS: 6.612792709520796e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 21/71 | LOSS: 6.6794613386181565e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 22/71 | LOSS: 6.704081047444007e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 23/71 | LOSS: 6.618604080207054e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 24/71 | LOSS: 6.551461319759256e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 25/71 | LOSS: 6.650630211879616e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 26/71 | LOSS: 6.580614404406839e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 27/71 | LOSS: 6.560470556645928e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 28/71 | LOSS: 6.583689041847187e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 29/71 | LOSS: 6.595435646280142e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 30/71 | LOSS: 6.600778144803048e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 31/71 | LOSS: 6.5963407820390785e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 32/71 | LOSS: 6.601285225439245e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 33/71 | LOSS: 6.567631383886157e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 34/71 | LOSS: 6.518532986124878e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 35/71 | LOSS: 6.4852627524992895e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 36/71 | LOSS: 6.4404594996704945e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 37/71 | LOSS: 6.489921002196895e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 38/71 | LOSS: 6.450376298371106e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 39/71 | LOSS: 6.491770591310342e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 40/71 | LOSS: 6.4698457740965614e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 41/71 | LOSS: 6.4804882012616696e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 42/71 | LOSS: 6.4567193476250395e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 43/71 | LOSS: 6.502010468135598e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 44/71 | LOSS: 6.4605704008297634e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 45/71 | LOSS: 6.487925742001845e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 46/71 | LOSS: 6.43366047370364e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 47/71 | LOSS: 6.430078296186063e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 48/71 | LOSS: 6.4189838184927574e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 49/71 | LOSS: 6.4038851087389045e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 50/71 | LOSS: 6.409736607904551e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 51/71 | LOSS: 6.365774661040073e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 52/71 | LOSS: 6.339024529775345e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 53/71 | LOSS: 6.3256536998745074e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 54/71 | LOSS: 6.278202206224457e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 55/71 | LOSS: 6.263852518324191e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 56/71 | LOSS: 6.252261420358034e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 57/71 | LOSS: 6.213451131080891e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 58/71 | LOSS: 6.183730412454155e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 59/71 | LOSS: 6.156078423676566e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 60/71 | LOSS: 6.126479972602387e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 61/71 | LOSS: 6.123860540326625e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 62/71 | LOSS: 6.129359924613262e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 63/71 | LOSS: 6.115646968396504e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 64/71 | LOSS: 6.096802231835542e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 65/71 | LOSS: 6.0688581594480304e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 66/71 | LOSS: 6.070111495902703e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 67/71 | LOSS: 6.070916918040038e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 68/71 | LOSS: 6.066093343785421e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 69/71 | LOSS: 6.058126843657062e-06\n",
      "TRAIN: EPOCH 186/1000 | BATCH 70/71 | LOSS: 6.097554042228271e-06\n",
      "VAL: EPOCH 186/1000 | BATCH 0/8 | LOSS: 9.839513040788006e-06\n",
      "VAL: EPOCH 186/1000 | BATCH 1/8 | LOSS: 8.50852370604116e-06\n",
      "VAL: EPOCH 186/1000 | BATCH 2/8 | LOSS: 9.283427213328347e-06\n",
      "VAL: EPOCH 186/1000 | BATCH 3/8 | LOSS: 9.35916807520698e-06\n",
      "VAL: EPOCH 186/1000 | BATCH 4/8 | LOSS: 9.41483094720752e-06\n",
      "VAL: EPOCH 186/1000 | BATCH 5/8 | LOSS: 9.448813443668769e-06\n",
      "VAL: EPOCH 186/1000 | BATCH 6/8 | LOSS: 9.234371840908093e-06\n",
      "VAL: EPOCH 186/1000 | BATCH 7/8 | LOSS: 9.371433463911671e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 0/71 | LOSS: 9.60903344093822e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 1/71 | LOSS: 6.85470240568975e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 2/71 | LOSS: 7.934785874870917e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 3/71 | LOSS: 7.333832854783395e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 4/71 | LOSS: 7.509542047046125e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 5/71 | LOSS: 7.486586052133741e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 6/71 | LOSS: 7.417943509478521e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 7/71 | LOSS: 7.387154255411588e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 8/71 | LOSS: 7.252167557049284e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 9/71 | LOSS: 7.151144791350816e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 10/71 | LOSS: 7.238669612971452e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 11/71 | LOSS: 7.347877385655011e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 12/71 | LOSS: 7.345054095472174e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 13/71 | LOSS: 7.251542878553405e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 14/71 | LOSS: 7.566103886347264e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 15/71 | LOSS: 7.43184631346594e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 16/71 | LOSS: 7.6748057087089e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 17/71 | LOSS: 7.678269867028575e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 18/71 | LOSS: 7.78936512353156e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 19/71 | LOSS: 7.76836998284125e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 20/71 | LOSS: 7.800556691758434e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 21/71 | LOSS: 7.746195168775474e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 22/71 | LOSS: 7.6001312031028485e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 23/71 | LOSS: 7.599004372120059e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 24/71 | LOSS: 7.5014635876868855e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 25/71 | LOSS: 7.381456795813462e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 26/71 | LOSS: 7.431282972026599e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 27/71 | LOSS: 7.378390819212655e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 28/71 | LOSS: 7.398829470260133e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 29/71 | LOSS: 7.360200015682494e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 30/71 | LOSS: 7.336808824132482e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 31/71 | LOSS: 7.358429030546176e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 32/71 | LOSS: 7.300261407208657e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 33/71 | LOSS: 7.341971655371119e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 34/71 | LOSS: 7.2776000836581395e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 35/71 | LOSS: 7.337095515443555e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 36/71 | LOSS: 7.292203254355873e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 37/71 | LOSS: 7.273378382945482e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 38/71 | LOSS: 7.29714972690906e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 39/71 | LOSS: 7.3616407007648375e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 40/71 | LOSS: 7.366853002700159e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 41/71 | LOSS: 7.340684966211916e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 42/71 | LOSS: 7.3384614002269556e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 43/71 | LOSS: 7.285289391554463e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 44/71 | LOSS: 7.2415385855290675e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 45/71 | LOSS: 7.215322699741354e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 46/71 | LOSS: 7.162325936347925e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 47/71 | LOSS: 7.1456528398054315e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 48/71 | LOSS: 7.141684633023367e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 49/71 | LOSS: 7.112960183803807e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 50/71 | LOSS: 7.074030753417401e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 51/71 | LOSS: 7.036859773716759e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 52/71 | LOSS: 6.999541393218712e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 53/71 | LOSS: 6.962652555052449e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 54/71 | LOSS: 6.9666470732375854e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 55/71 | LOSS: 6.936172967795885e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 56/71 | LOSS: 6.992147103901197e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 57/71 | LOSS: 7.005089448033812e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 58/71 | LOSS: 6.99112352021522e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 59/71 | LOSS: 7.031387250814684e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 60/71 | LOSS: 7.006229978168315e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 61/71 | LOSS: 7.034679525062412e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 62/71 | LOSS: 7.027628112515982e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 63/71 | LOSS: 7.068050678071813e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 64/71 | LOSS: 7.067582534476129e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 65/71 | LOSS: 7.071786478373917e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 66/71 | LOSS: 7.1155666556655e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 67/71 | LOSS: 7.106195171753129e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 68/71 | LOSS: 7.1154599145697634e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 69/71 | LOSS: 7.077253771318023e-06\n",
      "TRAIN: EPOCH 187/1000 | BATCH 70/71 | LOSS: 7.042637950600154e-06\n",
      "VAL: EPOCH 187/1000 | BATCH 0/8 | LOSS: 1.0154791198146995e-05\n",
      "VAL: EPOCH 187/1000 | BATCH 1/8 | LOSS: 1.0096689038618933e-05\n",
      "VAL: EPOCH 187/1000 | BATCH 2/8 | LOSS: 8.995564712677151e-06\n",
      "VAL: EPOCH 187/1000 | BATCH 3/8 | LOSS: 9.579868674336467e-06\n",
      "VAL: EPOCH 187/1000 | BATCH 4/8 | LOSS: 9.450153447687626e-06\n",
      "VAL: EPOCH 187/1000 | BATCH 5/8 | LOSS: 9.054814730310076e-06\n",
      "VAL: EPOCH 187/1000 | BATCH 6/8 | LOSS: 9.029760739005205e-06\n",
      "VAL: EPOCH 187/1000 | BATCH 7/8 | LOSS: 8.907039102723502e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 0/71 | LOSS: 8.077819074969739e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 1/71 | LOSS: 6.285404651862336e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 2/71 | LOSS: 6.941629483966001e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 3/71 | LOSS: 6.520958322653314e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 4/71 | LOSS: 6.784205834264867e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 5/71 | LOSS: 6.6037064243573695e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 6/71 | LOSS: 6.552301978704886e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 7/71 | LOSS: 6.544094901528297e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 8/71 | LOSS: 6.372273724587609e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 9/71 | LOSS: 6.405311023627292e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 10/71 | LOSS: 6.4648000013469504e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 11/71 | LOSS: 6.40557804369261e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 12/71 | LOSS: 6.427295270091586e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 13/71 | LOSS: 6.32983486476795e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 14/71 | LOSS: 6.295914893901985e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 15/71 | LOSS: 6.2532962203931675e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 16/71 | LOSS: 6.2229019289695905e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 17/71 | LOSS: 6.1351770202438475e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 18/71 | LOSS: 6.194074079470929e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 19/71 | LOSS: 6.130097062850837e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 20/71 | LOSS: 6.11528667789008e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 21/71 | LOSS: 6.152687919066838e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 22/71 | LOSS: 6.136266758561681e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 23/71 | LOSS: 6.172104311493361e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 24/71 | LOSS: 6.144768485683017e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 25/71 | LOSS: 6.1328411482883475e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 26/71 | LOSS: 6.102995973412396e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 27/71 | LOSS: 6.104245585447643e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 28/71 | LOSS: 6.044737237344039e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 29/71 | LOSS: 6.02065488237713e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 30/71 | LOSS: 6.005294732946617e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 31/71 | LOSS: 5.969270489458722e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 32/71 | LOSS: 5.9526980128959455e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 33/71 | LOSS: 5.926493860683609e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 34/71 | LOSS: 5.9204149952815245e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 35/71 | LOSS: 5.873607165085559e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 36/71 | LOSS: 5.84791814617347e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 37/71 | LOSS: 5.840452762353571e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 38/71 | LOSS: 5.805714303838054e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 39/71 | LOSS: 5.785237635791418e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 40/71 | LOSS: 5.788043115278081e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 41/71 | LOSS: 5.765326769895702e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 42/71 | LOSS: 5.805264949188757e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 43/71 | LOSS: 5.792864767913257e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 44/71 | LOSS: 5.808637807705155e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 45/71 | LOSS: 5.81885831746584e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 46/71 | LOSS: 5.839631343251119e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 47/71 | LOSS: 5.817932835346558e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 48/71 | LOSS: 5.836129229446298e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 49/71 | LOSS: 5.861904637640691e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 50/71 | LOSS: 5.876016209239263e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 51/71 | LOSS: 5.869456283737516e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 52/71 | LOSS: 5.884935266946724e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 53/71 | LOSS: 5.8847535237199975e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 54/71 | LOSS: 5.875944738032889e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 55/71 | LOSS: 5.900151092776339e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 56/71 | LOSS: 5.909586785460226e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 57/71 | LOSS: 5.918099736061714e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 58/71 | LOSS: 5.909831419927995e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 59/71 | LOSS: 5.9394650861577246e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 60/71 | LOSS: 5.940802417231292e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 61/71 | LOSS: 5.937131823708525e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 62/71 | LOSS: 5.959521259351205e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 63/71 | LOSS: 5.928215358608213e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 64/71 | LOSS: 5.91985979274166e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 65/71 | LOSS: 5.9134161788141215e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 66/71 | LOSS: 5.908045573933258e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 67/71 | LOSS: 5.920330134840162e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 68/71 | LOSS: 5.929456540837672e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 69/71 | LOSS: 5.943781882576462e-06\n",
      "TRAIN: EPOCH 188/1000 | BATCH 70/71 | LOSS: 5.958711733936224e-06\n",
      "VAL: EPOCH 188/1000 | BATCH 0/8 | LOSS: 7.172673576860689e-06\n",
      "VAL: EPOCH 188/1000 | BATCH 1/8 | LOSS: 6.15325757280516e-06\n",
      "VAL: EPOCH 188/1000 | BATCH 2/8 | LOSS: 5.8271051178356475e-06\n",
      "VAL: EPOCH 188/1000 | BATCH 3/8 | LOSS: 5.999587187943689e-06\n",
      "VAL: EPOCH 188/1000 | BATCH 4/8 | LOSS: 5.922344553255243e-06\n",
      "VAL: EPOCH 188/1000 | BATCH 5/8 | LOSS: 5.6548379537465126e-06\n",
      "VAL: EPOCH 188/1000 | BATCH 6/8 | LOSS: 5.558864813792752e-06\n",
      "VAL: EPOCH 188/1000 | BATCH 7/8 | LOSS: 5.505454168996948e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 0/71 | LOSS: 5.4341894610843156e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 1/71 | LOSS: 6.925081152076018e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 2/71 | LOSS: 6.621442328954193e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 3/71 | LOSS: 6.046566682016419e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 4/71 | LOSS: 5.931588748353533e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 5/71 | LOSS: 5.742084795201663e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 6/71 | LOSS: 5.788566081069543e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 7/71 | LOSS: 5.690680154657457e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 8/71 | LOSS: 5.588174038469636e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 9/71 | LOSS: 5.599119504040573e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 10/71 | LOSS: 5.5609863342066426e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 11/71 | LOSS: 5.792695484766834e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 12/71 | LOSS: 5.794550781082273e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 13/71 | LOSS: 5.8531639816854816e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 14/71 | LOSS: 5.774623908412954e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 15/71 | LOSS: 5.790522209281335e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 16/71 | LOSS: 5.749732415150741e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 17/71 | LOSS: 5.762828146139833e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 18/71 | LOSS: 5.6960219281838935e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 19/71 | LOSS: 5.764625007032009e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 20/71 | LOSS: 5.7469292187306564e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 21/71 | LOSS: 5.702910770692555e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 22/71 | LOSS: 5.797634853281678e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 23/71 | LOSS: 5.789515379698666e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 24/71 | LOSS: 5.772429994976847e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 25/71 | LOSS: 5.747310034656459e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 26/71 | LOSS: 5.700551523828095e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 27/71 | LOSS: 5.661932976051633e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 28/71 | LOSS: 5.705999165942997e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 29/71 | LOSS: 5.727425059376401e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 30/71 | LOSS: 5.741174816414e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 31/71 | LOSS: 5.773910814355077e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 32/71 | LOSS: 5.7855809383468255e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 33/71 | LOSS: 5.763383154878872e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 34/71 | LOSS: 5.865965613338631e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 35/71 | LOSS: 5.885278900071474e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 36/71 | LOSS: 5.910150333432752e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 37/71 | LOSS: 5.904026860853743e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 38/71 | LOSS: 5.924622912108391e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 39/71 | LOSS: 5.901827228171896e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 40/71 | LOSS: 5.8606783790221415e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 41/71 | LOSS: 5.861543178027551e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 42/71 | LOSS: 5.8888664001600305e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 43/71 | LOSS: 5.8710584715465135e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 44/71 | LOSS: 5.872130158321751e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 45/71 | LOSS: 5.856508758607442e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 46/71 | LOSS: 5.90553155872956e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 47/71 | LOSS: 5.909033281644345e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 48/71 | LOSS: 5.95077912925151e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 49/71 | LOSS: 5.928349537498434e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 50/71 | LOSS: 5.983157877256185e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 51/71 | LOSS: 5.953693857918552e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 52/71 | LOSS: 5.952674398014217e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 53/71 | LOSS: 5.952105758061171e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 54/71 | LOSS: 5.937251783259192e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 55/71 | LOSS: 5.904113525697799e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 56/71 | LOSS: 5.913029262796555e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 57/71 | LOSS: 5.89525904678308e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 58/71 | LOSS: 5.90645741039672e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 59/71 | LOSS: 5.898999014182967e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 60/71 | LOSS: 5.9000262510499985e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 61/71 | LOSS: 5.9097743184294465e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 62/71 | LOSS: 5.8939307174504e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 63/71 | LOSS: 5.902739239616039e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 64/71 | LOSS: 5.9508550573529925e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 65/71 | LOSS: 5.949313597077642e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 66/71 | LOSS: 5.941362502850477e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 67/71 | LOSS: 5.92139709249964e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 68/71 | LOSS: 5.910145681401195e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 69/71 | LOSS: 5.897688418242199e-06\n",
      "TRAIN: EPOCH 189/1000 | BATCH 70/71 | LOSS: 5.879528960346287e-06\n",
      "VAL: EPOCH 189/1000 | BATCH 0/8 | LOSS: 8.02924478193745e-06\n",
      "VAL: EPOCH 189/1000 | BATCH 1/8 | LOSS: 7.11453731128131e-06\n",
      "VAL: EPOCH 189/1000 | BATCH 2/8 | LOSS: 6.515515906357905e-06\n",
      "VAL: EPOCH 189/1000 | BATCH 3/8 | LOSS: 6.720044552821491e-06\n",
      "VAL: EPOCH 189/1000 | BATCH 4/8 | LOSS: 6.630070220126072e-06\n",
      "VAL: EPOCH 189/1000 | BATCH 5/8 | LOSS: 6.304362083634866e-06\n",
      "VAL: EPOCH 189/1000 | BATCH 6/8 | LOSS: 6.247004811094874e-06\n",
      "VAL: EPOCH 189/1000 | BATCH 7/8 | LOSS: 6.1574590972668375e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 0/71 | LOSS: 5.1319002523086965e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 1/71 | LOSS: 4.979727691534208e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 2/71 | LOSS: 5.038275655048589e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 3/71 | LOSS: 4.9038535507861525e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 4/71 | LOSS: 5.38054837306845e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 5/71 | LOSS: 5.676941327692475e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 6/71 | LOSS: 6.359851275712052e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 7/71 | LOSS: 6.2452317024508375e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 8/71 | LOSS: 6.649482404302236e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 9/71 | LOSS: 6.830042002547998e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 10/71 | LOSS: 6.846040584853406e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 11/71 | LOSS: 6.97606715978812e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 12/71 | LOSS: 7.046884058162015e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 13/71 | LOSS: 7.030219096902458e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 14/71 | LOSS: 6.929227583896136e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 15/71 | LOSS: 6.812820032564559e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 16/71 | LOSS: 6.8379770656225876e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 17/71 | LOSS: 6.749784006387927e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 18/71 | LOSS: 6.882809382085198e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 19/71 | LOSS: 6.949795715627261e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 20/71 | LOSS: 6.9318934731113366e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 21/71 | LOSS: 6.871685048860275e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 22/71 | LOSS: 6.7964749929239545e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 23/71 | LOSS: 6.865891975849081e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 24/71 | LOSS: 6.833490060671466e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 25/71 | LOSS: 6.7926034192238985e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 26/71 | LOSS: 6.743282601409764e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 27/71 | LOSS: 6.792150380923496e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 28/71 | LOSS: 6.72981648927691e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 29/71 | LOSS: 6.694491048619966e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 30/71 | LOSS: 6.704320155897163e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 31/71 | LOSS: 6.682827148551951e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 32/71 | LOSS: 6.6758705157333296e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 33/71 | LOSS: 6.6459625513208965e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 34/71 | LOSS: 6.668314953068537e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 35/71 | LOSS: 6.6416442273799075e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 36/71 | LOSS: 6.632258572534664e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 37/71 | LOSS: 6.657912272681929e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 38/71 | LOSS: 6.672655614145184e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 39/71 | LOSS: 6.756998845958151e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 40/71 | LOSS: 6.744221991539234e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 41/71 | LOSS: 6.763547552119624e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 42/71 | LOSS: 6.7427090682538745e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 43/71 | LOSS: 6.74663614187342e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 44/71 | LOSS: 6.774471795425699e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 45/71 | LOSS: 6.77249280393564e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 46/71 | LOSS: 6.7512746198267546e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 47/71 | LOSS: 6.724809992419978e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 48/71 | LOSS: 6.705735303308311e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 49/71 | LOSS: 6.692473198199877e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 50/71 | LOSS: 6.669062356962183e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 51/71 | LOSS: 6.629924944088718e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 52/71 | LOSS: 6.5930919433567226e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 53/71 | LOSS: 6.575745517154617e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 54/71 | LOSS: 6.5609159801996695e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 55/71 | LOSS: 6.538704407635773e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 56/71 | LOSS: 6.5750757146401025e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 57/71 | LOSS: 6.542294041255131e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 58/71 | LOSS: 6.559481967010701e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 59/71 | LOSS: 6.527931501902155e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 60/71 | LOSS: 6.508346605634691e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 61/71 | LOSS: 6.507607666560122e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 62/71 | LOSS: 6.497386399883359e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 63/71 | LOSS: 6.496368705199984e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 64/71 | LOSS: 6.4746044140170295e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 65/71 | LOSS: 6.487381679934598e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 66/71 | LOSS: 6.4866538928954605e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 67/71 | LOSS: 6.499813533831484e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 68/71 | LOSS: 6.491328006803457e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 69/71 | LOSS: 6.5042779689455135e-06\n",
      "TRAIN: EPOCH 190/1000 | BATCH 70/71 | LOSS: 6.498844432941733e-06\n",
      "VAL: EPOCH 190/1000 | BATCH 0/8 | LOSS: 8.834081199893262e-06\n",
      "VAL: EPOCH 190/1000 | BATCH 1/8 | LOSS: 7.843084858905058e-06\n",
      "VAL: EPOCH 190/1000 | BATCH 2/8 | LOSS: 7.0837641032994725e-06\n",
      "VAL: EPOCH 190/1000 | BATCH 3/8 | LOSS: 7.547246468675439e-06\n",
      "VAL: EPOCH 190/1000 | BATCH 4/8 | LOSS: 7.315538641705643e-06\n",
      "VAL: EPOCH 190/1000 | BATCH 5/8 | LOSS: 7.0270624140296905e-06\n",
      "VAL: EPOCH 190/1000 | BATCH 6/8 | LOSS: 6.976055569144332e-06\n",
      "VAL: EPOCH 190/1000 | BATCH 7/8 | LOSS: 6.897323089560814e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 0/71 | LOSS: 6.395101991074625e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 1/71 | LOSS: 5.25025916431332e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 2/71 | LOSS: 6.401631859868455e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 3/71 | LOSS: 6.484418008767534e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 4/71 | LOSS: 6.267630851652939e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 5/71 | LOSS: 6.145167844806565e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 6/71 | LOSS: 6.216795489828135e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 7/71 | LOSS: 6.377955457992357e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 8/71 | LOSS: 6.200522622950504e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 9/71 | LOSS: 6.29936243967677e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 10/71 | LOSS: 6.24306854164884e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 11/71 | LOSS: 6.175906188824835e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 12/71 | LOSS: 6.056085882077101e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 13/71 | LOSS: 6.00893339911376e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 14/71 | LOSS: 5.951639407915839e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 15/71 | LOSS: 5.972032511181169e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 16/71 | LOSS: 6.030872260881177e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 17/71 | LOSS: 5.999975731659409e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 18/71 | LOSS: 5.993852434920355e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 19/71 | LOSS: 6.0407207911339356e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 20/71 | LOSS: 5.99949007231598e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 21/71 | LOSS: 5.949286642961934e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 22/71 | LOSS: 5.901074770087649e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 23/71 | LOSS: 5.8712907389235625e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 24/71 | LOSS: 5.860611981916008e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 25/71 | LOSS: 5.807174637504011e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 26/71 | LOSS: 5.7813418708166055e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 27/71 | LOSS: 5.808036592368028e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 28/71 | LOSS: 5.858064092723813e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 29/71 | LOSS: 5.900474737548696e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 30/71 | LOSS: 5.99364423784033e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 31/71 | LOSS: 5.999634353770489e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 32/71 | LOSS: 5.98117095719455e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 33/71 | LOSS: 5.990736567315146e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 34/71 | LOSS: 5.9838408690536326e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 35/71 | LOSS: 5.973987413199211e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 36/71 | LOSS: 5.955979451902897e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 37/71 | LOSS: 5.964958590036411e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 38/71 | LOSS: 5.966176559013547e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 39/71 | LOSS: 5.956936308848526e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 40/71 | LOSS: 5.986831627346763e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 41/71 | LOSS: 5.989281254144208e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 42/71 | LOSS: 5.953759492238236e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 43/71 | LOSS: 5.958030866085292e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 44/71 | LOSS: 5.945559142775083e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 45/71 | LOSS: 5.9282128760336805e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 46/71 | LOSS: 5.893038400864362e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 47/71 | LOSS: 5.8859910438968654e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 48/71 | LOSS: 5.864354209604374e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 49/71 | LOSS: 5.845691493959748e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 50/71 | LOSS: 5.8528177813808295e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 51/71 | LOSS: 5.846495302928535e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 52/71 | LOSS: 5.81889844397665e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 53/71 | LOSS: 5.794188315054204e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 54/71 | LOSS: 5.795111825467426e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 55/71 | LOSS: 5.788946274216349e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 56/71 | LOSS: 5.805043209667865e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 57/71 | LOSS: 5.8016172527553365e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 58/71 | LOSS: 5.807197779223177e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 59/71 | LOSS: 5.834670052233074e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 60/71 | LOSS: 5.823152173061465e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 61/71 | LOSS: 5.826639960193255e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 62/71 | LOSS: 5.847547736410643e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 63/71 | LOSS: 5.8596555163603625e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 64/71 | LOSS: 5.837413062601207e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 65/71 | LOSS: 5.842328178142921e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 66/71 | LOSS: 5.821127730874352e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 67/71 | LOSS: 5.823409403948223e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 68/71 | LOSS: 5.84139797865646e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 69/71 | LOSS: 5.8252430660234364e-06\n",
      "TRAIN: EPOCH 191/1000 | BATCH 70/71 | LOSS: 5.808493485129078e-06\n",
      "VAL: EPOCH 191/1000 | BATCH 0/8 | LOSS: 7.164386715885485e-06\n",
      "VAL: EPOCH 191/1000 | BATCH 1/8 | LOSS: 6.8544293299055425e-06\n",
      "VAL: EPOCH 191/1000 | BATCH 2/8 | LOSS: 6.412378600847053e-06\n",
      "VAL: EPOCH 191/1000 | BATCH 3/8 | LOSS: 7.20049513347476e-06\n",
      "VAL: EPOCH 191/1000 | BATCH 4/8 | LOSS: 7.039915453788126e-06\n",
      "VAL: EPOCH 191/1000 | BATCH 5/8 | LOSS: 6.8650394950964255e-06\n",
      "VAL: EPOCH 191/1000 | BATCH 6/8 | LOSS: 6.761900034949317e-06\n",
      "VAL: EPOCH 191/1000 | BATCH 7/8 | LOSS: 6.603656800052704e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 0/71 | LOSS: 5.4828860811539926e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 1/71 | LOSS: 6.77244452162995e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 2/71 | LOSS: 6.143551824303965e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 3/71 | LOSS: 5.964897582089179e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 4/71 | LOSS: 6.439710887207184e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 5/71 | LOSS: 6.345493602566421e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 6/71 | LOSS: 6.349698651320068e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 7/71 | LOSS: 6.370985659032158e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 8/71 | LOSS: 6.189390660438221e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 9/71 | LOSS: 6.7220574237580875e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 10/71 | LOSS: 6.582482207952787e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 11/71 | LOSS: 6.754668068727672e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 12/71 | LOSS: 6.603489020143081e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 13/71 | LOSS: 6.635704201991237e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 14/71 | LOSS: 6.575727244732358e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 15/71 | LOSS: 6.518680919498365e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 16/71 | LOSS: 6.481009142375756e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 17/71 | LOSS: 6.422166658618759e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 18/71 | LOSS: 6.472147369525996e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 19/71 | LOSS: 6.395525338120933e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 20/71 | LOSS: 6.366534892419752e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 21/71 | LOSS: 6.423580667945895e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 22/71 | LOSS: 6.426833193551789e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 23/71 | LOSS: 6.3341996110466425e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 24/71 | LOSS: 6.360815950756659e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 25/71 | LOSS: 6.385293755346748e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 26/71 | LOSS: 6.440095435041089e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 27/71 | LOSS: 6.414591764171616e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 28/71 | LOSS: 6.423811822154174e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 29/71 | LOSS: 6.432025232546342e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 30/71 | LOSS: 6.454316257253184e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 31/71 | LOSS: 6.430091985976105e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 32/71 | LOSS: 6.498742633001794e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 33/71 | LOSS: 6.4336381242088355e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 34/71 | LOSS: 6.398549898481828e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 35/71 | LOSS: 6.370708736843274e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 36/71 | LOSS: 6.3194602999656865e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 37/71 | LOSS: 6.3037275834320005e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 38/71 | LOSS: 6.303460920137765e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 39/71 | LOSS: 6.3173477997224834e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 40/71 | LOSS: 6.3089557066055796e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 41/71 | LOSS: 6.320164922206979e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 42/71 | LOSS: 6.2924857237978655e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 43/71 | LOSS: 6.276527353127841e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 44/71 | LOSS: 6.2521941698731906e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 45/71 | LOSS: 6.25576363626865e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 46/71 | LOSS: 6.2339159894884475e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 47/71 | LOSS: 6.218967039709848e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 48/71 | LOSS: 6.237754396166491e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 49/71 | LOSS: 6.19403711425548e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 50/71 | LOSS: 6.1935476308052804e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 51/71 | LOSS: 6.168297073390232e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 52/71 | LOSS: 6.173601755194194e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 53/71 | LOSS: 6.153548535318502e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 54/71 | LOSS: 6.146689520392101e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 55/71 | LOSS: 6.153520424894461e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 56/71 | LOSS: 6.15714579429263e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 57/71 | LOSS: 6.146576722663855e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 58/71 | LOSS: 6.156450666112814e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 59/71 | LOSS: 6.16694808286411e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 60/71 | LOSS: 6.1718730693592374e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 61/71 | LOSS: 6.195454679689119e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 62/71 | LOSS: 6.213222056272022e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 63/71 | LOSS: 6.213105024244214e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 64/71 | LOSS: 6.179426054586656e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 65/71 | LOSS: 6.191025973252677e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 66/71 | LOSS: 6.157482219812175e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 67/71 | LOSS: 6.164617796939103e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 68/71 | LOSS: 6.153838473002669e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 69/71 | LOSS: 6.156609535017716e-06\n",
      "TRAIN: EPOCH 192/1000 | BATCH 70/71 | LOSS: 6.141920724550006e-06\n",
      "VAL: EPOCH 192/1000 | BATCH 0/8 | LOSS: 7.487546099582687e-06\n",
      "VAL: EPOCH 192/1000 | BATCH 1/8 | LOSS: 6.342073447740404e-06\n",
      "VAL: EPOCH 192/1000 | BATCH 2/8 | LOSS: 6.27538793196436e-06\n",
      "VAL: EPOCH 192/1000 | BATCH 3/8 | LOSS: 6.357500978992903e-06\n",
      "VAL: EPOCH 192/1000 | BATCH 4/8 | LOSS: 6.382465835486073e-06\n",
      "VAL: EPOCH 192/1000 | BATCH 5/8 | LOSS: 6.097349796618801e-06\n",
      "VAL: EPOCH 192/1000 | BATCH 6/8 | LOSS: 5.960788647436337e-06\n",
      "VAL: EPOCH 192/1000 | BATCH 7/8 | LOSS: 6.037088837729243e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 0/71 | LOSS: 6.6266129579162225e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 1/71 | LOSS: 5.4922466006246395e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 2/71 | LOSS: 5.499724162897716e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 3/71 | LOSS: 5.563660579355201e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 4/71 | LOSS: 5.678180968970992e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 5/71 | LOSS: 5.706473984901095e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 6/71 | LOSS: 5.840957718256894e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 7/71 | LOSS: 5.852366200542747e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 8/71 | LOSS: 5.800415945284638e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 9/71 | LOSS: 5.758816314482829e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 10/71 | LOSS: 5.801312578445174e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 11/71 | LOSS: 5.7801228194875875e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 12/71 | LOSS: 5.745021203788033e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 13/71 | LOSS: 5.845002630979122e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 14/71 | LOSS: 5.84195765137944e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 15/71 | LOSS: 5.9336132096632355e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 16/71 | LOSS: 5.98134339965969e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 17/71 | LOSS: 6.108139864914443e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 18/71 | LOSS: 6.106540096975789e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 19/71 | LOSS: 6.065329216653481e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 20/71 | LOSS: 6.083362807528049e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 21/71 | LOSS: 6.102474137812599e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 22/71 | LOSS: 6.113256226260843e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 23/71 | LOSS: 6.064729272263018e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 24/71 | LOSS: 6.044821184332249e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 25/71 | LOSS: 6.037368183779808e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 26/71 | LOSS: 5.993304141695801e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 27/71 | LOSS: 5.992503214916464e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 28/71 | LOSS: 5.975570180955151e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 29/71 | LOSS: 5.901965217465962e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 30/71 | LOSS: 5.916228122171766e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 31/71 | LOSS: 5.9139048858014576e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 32/71 | LOSS: 5.8958476212424244e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 33/71 | LOSS: 5.894267948914442e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 34/71 | LOSS: 5.969816993456334e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 35/71 | LOSS: 5.97204924209412e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 36/71 | LOSS: 6.026334680476843e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 37/71 | LOSS: 6.043952919897397e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 38/71 | LOSS: 6.015094116334624e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 39/71 | LOSS: 6.017950238401682e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 40/71 | LOSS: 6.113116319838588e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 41/71 | LOSS: 6.1404921433720684e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 42/71 | LOSS: 6.2065564105618845e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 43/71 | LOSS: 6.211899135037542e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 44/71 | LOSS: 6.181054060208327e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 45/71 | LOSS: 6.144872051426438e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 46/71 | LOSS: 6.156242374771032e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 47/71 | LOSS: 6.137179269671833e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 48/71 | LOSS: 6.136756995574058e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 49/71 | LOSS: 6.125744985183701e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 50/71 | LOSS: 6.109260539233219e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 51/71 | LOSS: 6.121070292955175e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 52/71 | LOSS: 6.093308448331334e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 53/71 | LOSS: 6.11021029591804e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 54/71 | LOSS: 6.093956281719412e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 55/71 | LOSS: 6.092044925156058e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 56/71 | LOSS: 6.068139100534609e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 57/71 | LOSS: 6.034221541995066e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 58/71 | LOSS: 6.030111076836613e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 59/71 | LOSS: 6.03239073673952e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 60/71 | LOSS: 6.041150493833588e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 61/71 | LOSS: 6.026871863967596e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 62/71 | LOSS: 6.016122853676308e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 63/71 | LOSS: 6.0184427752574265e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 64/71 | LOSS: 6.011358024037551e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 65/71 | LOSS: 6.007691367950898e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 66/71 | LOSS: 6.0058507472997765e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 67/71 | LOSS: 6.028592418474813e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 68/71 | LOSS: 6.0236378279417595e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 69/71 | LOSS: 6.007730377989771e-06\n",
      "TRAIN: EPOCH 193/1000 | BATCH 70/71 | LOSS: 5.963761866592571e-06\n",
      "VAL: EPOCH 193/1000 | BATCH 0/8 | LOSS: 8.467355655739084e-06\n",
      "VAL: EPOCH 193/1000 | BATCH 1/8 | LOSS: 7.295830528164515e-06\n",
      "VAL: EPOCH 193/1000 | BATCH 2/8 | LOSS: 7.069760310211374e-06\n",
      "VAL: EPOCH 193/1000 | BATCH 3/8 | LOSS: 6.8815295435342705e-06\n",
      "VAL: EPOCH 193/1000 | BATCH 4/8 | LOSS: 7.019039185252041e-06\n",
      "VAL: EPOCH 193/1000 | BATCH 5/8 | LOSS: 6.723504460145098e-06\n",
      "VAL: EPOCH 193/1000 | BATCH 6/8 | LOSS: 6.68627548553299e-06\n",
      "VAL: EPOCH 193/1000 | BATCH 7/8 | LOSS: 6.727966535891028e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 0/71 | LOSS: 5.480315849126782e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 1/71 | LOSS: 5.006815626984462e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 2/71 | LOSS: 5.56628644214167e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 3/71 | LOSS: 5.506939828592294e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 4/71 | LOSS: 5.763530862168409e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 5/71 | LOSS: 5.657857855112525e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 6/71 | LOSS: 5.783556259432641e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 7/71 | LOSS: 5.873379848253535e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 8/71 | LOSS: 6.199507323698425e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 9/71 | LOSS: 6.193234685269999e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 10/71 | LOSS: 6.382205893632173e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 11/71 | LOSS: 6.350848821057298e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 12/71 | LOSS: 6.525797813922579e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 13/71 | LOSS: 6.513040164333818e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 14/71 | LOSS: 6.6467531420736725e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 15/71 | LOSS: 6.58957330301746e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 16/71 | LOSS: 6.634298321895424e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 17/71 | LOSS: 6.616254192219155e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 18/71 | LOSS: 6.6290611022467554e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 19/71 | LOSS: 6.522497187688714e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 20/71 | LOSS: 6.516514076793101e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 21/71 | LOSS: 6.55292309271648e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 22/71 | LOSS: 6.4939681974267485e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 23/71 | LOSS: 6.516850570884951e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 24/71 | LOSS: 6.476559265138348e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 25/71 | LOSS: 6.396986883863485e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 26/71 | LOSS: 6.341104734828049e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 27/71 | LOSS: 6.302654761774258e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 28/71 | LOSS: 6.269191058451944e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 29/71 | LOSS: 6.219889489026779e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 30/71 | LOSS: 6.202113567152053e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 31/71 | LOSS: 6.143561961380328e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 32/71 | LOSS: 6.118196448700143e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 33/71 | LOSS: 6.09853576433499e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 34/71 | LOSS: 6.098986913067555e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 35/71 | LOSS: 6.0953701045542e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 36/71 | LOSS: 6.06858692835148e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 37/71 | LOSS: 6.047098000444269e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 38/71 | LOSS: 6.024620863452303e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 39/71 | LOSS: 6.042694008101535e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 40/71 | LOSS: 6.044307333579083e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 41/71 | LOSS: 6.031336115917102e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 42/71 | LOSS: 6.00975486016724e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 43/71 | LOSS: 5.99650642966894e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 44/71 | LOSS: 6.014165844842662e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 45/71 | LOSS: 5.99025755669256e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 46/71 | LOSS: 5.9510094867251375e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 47/71 | LOSS: 5.921142189890816e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 48/71 | LOSS: 5.919376550936045e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 49/71 | LOSS: 5.9016229351982475e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 50/71 | LOSS: 5.866857998840073e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 51/71 | LOSS: 5.8347361048688e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 52/71 | LOSS: 5.813730257494103e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 53/71 | LOSS: 5.779190814791946e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 54/71 | LOSS: 5.781451777279885e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 55/71 | LOSS: 5.779915235442397e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 56/71 | LOSS: 5.769550928865404e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 57/71 | LOSS: 5.764400361056624e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 58/71 | LOSS: 5.764368941496379e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 59/71 | LOSS: 5.753807264833691e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 60/71 | LOSS: 5.7572551146431135e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 61/71 | LOSS: 5.752468180822513e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 62/71 | LOSS: 5.733622676041069e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 63/71 | LOSS: 5.732020170512442e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 64/71 | LOSS: 5.731043620471609e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 65/71 | LOSS: 5.713086776185347e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 66/71 | LOSS: 5.714260087595686e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 67/71 | LOSS: 5.7257715699212415e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 68/71 | LOSS: 5.74163361992417e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 69/71 | LOSS: 5.736964379009026e-06\n",
      "TRAIN: EPOCH 194/1000 | BATCH 70/71 | LOSS: 5.717212491323204e-06\n",
      "VAL: EPOCH 194/1000 | BATCH 0/8 | LOSS: 7.090874987625284e-06\n",
      "VAL: EPOCH 194/1000 | BATCH 1/8 | LOSS: 6.4536172885709675e-06\n",
      "VAL: EPOCH 194/1000 | BATCH 2/8 | LOSS: 6.065958586987108e-06\n",
      "VAL: EPOCH 194/1000 | BATCH 3/8 | LOSS: 6.70486315357266e-06\n",
      "VAL: EPOCH 194/1000 | BATCH 4/8 | LOSS: 6.55144849588396e-06\n",
      "VAL: EPOCH 194/1000 | BATCH 5/8 | LOSS: 6.249140066453644e-06\n",
      "VAL: EPOCH 194/1000 | BATCH 6/8 | LOSS: 6.062094923566162e-06\n",
      "VAL: EPOCH 194/1000 | BATCH 7/8 | LOSS: 5.972375674900832e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 0/71 | LOSS: 6.818122074037092e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 1/71 | LOSS: 6.750057536919485e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 2/71 | LOSS: 6.246346856642049e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 3/71 | LOSS: 6.484547384388861e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 4/71 | LOSS: 7.012832611508202e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 5/71 | LOSS: 6.873504541241952e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 6/71 | LOSS: 6.424570918690213e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 7/71 | LOSS: 6.632100166825694e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 8/71 | LOSS: 6.5153943877602514e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 9/71 | LOSS: 6.657867652393179e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 10/71 | LOSS: 6.764162125446918e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 11/71 | LOSS: 6.704679359851677e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 12/71 | LOSS: 6.665042047433627e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 13/71 | LOSS: 6.531500470633286e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 14/71 | LOSS: 6.5216699416244715e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 15/71 | LOSS: 6.477515086089625e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 16/71 | LOSS: 6.507153370073545e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 17/71 | LOSS: 6.4718845907110435e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 18/71 | LOSS: 6.518371049567481e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 19/71 | LOSS: 6.4394591618111004e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 20/71 | LOSS: 6.423835159131115e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 21/71 | LOSS: 6.42541621428708e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 22/71 | LOSS: 6.373988407690574e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 23/71 | LOSS: 6.366448455234301e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 24/71 | LOSS: 6.337551076285308e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 25/71 | LOSS: 6.276667923325126e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 26/71 | LOSS: 6.227362148449713e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 27/71 | LOSS: 6.217013817279492e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 28/71 | LOSS: 6.174178875441832e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 29/71 | LOSS: 6.133165182594288e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 30/71 | LOSS: 6.08540032713321e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 31/71 | LOSS: 6.096883197415082e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 32/71 | LOSS: 6.115310029461838e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 33/71 | LOSS: 6.14279439800799e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 34/71 | LOSS: 6.134048674409444e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 35/71 | LOSS: 6.1849015082771075e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 36/71 | LOSS: 6.178045402299864e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 37/71 | LOSS: 6.235424902306956e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 38/71 | LOSS: 6.205934077558758e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 39/71 | LOSS: 6.2384023294725924e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 40/71 | LOSS: 6.2205192812032735e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 41/71 | LOSS: 6.205663285505753e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 42/71 | LOSS: 6.18908034296108e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 43/71 | LOSS: 6.173500682135372e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 44/71 | LOSS: 6.137798007128165e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 45/71 | LOSS: 6.122945528066181e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 46/71 | LOSS: 6.130103197102336e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 47/71 | LOSS: 6.134103027003827e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 48/71 | LOSS: 6.131984320040364e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 49/71 | LOSS: 6.0959894290135706e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 50/71 | LOSS: 6.107440702751274e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 51/71 | LOSS: 6.092369718456981e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 52/71 | LOSS: 6.088970613351228e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 53/71 | LOSS: 6.101072619491822e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 54/71 | LOSS: 6.104746245744701e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 55/71 | LOSS: 6.097600451734511e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 56/71 | LOSS: 6.095761539638675e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 57/71 | LOSS: 6.0815512934541085e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 58/71 | LOSS: 6.047600282280177e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 59/71 | LOSS: 6.022684950342713e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 60/71 | LOSS: 6.022422268393175e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 61/71 | LOSS: 5.999553936377314e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 62/71 | LOSS: 5.990493928182996e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 63/71 | LOSS: 5.976709118726831e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 64/71 | LOSS: 5.969614553927945e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 65/71 | LOSS: 5.99021282279423e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 66/71 | LOSS: 5.968385266896803e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 67/71 | LOSS: 5.973448999023222e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 68/71 | LOSS: 5.965859310126161e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 69/71 | LOSS: 5.991814578010235e-06\n",
      "TRAIN: EPOCH 195/1000 | BATCH 70/71 | LOSS: 5.999541109547504e-06\n",
      "VAL: EPOCH 195/1000 | BATCH 0/8 | LOSS: 9.24062442209106e-06\n",
      "VAL: EPOCH 195/1000 | BATCH 1/8 | LOSS: 8.263377822004259e-06\n",
      "VAL: EPOCH 195/1000 | BATCH 2/8 | LOSS: 7.55082510295324e-06\n",
      "VAL: EPOCH 195/1000 | BATCH 3/8 | LOSS: 7.807728934494662e-06\n",
      "VAL: EPOCH 195/1000 | BATCH 4/8 | LOSS: 7.717523294559214e-06\n",
      "VAL: EPOCH 195/1000 | BATCH 5/8 | LOSS: 7.3145228422314785e-06\n",
      "VAL: EPOCH 195/1000 | BATCH 6/8 | LOSS: 7.303587868331565e-06\n",
      "VAL: EPOCH 195/1000 | BATCH 7/8 | LOSS: 7.311585477509652e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 0/71 | LOSS: 5.667059213010361e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 1/71 | LOSS: 8.46810485199967e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 2/71 | LOSS: 7.716719361875827e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 3/71 | LOSS: 7.57203974899312e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 4/71 | LOSS: 7.764077054162044e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 5/71 | LOSS: 7.618172579289724e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 6/71 | LOSS: 7.675096769941905e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 7/71 | LOSS: 7.665590317174065e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 8/71 | LOSS: 7.912298668897063e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 9/71 | LOSS: 7.622660359629663e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 10/71 | LOSS: 7.777981739888094e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 11/71 | LOSS: 7.587016777203341e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 12/71 | LOSS: 7.4776708760719675e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 13/71 | LOSS: 7.451047232669745e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 14/71 | LOSS: 7.2055556756822625e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 15/71 | LOSS: 7.317073468016133e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 16/71 | LOSS: 7.2783001381621404e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 17/71 | LOSS: 7.2947735058429925e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 18/71 | LOSS: 7.357151568947684e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 19/71 | LOSS: 7.385533774595388e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 20/71 | LOSS: 7.275734365716094e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 21/71 | LOSS: 7.258359108742628e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 22/71 | LOSS: 7.326512322043013e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 23/71 | LOSS: 7.253335079819105e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 24/71 | LOSS: 7.272648263096926e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 25/71 | LOSS: 7.250337122199058e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 26/71 | LOSS: 7.268215498428347e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 27/71 | LOSS: 7.218710452759426e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 28/71 | LOSS: 7.237424464118044e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 29/71 | LOSS: 7.239227466016019e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 30/71 | LOSS: 7.2196284485647585e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 31/71 | LOSS: 7.239135761949456e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 32/71 | LOSS: 7.2312988956555e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 33/71 | LOSS: 7.274741893953058e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 34/71 | LOSS: 7.278807535320603e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 35/71 | LOSS: 7.29158155207996e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 36/71 | LOSS: 7.359161464911587e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 37/71 | LOSS: 7.305622704590956e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 38/71 | LOSS: 7.3271573210849265e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 39/71 | LOSS: 7.297504924963505e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 40/71 | LOSS: 7.294260745854954e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 41/71 | LOSS: 7.334951874327089e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 42/71 | LOSS: 7.360388709394313e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 43/71 | LOSS: 7.390150973275343e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 44/71 | LOSS: 7.3646578382775055e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 45/71 | LOSS: 7.4207024454153725e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 46/71 | LOSS: 7.420519025006488e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 47/71 | LOSS: 7.383594014716739e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 48/71 | LOSS: 7.329387175076943e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 49/71 | LOSS: 7.277079325831437e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 50/71 | LOSS: 7.282022343311908e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 51/71 | LOSS: 7.28065526363235e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 52/71 | LOSS: 7.248457167115016e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 53/71 | LOSS: 7.188855411860223e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 54/71 | LOSS: 7.141403086114389e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 55/71 | LOSS: 7.094705718405489e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 56/71 | LOSS: 7.051129347146662e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 57/71 | LOSS: 7.051531262167655e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 58/71 | LOSS: 7.0255716995450065e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 59/71 | LOSS: 6.996393862361098e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 60/71 | LOSS: 6.984894443881845e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 61/71 | LOSS: 6.9622747435372025e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 62/71 | LOSS: 6.951691776044927e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 63/71 | LOSS: 6.930892983092463e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 64/71 | LOSS: 6.917426326953529e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 65/71 | LOSS: 6.920425996461497e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 66/71 | LOSS: 6.899529855435327e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 67/71 | LOSS: 6.902004225353534e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 68/71 | LOSS: 6.894267629752943e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 69/71 | LOSS: 6.880808362958695e-06\n",
      "TRAIN: EPOCH 196/1000 | BATCH 70/71 | LOSS: 6.939220713935407e-06\n",
      "VAL: EPOCH 196/1000 | BATCH 0/8 | LOSS: 7.048835868772585e-06\n",
      "VAL: EPOCH 196/1000 | BATCH 1/8 | LOSS: 6.304020189418225e-06\n",
      "VAL: EPOCH 196/1000 | BATCH 2/8 | LOSS: 5.872391435938577e-06\n",
      "VAL: EPOCH 196/1000 | BATCH 3/8 | LOSS: 6.323734396573855e-06\n",
      "VAL: EPOCH 196/1000 | BATCH 4/8 | LOSS: 6.239677986741299e-06\n",
      "VAL: EPOCH 196/1000 | BATCH 5/8 | LOSS: 5.854232161558078e-06\n",
      "VAL: EPOCH 196/1000 | BATCH 6/8 | LOSS: 5.721199418725778e-06\n",
      "VAL: EPOCH 196/1000 | BATCH 7/8 | LOSS: 5.635298407469236e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 0/71 | LOSS: 5.510585651791189e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 1/71 | LOSS: 5.6849924021662446e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 2/71 | LOSS: 5.702920892266168e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 3/71 | LOSS: 5.903675400986685e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 4/71 | LOSS: 5.996469462843379e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 5/71 | LOSS: 5.889417100964541e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 6/71 | LOSS: 5.740933959584384e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 7/71 | LOSS: 5.59787321208205e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 8/71 | LOSS: 5.525632483719771e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 9/71 | LOSS: 5.639578103000531e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 10/71 | LOSS: 5.7178503993782215e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 11/71 | LOSS: 5.704508491059339e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 12/71 | LOSS: 5.66950799251432e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 13/71 | LOSS: 5.641152029056684e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 14/71 | LOSS: 5.6566356154993025e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 15/71 | LOSS: 5.622867462307113e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 16/71 | LOSS: 5.738652442446705e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 17/71 | LOSS: 5.67957375905179e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 18/71 | LOSS: 5.82192914359774e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 19/71 | LOSS: 5.85412637974514e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 20/71 | LOSS: 5.971784113561236e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 21/71 | LOSS: 6.024637617553129e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 22/71 | LOSS: 6.006135880971155e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 23/71 | LOSS: 6.286924929099769e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 24/71 | LOSS: 6.294530921877595e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 25/71 | LOSS: 6.413950481706147e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 26/71 | LOSS: 6.557425658010524e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 27/71 | LOSS: 6.613025967843507e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 28/71 | LOSS: 6.64438410804623e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 29/71 | LOSS: 6.731835431613338e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 30/71 | LOSS: 6.725811867762522e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 31/71 | LOSS: 6.7309331228671e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 32/71 | LOSS: 6.807383835455932e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 33/71 | LOSS: 6.791169733546935e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 34/71 | LOSS: 6.803498477633444e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 35/71 | LOSS: 6.7430621331570565e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 36/71 | LOSS: 6.83164922225547e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 37/71 | LOSS: 6.824357904789442e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 38/71 | LOSS: 6.829686124267308e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 39/71 | LOSS: 6.843074015705497e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 40/71 | LOSS: 6.873023089499883e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 41/71 | LOSS: 6.8651078046449195e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 42/71 | LOSS: 6.834819064246149e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 43/71 | LOSS: 6.856115481406794e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 44/71 | LOSS: 6.811752245994285e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 45/71 | LOSS: 6.85854616755893e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 46/71 | LOSS: 6.794555702072103e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 47/71 | LOSS: 6.782734743637775e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 48/71 | LOSS: 6.743320162578519e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 49/71 | LOSS: 6.7692222637560916e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 50/71 | LOSS: 6.753966478273094e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 51/71 | LOSS: 6.733893078839733e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 52/71 | LOSS: 6.722806634604909e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 53/71 | LOSS: 6.695029747384575e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 54/71 | LOSS: 6.704187977778599e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 55/71 | LOSS: 6.660706990909862e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 56/71 | LOSS: 6.675212086536289e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 57/71 | LOSS: 6.663939600535929e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 58/71 | LOSS: 6.662775276554847e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 59/71 | LOSS: 6.677021876081805e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 60/71 | LOSS: 6.65752243355096e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 61/71 | LOSS: 6.737031673119711e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 62/71 | LOSS: 6.722495794429773e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 63/71 | LOSS: 6.753401528669656e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 64/71 | LOSS: 6.7346991342604335e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 65/71 | LOSS: 6.734337499768696e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 66/71 | LOSS: 6.738068635452542e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 67/71 | LOSS: 6.763561092317532e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 68/71 | LOSS: 6.748154541181039e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 69/71 | LOSS: 6.710152729283436e-06\n",
      "TRAIN: EPOCH 197/1000 | BATCH 70/71 | LOSS: 6.71055023675263e-06\n",
      "VAL: EPOCH 197/1000 | BATCH 0/8 | LOSS: 7.553727300546598e-06\n",
      "VAL: EPOCH 197/1000 | BATCH 1/8 | LOSS: 6.556094831466908e-06\n",
      "VAL: EPOCH 197/1000 | BATCH 2/8 | LOSS: 6.77344966485786e-06\n",
      "VAL: EPOCH 197/1000 | BATCH 3/8 | LOSS: 6.7865763639929355e-06\n",
      "VAL: EPOCH 197/1000 | BATCH 4/8 | LOSS: 6.883487549202982e-06\n",
      "VAL: EPOCH 197/1000 | BATCH 5/8 | LOSS: 6.798021634798109e-06\n",
      "VAL: EPOCH 197/1000 | BATCH 6/8 | LOSS: 6.709133491053113e-06\n",
      "VAL: EPOCH 197/1000 | BATCH 7/8 | LOSS: 6.857678954474977e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 0/71 | LOSS: 5.9075900935567915e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 1/71 | LOSS: 5.883992116650916e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 2/71 | LOSS: 6.2662682770072324e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 3/71 | LOSS: 5.858710323991545e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 4/71 | LOSS: 5.7459911658952475e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 5/71 | LOSS: 5.723577790680186e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 6/71 | LOSS: 5.570430078348311e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 7/71 | LOSS: 5.93664975667707e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 8/71 | LOSS: 5.879156661750231e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 9/71 | LOSS: 5.743518704548478e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 10/71 | LOSS: 5.577744022130818e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 11/71 | LOSS: 5.5642736545754206e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 12/71 | LOSS: 5.537927686068892e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 13/71 | LOSS: 5.555801115925922e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 14/71 | LOSS: 5.599630640062969e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 15/71 | LOSS: 5.581476642646521e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 16/71 | LOSS: 5.5394328174931136e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 17/71 | LOSS: 5.547576645565439e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 18/71 | LOSS: 5.56330764754277e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 19/71 | LOSS: 5.537604897654091e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 20/71 | LOSS: 5.535567977079024e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 21/71 | LOSS: 5.558710571759465e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 22/71 | LOSS: 5.594262485987122e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 23/71 | LOSS: 5.640396485280992e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 24/71 | LOSS: 5.637428930640454e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 25/71 | LOSS: 5.63082611594067e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 26/71 | LOSS: 5.675298763418579e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 27/71 | LOSS: 5.660949422495573e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 28/71 | LOSS: 5.6693393946324215e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 29/71 | LOSS: 5.649936307842533e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 30/71 | LOSS: 5.61493786130262e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 31/71 | LOSS: 5.618339869783995e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 32/71 | LOSS: 5.649731933358866e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 33/71 | LOSS: 5.695272822028046e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 34/71 | LOSS: 5.7215827447570125e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 35/71 | LOSS: 5.687198621570941e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 36/71 | LOSS: 5.70481752465885e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 37/71 | LOSS: 5.692769634965532e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 38/71 | LOSS: 5.69436259376324e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 39/71 | LOSS: 5.697297535789403e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 40/71 | LOSS: 5.698302463382034e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 41/71 | LOSS: 5.738552869973189e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 42/71 | LOSS: 5.7608771237218486e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 43/71 | LOSS: 5.730527432619552e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 44/71 | LOSS: 5.733745850496537e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 45/71 | LOSS: 5.819203945224138e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 46/71 | LOSS: 5.831146628476377e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 47/71 | LOSS: 5.819523048936996e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 48/71 | LOSS: 5.853852784713701e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 49/71 | LOSS: 5.830725449413876e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 50/71 | LOSS: 5.811850430701252e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 51/71 | LOSS: 5.81755780506053e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 52/71 | LOSS: 5.81025579039014e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 53/71 | LOSS: 5.787958663724432e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 54/71 | LOSS: 5.778584835008421e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 55/71 | LOSS: 5.812806551018314e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 56/71 | LOSS: 5.82626599728093e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 57/71 | LOSS: 5.831776779187153e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 58/71 | LOSS: 5.8232120925637545e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 59/71 | LOSS: 5.855620490062089e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 60/71 | LOSS: 5.833702676890888e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 61/71 | LOSS: 5.848008391518716e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 62/71 | LOSS: 5.8282905936519255e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 63/71 | LOSS: 5.8128390278966435e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 64/71 | LOSS: 5.822310329396994e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 65/71 | LOSS: 5.834528069085949e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 66/71 | LOSS: 5.861577209095651e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 67/71 | LOSS: 5.858267567258308e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 68/71 | LOSS: 5.923165295773314e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 69/71 | LOSS: 5.9194481959171495e-06\n",
      "TRAIN: EPOCH 198/1000 | BATCH 70/71 | LOSS: 5.943704661443866e-06\n",
      "VAL: EPOCH 198/1000 | BATCH 0/8 | LOSS: 8.007982614799403e-06\n",
      "VAL: EPOCH 198/1000 | BATCH 1/8 | LOSS: 6.972357368795201e-06\n",
      "VAL: EPOCH 198/1000 | BATCH 2/8 | LOSS: 6.981768668386697e-06\n",
      "VAL: EPOCH 198/1000 | BATCH 3/8 | LOSS: 7.020535690571705e-06\n",
      "VAL: EPOCH 198/1000 | BATCH 4/8 | LOSS: 7.06400232957094e-06\n",
      "VAL: EPOCH 198/1000 | BATCH 5/8 | LOSS: 6.763021625981007e-06\n",
      "VAL: EPOCH 198/1000 | BATCH 6/8 | LOSS: 6.6414314200561165e-06\n",
      "VAL: EPOCH 198/1000 | BATCH 7/8 | LOSS: 6.766775470623543e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 0/71 | LOSS: 6.228735401236918e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 1/71 | LOSS: 6.664995453320444e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 2/71 | LOSS: 7.200648421227622e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 3/71 | LOSS: 7.065548061291338e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 4/71 | LOSS: 7.394621934508905e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 5/71 | LOSS: 7.246778598831345e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 6/71 | LOSS: 7.183567634326339e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 7/71 | LOSS: 7.021335363788239e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 8/71 | LOSS: 6.971324283868954e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 9/71 | LOSS: 6.834153737145243e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 10/71 | LOSS: 6.809545745702715e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 11/71 | LOSS: 6.6493439211020205e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 12/71 | LOSS: 6.527114097620002e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 13/71 | LOSS: 6.528962168366499e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 14/71 | LOSS: 6.5093833654827906e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 15/71 | LOSS: 6.485390855459627e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 16/71 | LOSS: 6.4601018921701535e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 17/71 | LOSS: 6.405868665145438e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 18/71 | LOSS: 6.444002002880504e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 19/71 | LOSS: 6.569184620275337e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 20/71 | LOSS: 6.466052214125825e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 21/71 | LOSS: 6.432060093753865e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 22/71 | LOSS: 6.4426487132669026e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 23/71 | LOSS: 6.364000986044023e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 24/71 | LOSS: 6.355803634505719e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 25/71 | LOSS: 6.315438090845978e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 26/71 | LOSS: 6.326197291383761e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 27/71 | LOSS: 6.332555601927327e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 28/71 | LOSS: 6.2600328701905555e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 29/71 | LOSS: 6.248284838269076e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 30/71 | LOSS: 6.197074467749072e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 31/71 | LOSS: 6.1581330186299965e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 32/71 | LOSS: 6.105940525285956e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 33/71 | LOSS: 6.093477462426938e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 34/71 | LOSS: 6.101558106560592e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 35/71 | LOSS: 6.0636497588954325e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 36/71 | LOSS: 6.044536938724058e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 37/71 | LOSS: 6.013717319020591e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 38/71 | LOSS: 5.979618774103867e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 39/71 | LOSS: 5.935914816745935e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 40/71 | LOSS: 5.934476997403758e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 41/71 | LOSS: 5.928238936446308e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 42/71 | LOSS: 5.933891474722809e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 43/71 | LOSS: 5.941366225670208e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 44/71 | LOSS: 5.9280957733183945e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 45/71 | LOSS: 5.9280166822092095e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 46/71 | LOSS: 5.922861699683051e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 47/71 | LOSS: 5.90316471971164e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 48/71 | LOSS: 5.87941341870405e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 49/71 | LOSS: 5.8529032776277745e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 50/71 | LOSS: 5.829978051099483e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 51/71 | LOSS: 5.810531241387742e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 52/71 | LOSS: 5.79939458750129e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 53/71 | LOSS: 5.8047325503004555e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 54/71 | LOSS: 5.76711994323308e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 55/71 | LOSS: 5.747316436099936e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 56/71 | LOSS: 5.746845441503785e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 57/71 | LOSS: 5.731000336364277e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 58/71 | LOSS: 5.713897989754529e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 59/71 | LOSS: 5.710312120754679e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 60/71 | LOSS: 5.7044307826889385e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 61/71 | LOSS: 5.694381137976551e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 62/71 | LOSS: 5.674434818040084e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 63/71 | LOSS: 5.679416837267581e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 64/71 | LOSS: 5.66867196399611e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 65/71 | LOSS: 5.6823337286693984e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 66/71 | LOSS: 5.6650902384283096e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 67/71 | LOSS: 5.68545785614226e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 68/71 | LOSS: 5.703266078458198e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 69/71 | LOSS: 5.705939279973141e-06\n",
      "TRAIN: EPOCH 199/1000 | BATCH 70/71 | LOSS: 5.7253012489806206e-06\n",
      "VAL: EPOCH 199/1000 | BATCH 0/8 | LOSS: 6.5595722844591364e-06\n",
      "VAL: EPOCH 199/1000 | BATCH 1/8 | LOSS: 5.771003088739235e-06\n",
      "VAL: EPOCH 199/1000 | BATCH 2/8 | LOSS: 5.806430635857396e-06\n",
      "VAL: EPOCH 199/1000 | BATCH 3/8 | LOSS: 5.879178161194432e-06\n",
      "VAL: EPOCH 199/1000 | BATCH 4/8 | LOSS: 5.958044584986055e-06\n",
      "VAL: EPOCH 199/1000 | BATCH 5/8 | LOSS: 5.684378947989899e-06\n",
      "VAL: EPOCH 199/1000 | BATCH 6/8 | LOSS: 5.590971016187853e-06\n",
      "VAL: EPOCH 199/1000 | BATCH 7/8 | LOSS: 5.5940875540727575e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 0/71 | LOSS: 5.228426743997261e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 1/71 | LOSS: 5.24537563251215e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 2/71 | LOSS: 5.685271692830914e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 3/71 | LOSS: 5.492550371855032e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 4/71 | LOSS: 5.483701716002542e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 5/71 | LOSS: 5.408864581113448e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 6/71 | LOSS: 5.415654608701256e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 7/71 | LOSS: 5.516666476523824e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 8/71 | LOSS: 5.485067908416062e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 9/71 | LOSS: 5.600828762908349e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 10/71 | LOSS: 5.576003910722317e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 11/71 | LOSS: 5.583827601185476e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 12/71 | LOSS: 5.611143443764797e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 13/71 | LOSS: 5.562968778706688e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 14/71 | LOSS: 5.556367250392214e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 15/71 | LOSS: 5.563080719639402e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 16/71 | LOSS: 5.5748929459214735e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 17/71 | LOSS: 5.5755478772173065e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 18/71 | LOSS: 5.605958856329763e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 19/71 | LOSS: 5.604230454991921e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 20/71 | LOSS: 5.6848304579727394e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 21/71 | LOSS: 5.712071842109171e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 22/71 | LOSS: 5.675715091574998e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 23/71 | LOSS: 5.679196116640621e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 24/71 | LOSS: 5.7149227359332144e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 25/71 | LOSS: 5.725161634892664e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 26/71 | LOSS: 5.6994468583003184e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 27/71 | LOSS: 5.6818010014987e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 28/71 | LOSS: 5.689173984388492e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 29/71 | LOSS: 5.659264176453386e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 30/71 | LOSS: 5.638025633684705e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 31/71 | LOSS: 5.653420402040865e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 32/71 | LOSS: 5.646549129089566e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 33/71 | LOSS: 5.6927688092400786e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 34/71 | LOSS: 5.685445947684846e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 35/71 | LOSS: 5.718544305940061e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 36/71 | LOSS: 5.720009883933361e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 37/71 | LOSS: 5.776619529665607e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 38/71 | LOSS: 5.738904805277856e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 39/71 | LOSS: 5.7378545534447765e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 40/71 | LOSS: 5.7116055203794794e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 41/71 | LOSS: 5.69428403824055e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 42/71 | LOSS: 5.6667763813091495e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 43/71 | LOSS: 5.6919352056700685e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 44/71 | LOSS: 5.677075761114894e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 45/71 | LOSS: 5.657353609501075e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 46/71 | LOSS: 5.648813636613108e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 47/71 | LOSS: 5.625591162091344e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 48/71 | LOSS: 5.625770158150437e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 49/71 | LOSS: 5.611889546344173e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 50/71 | LOSS: 5.587864979686713e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 51/71 | LOSS: 5.568908390019855e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 52/71 | LOSS: 5.564035720179266e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 53/71 | LOSS: 5.555004404582347e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 54/71 | LOSS: 5.551592022873757e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 55/71 | LOSS: 5.575477376039219e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 56/71 | LOSS: 5.563579634326743e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 57/71 | LOSS: 5.547069192822352e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 58/71 | LOSS: 5.53406972219325e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 59/71 | LOSS: 5.530682013462259e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 60/71 | LOSS: 5.5271891932400155e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 61/71 | LOSS: 5.5304764329423876e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 62/71 | LOSS: 5.527696221034848e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 63/71 | LOSS: 5.535661713906848e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 64/71 | LOSS: 5.536567130967831e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 65/71 | LOSS: 5.555824510806953e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 66/71 | LOSS: 5.558912107517244e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 67/71 | LOSS: 5.55634518578325e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 68/71 | LOSS: 5.559150144688539e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 69/71 | LOSS: 5.554986630938531e-06\n",
      "TRAIN: EPOCH 200/1000 | BATCH 70/71 | LOSS: 5.556225549836699e-06\n",
      "VAL: EPOCH 200/1000 | BATCH 0/8 | LOSS: 8.12259895610623e-06\n",
      "VAL: EPOCH 200/1000 | BATCH 1/8 | LOSS: 7.136708973121131e-06\n",
      "VAL: EPOCH 200/1000 | BATCH 2/8 | LOSS: 6.809819903234408e-06\n",
      "VAL: EPOCH 200/1000 | BATCH 3/8 | LOSS: 6.902317750245857e-06\n",
      "VAL: EPOCH 200/1000 | BATCH 4/8 | LOSS: 6.926152309461031e-06\n",
      "VAL: EPOCH 200/1000 | BATCH 5/8 | LOSS: 6.648745056736516e-06\n",
      "VAL: EPOCH 200/1000 | BATCH 6/8 | LOSS: 6.685010443366732e-06\n",
      "VAL: EPOCH 200/1000 | BATCH 7/8 | LOSS: 6.672931249340763e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 0/71 | LOSS: 7.456537787220441e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 1/71 | LOSS: 6.926202331669629e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 2/71 | LOSS: 6.565985434766238e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 3/71 | LOSS: 6.27698318567127e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 4/71 | LOSS: 5.942611460341141e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 5/71 | LOSS: 6.083435664550052e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 6/71 | LOSS: 5.819737192983407e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 7/71 | LOSS: 5.794145977233711e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 8/71 | LOSS: 5.969679376802459e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 9/71 | LOSS: 5.882406230739434e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 10/71 | LOSS: 5.95658692955939e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 11/71 | LOSS: 6.034545890543086e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 12/71 | LOSS: 6.153827808492889e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 13/71 | LOSS: 6.1263737864335e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 14/71 | LOSS: 6.084574579290347e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 15/71 | LOSS: 6.0483569370717305e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 16/71 | LOSS: 6.014205583222279e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 17/71 | LOSS: 6.102419269558999e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 18/71 | LOSS: 6.053924727510955e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 19/71 | LOSS: 6.0488696817628805e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 20/71 | LOSS: 6.255922579882844e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 21/71 | LOSS: 6.244238193175988e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 22/71 | LOSS: 6.3022662288858555e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 23/71 | LOSS: 6.338817816716376e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 24/71 | LOSS: 6.365268582158024e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 25/71 | LOSS: 6.305595346790282e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 26/71 | LOSS: 6.317627274391405e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 27/71 | LOSS: 6.31599315121483e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 28/71 | LOSS: 6.277593629106534e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 29/71 | LOSS: 6.32446285635524e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 30/71 | LOSS: 6.2901062847231515e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 31/71 | LOSS: 6.291410301173528e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 32/71 | LOSS: 6.272637206579517e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 33/71 | LOSS: 6.2553926106963495e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 34/71 | LOSS: 6.22048389362005e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 35/71 | LOSS: 6.181757347197466e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 36/71 | LOSS: 6.175169358824418e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 37/71 | LOSS: 6.128842055898674e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 38/71 | LOSS: 6.116496642323163e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 39/71 | LOSS: 6.065684260647686e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 40/71 | LOSS: 6.086329127666853e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 41/71 | LOSS: 6.041664760015833e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 42/71 | LOSS: 5.987981287331041e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 43/71 | LOSS: 5.9892972837422516e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 44/71 | LOSS: 5.965096660373901e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 45/71 | LOSS: 5.944020996218529e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 46/71 | LOSS: 5.956634048685528e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 47/71 | LOSS: 5.972751632536226e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 48/71 | LOSS: 5.973709175104515e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 49/71 | LOSS: 5.942515276728954e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 50/71 | LOSS: 5.9653758514697054e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 51/71 | LOSS: 5.971140191937034e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 52/71 | LOSS: 5.965011706513974e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 53/71 | LOSS: 5.93173238198856e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 54/71 | LOSS: 5.95301592676895e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 55/71 | LOSS: 5.945490162925385e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 56/71 | LOSS: 5.9357212393750385e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 57/71 | LOSS: 5.958622307781329e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 58/71 | LOSS: 5.956532119190512e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 59/71 | LOSS: 5.960855382151445e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 60/71 | LOSS: 5.969436601770242e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 61/71 | LOSS: 5.968305308376711e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 62/71 | LOSS: 5.993658395897318e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 63/71 | LOSS: 5.962516379298677e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 64/71 | LOSS: 6.000972808368925e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 65/71 | LOSS: 5.974215083149444e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 66/71 | LOSS: 5.961379710880912e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 67/71 | LOSS: 5.953880735673652e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 68/71 | LOSS: 5.973713511949704e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 69/71 | LOSS: 5.969716010310679e-06\n",
      "TRAIN: EPOCH 201/1000 | BATCH 70/71 | LOSS: 5.970293927994519e-06\n",
      "VAL: EPOCH 201/1000 | BATCH 0/8 | LOSS: 9.673055501480121e-06\n",
      "VAL: EPOCH 201/1000 | BATCH 1/8 | LOSS: 8.61780881677987e-06\n",
      "VAL: EPOCH 201/1000 | BATCH 2/8 | LOSS: 8.164313840097748e-06\n",
      "VAL: EPOCH 201/1000 | BATCH 3/8 | LOSS: 8.295424322568579e-06\n",
      "VAL: EPOCH 201/1000 | BATCH 4/8 | LOSS: 8.290728510473855e-06\n",
      "VAL: EPOCH 201/1000 | BATCH 5/8 | LOSS: 7.895509043009952e-06\n",
      "VAL: EPOCH 201/1000 | BATCH 6/8 | LOSS: 8.012713806238025e-06\n",
      "VAL: EPOCH 201/1000 | BATCH 7/8 | LOSS: 7.91838937175271e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 0/71 | LOSS: 7.736662155366503e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 1/71 | LOSS: 6.248619683901779e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 2/71 | LOSS: 6.591226489642092e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 3/71 | LOSS: 6.570523964910535e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 4/71 | LOSS: 6.229984319361393e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 5/71 | LOSS: 6.087999357381098e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 6/71 | LOSS: 5.920782639025544e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 7/71 | LOSS: 6.0014781979589316e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 8/71 | LOSS: 5.862409883169069e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 9/71 | LOSS: 5.837854541823617e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 10/71 | LOSS: 5.960248059140709e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 11/71 | LOSS: 5.966688263470132e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 12/71 | LOSS: 6.001125641957445e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 13/71 | LOSS: 5.890932860503588e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 14/71 | LOSS: 5.932584069038664e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 15/71 | LOSS: 5.883884227841918e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 16/71 | LOSS: 5.8781370706342656e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 17/71 | LOSS: 5.847528478221244e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 18/71 | LOSS: 5.88816845648628e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 19/71 | LOSS: 5.906646447328967e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 20/71 | LOSS: 5.857353504065291e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 21/71 | LOSS: 5.787867890334879e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 22/71 | LOSS: 5.830794179928489e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 23/71 | LOSS: 5.849165688687208e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 24/71 | LOSS: 5.829257261211751e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 25/71 | LOSS: 5.81980284900615e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 26/71 | LOSS: 5.85785547055248e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 27/71 | LOSS: 5.864512867706903e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 28/71 | LOSS: 5.839558854921929e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 29/71 | LOSS: 5.909292030992219e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 30/71 | LOSS: 5.840997602952543e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 31/71 | LOSS: 5.90199795880153e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 32/71 | LOSS: 5.878283452671674e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 33/71 | LOSS: 5.889210977811137e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 34/71 | LOSS: 5.894212316499242e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 35/71 | LOSS: 6.021697023091595e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 36/71 | LOSS: 6.017680770791894e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 37/71 | LOSS: 6.0201886071808985e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 38/71 | LOSS: 5.973538143221865e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 39/71 | LOSS: 5.993017106220578e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 40/71 | LOSS: 5.982467766399368e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 41/71 | LOSS: 6.0088790730463056e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 42/71 | LOSS: 6.032891829014515e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 43/71 | LOSS: 6.012630773387752e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 44/71 | LOSS: 6.0822927834023076e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 45/71 | LOSS: 6.06023070304218e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 46/71 | LOSS: 6.090455841292575e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 47/71 | LOSS: 6.098824708071031e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 48/71 | LOSS: 6.086496550407753e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 49/71 | LOSS: 6.0934433849979545e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 50/71 | LOSS: 6.081456300914094e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 51/71 | LOSS: 6.055428085661473e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 52/71 | LOSS: 6.079877189308117e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 53/71 | LOSS: 6.083931436006861e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 54/71 | LOSS: 6.052702292436152e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 55/71 | LOSS: 6.100984563772727e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 56/71 | LOSS: 6.102572039405156e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 57/71 | LOSS: 6.111238647086981e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 58/71 | LOSS: 6.1328021505903315e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 59/71 | LOSS: 6.178639178718489e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 60/71 | LOSS: 6.147810355570505e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 61/71 | LOSS: 6.180318586478778e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 62/71 | LOSS: 6.18418829660749e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 63/71 | LOSS: 6.191362235341558e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 64/71 | LOSS: 6.186002264467579e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 65/71 | LOSS: 6.222542710378493e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 66/71 | LOSS: 6.2165134926572e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 67/71 | LOSS: 6.198793073508568e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 68/71 | LOSS: 6.181243080667944e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 69/71 | LOSS: 6.2098295692132e-06\n",
      "TRAIN: EPOCH 202/1000 | BATCH 70/71 | LOSS: 6.2261911122258306e-06\n",
      "VAL: EPOCH 202/1000 | BATCH 0/8 | LOSS: 6.255480911931954e-06\n",
      "VAL: EPOCH 202/1000 | BATCH 1/8 | LOSS: 5.744095233239932e-06\n",
      "VAL: EPOCH 202/1000 | BATCH 2/8 | LOSS: 5.4763413572800346e-06\n",
      "VAL: EPOCH 202/1000 | BATCH 3/8 | LOSS: 6.076649697206449e-06\n",
      "VAL: EPOCH 202/1000 | BATCH 4/8 | LOSS: 5.97177458985243e-06\n",
      "VAL: EPOCH 202/1000 | BATCH 5/8 | LOSS: 5.763173248851672e-06\n",
      "VAL: EPOCH 202/1000 | BATCH 6/8 | LOSS: 5.606895748704639e-06\n",
      "VAL: EPOCH 202/1000 | BATCH 7/8 | LOSS: 5.51669961623702e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 0/71 | LOSS: 4.875464128417661e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 1/71 | LOSS: 5.959855798209901e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 2/71 | LOSS: 6.043994593104192e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 3/71 | LOSS: 6.042073664502823e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 4/71 | LOSS: 6.022538400429767e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 5/71 | LOSS: 5.962967103793441e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 6/71 | LOSS: 5.791785237566468e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 7/71 | LOSS: 5.623874585580779e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 8/71 | LOSS: 5.888400361679184e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 9/71 | LOSS: 5.910530126129742e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 10/71 | LOSS: 5.887250402090351e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 11/71 | LOSS: 5.889550190355901e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 12/71 | LOSS: 5.872630516443258e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 13/71 | LOSS: 5.8233819605188075e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 14/71 | LOSS: 5.774727924290346e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 15/71 | LOSS: 5.722534467622609e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 16/71 | LOSS: 5.738614591417204e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 17/71 | LOSS: 5.692795967762423e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 18/71 | LOSS: 5.672030870061319e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 19/71 | LOSS: 5.60534183478012e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 20/71 | LOSS: 5.613229534771692e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 21/71 | LOSS: 5.598727346461436e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 22/71 | LOSS: 5.610637740376304e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 23/71 | LOSS: 5.575175218079191e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 24/71 | LOSS: 5.559883265959797e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 25/71 | LOSS: 5.534547863834842e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 26/71 | LOSS: 5.515870051803412e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 27/71 | LOSS: 5.492738150028994e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 28/71 | LOSS: 5.451409598553537e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 29/71 | LOSS: 5.453822662578508e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 30/71 | LOSS: 5.443167948772409e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 31/71 | LOSS: 5.538719165087969e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 32/71 | LOSS: 5.5270505926631755e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 33/71 | LOSS: 5.571467762124752e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 34/71 | LOSS: 5.576454649209544e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 35/71 | LOSS: 5.631990815244434e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 36/71 | LOSS: 5.610109272655225e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 37/71 | LOSS: 5.609897279531364e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 38/71 | LOSS: 5.6180095690741346e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 39/71 | LOSS: 5.6036612591015e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 40/71 | LOSS: 5.5888028949017855e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 41/71 | LOSS: 5.552899578798956e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 42/71 | LOSS: 5.549048108700528e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 43/71 | LOSS: 5.541091974753892e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 44/71 | LOSS: 5.528672560709917e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 45/71 | LOSS: 5.564060806453041e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 46/71 | LOSS: 5.57142218778918e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 47/71 | LOSS: 5.605106584501603e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 48/71 | LOSS: 5.6265498085626955e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 49/71 | LOSS: 5.642554133373778e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 50/71 | LOSS: 5.630649875278252e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 51/71 | LOSS: 5.63049874157374e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 52/71 | LOSS: 5.620933090952194e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 53/71 | LOSS: 5.600144715100652e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 54/71 | LOSS: 5.641979714710032e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 55/71 | LOSS: 5.609148956864374e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 56/71 | LOSS: 5.605972993387759e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 57/71 | LOSS: 5.595049640697684e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 58/71 | LOSS: 5.595352561967803e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 59/71 | LOSS: 5.58745431741651e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 60/71 | LOSS: 5.57460669101527e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 61/71 | LOSS: 5.555704649762653e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 62/71 | LOSS: 5.540530504991434e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 63/71 | LOSS: 5.5330995536451155e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 64/71 | LOSS: 5.526431582089012e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 65/71 | LOSS: 5.510068011503653e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 66/71 | LOSS: 5.496893697739717e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 67/71 | LOSS: 5.50841611905472e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 68/71 | LOSS: 5.522849317893143e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 69/71 | LOSS: 5.536343431750928e-06\n",
      "TRAIN: EPOCH 203/1000 | BATCH 70/71 | LOSS: 5.51979818622603e-06\n",
      "VAL: EPOCH 203/1000 | BATCH 0/8 | LOSS: 1.1370894753781613e-05\n",
      "VAL: EPOCH 203/1000 | BATCH 1/8 | LOSS: 1.054867743732757e-05\n",
      "VAL: EPOCH 203/1000 | BATCH 2/8 | LOSS: 9.698563189886045e-06\n",
      "VAL: EPOCH 203/1000 | BATCH 3/8 | LOSS: 1.0043624342870316e-05\n",
      "VAL: EPOCH 203/1000 | BATCH 4/8 | LOSS: 9.856131873675622e-06\n",
      "VAL: EPOCH 203/1000 | BATCH 5/8 | LOSS: 9.382290196905766e-06\n",
      "VAL: EPOCH 203/1000 | BATCH 6/8 | LOSS: 9.448895980312955e-06\n",
      "VAL: EPOCH 203/1000 | BATCH 7/8 | LOSS: 9.267943255508726e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 0/71 | LOSS: 7.788607945258263e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 1/71 | LOSS: 7.276193173311185e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 2/71 | LOSS: 7.46099552391873e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 3/71 | LOSS: 7.416683047267725e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 4/71 | LOSS: 7.302450103452429e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 5/71 | LOSS: 7.410485826161069e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 6/71 | LOSS: 7.212465659124843e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 7/71 | LOSS: 7.0151932050066534e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 8/71 | LOSS: 6.83977264594028e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 9/71 | LOSS: 6.7453098381520246e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 10/71 | LOSS: 6.513645323800367e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 11/71 | LOSS: 6.467894574295012e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 12/71 | LOSS: 6.388223342722855e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 13/71 | LOSS: 6.2756012734358336e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 14/71 | LOSS: 6.261339391736935e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 15/71 | LOSS: 6.170525551851824e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 16/71 | LOSS: 6.1267292039701715e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 17/71 | LOSS: 6.015521446695655e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 18/71 | LOSS: 5.953500237266876e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 19/71 | LOSS: 5.9915304518654015e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 20/71 | LOSS: 5.984916264250864e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 21/71 | LOSS: 5.963523459616805e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 22/71 | LOSS: 5.926693832051292e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 23/71 | LOSS: 5.886279116869749e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 24/71 | LOSS: 5.882211989955977e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 25/71 | LOSS: 5.8340403064413786e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 26/71 | LOSS: 5.813794949197607e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 27/71 | LOSS: 5.809918385628927e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 28/71 | LOSS: 5.807138442268579e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 29/71 | LOSS: 5.801398234931791e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 30/71 | LOSS: 5.783534356117842e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 31/71 | LOSS: 5.74772614925223e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 32/71 | LOSS: 5.73662144674789e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 33/71 | LOSS: 5.72883607888156e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 34/71 | LOSS: 5.715029874409083e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 35/71 | LOSS: 5.68467740120266e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 36/71 | LOSS: 5.654705607928524e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 37/71 | LOSS: 5.621728656766017e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 38/71 | LOSS: 5.61078193128103e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 39/71 | LOSS: 5.6084037964865274e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 40/71 | LOSS: 5.603078789579563e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 41/71 | LOSS: 5.623045418750345e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 42/71 | LOSS: 5.6569159252915615e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 43/71 | LOSS: 5.6396201505942205e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 44/71 | LOSS: 5.614182555291336e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 45/71 | LOSS: 5.613194349754612e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 46/71 | LOSS: 5.5962536475333845e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 47/71 | LOSS: 5.591802685482132e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 48/71 | LOSS: 5.559999770374983e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 49/71 | LOSS: 5.574076740231249e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 50/71 | LOSS: 5.5492005765914015e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 51/71 | LOSS: 5.530601662520405e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 52/71 | LOSS: 5.514834304809689e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 53/71 | LOSS: 5.497625327027183e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 54/71 | LOSS: 5.516257665889994e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 55/71 | LOSS: 5.539415586294386e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 56/71 | LOSS: 5.533511803310319e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 57/71 | LOSS: 5.525748551447358e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 58/71 | LOSS: 5.569814312998383e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 59/71 | LOSS: 5.535248033083917e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 60/71 | LOSS: 5.552229065899871e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 61/71 | LOSS: 5.530344856930652e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 62/71 | LOSS: 5.56052337121819e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 63/71 | LOSS: 5.5648104932970455e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 64/71 | LOSS: 5.57849924427081e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 65/71 | LOSS: 5.566674217609971e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 66/71 | LOSS: 5.580156458061321e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 67/71 | LOSS: 5.5604384068133455e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 68/71 | LOSS: 5.573212086122679e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 69/71 | LOSS: 5.567358831675457e-06\n",
      "TRAIN: EPOCH 204/1000 | BATCH 70/71 | LOSS: 5.5512681504130255e-06\n",
      "VAL: EPOCH 204/1000 | BATCH 0/8 | LOSS: 7.24415440345183e-06\n",
      "VAL: EPOCH 204/1000 | BATCH 1/8 | LOSS: 6.515781251437147e-06\n",
      "VAL: EPOCH 204/1000 | BATCH 2/8 | LOSS: 5.948492647197175e-06\n",
      "VAL: EPOCH 204/1000 | BATCH 3/8 | LOSS: 6.242246286092268e-06\n",
      "VAL: EPOCH 204/1000 | BATCH 4/8 | LOSS: 6.128429504315136e-06\n",
      "VAL: EPOCH 204/1000 | BATCH 5/8 | LOSS: 5.782659097045932e-06\n",
      "VAL: EPOCH 204/1000 | BATCH 6/8 | LOSS: 5.772643687253419e-06\n",
      "VAL: EPOCH 204/1000 | BATCH 7/8 | LOSS: 5.694215587936924e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 0/71 | LOSS: 6.2213148339651525e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 1/71 | LOSS: 5.5788254940125626e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 2/71 | LOSS: 5.2481864258879796e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 3/71 | LOSS: 5.082531629341247e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 4/71 | LOSS: 5.288414013193688e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 5/71 | LOSS: 5.189055400478537e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 6/71 | LOSS: 5.11582636266082e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 7/71 | LOSS: 5.140156872585067e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 8/71 | LOSS: 5.2545261092341716e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 9/71 | LOSS: 5.18065949108859e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 10/71 | LOSS: 5.168904036426366e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 11/71 | LOSS: 5.1457701223019585e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 12/71 | LOSS: 5.118432804570498e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 13/71 | LOSS: 5.105724929827764e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 14/71 | LOSS: 5.036645355479171e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 15/71 | LOSS: 5.0901085160148796e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 16/71 | LOSS: 5.065120175833513e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 17/71 | LOSS: 5.100634350835915e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 18/71 | LOSS: 5.130060440263294e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 19/71 | LOSS: 5.130987028678647e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 20/71 | LOSS: 5.171246075839181e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 21/71 | LOSS: 5.211732886064882e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 22/71 | LOSS: 5.22417424334613e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 23/71 | LOSS: 5.201891800273491e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 24/71 | LOSS: 5.192710668779909e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 25/71 | LOSS: 5.168819842975398e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 26/71 | LOSS: 5.215241479298264e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 27/71 | LOSS: 5.238954518712749e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 28/71 | LOSS: 5.304445996193079e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 29/71 | LOSS: 5.287325378352155e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 30/71 | LOSS: 5.310809096807779e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 31/71 | LOSS: 5.341241518408424e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 32/71 | LOSS: 5.351406371394687e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 33/71 | LOSS: 5.357628598336733e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 34/71 | LOSS: 5.379690628615208e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 35/71 | LOSS: 5.398078807653898e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 36/71 | LOSS: 5.362203766609939e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 37/71 | LOSS: 5.38632207280898e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 38/71 | LOSS: 5.41018388479969e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 39/71 | LOSS: 5.403526392910862e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 40/71 | LOSS: 5.4023720866364494e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 41/71 | LOSS: 5.383726818850153e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 42/71 | LOSS: 5.371441824705513e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 43/71 | LOSS: 5.3561337526017185e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 44/71 | LOSS: 5.338435221347026e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 45/71 | LOSS: 5.334179772746764e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 46/71 | LOSS: 5.321403855208352e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 47/71 | LOSS: 5.3149094393726655e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 48/71 | LOSS: 5.320052878232673e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 49/71 | LOSS: 5.304544301907299e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 50/71 | LOSS: 5.3017580356419215e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 51/71 | LOSS: 5.297257950167897e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 52/71 | LOSS: 5.297083932744945e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 53/71 | LOSS: 5.31403493705848e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 54/71 | LOSS: 5.304893685123799e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 55/71 | LOSS: 5.318467206702085e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 56/71 | LOSS: 5.318338799840779e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 57/71 | LOSS: 5.310240512385619e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 58/71 | LOSS: 5.324311147841538e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 59/71 | LOSS: 5.302919877673654e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 60/71 | LOSS: 5.318932589496578e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 61/71 | LOSS: 5.326075368781946e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 62/71 | LOSS: 5.325962868641549e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 63/71 | LOSS: 5.314638549691608e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 64/71 | LOSS: 5.3271014551753e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 65/71 | LOSS: 5.317302173518163e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 66/71 | LOSS: 5.307566880219383e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 67/71 | LOSS: 5.319797189847176e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 68/71 | LOSS: 5.300145383681292e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 69/71 | LOSS: 5.30234050789399e-06\n",
      "TRAIN: EPOCH 205/1000 | BATCH 70/71 | LOSS: 5.303596814153191e-06\n",
      "VAL: EPOCH 205/1000 | BATCH 0/8 | LOSS: 6.468824267358286e-06\n",
      "VAL: EPOCH 205/1000 | BATCH 1/8 | LOSS: 5.661125442202319e-06\n",
      "VAL: EPOCH 205/1000 | BATCH 2/8 | LOSS: 5.6135224137202995e-06\n",
      "VAL: EPOCH 205/1000 | BATCH 3/8 | LOSS: 5.930306656409812e-06\n",
      "VAL: EPOCH 205/1000 | BATCH 4/8 | LOSS: 5.881499600945972e-06\n",
      "VAL: EPOCH 205/1000 | BATCH 5/8 | LOSS: 5.844467523274943e-06\n",
      "VAL: EPOCH 205/1000 | BATCH 6/8 | LOSS: 5.800941900296104e-06\n",
      "VAL: EPOCH 205/1000 | BATCH 7/8 | LOSS: 5.765180333128228e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 0/71 | LOSS: 5.220006642048247e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 1/71 | LOSS: 6.040349035174586e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 2/71 | LOSS: 5.700085542533391e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 3/71 | LOSS: 5.6619821862113895e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 4/71 | LOSS: 5.8032070228364315e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 5/71 | LOSS: 5.8514152290930115e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 6/71 | LOSS: 5.6746945509595596e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 7/71 | LOSS: 5.5022895253387105e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 8/71 | LOSS: 5.381819391914178e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 9/71 | LOSS: 5.288116926749353e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 10/71 | LOSS: 5.339581755172483e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 11/71 | LOSS: 5.4698840585842845e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 12/71 | LOSS: 5.439275845184546e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 13/71 | LOSS: 5.495665228798836e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 14/71 | LOSS: 5.593337997803853e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 15/71 | LOSS: 5.61367787099698e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 16/71 | LOSS: 5.622261329228743e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 17/71 | LOSS: 5.568586958462321e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 18/71 | LOSS: 5.6980988072035345e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 19/71 | LOSS: 5.758941551903263e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 20/71 | LOSS: 5.749948849413722e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 21/71 | LOSS: 5.794281729653118e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 22/71 | LOSS: 5.803388850598941e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 23/71 | LOSS: 5.764615555866233e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 24/71 | LOSS: 5.8564615392242555e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 25/71 | LOSS: 5.845322378333479e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 26/71 | LOSS: 5.925358891545329e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 27/71 | LOSS: 5.870940997608289e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 28/71 | LOSS: 5.9387822806582275e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 29/71 | LOSS: 5.9094114931212974e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 30/71 | LOSS: 5.919557503558838e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 31/71 | LOSS: 5.92498312812495e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 32/71 | LOSS: 5.982326387531815e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 33/71 | LOSS: 5.981771397166406e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 34/71 | LOSS: 5.99096673405646e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 35/71 | LOSS: 5.954308474227017e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 36/71 | LOSS: 5.930869335586337e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 37/71 | LOSS: 5.904321190093087e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 38/71 | LOSS: 5.893264980654632e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 39/71 | LOSS: 5.903029546061589e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 40/71 | LOSS: 5.867078659609499e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 41/71 | LOSS: 5.883305913725746e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 42/71 | LOSS: 5.869079158464744e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 43/71 | LOSS: 5.8725572671392e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 44/71 | LOSS: 5.8535943026072346e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 45/71 | LOSS: 5.870676797870977e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 46/71 | LOSS: 5.8608459747777854e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 47/71 | LOSS: 5.874203585184053e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 48/71 | LOSS: 5.87254629751943e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 49/71 | LOSS: 5.876846353203291e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 50/71 | LOSS: 5.898949635308122e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 51/71 | LOSS: 5.882247076162737e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 52/71 | LOSS: 5.8788457194305865e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 53/71 | LOSS: 5.852359023534788e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 54/71 | LOSS: 5.830020596807696e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 55/71 | LOSS: 5.8568513980260055e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 56/71 | LOSS: 5.881528921778673e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 57/71 | LOSS: 5.881380996564483e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 58/71 | LOSS: 5.875909495043126e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 59/71 | LOSS: 5.888117175345542e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 60/71 | LOSS: 5.8745243401718935e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 61/71 | LOSS: 5.876882002461747e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 62/71 | LOSS: 5.8607567321843584e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 63/71 | LOSS: 5.869582793138761e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 64/71 | LOSS: 5.87345043291302e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 65/71 | LOSS: 5.889826339129782e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 66/71 | LOSS: 5.8772694542109205e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 67/71 | LOSS: 5.8761231818660985e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 68/71 | LOSS: 5.882724047072427e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 69/71 | LOSS: 5.8869760518324e-06\n",
      "TRAIN: EPOCH 206/1000 | BATCH 70/71 | LOSS: 5.8690777773912306e-06\n",
      "VAL: EPOCH 206/1000 | BATCH 0/8 | LOSS: 6.224317075975705e-06\n",
      "VAL: EPOCH 206/1000 | BATCH 1/8 | LOSS: 5.617052920570131e-06\n",
      "VAL: EPOCH 206/1000 | BATCH 2/8 | LOSS: 5.3515041145146824e-06\n",
      "VAL: EPOCH 206/1000 | BATCH 3/8 | LOSS: 5.820864316774532e-06\n",
      "VAL: EPOCH 206/1000 | BATCH 4/8 | LOSS: 5.695541585737374e-06\n",
      "VAL: EPOCH 206/1000 | BATCH 5/8 | LOSS: 5.60088005840953e-06\n",
      "VAL: EPOCH 206/1000 | BATCH 6/8 | LOSS: 5.523598117308991e-06\n",
      "VAL: EPOCH 206/1000 | BATCH 7/8 | LOSS: 5.4280629342429165e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 0/71 | LOSS: 4.6327113523148e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 1/71 | LOSS: 6.119269073678879e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 2/71 | LOSS: 5.718071709755653e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 3/71 | LOSS: 5.568079018303251e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 4/71 | LOSS: 6.412252059817547e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 5/71 | LOSS: 6.297269616576766e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 6/71 | LOSS: 7.005302255233151e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 7/71 | LOSS: 7.0217354846136e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 8/71 | LOSS: 7.377728707069764e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 9/71 | LOSS: 7.2920753609651005e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 10/71 | LOSS: 7.327545517910039e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 11/71 | LOSS: 7.073785089536007e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 12/71 | LOSS: 7.0012625446207285e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 13/71 | LOSS: 7.126950322344783e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 14/71 | LOSS: 7.025386033395383e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 15/71 | LOSS: 6.9062019179000345e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 16/71 | LOSS: 6.944266180439359e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 17/71 | LOSS: 7.1736403192011894e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 18/71 | LOSS: 7.086915047693765e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 19/71 | LOSS: 7.10390897893376e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 20/71 | LOSS: 7.181338571085473e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 21/71 | LOSS: 7.187633834539818e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 22/71 | LOSS: 7.186057068200013e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 23/71 | LOSS: 7.133158798448373e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 24/71 | LOSS: 7.1434598976338746e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 25/71 | LOSS: 7.105842367029534e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 26/71 | LOSS: 7.070586047188111e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 27/71 | LOSS: 7.088269334027635e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 28/71 | LOSS: 7.019570759815718e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 29/71 | LOSS: 6.994237158626977e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 30/71 | LOSS: 7.005796071039626e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 31/71 | LOSS: 6.976583634354938e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 32/71 | LOSS: 6.929177247498459e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 33/71 | LOSS: 6.963779803888482e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 34/71 | LOSS: 6.928684160811826e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 35/71 | LOSS: 6.916952720025114e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 36/71 | LOSS: 6.892819660483715e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 37/71 | LOSS: 6.860933114550885e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 38/71 | LOSS: 6.8047043103620244e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 39/71 | LOSS: 6.772686765543767e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 40/71 | LOSS: 6.730423792316465e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 41/71 | LOSS: 6.712037247780245e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 42/71 | LOSS: 6.706696716042474e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 43/71 | LOSS: 6.6873987572597056e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 44/71 | LOSS: 6.658997305445761e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 45/71 | LOSS: 6.606345194719008e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 46/71 | LOSS: 6.588717574328605e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 47/71 | LOSS: 6.584439991759912e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 48/71 | LOSS: 6.572111293020758e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 49/71 | LOSS: 6.550553198394482e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 50/71 | LOSS: 6.545359673170834e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 51/71 | LOSS: 6.52801663118925e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 52/71 | LOSS: 6.511432352368193e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 53/71 | LOSS: 6.483632820096251e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 54/71 | LOSS: 6.4502628695415545e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 55/71 | LOSS: 6.468377884110461e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 56/71 | LOSS: 6.472364323902487e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 57/71 | LOSS: 6.510517629255352e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 58/71 | LOSS: 6.496990493560897e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 59/71 | LOSS: 6.519914601691805e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 60/71 | LOSS: 6.510952688153875e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 61/71 | LOSS: 6.521941530433739e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 62/71 | LOSS: 6.500214495313158e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 63/71 | LOSS: 6.49549546238859e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 64/71 | LOSS: 6.529008582079909e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 65/71 | LOSS: 6.512636156747414e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 66/71 | LOSS: 6.565299033232506e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 67/71 | LOSS: 6.547380093359286e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 68/71 | LOSS: 6.590709942785274e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 69/71 | LOSS: 6.584813192083467e-06\n",
      "TRAIN: EPOCH 207/1000 | BATCH 70/71 | LOSS: 6.681923660438206e-06\n",
      "VAL: EPOCH 207/1000 | BATCH 0/8 | LOSS: 6.3128845795290545e-06\n",
      "VAL: EPOCH 207/1000 | BATCH 1/8 | LOSS: 5.513285941560753e-06\n",
      "VAL: EPOCH 207/1000 | BATCH 2/8 | LOSS: 5.36282489823255e-06\n",
      "VAL: EPOCH 207/1000 | BATCH 3/8 | LOSS: 5.670586460837512e-06\n",
      "VAL: EPOCH 207/1000 | BATCH 4/8 | LOSS: 5.593103742285166e-06\n",
      "VAL: EPOCH 207/1000 | BATCH 5/8 | LOSS: 5.3938385159805575e-06\n",
      "VAL: EPOCH 207/1000 | BATCH 6/8 | LOSS: 5.284594603186374e-06\n",
      "VAL: EPOCH 207/1000 | BATCH 7/8 | LOSS: 5.265744221105706e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 0/71 | LOSS: 4.493202595767798e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 1/71 | LOSS: 7.0744974891567836e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 2/71 | LOSS: 6.589067349220083e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 3/71 | LOSS: 6.755796789548185e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 4/71 | LOSS: 6.627917082369095e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 5/71 | LOSS: 6.405686311457733e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 6/71 | LOSS: 6.23867013668392e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 7/71 | LOSS: 6.094164689329773e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 8/71 | LOSS: 5.985878740628121e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 9/71 | LOSS: 5.9168542975385204e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 10/71 | LOSS: 5.875770015832544e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 11/71 | LOSS: 5.948839998382027e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 12/71 | LOSS: 5.958793560869079e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 13/71 | LOSS: 6.133403368429364e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 14/71 | LOSS: 6.039013927268873e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 15/71 | LOSS: 6.098316106317725e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 16/71 | LOSS: 6.040598183223183e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 17/71 | LOSS: 6.074118143967806e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 18/71 | LOSS: 5.988206220009536e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 19/71 | LOSS: 5.9335341575206256e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 20/71 | LOSS: 5.885585064139118e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 21/71 | LOSS: 5.866555718305542e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 22/71 | LOSS: 5.81731989729198e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 23/71 | LOSS: 5.869729667059194e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 24/71 | LOSS: 5.858714630448958e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 25/71 | LOSS: 5.9246928265985425e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 26/71 | LOSS: 5.9115073714498e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 27/71 | LOSS: 6.0327247898125535e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 28/71 | LOSS: 6.002667305438081e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 29/71 | LOSS: 6.046833398916837e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 30/71 | LOSS: 5.999993141637289e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 31/71 | LOSS: 6.06435375516412e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 32/71 | LOSS: 6.036667400621809e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 33/71 | LOSS: 6.020412242565619e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 34/71 | LOSS: 6.065611744686196e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 35/71 | LOSS: 6.098647910827519e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 36/71 | LOSS: 6.0572965963188295e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 37/71 | LOSS: 6.03125383684911e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 38/71 | LOSS: 5.999438902411687e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 39/71 | LOSS: 5.995339108721964e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 40/71 | LOSS: 5.939367954896425e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 41/71 | LOSS: 5.918830933773106e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 42/71 | LOSS: 5.910229907216447e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 43/71 | LOSS: 5.877507579622364e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 44/71 | LOSS: 5.863279607688633e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 45/71 | LOSS: 5.863087751648614e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 46/71 | LOSS: 5.850606636700599e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 47/71 | LOSS: 5.843809679125418e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 48/71 | LOSS: 5.838605945928994e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 49/71 | LOSS: 5.81946841066383e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 50/71 | LOSS: 5.783962010622524e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 51/71 | LOSS: 5.802202268184304e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 52/71 | LOSS: 5.807308105191486e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 53/71 | LOSS: 5.818540595707115e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 54/71 | LOSS: 5.788370009957527e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 55/71 | LOSS: 5.772841888074254e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 56/71 | LOSS: 5.787019238580302e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 57/71 | LOSS: 5.814563153229568e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 58/71 | LOSS: 5.803694355532663e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 59/71 | LOSS: 5.822082065757664e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 60/71 | LOSS: 5.824040626379667e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 61/71 | LOSS: 5.836307078395968e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 62/71 | LOSS: 5.835311925231566e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 63/71 | LOSS: 5.8484067686492835e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 64/71 | LOSS: 5.8226706816920065e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 65/71 | LOSS: 5.813704192267764e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 66/71 | LOSS: 5.8148352594282945e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 67/71 | LOSS: 5.829359194569192e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 68/71 | LOSS: 5.824414928070652e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 69/71 | LOSS: 5.810330740782124e-06\n",
      "TRAIN: EPOCH 208/1000 | BATCH 70/71 | LOSS: 5.797584432438155e-06\n",
      "VAL: EPOCH 208/1000 | BATCH 0/8 | LOSS: 6.874586688354611e-06\n",
      "VAL: EPOCH 208/1000 | BATCH 1/8 | LOSS: 6.022612524247961e-06\n",
      "VAL: EPOCH 208/1000 | BATCH 2/8 | LOSS: 5.830712931735131e-06\n",
      "VAL: EPOCH 208/1000 | BATCH 3/8 | LOSS: 6.31034799880581e-06\n",
      "VAL: EPOCH 208/1000 | BATCH 4/8 | LOSS: 6.131461486802437e-06\n",
      "VAL: EPOCH 208/1000 | BATCH 5/8 | LOSS: 6.0091288105468266e-06\n",
      "VAL: EPOCH 208/1000 | BATCH 6/8 | LOSS: 5.877892622915429e-06\n",
      "VAL: EPOCH 208/1000 | BATCH 7/8 | LOSS: 5.827223560572747e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 0/71 | LOSS: 5.550738933379762e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 1/71 | LOSS: 5.029371777709457e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 2/71 | LOSS: 6.288457825576188e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 3/71 | LOSS: 6.0988040786469355e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 4/71 | LOSS: 5.978776516712969e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 5/71 | LOSS: 5.973573252049391e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 6/71 | LOSS: 5.849737720252181e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 7/71 | LOSS: 5.757908638770459e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 8/71 | LOSS: 5.690837345658413e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 9/71 | LOSS: 5.7587771607359174e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 10/71 | LOSS: 5.644603375416906e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 11/71 | LOSS: 5.581033860835305e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 12/71 | LOSS: 5.535296867702317e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 13/71 | LOSS: 5.614176773503589e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 14/71 | LOSS: 5.577490931803671e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 15/71 | LOSS: 5.629937589901601e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 16/71 | LOSS: 5.67137505961856e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 17/71 | LOSS: 5.658563825111034e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 18/71 | LOSS: 5.71845650579474e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 19/71 | LOSS: 5.696295511370409e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 20/71 | LOSS: 5.738849722547457e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 21/71 | LOSS: 5.699750297274229e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 22/71 | LOSS: 5.797450324715338e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 23/71 | LOSS: 5.790088816108134e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 24/71 | LOSS: 5.7604271933087145e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 25/71 | LOSS: 5.779826743981595e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 26/71 | LOSS: 5.77795049981365e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 27/71 | LOSS: 5.817547958031355e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 28/71 | LOSS: 5.807181643266913e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 29/71 | LOSS: 5.7955179575704585e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 30/71 | LOSS: 5.847697755070253e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 31/71 | LOSS: 5.898642598367587e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 32/71 | LOSS: 5.9716970402708585e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 33/71 | LOSS: 5.9624845856459985e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 34/71 | LOSS: 6.008947886065081e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 35/71 | LOSS: 5.973881469698325e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 36/71 | LOSS: 5.921563840545147e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 37/71 | LOSS: 5.925616350348144e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 38/71 | LOSS: 5.932592872480969e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 39/71 | LOSS: 5.911444316097914e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 40/71 | LOSS: 5.924033214286305e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 41/71 | LOSS: 5.929333101881812e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 42/71 | LOSS: 5.921871021666109e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 43/71 | LOSS: 5.9401667158454075e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 44/71 | LOSS: 5.9225523829000954e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 45/71 | LOSS: 5.92484913953661e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 46/71 | LOSS: 5.943821750097026e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 47/71 | LOSS: 5.927455788423686e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 48/71 | LOSS: 5.902449351085625e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 49/71 | LOSS: 5.885013879378676e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 50/71 | LOSS: 5.856525987445482e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 51/71 | LOSS: 5.8709718604614654e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 52/71 | LOSS: 5.852073086917133e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 53/71 | LOSS: 5.847446312111936e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 54/71 | LOSS: 5.856429197592661e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 55/71 | LOSS: 5.834961176073453e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 56/71 | LOSS: 5.8258306923738e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 57/71 | LOSS: 5.819665743886084e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 58/71 | LOSS: 5.819599857081273e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 59/71 | LOSS: 5.829309611726785e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 60/71 | LOSS: 5.806723463331868e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 61/71 | LOSS: 5.810878835085708e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 62/71 | LOSS: 5.783879007342168e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 63/71 | LOSS: 5.788214430424432e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 64/71 | LOSS: 5.76351438632208e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 65/71 | LOSS: 5.753260411162928e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 66/71 | LOSS: 5.770981851359221e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 67/71 | LOSS: 5.761686231701734e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 68/71 | LOSS: 5.752617989460503e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 69/71 | LOSS: 5.7609111146510774e-06\n",
      "TRAIN: EPOCH 209/1000 | BATCH 70/71 | LOSS: 5.788897217659634e-06\n",
      "VAL: EPOCH 209/1000 | BATCH 0/8 | LOSS: 1.0099532119056676e-05\n",
      "VAL: EPOCH 209/1000 | BATCH 1/8 | LOSS: 8.821892834021128e-06\n",
      "VAL: EPOCH 209/1000 | BATCH 2/8 | LOSS: 9.585480711393757e-06\n",
      "VAL: EPOCH 209/1000 | BATCH 3/8 | LOSS: 9.302852163273201e-06\n",
      "VAL: EPOCH 209/1000 | BATCH 4/8 | LOSS: 9.429584406461799e-06\n",
      "VAL: EPOCH 209/1000 | BATCH 5/8 | LOSS: 9.4258212660255e-06\n",
      "VAL: EPOCH 209/1000 | BATCH 6/8 | LOSS: 9.278491169035468e-06\n",
      "VAL: EPOCH 209/1000 | BATCH 7/8 | LOSS: 9.551736468438321e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 0/71 | LOSS: 1.0926374670816585e-05\n",
      "TRAIN: EPOCH 210/1000 | BATCH 1/71 | LOSS: 8.484211548420717e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 2/71 | LOSS: 9.539361447726455e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 3/71 | LOSS: 9.275402021557966e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 4/71 | LOSS: 1.0025302162830486e-05\n",
      "TRAIN: EPOCH 210/1000 | BATCH 5/71 | LOSS: 9.45612706952185e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 6/71 | LOSS: 9.243158664113643e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 7/71 | LOSS: 8.803964703929523e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 8/71 | LOSS: 8.55874981829806e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 9/71 | LOSS: 8.703118828634615e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 10/71 | LOSS: 8.491964804720324e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 11/71 | LOSS: 8.335853218947401e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 12/71 | LOSS: 8.218708444474032e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 13/71 | LOSS: 8.214529608656968e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 14/71 | LOSS: 8.085721977598345e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 15/71 | LOSS: 7.991367056092713e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 16/71 | LOSS: 8.020128060546567e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 17/71 | LOSS: 7.890532186946883e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 18/71 | LOSS: 7.836574169280203e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 19/71 | LOSS: 7.717302628407196e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 20/71 | LOSS: 7.723655839765256e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 21/71 | LOSS: 7.595073611810221e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 22/71 | LOSS: 7.603292329883476e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 23/71 | LOSS: 7.59716819705621e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 24/71 | LOSS: 7.5133585960429624e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 25/71 | LOSS: 7.529003788966713e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 26/71 | LOSS: 7.5046139634785635e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 27/71 | LOSS: 7.386139080673664e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 28/71 | LOSS: 7.320164795914777e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 29/71 | LOSS: 7.2374065287779865e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 30/71 | LOSS: 7.252184332173783e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 31/71 | LOSS: 7.20682096755354e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 32/71 | LOSS: 7.169941055150103e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 33/71 | LOSS: 7.151535903497407e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 34/71 | LOSS: 7.137138163670898e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 35/71 | LOSS: 7.2055074977268105e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 36/71 | LOSS: 7.186207447965579e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 37/71 | LOSS: 7.2975511246017716e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 38/71 | LOSS: 7.2271073827906575e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 39/71 | LOSS: 7.233980215914926e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 40/71 | LOSS: 7.227219168448866e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 41/71 | LOSS: 7.292519642630525e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 42/71 | LOSS: 7.270872396644378e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 43/71 | LOSS: 7.238464449570032e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 44/71 | LOSS: 7.216683499993653e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 45/71 | LOSS: 7.2225905114464695e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 46/71 | LOSS: 7.230266080714113e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 47/71 | LOSS: 7.191720717022084e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 48/71 | LOSS: 7.141417464314442e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 49/71 | LOSS: 7.198900393632357e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 50/71 | LOSS: 7.152053914229755e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 51/71 | LOSS: 7.219499549976787e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 52/71 | LOSS: 7.187843622440991e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 53/71 | LOSS: 7.278988202112184e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 54/71 | LOSS: 7.2390697989083655e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 55/71 | LOSS: 7.218223320900766e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 56/71 | LOSS: 7.262224778714242e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 57/71 | LOSS: 7.2206061508166325e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 58/71 | LOSS: 7.243360869324533e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 59/71 | LOSS: 7.200173445198743e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 60/71 | LOSS: 7.200854128552506e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 61/71 | LOSS: 7.158131512046713e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 62/71 | LOSS: 7.133449040945076e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 63/71 | LOSS: 7.099255221021394e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 64/71 | LOSS: 7.081638138445739e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 65/71 | LOSS: 7.071748989277914e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 66/71 | LOSS: 7.024161847644493e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 67/71 | LOSS: 6.999295010245987e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 68/71 | LOSS: 6.958997267287012e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 69/71 | LOSS: 6.929513931806598e-06\n",
      "TRAIN: EPOCH 210/1000 | BATCH 70/71 | LOSS: 6.955895194448498e-06\n",
      "VAL: EPOCH 210/1000 | BATCH 0/8 | LOSS: 6.675458280369639e-06\n",
      "VAL: EPOCH 210/1000 | BATCH 1/8 | LOSS: 6.175019052534481e-06\n",
      "VAL: EPOCH 210/1000 | BATCH 2/8 | LOSS: 5.640015691218044e-06\n",
      "VAL: EPOCH 210/1000 | BATCH 3/8 | LOSS: 6.056002575860475e-06\n",
      "VAL: EPOCH 210/1000 | BATCH 4/8 | LOSS: 5.961323404335417e-06\n",
      "VAL: EPOCH 210/1000 | BATCH 5/8 | LOSS: 5.607435696219909e-06\n",
      "VAL: EPOCH 210/1000 | BATCH 6/8 | LOSS: 5.484554419256581e-06\n",
      "VAL: EPOCH 210/1000 | BATCH 7/8 | LOSS: 5.372664872993482e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 0/71 | LOSS: 5.426081770565361e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 1/71 | LOSS: 5.077931291452842e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 2/71 | LOSS: 4.971626670643066e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 3/71 | LOSS: 5.166079176888161e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 4/71 | LOSS: 5.2081369176448785e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 5/71 | LOSS: 5.250894673736184e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 6/71 | LOSS: 5.2744238538642615e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 7/71 | LOSS: 5.305925014909008e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 8/71 | LOSS: 5.254090208634605e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 9/71 | LOSS: 5.365791048461688e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 10/71 | LOSS: 5.464526897081999e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 11/71 | LOSS: 5.530171089655293e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 12/71 | LOSS: 5.534605686709536e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 13/71 | LOSS: 5.561119158820864e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 14/71 | LOSS: 5.523819830462647e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 15/71 | LOSS: 5.702598400603165e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 16/71 | LOSS: 5.789914987818681e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 17/71 | LOSS: 5.739697396873251e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 18/71 | LOSS: 5.681648656862212e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 19/71 | LOSS: 5.7932228855861466e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 20/71 | LOSS: 5.842744269016077e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 21/71 | LOSS: 5.875831427395249e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 22/71 | LOSS: 5.821100983113233e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 23/71 | LOSS: 5.8572787982787604e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 24/71 | LOSS: 5.894014975638129e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 25/71 | LOSS: 5.874448981138323e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 26/71 | LOSS: 5.8391505835633895e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 27/71 | LOSS: 5.824297090839536e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 28/71 | LOSS: 5.841301885836934e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 29/71 | LOSS: 5.854412256667274e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 30/71 | LOSS: 5.846565786229011e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 31/71 | LOSS: 5.867871848863615e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 32/71 | LOSS: 5.846708476879443e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 33/71 | LOSS: 5.848850766115527e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 34/71 | LOSS: 5.862850412086118e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 35/71 | LOSS: 5.851437549608818e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 36/71 | LOSS: 5.831292374087089e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 37/71 | LOSS: 5.823393513836843e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 38/71 | LOSS: 5.783258599806756e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 39/71 | LOSS: 5.758786221576883e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 40/71 | LOSS: 5.752399729539232e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 41/71 | LOSS: 5.712256541387649e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 42/71 | LOSS: 5.71367151446053e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 43/71 | LOSS: 5.694856709355505e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 44/71 | LOSS: 5.6574707943784435e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 45/71 | LOSS: 5.6998795551645225e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 46/71 | LOSS: 5.689802095575748e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 47/71 | LOSS: 5.690270503085533e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 48/71 | LOSS: 5.733942107697806e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 49/71 | LOSS: 5.78208826482296e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 50/71 | LOSS: 5.770429679079611e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 51/71 | LOSS: 5.745629795665781e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 52/71 | LOSS: 5.744803463628314e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 53/71 | LOSS: 5.752498896914336e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 54/71 | LOSS: 5.741163576286371e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 55/71 | LOSS: 5.76167583078911e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 56/71 | LOSS: 5.739183092245775e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 57/71 | LOSS: 5.731509716121042e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 58/71 | LOSS: 5.722813396352063e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 59/71 | LOSS: 5.716567413098043e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 60/71 | LOSS: 5.707984916521351e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 61/71 | LOSS: 5.698410639583486e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 62/71 | LOSS: 5.683053796163886e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 63/71 | LOSS: 5.673643215686752e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 64/71 | LOSS: 5.690477267377831e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 65/71 | LOSS: 5.695794822134882e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 66/71 | LOSS: 5.702812087740802e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 67/71 | LOSS: 5.6890767756853805e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 68/71 | LOSS: 5.65674640974844e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 69/71 | LOSS: 5.648450285532038e-06\n",
      "TRAIN: EPOCH 211/1000 | BATCH 70/71 | LOSS: 5.6503774245151955e-06\n",
      "VAL: EPOCH 211/1000 | BATCH 0/8 | LOSS: 7.808271220710594e-06\n",
      "VAL: EPOCH 211/1000 | BATCH 1/8 | LOSS: 6.8017361627426e-06\n",
      "VAL: EPOCH 211/1000 | BATCH 2/8 | LOSS: 7.319243195524905e-06\n",
      "VAL: EPOCH 211/1000 | BATCH 3/8 | LOSS: 7.438988632202381e-06\n",
      "VAL: EPOCH 211/1000 | BATCH 4/8 | LOSS: 7.4831023084698245e-06\n",
      "VAL: EPOCH 211/1000 | BATCH 5/8 | LOSS: 7.553562682005577e-06\n",
      "VAL: EPOCH 211/1000 | BATCH 6/8 | LOSS: 7.458899225249687e-06\n",
      "VAL: EPOCH 211/1000 | BATCH 7/8 | LOSS: 7.63113644097757e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 0/71 | LOSS: 7.304269274754915e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 1/71 | LOSS: 7.342532626353204e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 2/71 | LOSS: 7.379444317242208e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 3/71 | LOSS: 6.816331733716652e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 4/71 | LOSS: 6.651760941167595e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 5/71 | LOSS: 6.451596846090979e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 6/71 | LOSS: 6.279022207828737e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 7/71 | LOSS: 6.138105220543366e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 8/71 | LOSS: 6.225668029300222e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 9/71 | LOSS: 6.105766669861623e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 10/71 | LOSS: 6.0972288751641335e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 11/71 | LOSS: 6.02688104815267e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 12/71 | LOSS: 6.173694495933328e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 13/71 | LOSS: 6.194960893221183e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 14/71 | LOSS: 6.20294816447616e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 15/71 | LOSS: 6.147584343807466e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 16/71 | LOSS: 6.2209985170578446e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 17/71 | LOSS: 6.131776924828753e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 18/71 | LOSS: 6.0957389857656765e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 19/71 | LOSS: 6.0995155990895e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 20/71 | LOSS: 6.044505490925596e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 21/71 | LOSS: 6.031599551451191e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 22/71 | LOSS: 5.97552339652042e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 23/71 | LOSS: 5.998553623006349e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 24/71 | LOSS: 5.976167776680086e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 25/71 | LOSS: 6.042328620354126e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 26/71 | LOSS: 6.088712999135842e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 27/71 | LOSS: 6.082007806591199e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 28/71 | LOSS: 6.164622783248417e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 29/71 | LOSS: 6.114586449257331e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 30/71 | LOSS: 6.170130408492931e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 31/71 | LOSS: 6.15007333237827e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 32/71 | LOSS: 6.106393302064989e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 33/71 | LOSS: 6.090897573705982e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 34/71 | LOSS: 6.075271903682733e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 35/71 | LOSS: 6.0388202377402595e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 36/71 | LOSS: 6.007476701130587e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 37/71 | LOSS: 6.034319516115111e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 38/71 | LOSS: 6.02010391631689e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 39/71 | LOSS: 5.997871733143257e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 40/71 | LOSS: 5.9720324848390965e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 41/71 | LOSS: 5.940409805372751e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 42/71 | LOSS: 5.918947852817012e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 43/71 | LOSS: 5.885454387763192e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 44/71 | LOSS: 5.88189179527237e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 45/71 | LOSS: 5.86700237007513e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 46/71 | LOSS: 5.83818455004106e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 47/71 | LOSS: 5.834231596206034e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 48/71 | LOSS: 5.8217459790823194e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 49/71 | LOSS: 5.824917079735314e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 50/71 | LOSS: 5.820894704597509e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 51/71 | LOSS: 5.79346532539113e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 52/71 | LOSS: 5.847680502072008e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 53/71 | LOSS: 5.862278621188361e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 54/71 | LOSS: 5.894313587553122e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 55/71 | LOSS: 5.877709985010629e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 56/71 | LOSS: 5.888796749786707e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 57/71 | LOSS: 5.863016330577325e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 58/71 | LOSS: 5.873817094575547e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 59/71 | LOSS: 5.8604743344403685e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 60/71 | LOSS: 5.829668799513039e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 61/71 | LOSS: 5.846887842703689e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 62/71 | LOSS: 5.83859696595778e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 63/71 | LOSS: 5.816968005945e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 64/71 | LOSS: 5.8487714830754645e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 65/71 | LOSS: 5.873977505127021e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 66/71 | LOSS: 5.849523234640252e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 67/71 | LOSS: 5.85977860576831e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 68/71 | LOSS: 5.868111004573343e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 69/71 | LOSS: 5.869827360324312e-06\n",
      "TRAIN: EPOCH 212/1000 | BATCH 70/71 | LOSS: 5.8289830429228616e-06\n",
      "VAL: EPOCH 212/1000 | BATCH 0/8 | LOSS: 6.976586519158445e-06\n",
      "VAL: EPOCH 212/1000 | BATCH 1/8 | LOSS: 5.9553494793362916e-06\n",
      "VAL: EPOCH 212/1000 | BATCH 2/8 | LOSS: 5.852009053342044e-06\n",
      "VAL: EPOCH 212/1000 | BATCH 3/8 | LOSS: 5.964502179267583e-06\n",
      "VAL: EPOCH 212/1000 | BATCH 4/8 | LOSS: 5.9592381148831915e-06\n",
      "VAL: EPOCH 212/1000 | BATCH 5/8 | LOSS: 5.731082637794316e-06\n",
      "VAL: EPOCH 212/1000 | BATCH 6/8 | LOSS: 5.683800801697154e-06\n",
      "VAL: EPOCH 212/1000 | BATCH 7/8 | LOSS: 5.6556730783086095e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 0/71 | LOSS: 5.423048150987597e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 1/71 | LOSS: 5.006795390727348e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 2/71 | LOSS: 5.403219953829345e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 3/71 | LOSS: 5.009260917177016e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 4/71 | LOSS: 5.234252694208408e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 5/71 | LOSS: 5.161267154107918e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 6/71 | LOSS: 5.055653478588543e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 7/71 | LOSS: 5.09822189087572e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 8/71 | LOSS: 5.271113119912722e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 9/71 | LOSS: 5.298531823427765e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 10/71 | LOSS: 5.315125814707823e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 11/71 | LOSS: 5.317776867741486e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 12/71 | LOSS: 5.319226777198939e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 13/71 | LOSS: 5.355933288748409e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 14/71 | LOSS: 5.36872894372209e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 15/71 | LOSS: 5.4650639640385634e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 16/71 | LOSS: 5.468163715918576e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 17/71 | LOSS: 5.42808382104138e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 18/71 | LOSS: 5.4189770851639635e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 19/71 | LOSS: 5.51124553567206e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 20/71 | LOSS: 5.437794286644064e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 21/71 | LOSS: 5.569856346648356e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 22/71 | LOSS: 5.646497930169773e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 23/71 | LOSS: 5.59315083137335e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 24/71 | LOSS: 5.665132466674549e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 25/71 | LOSS: 5.743092078074159e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 26/71 | LOSS: 5.694654511764249e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 27/71 | LOSS: 5.669688058722906e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 28/71 | LOSS: 5.7279787205484826e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 29/71 | LOSS: 5.752472240298326e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 30/71 | LOSS: 5.75653772220023e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 31/71 | LOSS: 5.7855502291204175e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 32/71 | LOSS: 5.778752121400568e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 33/71 | LOSS: 5.746846641961704e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 34/71 | LOSS: 5.699917957307272e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 35/71 | LOSS: 5.745348112719108e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 36/71 | LOSS: 5.789312851718293e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 37/71 | LOSS: 5.759867558720687e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 38/71 | LOSS: 5.806350926808917e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 39/71 | LOSS: 5.802741713978321e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 40/71 | LOSS: 5.781288397950603e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 41/71 | LOSS: 5.7926921445392685e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 42/71 | LOSS: 5.790395789517788e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 43/71 | LOSS: 5.7571353822409454e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 44/71 | LOSS: 5.7496645442572315e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 45/71 | LOSS: 5.766576162821603e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 46/71 | LOSS: 5.778497034817577e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 47/71 | LOSS: 5.752570748048432e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 48/71 | LOSS: 5.774014082316153e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 49/71 | LOSS: 5.77785559471522e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 50/71 | LOSS: 5.7675741599643305e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 51/71 | LOSS: 5.761780401441613e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 52/71 | LOSS: 5.771965617278288e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 53/71 | LOSS: 5.755040614598396e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 54/71 | LOSS: 5.735852259518156e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 55/71 | LOSS: 5.712348532987173e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 56/71 | LOSS: 5.684673987273352e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 57/71 | LOSS: 5.6846626162511934e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 58/71 | LOSS: 5.68608716016801e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 59/71 | LOSS: 5.6835230831590405e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 60/71 | LOSS: 5.674055523368373e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 61/71 | LOSS: 5.672053043048025e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 62/71 | LOSS: 5.645090361270824e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 63/71 | LOSS: 5.634721617298055e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 64/71 | LOSS: 5.6307306364872675e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 65/71 | LOSS: 5.609143231181819e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 66/71 | LOSS: 5.593973616954311e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 67/71 | LOSS: 5.606925142006623e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 68/71 | LOSS: 5.620786923564214e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 69/71 | LOSS: 5.620524643745739e-06\n",
      "TRAIN: EPOCH 213/1000 | BATCH 70/71 | LOSS: 5.632249449775718e-06\n",
      "VAL: EPOCH 213/1000 | BATCH 0/8 | LOSS: 7.399630703730509e-06\n",
      "VAL: EPOCH 213/1000 | BATCH 1/8 | LOSS: 7.347705832216889e-06\n",
      "VAL: EPOCH 213/1000 | BATCH 2/8 | LOSS: 6.7025155961649334e-06\n",
      "VAL: EPOCH 213/1000 | BATCH 3/8 | LOSS: 7.392073825940315e-06\n",
      "VAL: EPOCH 213/1000 | BATCH 4/8 | LOSS: 7.227930200315313e-06\n",
      "VAL: EPOCH 213/1000 | BATCH 5/8 | LOSS: 6.873200087890534e-06\n",
      "VAL: EPOCH 213/1000 | BATCH 6/8 | LOSS: 6.777319348267545e-06\n",
      "VAL: EPOCH 213/1000 | BATCH 7/8 | LOSS: 6.588462838408304e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 0/71 | LOSS: 6.699787263642065e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 1/71 | LOSS: 5.702507678506663e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 2/71 | LOSS: 5.748445952728313e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 3/71 | LOSS: 5.803524913972069e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 4/71 | LOSS: 5.568494634644594e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 5/71 | LOSS: 5.577169000995734e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 6/71 | LOSS: 5.692353624908719e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 7/71 | LOSS: 5.6162246551139106e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 8/71 | LOSS: 5.633829510366519e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 9/71 | LOSS: 5.838660172230448e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 10/71 | LOSS: 5.8247042034054175e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 11/71 | LOSS: 5.839499901109472e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 12/71 | LOSS: 5.89330508004283e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 13/71 | LOSS: 6.060626284514521e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 14/71 | LOSS: 6.0599580137932206e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 15/71 | LOSS: 6.169642773556916e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 16/71 | LOSS: 6.099440965686774e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 17/71 | LOSS: 6.130220930976469e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 18/71 | LOSS: 6.052713567777046e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 19/71 | LOSS: 6.051818945707055e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 20/71 | LOSS: 6.037364506656082e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 21/71 | LOSS: 6.0213195948480545e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 22/71 | LOSS: 5.97178355430673e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 23/71 | LOSS: 5.890640958720421e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 24/71 | LOSS: 5.852228005096549e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 25/71 | LOSS: 5.8659510124506105e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 26/71 | LOSS: 5.853600534330191e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 27/71 | LOSS: 5.787581260717291e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 28/71 | LOSS: 5.838133488552934e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 29/71 | LOSS: 5.8411174601739425e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 30/71 | LOSS: 5.857517129287214e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 31/71 | LOSS: 5.819930819939145e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 32/71 | LOSS: 5.834988562789724e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 33/71 | LOSS: 5.8727457599161426e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 34/71 | LOSS: 5.905400627982869e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 35/71 | LOSS: 5.8692806861573545e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 36/71 | LOSS: 5.885703423110819e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 37/71 | LOSS: 5.860307570209873e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 38/71 | LOSS: 5.8340485618548255e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 39/71 | LOSS: 5.818528552481439e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 40/71 | LOSS: 5.823110147569579e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 41/71 | LOSS: 5.807286329616632e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 42/71 | LOSS: 5.808903816339511e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 43/71 | LOSS: 5.772993352531392e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 44/71 | LOSS: 5.766981464855942e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 45/71 | LOSS: 5.729089173695209e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 46/71 | LOSS: 5.704639688895228e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 47/71 | LOSS: 5.690624779693583e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 48/71 | LOSS: 5.675939742011868e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 49/71 | LOSS: 5.666619936164352e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 50/71 | LOSS: 5.6599635671791116e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 51/71 | LOSS: 5.664468464881513e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 52/71 | LOSS: 5.677181640217072e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 53/71 | LOSS: 5.653088481738922e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 54/71 | LOSS: 5.673071692316708e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 55/71 | LOSS: 5.6621015411500205e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 56/71 | LOSS: 5.729163970962665e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 57/71 | LOSS: 5.718337817974128e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 58/71 | LOSS: 5.757223696769037e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 59/71 | LOSS: 5.750416888380035e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 60/71 | LOSS: 5.7368131253541995e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 61/71 | LOSS: 5.7605401663397885e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 62/71 | LOSS: 5.736079221230558e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 63/71 | LOSS: 5.773132251363222e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 64/71 | LOSS: 5.7599293820833105e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 65/71 | LOSS: 5.773843674956659e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 66/71 | LOSS: 5.769998925141191e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 67/71 | LOSS: 5.782880367298597e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 68/71 | LOSS: 5.769913843613577e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 69/71 | LOSS: 5.758140150646795e-06\n",
      "TRAIN: EPOCH 214/1000 | BATCH 70/71 | LOSS: 5.739720672607572e-06\n",
      "VAL: EPOCH 214/1000 | BATCH 0/8 | LOSS: 6.487185146397678e-06\n",
      "VAL: EPOCH 214/1000 | BATCH 1/8 | LOSS: 5.566544814428198e-06\n",
      "VAL: EPOCH 214/1000 | BATCH 2/8 | LOSS: 5.246459447031763e-06\n",
      "VAL: EPOCH 214/1000 | BATCH 3/8 | LOSS: 5.547371642933285e-06\n",
      "VAL: EPOCH 214/1000 | BATCH 4/8 | LOSS: 5.438441985461395e-06\n",
      "VAL: EPOCH 214/1000 | BATCH 5/8 | LOSS: 5.186310393886136e-06\n",
      "VAL: EPOCH 214/1000 | BATCH 6/8 | LOSS: 5.070248140067893e-06\n",
      "VAL: EPOCH 214/1000 | BATCH 7/8 | LOSS: 4.967454287907458e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 0/71 | LOSS: 4.0488102968083695e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 1/71 | LOSS: 5.22021855431376e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 2/71 | LOSS: 5.2287552231670515e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 3/71 | LOSS: 5.241661483523785e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 4/71 | LOSS: 4.906274716631742e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 5/71 | LOSS: 4.968737130184309e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 6/71 | LOSS: 4.989380678515383e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 7/71 | LOSS: 5.1575188422248175e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 8/71 | LOSS: 5.31524953354771e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 9/71 | LOSS: 5.332856335371616e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 10/71 | LOSS: 5.427103546522134e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 11/71 | LOSS: 5.440924761993908e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 12/71 | LOSS: 5.401693775079016e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 13/71 | LOSS: 5.356535016939493e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 14/71 | LOSS: 5.35648384053881e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 15/71 | LOSS: 5.298389481822596e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 16/71 | LOSS: 5.341036472493536e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 17/71 | LOSS: 5.271804487468519e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 18/71 | LOSS: 5.322323072744582e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 19/71 | LOSS: 5.294794277688197e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 20/71 | LOSS: 5.378563465845738e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 21/71 | LOSS: 5.403962466681895e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 22/71 | LOSS: 5.458056856023967e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 23/71 | LOSS: 5.482730803881471e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 24/71 | LOSS: 5.513349406101043e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 25/71 | LOSS: 5.514651320481789e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 26/71 | LOSS: 5.521928970766668e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 27/71 | LOSS: 5.523767494456219e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 28/71 | LOSS: 5.5445434084284555e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 29/71 | LOSS: 5.54032171748986e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 30/71 | LOSS: 5.518282015996632e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 31/71 | LOSS: 5.551556512273237e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 32/71 | LOSS: 5.5406776895820675e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 33/71 | LOSS: 5.5464149280690915e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 34/71 | LOSS: 5.474861271587932e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 35/71 | LOSS: 5.521605253812191e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 36/71 | LOSS: 5.50983039018418e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 37/71 | LOSS: 5.499556460850535e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 38/71 | LOSS: 5.476640965823138e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 39/71 | LOSS: 5.44139360272311e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 40/71 | LOSS: 5.452224197541931e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 41/71 | LOSS: 5.4554723300541465e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 42/71 | LOSS: 5.502493539012882e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 43/71 | LOSS: 5.52727967508624e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 44/71 | LOSS: 5.5389070439559875e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 45/71 | LOSS: 5.511362943529409e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 46/71 | LOSS: 5.5208030991049415e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 47/71 | LOSS: 5.503555703019932e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 48/71 | LOSS: 5.514627900993219e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 49/71 | LOSS: 5.515340080819442e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 50/71 | LOSS: 5.528294298296803e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 51/71 | LOSS: 5.538560370811096e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 52/71 | LOSS: 5.543460753509656e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 53/71 | LOSS: 5.544432202374015e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 54/71 | LOSS: 5.5276692116670095e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 55/71 | LOSS: 5.520520679575773e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 56/71 | LOSS: 5.51977600001632e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 57/71 | LOSS: 5.534870234473131e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 58/71 | LOSS: 5.528372408446296e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 59/71 | LOSS: 5.524211057187737e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 60/71 | LOSS: 5.513170801022483e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 61/71 | LOSS: 5.492394708902969e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 62/71 | LOSS: 5.514136961527771e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 63/71 | LOSS: 5.5406818333381125e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 64/71 | LOSS: 5.525798650733822e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 65/71 | LOSS: 5.5502013600000115e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 66/71 | LOSS: 5.559207068866122e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 67/71 | LOSS: 5.580132725459835e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 68/71 | LOSS: 5.5985123899649744e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 69/71 | LOSS: 5.592544299101324e-06\n",
      "TRAIN: EPOCH 215/1000 | BATCH 70/71 | LOSS: 5.5879165852851475e-06\n",
      "VAL: EPOCH 215/1000 | BATCH 0/8 | LOSS: 8.616902050562203e-06\n",
      "VAL: EPOCH 215/1000 | BATCH 1/8 | LOSS: 7.590594123030314e-06\n",
      "VAL: EPOCH 215/1000 | BATCH 2/8 | LOSS: 7.161675057432149e-06\n",
      "VAL: EPOCH 215/1000 | BATCH 3/8 | LOSS: 7.3928406436607474e-06\n",
      "VAL: EPOCH 215/1000 | BATCH 4/8 | LOSS: 7.195471880550031e-06\n",
      "VAL: EPOCH 215/1000 | BATCH 5/8 | LOSS: 6.86608094232118e-06\n",
      "VAL: EPOCH 215/1000 | BATCH 6/8 | LOSS: 6.85347569222878e-06\n",
      "VAL: EPOCH 215/1000 | BATCH 7/8 | LOSS: 6.7858715055990615e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 0/71 | LOSS: 6.779116120014805e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 1/71 | LOSS: 7.257627203216543e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 2/71 | LOSS: 7.094620741554536e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 3/71 | LOSS: 7.520392500737216e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 4/71 | LOSS: 7.142501453927253e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 5/71 | LOSS: 6.95224533349877e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 6/71 | LOSS: 6.9637022923935935e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 7/71 | LOSS: 6.999157449172344e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 8/71 | LOSS: 6.957039962192842e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 9/71 | LOSS: 6.926127343831467e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 10/71 | LOSS: 6.948994723643409e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 11/71 | LOSS: 6.988451787037775e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 12/71 | LOSS: 7.028243159696173e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 13/71 | LOSS: 7.0581236611490435e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 14/71 | LOSS: 6.988222321524518e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 15/71 | LOSS: 7.011704070691849e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 16/71 | LOSS: 6.891554034261978e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 17/71 | LOSS: 6.932154014470547e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 18/71 | LOSS: 6.916163759964739e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 19/71 | LOSS: 6.929183837200981e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 20/71 | LOSS: 6.870496387515838e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 21/71 | LOSS: 6.822778976294847e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 22/71 | LOSS: 6.847823066852035e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 23/71 | LOSS: 6.76049194225925e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 24/71 | LOSS: 6.787263919250108e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 25/71 | LOSS: 6.70733722402544e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 26/71 | LOSS: 6.758072703195029e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 27/71 | LOSS: 6.682970966461913e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 28/71 | LOSS: 6.656613801043994e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 29/71 | LOSS: 6.598089673085875e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 30/71 | LOSS: 6.581195506324518e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 31/71 | LOSS: 6.557240823212851e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 32/71 | LOSS: 6.466298104483443e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 33/71 | LOSS: 6.4003539886043115e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 34/71 | LOSS: 6.3486473923798516e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 35/71 | LOSS: 6.294536433415487e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 36/71 | LOSS: 6.284496054364796e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 37/71 | LOSS: 6.2394856046778026e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 38/71 | LOSS: 6.234583370547998e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 39/71 | LOSS: 6.213661538367887e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 40/71 | LOSS: 6.174838556656075e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 41/71 | LOSS: 6.141140201656353e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 42/71 | LOSS: 6.106158961365297e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 43/71 | LOSS: 6.090624093634357e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 44/71 | LOSS: 6.071491791064748e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 45/71 | LOSS: 6.0194472355404916e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 46/71 | LOSS: 6.004045934922884e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 47/71 | LOSS: 5.964412859308747e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 48/71 | LOSS: 5.9305424545119445e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 49/71 | LOSS: 5.911361113248859e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 50/71 | LOSS: 5.8834785865174665e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 51/71 | LOSS: 5.8740366039973405e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 52/71 | LOSS: 5.853015289107614e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 53/71 | LOSS: 5.83525688516265e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 54/71 | LOSS: 5.818531330574346e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 55/71 | LOSS: 5.793272107114587e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 56/71 | LOSS: 5.777550612899511e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 57/71 | LOSS: 5.76349556279732e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 58/71 | LOSS: 5.77364348288738e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 59/71 | LOSS: 5.743428058243202e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 60/71 | LOSS: 5.726843482989352e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 61/71 | LOSS: 5.711604180466947e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 62/71 | LOSS: 5.6923126976471394e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 63/71 | LOSS: 5.69347628243122e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 64/71 | LOSS: 5.669477202620608e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 65/71 | LOSS: 5.663827344257945e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 66/71 | LOSS: 5.660707532780491e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 67/71 | LOSS: 5.62295093352398e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 68/71 | LOSS: 5.610139561358137e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 69/71 | LOSS: 5.610325661109527e-06\n",
      "TRAIN: EPOCH 216/1000 | BATCH 70/71 | LOSS: 5.579441378845523e-06\n",
      "VAL: EPOCH 216/1000 | BATCH 0/8 | LOSS: 6.838403805886628e-06\n",
      "VAL: EPOCH 216/1000 | BATCH 1/8 | LOSS: 5.882922550881631e-06\n",
      "VAL: EPOCH 216/1000 | BATCH 2/8 | LOSS: 6.089454321530259e-06\n",
      "VAL: EPOCH 216/1000 | BATCH 3/8 | LOSS: 6.191723855408782e-06\n",
      "VAL: EPOCH 216/1000 | BATCH 4/8 | LOSS: 6.21086874161847e-06\n",
      "VAL: EPOCH 216/1000 | BATCH 5/8 | LOSS: 6.137027185104671e-06\n",
      "VAL: EPOCH 216/1000 | BATCH 6/8 | LOSS: 6.00346005999849e-06\n",
      "VAL: EPOCH 216/1000 | BATCH 7/8 | LOSS: 6.027964900567895e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 0/71 | LOSS: 7.554648618679494e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 1/71 | LOSS: 6.732096608175198e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 2/71 | LOSS: 7.128375121586335e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 3/71 | LOSS: 6.481846185124596e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 4/71 | LOSS: 6.7400380430626685e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 5/71 | LOSS: 6.558401385821829e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 6/71 | LOSS: 6.374802004367146e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 7/71 | LOSS: 6.226546361176588e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 8/71 | LOSS: 6.313631426261661e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 9/71 | LOSS: 6.338258071991732e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 10/71 | LOSS: 6.105077525055756e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 11/71 | LOSS: 6.1538073623523815e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 12/71 | LOSS: 6.051804050982285e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 13/71 | LOSS: 6.142169013401144e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 14/71 | LOSS: 6.078258896498785e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 15/71 | LOSS: 6.06722238671864e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 16/71 | LOSS: 5.955887846700053e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 17/71 | LOSS: 5.88019065617118e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 18/71 | LOSS: 5.8174172361185285e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 19/71 | LOSS: 5.763196486441302e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 20/71 | LOSS: 5.841627496140843e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 21/71 | LOSS: 5.790489942889756e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 22/71 | LOSS: 5.779621005785884e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 23/71 | LOSS: 5.730412719155235e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 24/71 | LOSS: 5.820619553560391e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 25/71 | LOSS: 5.743883933134314e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 26/71 | LOSS: 5.738428744397997e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 27/71 | LOSS: 5.739779924494672e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 28/71 | LOSS: 5.753483931365391e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 29/71 | LOSS: 5.757507309075057e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 30/71 | LOSS: 5.7574992488005064e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 31/71 | LOSS: 5.852179484122644e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 32/71 | LOSS: 5.860092421767838e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 33/71 | LOSS: 5.857497586368802e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 34/71 | LOSS: 5.865259793478929e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 35/71 | LOSS: 5.865838602403528e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 36/71 | LOSS: 5.856867266777192e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 37/71 | LOSS: 5.852598262304366e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 38/71 | LOSS: 5.837594413512703e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 39/71 | LOSS: 5.8371393947709295e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 40/71 | LOSS: 5.817058624926154e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 41/71 | LOSS: 5.80144660489168e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 42/71 | LOSS: 5.797090696348257e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 43/71 | LOSS: 5.77793563975733e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 44/71 | LOSS: 5.771589419762474e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 45/71 | LOSS: 5.7716265078140525e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 46/71 | LOSS: 5.769435758454765e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 47/71 | LOSS: 5.777424595028909e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 48/71 | LOSS: 5.772091364113814e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 49/71 | LOSS: 5.7863465826812895e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 50/71 | LOSS: 5.778350004773386e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 51/71 | LOSS: 5.730760541120197e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 52/71 | LOSS: 5.728550780437423e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 53/71 | LOSS: 5.719715406687303e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 54/71 | LOSS: 5.70438386272475e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 55/71 | LOSS: 5.686252259400655e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 56/71 | LOSS: 5.695064361900318e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 57/71 | LOSS: 5.690650905778276e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 58/71 | LOSS: 5.6777933122695254e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 59/71 | LOSS: 5.6987957464116334e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 60/71 | LOSS: 5.697892123355471e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 61/71 | LOSS: 5.714800339216797e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 62/71 | LOSS: 5.7302417269330895e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 63/71 | LOSS: 5.723704536109153e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 64/71 | LOSS: 5.7428606326608975e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 65/71 | LOSS: 5.741416788568489e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 66/71 | LOSS: 5.758314887655737e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 67/71 | LOSS: 5.78939785648746e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 68/71 | LOSS: 5.7898361240444025e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 69/71 | LOSS: 5.797723286247804e-06\n",
      "TRAIN: EPOCH 217/1000 | BATCH 70/71 | LOSS: 5.83160407535308e-06\n",
      "VAL: EPOCH 217/1000 | BATCH 0/8 | LOSS: 9.117205991060473e-06\n",
      "VAL: EPOCH 217/1000 | BATCH 1/8 | LOSS: 8.473187335766852e-06\n",
      "VAL: EPOCH 217/1000 | BATCH 2/8 | LOSS: 7.643112439836841e-06\n",
      "VAL: EPOCH 217/1000 | BATCH 3/8 | LOSS: 8.32098498904088e-06\n",
      "VAL: EPOCH 217/1000 | BATCH 4/8 | LOSS: 7.99553563410882e-06\n",
      "VAL: EPOCH 217/1000 | BATCH 5/8 | LOSS: 7.752425290163956e-06\n",
      "VAL: EPOCH 217/1000 | BATCH 6/8 | LOSS: 7.576026356608574e-06\n",
      "VAL: EPOCH 217/1000 | BATCH 7/8 | LOSS: 7.294848217043182e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 0/71 | LOSS: 5.523166237253463e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 1/71 | LOSS: 5.150364586370415e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 2/71 | LOSS: 5.7639337380048046e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 3/71 | LOSS: 5.805555019833264e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 4/71 | LOSS: 5.6969939578266345e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 5/71 | LOSS: 5.616684044677338e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 6/71 | LOSS: 5.7361688569861665e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 7/71 | LOSS: 5.81838099833476e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 8/71 | LOSS: 5.930348126841838e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 9/71 | LOSS: 5.914526354899863e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 10/71 | LOSS: 5.966898904510097e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 11/71 | LOSS: 5.877550885694897e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 12/71 | LOSS: 5.901763625633276e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 13/71 | LOSS: 5.8378611161710325e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 14/71 | LOSS: 5.790594180628735e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 15/71 | LOSS: 5.780450777592705e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 16/71 | LOSS: 5.758889635805698e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 17/71 | LOSS: 5.825231508222512e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 18/71 | LOSS: 5.902600653491008e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 19/71 | LOSS: 5.974236637484865e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 20/71 | LOSS: 5.982782654644411e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 21/71 | LOSS: 6.0604948802475436e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 22/71 | LOSS: 5.985445647599155e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 23/71 | LOSS: 6.017743923318146e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 24/71 | LOSS: 5.995106912450865e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 25/71 | LOSS: 5.954176860821182e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 26/71 | LOSS: 6.004419918570892e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 27/71 | LOSS: 5.92185230224069e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 28/71 | LOSS: 5.902643242107247e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 29/71 | LOSS: 5.905935404371121e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 30/71 | LOSS: 5.936460711968868e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 31/71 | LOSS: 5.932256669893832e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 32/71 | LOSS: 5.9204355211201776e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 33/71 | LOSS: 5.914829791757202e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 34/71 | LOSS: 5.853791520036923e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 35/71 | LOSS: 5.831362639380839e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 36/71 | LOSS: 5.797079407981269e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 37/71 | LOSS: 5.7757552703582115e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 38/71 | LOSS: 5.772942761093593e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 39/71 | LOSS: 5.768075948253682e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 40/71 | LOSS: 5.755279821974776e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 41/71 | LOSS: 5.703542797164118e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 42/71 | LOSS: 5.71325835534121e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 43/71 | LOSS: 5.68581991315527e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 44/71 | LOSS: 5.677026016807455e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 45/71 | LOSS: 5.704343953252448e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 46/71 | LOSS: 5.715221925825933e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 47/71 | LOSS: 5.721426840447445e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 48/71 | LOSS: 5.712474872705905e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 49/71 | LOSS: 5.690658485946187e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 50/71 | LOSS: 5.691675654566543e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 51/71 | LOSS: 5.669892826832578e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 52/71 | LOSS: 5.646014367915275e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 53/71 | LOSS: 5.642175720081637e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 54/71 | LOSS: 5.6369070948262975e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 55/71 | LOSS: 5.6229318496597055e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 56/71 | LOSS: 5.607801831661562e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 57/71 | LOSS: 5.631288484531477e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 58/71 | LOSS: 5.614230469476197e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 59/71 | LOSS: 5.624875958195237e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 60/71 | LOSS: 5.616441587305611e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 61/71 | LOSS: 5.6172988472040275e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 62/71 | LOSS: 5.606219369765891e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 63/71 | LOSS: 5.6034706368279785e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 64/71 | LOSS: 5.607388255577042e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 65/71 | LOSS: 5.616571360350821e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 66/71 | LOSS: 5.603623725646549e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 67/71 | LOSS: 5.610384328199015e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 68/71 | LOSS: 5.606177996858183e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 69/71 | LOSS: 5.607989763640424e-06\n",
      "TRAIN: EPOCH 218/1000 | BATCH 70/71 | LOSS: 5.600961137940725e-06\n",
      "VAL: EPOCH 218/1000 | BATCH 0/8 | LOSS: 1.089294528355822e-05\n",
      "VAL: EPOCH 218/1000 | BATCH 1/8 | LOSS: 9.714673069538549e-06\n",
      "VAL: EPOCH 218/1000 | BATCH 2/8 | LOSS: 9.897258072063172e-06\n",
      "VAL: EPOCH 218/1000 | BATCH 3/8 | LOSS: 9.553930340189254e-06\n",
      "VAL: EPOCH 218/1000 | BATCH 4/8 | LOSS: 9.743881309987047e-06\n",
      "VAL: EPOCH 218/1000 | BATCH 5/8 | LOSS: 9.279190256468913e-06\n",
      "VAL: EPOCH 218/1000 | BATCH 6/8 | LOSS: 9.364120484990832e-06\n",
      "VAL: EPOCH 218/1000 | BATCH 7/8 | LOSS: 9.405240632531786e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 0/71 | LOSS: 6.478911018348299e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 1/71 | LOSS: 7.759193977108225e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 2/71 | LOSS: 7.044708278651039e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 3/71 | LOSS: 7.053844797155762e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 4/71 | LOSS: 6.865559953439515e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 5/71 | LOSS: 6.625830640890247e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 6/71 | LOSS: 6.3992891747537735e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 7/71 | LOSS: 6.330311634883401e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 8/71 | LOSS: 6.361008369519065e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 9/71 | LOSS: 6.168807931317133e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 10/71 | LOSS: 6.111534110624978e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 11/71 | LOSS: 6.194848917099686e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 12/71 | LOSS: 6.190262586012697e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 13/71 | LOSS: 6.11303997694839e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 14/71 | LOSS: 6.094499531172914e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 15/71 | LOSS: 6.283724445665939e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 16/71 | LOSS: 6.2681016221604834e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 17/71 | LOSS: 6.271823369590695e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 18/71 | LOSS: 6.302117789776898e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 19/71 | LOSS: 6.233685780898668e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 20/71 | LOSS: 6.244193129878979e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 21/71 | LOSS: 6.179639988410847e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 22/71 | LOSS: 6.203568516313048e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 23/71 | LOSS: 6.120411001120374e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 24/71 | LOSS: 6.124917072156677e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 25/71 | LOSS: 6.160088087898867e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 26/71 | LOSS: 6.1106814731222885e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 27/71 | LOSS: 6.062108907047202e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 28/71 | LOSS: 6.117845902641922e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 29/71 | LOSS: 6.132811070832152e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 30/71 | LOSS: 6.1278314968794505e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 31/71 | LOSS: 6.108248143732453e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 32/71 | LOSS: 6.112674189794096e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 33/71 | LOSS: 6.171933565008872e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 34/71 | LOSS: 6.178136274164509e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 35/71 | LOSS: 6.174923555590794e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 36/71 | LOSS: 6.132009115685218e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 37/71 | LOSS: 6.105173198241238e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 38/71 | LOSS: 6.064019649224624e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 39/71 | LOSS: 6.039238451194251e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 40/71 | LOSS: 6.031806206897826e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 41/71 | LOSS: 6.018774120959625e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 42/71 | LOSS: 6.0099447647760745e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 43/71 | LOSS: 5.998047144699243e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 44/71 | LOSS: 5.994726163610014e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 45/71 | LOSS: 6.002400378628923e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 46/71 | LOSS: 5.999184772821188e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 47/71 | LOSS: 6.021161558085926e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 48/71 | LOSS: 6.022325662194575e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 49/71 | LOSS: 6.0388601104932606e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 50/71 | LOSS: 6.048548201243987e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 51/71 | LOSS: 6.037398511930055e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 52/71 | LOSS: 6.027645625026879e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 53/71 | LOSS: 6.039951249140022e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 54/71 | LOSS: 6.031712853820698e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 55/71 | LOSS: 6.025633865647251e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 56/71 | LOSS: 6.024101279316503e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 57/71 | LOSS: 6.047245145462762e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 58/71 | LOSS: 6.076497141031599e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 59/71 | LOSS: 6.097867018676576e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 60/71 | LOSS: 6.126108593114205e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 61/71 | LOSS: 6.102060047675378e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 62/71 | LOSS: 6.132419977283072e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 63/71 | LOSS: 6.152061324371516e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 64/71 | LOSS: 6.164938461622044e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 65/71 | LOSS: 6.203808921286096e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 66/71 | LOSS: 6.242626616075364e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 67/71 | LOSS: 6.2283391156704645e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 68/71 | LOSS: 6.220247337625841e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 69/71 | LOSS: 6.246211017111948e-06\n",
      "TRAIN: EPOCH 219/1000 | BATCH 70/71 | LOSS: 6.229606408941065e-06\n",
      "VAL: EPOCH 219/1000 | BATCH 0/8 | LOSS: 7.87235148891341e-06\n",
      "VAL: EPOCH 219/1000 | BATCH 1/8 | LOSS: 7.081721832946641e-06\n",
      "VAL: EPOCH 219/1000 | BATCH 2/8 | LOSS: 7.316134239469345e-06\n",
      "VAL: EPOCH 219/1000 | BATCH 3/8 | LOSS: 7.400434242299525e-06\n",
      "VAL: EPOCH 219/1000 | BATCH 4/8 | LOSS: 7.4190043960697946e-06\n",
      "VAL: EPOCH 219/1000 | BATCH 5/8 | LOSS: 7.251530708648109e-06\n",
      "VAL: EPOCH 219/1000 | BATCH 6/8 | LOSS: 7.0316985199627065e-06\n",
      "VAL: EPOCH 219/1000 | BATCH 7/8 | LOSS: 7.095147907421051e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 0/71 | LOSS: 7.044342510198476e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 1/71 | LOSS: 7.03864043316571e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 2/71 | LOSS: 6.742208370269509e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 3/71 | LOSS: 6.601862537536363e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 4/71 | LOSS: 6.342435244732769e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 5/71 | LOSS: 6.714757015894672e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 6/71 | LOSS: 6.4871424651106025e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 7/71 | LOSS: 6.555581876455108e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 8/71 | LOSS: 6.472274688146879e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 9/71 | LOSS: 6.4731991642474895e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 10/71 | LOSS: 6.402699000318535e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 11/71 | LOSS: 6.295753072057171e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 12/71 | LOSS: 6.520325943991058e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 13/71 | LOSS: 6.402419720481183e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 14/71 | LOSS: 6.426185124534337e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 15/71 | LOSS: 6.440476653324367e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 16/71 | LOSS: 6.341028491827755e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 17/71 | LOSS: 6.394333442787885e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 18/71 | LOSS: 6.371033793440897e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 19/71 | LOSS: 6.481883156084223e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 20/71 | LOSS: 6.542017131106972e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 21/71 | LOSS: 6.583666535417168e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 22/71 | LOSS: 6.690854246224499e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 23/71 | LOSS: 6.713584411954798e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 24/71 | LOSS: 6.836172578914557e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 25/71 | LOSS: 6.7589772138140125e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 26/71 | LOSS: 6.8405771782164705e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 27/71 | LOSS: 6.817722481043477e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 28/71 | LOSS: 6.791775753111418e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 29/71 | LOSS: 6.769671153961099e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 30/71 | LOSS: 6.7052261299742836e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 31/71 | LOSS: 6.654746016465651e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 32/71 | LOSS: 6.576307262576213e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 33/71 | LOSS: 6.55395545917974e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 34/71 | LOSS: 6.541586831839855e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 35/71 | LOSS: 6.542329768106154e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 36/71 | LOSS: 6.497979177003947e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 37/71 | LOSS: 6.461395311194601e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 38/71 | LOSS: 6.417734125417878e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 39/71 | LOSS: 6.424624586998107e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 40/71 | LOSS: 6.397393618255326e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 41/71 | LOSS: 6.416685567143889e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 42/71 | LOSS: 6.422411335607162e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 43/71 | LOSS: 6.397953866739541e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 44/71 | LOSS: 6.496703271194645e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 45/71 | LOSS: 6.480826364761173e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 46/71 | LOSS: 6.5934088254041546e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 47/71 | LOSS: 6.585816104613211e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 48/71 | LOSS: 6.758495527144393e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 49/71 | LOSS: 6.729382103003445e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 50/71 | LOSS: 6.84797843111001e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 51/71 | LOSS: 6.832803268690231e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 52/71 | LOSS: 6.931611036600101e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 53/71 | LOSS: 6.902159350795921e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 54/71 | LOSS: 6.922503317499914e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 55/71 | LOSS: 6.971302647278728e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 56/71 | LOSS: 6.943678088869651e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 57/71 | LOSS: 7.006350509480649e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 58/71 | LOSS: 7.0125248400426385e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 59/71 | LOSS: 7.1046973289412564e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 60/71 | LOSS: 7.077666977551655e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 61/71 | LOSS: 7.0842218358466784e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 62/71 | LOSS: 7.165429992926961e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 63/71 | LOSS: 7.164273981175029e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 64/71 | LOSS: 7.214037465858452e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 65/71 | LOSS: 7.233902813437233e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 66/71 | LOSS: 7.266398363333167e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 67/71 | LOSS: 7.226441954010036e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 68/71 | LOSS: 7.236948371481022e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 69/71 | LOSS: 7.217341226350981e-06\n",
      "TRAIN: EPOCH 220/1000 | BATCH 70/71 | LOSS: 7.190447447523901e-06\n",
      "VAL: EPOCH 220/1000 | BATCH 0/8 | LOSS: 7.215704499685671e-06\n",
      "VAL: EPOCH 220/1000 | BATCH 1/8 | LOSS: 6.58384897178621e-06\n",
      "VAL: EPOCH 220/1000 | BATCH 2/8 | LOSS: 6.284875629110805e-06\n",
      "VAL: EPOCH 220/1000 | BATCH 3/8 | LOSS: 6.7636061658049584e-06\n",
      "VAL: EPOCH 220/1000 | BATCH 4/8 | LOSS: 6.588567521248478e-06\n",
      "VAL: EPOCH 220/1000 | BATCH 5/8 | LOSS: 6.201681875002881e-06\n",
      "VAL: EPOCH 220/1000 | BATCH 6/8 | LOSS: 6.125997190663059e-06\n",
      "VAL: EPOCH 220/1000 | BATCH 7/8 | LOSS: 6.014480391058896e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 0/71 | LOSS: 4.971441285306355e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 1/71 | LOSS: 4.703455715571181e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 2/71 | LOSS: 5.3783049528040765e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 3/71 | LOSS: 5.108363779982028e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 4/71 | LOSS: 5.1980004172946795e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 5/71 | LOSS: 5.379824794241965e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 6/71 | LOSS: 5.978445382684835e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 7/71 | LOSS: 5.868913376616547e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 8/71 | LOSS: 6.353024218697101e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 9/71 | LOSS: 6.255652078834828e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 10/71 | LOSS: 6.147835351426196e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 11/71 | LOSS: 6.257086283767421e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 12/71 | LOSS: 6.222323008842068e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 13/71 | LOSS: 6.1551434750331935e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 14/71 | LOSS: 6.072029130640052e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 15/71 | LOSS: 6.047878628123726e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 16/71 | LOSS: 6.091643412861124e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 17/71 | LOSS: 6.0091811064921785e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 18/71 | LOSS: 6.078576150589231e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 19/71 | LOSS: 6.113011477282271e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 20/71 | LOSS: 6.096705181421601e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 21/71 | LOSS: 6.147816169356123e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 22/71 | LOSS: 6.201621466101168e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 23/71 | LOSS: 6.171446007859534e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 24/71 | LOSS: 6.206349626154406e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 25/71 | LOSS: 6.2467666758983305e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 26/71 | LOSS: 6.26903644202954e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 27/71 | LOSS: 6.305821200060434e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 28/71 | LOSS: 6.353946273834324e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 29/71 | LOSS: 6.358099941887e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 30/71 | LOSS: 6.338325962837365e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 31/71 | LOSS: 6.309799189807563e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 32/71 | LOSS: 6.359931596693279e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 33/71 | LOSS: 6.301221888219623e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 34/71 | LOSS: 6.27000610100887e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 35/71 | LOSS: 6.283318940505625e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 36/71 | LOSS: 6.310260207986151e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 37/71 | LOSS: 6.26450904699021e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 38/71 | LOSS: 6.225781572328952e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 39/71 | LOSS: 6.246238172025187e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 40/71 | LOSS: 6.235560781866164e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 41/71 | LOSS: 6.20890712828744e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 42/71 | LOSS: 6.233364063011195e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 43/71 | LOSS: 6.2284844530453976e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 44/71 | LOSS: 6.2184627192133725e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 45/71 | LOSS: 6.194370816165642e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 46/71 | LOSS: 6.183572835691442e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 47/71 | LOSS: 6.172066055872468e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 48/71 | LOSS: 6.1479411540078995e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 49/71 | LOSS: 6.196071890371968e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 50/71 | LOSS: 6.175275596914991e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 51/71 | LOSS: 6.205040082162528e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 52/71 | LOSS: 6.198168339667389e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 53/71 | LOSS: 6.238107449395035e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 54/71 | LOSS: 6.240770909988152e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 55/71 | LOSS: 6.240943418691001e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 56/71 | LOSS: 6.228033734052524e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 57/71 | LOSS: 6.220389156530065e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 58/71 | LOSS: 6.2157849211963995e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 59/71 | LOSS: 6.204913112621094e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 60/71 | LOSS: 6.1954644598834005e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 61/71 | LOSS: 6.171879396142979e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 62/71 | LOSS: 6.1637797516018585e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 63/71 | LOSS: 6.1931643671186976e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 64/71 | LOSS: 6.162083256920316e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 65/71 | LOSS: 6.143428656672988e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 66/71 | LOSS: 6.164005930286158e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 67/71 | LOSS: 6.161706912333851e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 68/71 | LOSS: 6.159350749036041e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 69/71 | LOSS: 6.172111430454866e-06\n",
      "TRAIN: EPOCH 221/1000 | BATCH 70/71 | LOSS: 6.166963654758432e-06\n",
      "VAL: EPOCH 221/1000 | BATCH 0/8 | LOSS: 6.938763817743165e-06\n",
      "VAL: EPOCH 221/1000 | BATCH 1/8 | LOSS: 6.201547193995793e-06\n",
      "VAL: EPOCH 221/1000 | BATCH 2/8 | LOSS: 5.7991264839074574e-06\n",
      "VAL: EPOCH 221/1000 | BATCH 3/8 | LOSS: 6.285500603553373e-06\n",
      "VAL: EPOCH 221/1000 | BATCH 4/8 | LOSS: 6.05331460974412e-06\n",
      "VAL: EPOCH 221/1000 | BATCH 5/8 | LOSS: 5.761923982087562e-06\n",
      "VAL: EPOCH 221/1000 | BATCH 6/8 | LOSS: 5.6349404076172505e-06\n",
      "VAL: EPOCH 221/1000 | BATCH 7/8 | LOSS: 5.508711581114767e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 0/71 | LOSS: 4.705392711912282e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 1/71 | LOSS: 6.181282969919266e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 2/71 | LOSS: 6.087358997319825e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 3/71 | LOSS: 5.6475789733667625e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 4/71 | LOSS: 5.818600220663938e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 5/71 | LOSS: 5.6944015038122115e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 6/71 | LOSS: 5.6071781468095395e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 7/71 | LOSS: 5.500442057382315e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 8/71 | LOSS: 5.525170409883786e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 9/71 | LOSS: 5.508119375008391e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 10/71 | LOSS: 5.495990039476468e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 11/71 | LOSS: 5.46752740622954e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 12/71 | LOSS: 5.397754683564506e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 13/71 | LOSS: 5.416816065917374e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 14/71 | LOSS: 5.378434131368218e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 15/71 | LOSS: 5.501317218659096e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 16/71 | LOSS: 5.477397895875209e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 17/71 | LOSS: 5.483063078928454e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 18/71 | LOSS: 5.4976781029836275e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 19/71 | LOSS: 5.576619150815531e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 20/71 | LOSS: 5.536182665569608e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 21/71 | LOSS: 5.498815116053183e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 22/71 | LOSS: 5.502280680008445e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 23/71 | LOSS: 5.4691808675973635e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 24/71 | LOSS: 5.44874717888888e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 25/71 | LOSS: 5.451479619995763e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 26/71 | LOSS: 5.424504740437476e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 27/71 | LOSS: 5.396884749383649e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 28/71 | LOSS: 5.42875436778935e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 29/71 | LOSS: 5.387538173332965e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 30/71 | LOSS: 5.382388280850137e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 31/71 | LOSS: 5.405215645737371e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 32/71 | LOSS: 5.3988229741331786e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 33/71 | LOSS: 5.375866601966931e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 34/71 | LOSS: 5.358267208066536e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 35/71 | LOSS: 5.350913726134523e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 36/71 | LOSS: 5.31378631225183e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 37/71 | LOSS: 5.3060496156831505e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 38/71 | LOSS: 5.306057911907257e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 39/71 | LOSS: 5.303408750023664e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 40/71 | LOSS: 5.302045493164503e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 41/71 | LOSS: 5.317814936591146e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 42/71 | LOSS: 5.313276222634243e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 43/71 | LOSS: 5.291897020304697e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 44/71 | LOSS: 5.280254774536135e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 45/71 | LOSS: 5.269773836240031e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 46/71 | LOSS: 5.25106365107894e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 47/71 | LOSS: 5.2474930309169094e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 48/71 | LOSS: 5.231469356435903e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 49/71 | LOSS: 5.217395073486841e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 50/71 | LOSS: 5.203904769515764e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 51/71 | LOSS: 5.209522494344408e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 52/71 | LOSS: 5.195562213009667e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 53/71 | LOSS: 5.183102791340423e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 54/71 | LOSS: 5.202674881068312e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 55/71 | LOSS: 5.212480500306681e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 56/71 | LOSS: 5.228490863373736e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 57/71 | LOSS: 5.24254592007843e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 58/71 | LOSS: 5.243742835172255e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 59/71 | LOSS: 5.236129140939738e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 60/71 | LOSS: 5.230378191612783e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 61/71 | LOSS: 5.24630638298526e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 62/71 | LOSS: 5.250395242044414e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 63/71 | LOSS: 5.254144383570747e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 64/71 | LOSS: 5.251976066434648e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 65/71 | LOSS: 5.27145274728272e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 66/71 | LOSS: 5.276821174271363e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 67/71 | LOSS: 5.295374294559331e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 68/71 | LOSS: 5.311698292854893e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 69/71 | LOSS: 5.316170154401334e-06\n",
      "TRAIN: EPOCH 222/1000 | BATCH 70/71 | LOSS: 5.293256416619661e-06\n",
      "VAL: EPOCH 222/1000 | BATCH 0/8 | LOSS: 8.124359737848863e-06\n",
      "VAL: EPOCH 222/1000 | BATCH 1/8 | LOSS: 7.511258900194662e-06\n",
      "VAL: EPOCH 222/1000 | BATCH 2/8 | LOSS: 7.458957649456958e-06\n",
      "VAL: EPOCH 222/1000 | BATCH 3/8 | LOSS: 7.245225788210519e-06\n",
      "VAL: EPOCH 222/1000 | BATCH 4/8 | LOSS: 7.392893530777656e-06\n",
      "VAL: EPOCH 222/1000 | BATCH 5/8 | LOSS: 7.006375502290514e-06\n",
      "VAL: EPOCH 222/1000 | BATCH 6/8 | LOSS: 6.927449378833574e-06\n",
      "VAL: EPOCH 222/1000 | BATCH 7/8 | LOSS: 6.964476881421433e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 0/71 | LOSS: 7.344852747337427e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 1/71 | LOSS: 7.2595294113853015e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 2/71 | LOSS: 6.7668045934018055e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 3/71 | LOSS: 6.370921596499102e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 4/71 | LOSS: 6.480221873061964e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 5/71 | LOSS: 6.593727979028093e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 6/71 | LOSS: 6.639758209660483e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 7/71 | LOSS: 6.561160091678175e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 8/71 | LOSS: 6.651937029447355e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 9/71 | LOSS: 6.520741953863762e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 10/71 | LOSS: 6.3067203138251155e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 11/71 | LOSS: 6.265424228028375e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 12/71 | LOSS: 6.278115926341763e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 13/71 | LOSS: 6.28166091181421e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 14/71 | LOSS: 6.146614759927616e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 15/71 | LOSS: 6.150360832180013e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 16/71 | LOSS: 6.072815749040969e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 17/71 | LOSS: 6.0070097384191795e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 18/71 | LOSS: 6.085131405454472e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 19/71 | LOSS: 6.0864578699693085e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 20/71 | LOSS: 6.006479452480562e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 21/71 | LOSS: 5.989134437682382e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 22/71 | LOSS: 6.0611464505200274e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 23/71 | LOSS: 6.051894255657923e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 24/71 | LOSS: 6.053029628674267e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 25/71 | LOSS: 6.05251886134703e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 26/71 | LOSS: 6.030706892353784e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 27/71 | LOSS: 6.015073479050313e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 28/71 | LOSS: 6.036770346013484e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 29/71 | LOSS: 5.977568237843419e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 30/71 | LOSS: 5.992005563096045e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 31/71 | LOSS: 5.953376003731137e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 32/71 | LOSS: 5.939206155083164e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 33/71 | LOSS: 5.969471618193304e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 34/71 | LOSS: 5.951049479335779e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 35/71 | LOSS: 5.964496608612535e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 36/71 | LOSS: 5.943772902281961e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 37/71 | LOSS: 5.9225475955540066e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 38/71 | LOSS: 5.884697832068643e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 39/71 | LOSS: 5.895972822145268e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 40/71 | LOSS: 5.9410937710043336e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 41/71 | LOSS: 5.919393234286136e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 42/71 | LOSS: 5.941295311522269e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 43/71 | LOSS: 5.901845931723735e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 44/71 | LOSS: 5.952548943039599e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 45/71 | LOSS: 5.950091482863291e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 46/71 | LOSS: 5.977805919447998e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 47/71 | LOSS: 5.9559773812149315e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 48/71 | LOSS: 5.9531415138976905e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 49/71 | LOSS: 5.9348149443394505e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 50/71 | LOSS: 5.91309086017895e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 51/71 | LOSS: 5.907173798592479e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 52/71 | LOSS: 5.888644297882357e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 53/71 | LOSS: 5.839710225969881e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 54/71 | LOSS: 5.841179253448817e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 55/71 | LOSS: 5.835876728659579e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 56/71 | LOSS: 5.835057707595856e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 57/71 | LOSS: 5.8201456513102284e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 58/71 | LOSS: 5.826172983395705e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 59/71 | LOSS: 5.844955110963686e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 60/71 | LOSS: 5.834033896969574e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 61/71 | LOSS: 5.8528910504983584e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 62/71 | LOSS: 5.862186306273096e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 63/71 | LOSS: 5.868683409460118e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 64/71 | LOSS: 5.851138282126228e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 65/71 | LOSS: 5.858197813391929e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 66/71 | LOSS: 5.8318558045873185e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 67/71 | LOSS: 5.821900171052702e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 68/71 | LOSS: 5.797988383099015e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 69/71 | LOSS: 5.809241214852331e-06\n",
      "TRAIN: EPOCH 223/1000 | BATCH 70/71 | LOSS: 5.7990115032814985e-06\n",
      "VAL: EPOCH 223/1000 | BATCH 0/8 | LOSS: 5.640398740069941e-06\n",
      "VAL: EPOCH 223/1000 | BATCH 1/8 | LOSS: 4.978301603841828e-06\n",
      "VAL: EPOCH 223/1000 | BATCH 2/8 | LOSS: 4.900110070593655e-06\n",
      "VAL: EPOCH 223/1000 | BATCH 3/8 | LOSS: 5.205040224609547e-06\n",
      "VAL: EPOCH 223/1000 | BATCH 4/8 | LOSS: 5.146643070474966e-06\n",
      "VAL: EPOCH 223/1000 | BATCH 5/8 | LOSS: 4.975527872375096e-06\n",
      "VAL: EPOCH 223/1000 | BATCH 6/8 | LOSS: 4.895367120687817e-06\n",
      "VAL: EPOCH 223/1000 | BATCH 7/8 | LOSS: 4.8713708906689135e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 0/71 | LOSS: 4.444366368261399e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 1/71 | LOSS: 4.8487902404303895e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 2/71 | LOSS: 4.9581760019160965e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 3/71 | LOSS: 5.264530159365677e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 4/71 | LOSS: 5.235720254859188e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 5/71 | LOSS: 5.441265557237784e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 6/71 | LOSS: 5.545539287205818e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 7/71 | LOSS: 5.534707838705799e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 8/71 | LOSS: 5.499096965245877e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 9/71 | LOSS: 5.4301991895044924e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 10/71 | LOSS: 5.451612485723507e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 11/71 | LOSS: 5.261185435756488e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 12/71 | LOSS: 5.295673925222498e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 13/71 | LOSS: 5.307869303448699e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 14/71 | LOSS: 5.21046408721304e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 15/71 | LOSS: 5.200956977091664e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 16/71 | LOSS: 5.269933848574189e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 17/71 | LOSS: 5.234729655967385e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 18/71 | LOSS: 5.205833902341271e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 19/71 | LOSS: 5.145500529124547e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 20/71 | LOSS: 5.136058342984706e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 21/71 | LOSS: 5.167761845438111e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 22/71 | LOSS: 5.189765577439254e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 23/71 | LOSS: 5.1117233207757335e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 24/71 | LOSS: 5.106517528474797e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 25/71 | LOSS: 5.0825946205944865e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 26/71 | LOSS: 5.086694660281797e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 27/71 | LOSS: 5.14592014561848e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 28/71 | LOSS: 5.139847793355424e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 29/71 | LOSS: 5.163814770033545e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 30/71 | LOSS: 5.161228595933343e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 31/71 | LOSS: 5.163754778436669e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 32/71 | LOSS: 5.176514756991916e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 33/71 | LOSS: 5.188459039452616e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 34/71 | LOSS: 5.160279930091097e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 35/71 | LOSS: 5.212407131693908e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 36/71 | LOSS: 5.219899125025935e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 37/71 | LOSS: 5.2355419962910485e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 38/71 | LOSS: 5.205812543023897e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 39/71 | LOSS: 5.177197670036548e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 40/71 | LOSS: 5.174920294471454e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 41/71 | LOSS: 5.172503939493686e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 42/71 | LOSS: 5.190295808482238e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 43/71 | LOSS: 5.154194597749169e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 44/71 | LOSS: 5.152975028168941e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 45/71 | LOSS: 5.1389332413790445e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 46/71 | LOSS: 5.140924437735985e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 47/71 | LOSS: 5.160036801991434e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 48/71 | LOSS: 5.193715307949769e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 49/71 | LOSS: 5.228499235272466e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 50/71 | LOSS: 5.24417405180358e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 51/71 | LOSS: 5.269851197471536e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 52/71 | LOSS: 5.262605001470942e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 53/71 | LOSS: 5.270796426278442e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 54/71 | LOSS: 5.252369841069925e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 55/71 | LOSS: 5.265808620639031e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 56/71 | LOSS: 5.277957462455736e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 57/71 | LOSS: 5.280096365038781e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 58/71 | LOSS: 5.288219958625416e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 59/71 | LOSS: 5.270995154660341e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 60/71 | LOSS: 5.282582483008899e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 61/71 | LOSS: 5.282713672268953e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 62/71 | LOSS: 5.287244296899492e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 63/71 | LOSS: 5.299539164838052e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 64/71 | LOSS: 5.309483566844182e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 65/71 | LOSS: 5.311571919374921e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 66/71 | LOSS: 5.348954634577402e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 67/71 | LOSS: 5.363780744841386e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 68/71 | LOSS: 5.3798734192558024e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 69/71 | LOSS: 5.4114787774649035e-06\n",
      "TRAIN: EPOCH 224/1000 | BATCH 70/71 | LOSS: 5.3962198346428995e-06\n",
      "VAL: EPOCH 224/1000 | BATCH 0/8 | LOSS: 5.861222689418355e-06\n",
      "VAL: EPOCH 224/1000 | BATCH 1/8 | LOSS: 5.176967306397273e-06\n",
      "VAL: EPOCH 224/1000 | BATCH 2/8 | LOSS: 5.361746540681149e-06\n",
      "VAL: EPOCH 224/1000 | BATCH 3/8 | LOSS: 5.734657406719634e-06\n",
      "VAL: EPOCH 224/1000 | BATCH 4/8 | LOSS: 5.671195140166674e-06\n",
      "VAL: EPOCH 224/1000 | BATCH 5/8 | LOSS: 5.629152989664969e-06\n",
      "VAL: EPOCH 224/1000 | BATCH 6/8 | LOSS: 5.566034035707292e-06\n",
      "VAL: EPOCH 224/1000 | BATCH 7/8 | LOSS: 5.536012224638398e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 0/71 | LOSS: 5.841761776537169e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 1/71 | LOSS: 5.972922281216597e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 2/71 | LOSS: 5.343684430651289e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 3/71 | LOSS: 6.34746663763508e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 4/71 | LOSS: 6.045456484571332e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 5/71 | LOSS: 6.116271682306736e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 6/71 | LOSS: 5.959700242133944e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 7/71 | LOSS: 6.155296944143629e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 8/71 | LOSS: 6.020926220015907e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 9/71 | LOSS: 5.9874646012758605e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 10/71 | LOSS: 5.851961552914211e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 11/71 | LOSS: 5.966346103984203e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 12/71 | LOSS: 5.874087177149844e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 13/71 | LOSS: 5.887226247328467e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 14/71 | LOSS: 5.869191015032508e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 15/71 | LOSS: 5.806094350191415e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 16/71 | LOSS: 5.836085379354806e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 17/71 | LOSS: 5.784673779392809e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 18/71 | LOSS: 5.722509285988053e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 19/71 | LOSS: 5.771497330897546e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 20/71 | LOSS: 5.693407880850525e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 21/71 | LOSS: 5.684368984888848e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 22/71 | LOSS: 5.675987722497666e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 23/71 | LOSS: 5.637654832450305e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 24/71 | LOSS: 5.589840984612238e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 25/71 | LOSS: 5.53583826191383e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 26/71 | LOSS: 5.510508041576638e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 27/71 | LOSS: 5.510273045469408e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 28/71 | LOSS: 5.480053710800411e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 29/71 | LOSS: 5.4616898978565585e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 30/71 | LOSS: 5.4320930171196515e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 31/71 | LOSS: 5.417672483076785e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 32/71 | LOSS: 5.411735690524273e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 33/71 | LOSS: 5.4315975616833275e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 34/71 | LOSS: 5.417940701590851e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 35/71 | LOSS: 5.3805032393938745e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 36/71 | LOSS: 5.368576706599482e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 37/71 | LOSS: 5.354118109990898e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 38/71 | LOSS: 5.3537322599321414e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 39/71 | LOSS: 5.340223435723601e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 40/71 | LOSS: 5.364264192083771e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 41/71 | LOSS: 5.339191299450856e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 42/71 | LOSS: 5.313460776030065e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 43/71 | LOSS: 5.314297461402682e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 44/71 | LOSS: 5.313485063298786e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 45/71 | LOSS: 5.307129096921847e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 46/71 | LOSS: 5.303377379954946e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 47/71 | LOSS: 5.29929825650773e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 48/71 | LOSS: 5.280513050820446e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 49/71 | LOSS: 5.282319771140464e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 50/71 | LOSS: 5.259686531644547e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 51/71 | LOSS: 5.246880750047029e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 52/71 | LOSS: 5.234392016784525e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 53/71 | LOSS: 5.2154703687982095e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 54/71 | LOSS: 5.224138278043194e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 55/71 | LOSS: 5.21455834651923e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 56/71 | LOSS: 5.2246654089877234e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 57/71 | LOSS: 5.220953433873264e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 58/71 | LOSS: 5.229222713720539e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 59/71 | LOSS: 5.245760416983103e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 60/71 | LOSS: 5.25673586245651e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 61/71 | LOSS: 5.293504749891825e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 62/71 | LOSS: 5.294188473011932e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 63/71 | LOSS: 5.3546684668503985e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 64/71 | LOSS: 5.3470509626025045e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 65/71 | LOSS: 5.410466077261265e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 66/71 | LOSS: 5.394725820821547e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 67/71 | LOSS: 5.427610009225164e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 68/71 | LOSS: 5.463341758381896e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 69/71 | LOSS: 5.468764330934001e-06\n",
      "TRAIN: EPOCH 225/1000 | BATCH 70/71 | LOSS: 5.477612683560591e-06\n",
      "VAL: EPOCH 225/1000 | BATCH 0/8 | LOSS: 6.739309355907608e-06\n",
      "VAL: EPOCH 225/1000 | BATCH 1/8 | LOSS: 5.7798704347078456e-06\n",
      "VAL: EPOCH 225/1000 | BATCH 2/8 | LOSS: 5.82953771299799e-06\n",
      "VAL: EPOCH 225/1000 | BATCH 3/8 | LOSS: 5.8656823966884986e-06\n",
      "VAL: EPOCH 225/1000 | BATCH 4/8 | LOSS: 5.906231308472343e-06\n",
      "VAL: EPOCH 225/1000 | BATCH 5/8 | LOSS: 5.772591975983232e-06\n",
      "VAL: EPOCH 225/1000 | BATCH 6/8 | LOSS: 5.719937170007532e-06\n",
      "VAL: EPOCH 225/1000 | BATCH 7/8 | LOSS: 5.768039329723251e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 0/71 | LOSS: 4.866946255788207e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 1/71 | LOSS: 5.833177510794485e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 2/71 | LOSS: 5.877731988827388e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 3/71 | LOSS: 6.185821916915302e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 4/71 | LOSS: 6.514218421216356e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 5/71 | LOSS: 6.684305996410937e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 6/71 | LOSS: 6.668872824125824e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 7/71 | LOSS: 6.875041378862079e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 8/71 | LOSS: 6.789809655553351e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 9/71 | LOSS: 6.699109781038714e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 10/71 | LOSS: 6.58241362378414e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 11/71 | LOSS: 6.4735360941388835e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 12/71 | LOSS: 6.414710521942023e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 13/71 | LOSS: 6.3492764508867235e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 14/71 | LOSS: 6.2531767980544824e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 15/71 | LOSS: 6.2345678486508405e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 16/71 | LOSS: 6.220692017343347e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 17/71 | LOSS: 6.234236353621883e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 18/71 | LOSS: 6.178518393234729e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 19/71 | LOSS: 6.224846060831624e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 20/71 | LOSS: 6.160100102057359e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 21/71 | LOSS: 6.083282063297123e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 22/71 | LOSS: 6.0657121139229275e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 23/71 | LOSS: 6.044895769719005e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 24/71 | LOSS: 6.09678811088088e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 25/71 | LOSS: 6.051313745886616e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 26/71 | LOSS: 6.092693840603008e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 27/71 | LOSS: 6.141659079211033e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 28/71 | LOSS: 6.233942258404568e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 29/71 | LOSS: 6.229049404282705e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 30/71 | LOSS: 6.203054278059074e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 31/71 | LOSS: 6.360735866905998e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 32/71 | LOSS: 6.307869529281919e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 33/71 | LOSS: 6.387585405237212e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 34/71 | LOSS: 6.356701890451534e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 35/71 | LOSS: 6.3592443919131056e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 36/71 | LOSS: 6.362926661814846e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 37/71 | LOSS: 6.366694259141743e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 38/71 | LOSS: 6.380741267956835e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 39/71 | LOSS: 6.373911833179591e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 40/71 | LOSS: 6.3801611226730665e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 41/71 | LOSS: 6.334798313002379e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 42/71 | LOSS: 6.324135980786766e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 43/71 | LOSS: 6.309913353354866e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 44/71 | LOSS: 6.30326199421284e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 45/71 | LOSS: 6.2803796738634166e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 46/71 | LOSS: 6.286853709133674e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 47/71 | LOSS: 6.256080657370451e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 48/71 | LOSS: 6.230083597204482e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 49/71 | LOSS: 6.1993887084099695e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 50/71 | LOSS: 6.192241168416028e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 51/71 | LOSS: 6.1804386265206494e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 52/71 | LOSS: 6.152595859638718e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 53/71 | LOSS: 6.11998900610531e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 54/71 | LOSS: 6.09371874650771e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 55/71 | LOSS: 6.072157248127041e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 56/71 | LOSS: 6.055798302552655e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 57/71 | LOSS: 6.01027046945217e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 58/71 | LOSS: 6.0199712822657724e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 59/71 | LOSS: 5.998091258637335e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 60/71 | LOSS: 5.989282816117935e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 61/71 | LOSS: 5.973512821994166e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 62/71 | LOSS: 5.9508464267035755e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 63/71 | LOSS: 5.943211277781302e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 64/71 | LOSS: 5.952782178680466e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 65/71 | LOSS: 5.960554235019972e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 66/71 | LOSS: 5.95738986282698e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 67/71 | LOSS: 5.98259236994636e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 68/71 | LOSS: 5.987139607843373e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 69/71 | LOSS: 5.9961425839511715e-06\n",
      "TRAIN: EPOCH 226/1000 | BATCH 70/71 | LOSS: 6.0527958864863456e-06\n",
      "VAL: EPOCH 226/1000 | BATCH 0/8 | LOSS: 8.518617505615111e-06\n",
      "VAL: EPOCH 226/1000 | BATCH 1/8 | LOSS: 8.067361250141403e-06\n",
      "VAL: EPOCH 226/1000 | BATCH 2/8 | LOSS: 7.422323506034445e-06\n",
      "VAL: EPOCH 226/1000 | BATCH 3/8 | LOSS: 8.08031541055243e-06\n",
      "VAL: EPOCH 226/1000 | BATCH 4/8 | LOSS: 7.752993406029418e-06\n",
      "VAL: EPOCH 226/1000 | BATCH 5/8 | LOSS: 7.471774551959243e-06\n",
      "VAL: EPOCH 226/1000 | BATCH 6/8 | LOSS: 7.309129874296819e-06\n",
      "VAL: EPOCH 226/1000 | BATCH 7/8 | LOSS: 7.0430623395623115e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 0/71 | LOSS: 7.283510512934299e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 1/71 | LOSS: 8.060589834713028e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 2/71 | LOSS: 7.89528197249941e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 3/71 | LOSS: 9.193186542688636e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 4/71 | LOSS: 8.60331656440394e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 5/71 | LOSS: 8.585061702130284e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 6/71 | LOSS: 8.43678981254925e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 7/71 | LOSS: 7.95191834868092e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 8/71 | LOSS: 7.843622218691356e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 9/71 | LOSS: 7.673248228456942e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 10/71 | LOSS: 7.459548214683309e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 11/71 | LOSS: 7.249602125132999e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 12/71 | LOSS: 7.072421759893097e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 13/71 | LOSS: 6.946536294500609e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 14/71 | LOSS: 6.771488066685075e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 15/71 | LOSS: 6.629322456319642e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 16/71 | LOSS: 6.489581691686694e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 17/71 | LOSS: 6.425150938108952e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 18/71 | LOSS: 6.378607203690703e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 19/71 | LOSS: 6.305357328528771e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 20/71 | LOSS: 6.27466871654698e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 21/71 | LOSS: 6.215059226054688e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 22/71 | LOSS: 6.139755125490331e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 23/71 | LOSS: 6.12520422767678e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 24/71 | LOSS: 6.10744924415485e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 25/71 | LOSS: 6.085721294696738e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 26/71 | LOSS: 6.1043524005476175e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 27/71 | LOSS: 6.074524565389895e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 28/71 | LOSS: 6.030941127893788e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 29/71 | LOSS: 6.186545336580214e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 30/71 | LOSS: 6.142460367095003e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 31/71 | LOSS: 6.219196691858997e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 32/71 | LOSS: 6.166663380016954e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 33/71 | LOSS: 6.136208853871829e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 34/71 | LOSS: 6.128851743726824e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 35/71 | LOSS: 6.090770903534172e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 36/71 | LOSS: 6.129027214400766e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 37/71 | LOSS: 6.07523810159412e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 38/71 | LOSS: 6.05444609811037e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 39/71 | LOSS: 6.045688451195019e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 40/71 | LOSS: 6.083372105289217e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 41/71 | LOSS: 6.051121588479007e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 42/71 | LOSS: 6.006480848449174e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 43/71 | LOSS: 6.029837818873189e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 44/71 | LOSS: 6.0038357737034355e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 45/71 | LOSS: 6.024374373658287e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 46/71 | LOSS: 5.999665982597705e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 47/71 | LOSS: 5.995528672049962e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 48/71 | LOSS: 5.965601322175792e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 49/71 | LOSS: 5.9849786066479285e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 50/71 | LOSS: 5.958471293150283e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 51/71 | LOSS: 5.956829542041165e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 52/71 | LOSS: 5.934320846413271e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 53/71 | LOSS: 5.962430855611779e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 54/71 | LOSS: 5.95250019779831e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 55/71 | LOSS: 5.948144413585916e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 56/71 | LOSS: 5.931228786306619e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 57/71 | LOSS: 5.924041527672169e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 58/71 | LOSS: 5.923155193590516e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 59/71 | LOSS: 5.910619279347884e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 60/71 | LOSS: 5.894347023207896e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 61/71 | LOSS: 5.862264214879213e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 62/71 | LOSS: 5.8858381490854155e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 63/71 | LOSS: 5.881758958992123e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 64/71 | LOSS: 5.871536730130454e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 65/71 | LOSS: 5.8844448658695026e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 66/71 | LOSS: 5.883768479369751e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 67/71 | LOSS: 5.914189326925707e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 68/71 | LOSS: 5.90258191484769e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 69/71 | LOSS: 5.909877245358075e-06\n",
      "TRAIN: EPOCH 227/1000 | BATCH 70/71 | LOSS: 5.8933637838365515e-06\n",
      "VAL: EPOCH 227/1000 | BATCH 0/8 | LOSS: 5.986582436889876e-06\n",
      "VAL: EPOCH 227/1000 | BATCH 1/8 | LOSS: 5.312536814017221e-06\n",
      "VAL: EPOCH 227/1000 | BATCH 2/8 | LOSS: 5.097673541361776e-06\n",
      "VAL: EPOCH 227/1000 | BATCH 3/8 | LOSS: 5.276056526781758e-06\n",
      "VAL: EPOCH 227/1000 | BATCH 4/8 | LOSS: 5.247739863989409e-06\n",
      "VAL: EPOCH 227/1000 | BATCH 5/8 | LOSS: 5.017560245808757e-06\n",
      "VAL: EPOCH 227/1000 | BATCH 6/8 | LOSS: 4.9564357271135255e-06\n",
      "VAL: EPOCH 227/1000 | BATCH 7/8 | LOSS: 4.913876807677298e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 0/71 | LOSS: 4.378686753625516e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 1/71 | LOSS: 5.2757191042474005e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 2/71 | LOSS: 5.162853691823936e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 3/71 | LOSS: 5.549636398427538e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 4/71 | LOSS: 5.7877550716511905e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 5/71 | LOSS: 5.8680687592035e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 6/71 | LOSS: 6.169178829752907e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 7/71 | LOSS: 5.960316798336862e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 8/71 | LOSS: 6.002272736319962e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 9/71 | LOSS: 5.729622012040636e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 10/71 | LOSS: 5.789149926468698e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 11/71 | LOSS: 5.748519072312774e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 12/71 | LOSS: 5.652499164800527e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 13/71 | LOSS: 5.82548255221939e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 14/71 | LOSS: 5.771736035361149e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 15/71 | LOSS: 5.903809849883146e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 16/71 | LOSS: 5.813318974854637e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 17/71 | LOSS: 5.83954492109721e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 18/71 | LOSS: 5.879366354795486e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 19/71 | LOSS: 5.910173092615878e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 20/71 | LOSS: 5.904652154987118e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 21/71 | LOSS: 5.860217108668373e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 22/71 | LOSS: 5.893249448016159e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 23/71 | LOSS: 5.88197903539367e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 24/71 | LOSS: 5.873635009265854e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 25/71 | LOSS: 5.911013842635909e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 26/71 | LOSS: 5.868200880151427e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 27/71 | LOSS: 5.877907361601891e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 28/71 | LOSS: 5.8696211438253724e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 29/71 | LOSS: 5.937808790198081e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 30/71 | LOSS: 5.929406252987739e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 31/71 | LOSS: 5.900881937748181e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 32/71 | LOSS: 5.9016294513143786e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 33/71 | LOSS: 5.89725001485503e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 34/71 | LOSS: 5.979610425908634e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 35/71 | LOSS: 6.007271022351941e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 36/71 | LOSS: 6.105761325965727e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 37/71 | LOSS: 6.1584297913884116e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 38/71 | LOSS: 6.19569463057045e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 39/71 | LOSS: 6.249220024301394e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 40/71 | LOSS: 6.24170655367091e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 41/71 | LOSS: 6.2621046156761754e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 42/71 | LOSS: 6.226180096555889e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 43/71 | LOSS: 6.260136444780073e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 44/71 | LOSS: 6.274899457518283e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 45/71 | LOSS: 6.275489681684121e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 46/71 | LOSS: 6.30571896587579e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 47/71 | LOSS: 6.287826887311591e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 48/71 | LOSS: 6.273235441062288e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 49/71 | LOSS: 6.257198006096587e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 50/71 | LOSS: 6.307995104804172e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 51/71 | LOSS: 6.314580765285007e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 52/71 | LOSS: 6.359341375842495e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 53/71 | LOSS: 6.3420239771235004e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 54/71 | LOSS: 6.34239650439254e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 55/71 | LOSS: 6.316223346759996e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 56/71 | LOSS: 6.305154740179556e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 57/71 | LOSS: 6.315976018664389e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 58/71 | LOSS: 6.313081512108526e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 59/71 | LOSS: 6.316698261343845e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 60/71 | LOSS: 6.313919185663881e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 61/71 | LOSS: 6.304592587976883e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 62/71 | LOSS: 6.274989706821904e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 63/71 | LOSS: 6.255082919182087e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 64/71 | LOSS: 6.2482673841329344e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 65/71 | LOSS: 6.242805997399873e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 66/71 | LOSS: 6.217303307358403e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 67/71 | LOSS: 6.202742413814335e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 68/71 | LOSS: 6.181305110842821e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 69/71 | LOSS: 6.155036450244162e-06\n",
      "TRAIN: EPOCH 228/1000 | BATCH 70/71 | LOSS: 6.132546834866754e-06\n",
      "VAL: EPOCH 228/1000 | BATCH 0/8 | LOSS: 5.72063527215505e-06\n",
      "VAL: EPOCH 228/1000 | BATCH 1/8 | LOSS: 5.0150729293818586e-06\n",
      "VAL: EPOCH 228/1000 | BATCH 2/8 | LOSS: 4.950441204224869e-06\n",
      "VAL: EPOCH 228/1000 | BATCH 3/8 | LOSS: 5.16967588737316e-06\n",
      "VAL: EPOCH 228/1000 | BATCH 4/8 | LOSS: 5.110507117933594e-06\n",
      "VAL: EPOCH 228/1000 | BATCH 5/8 | LOSS: 4.894564578232045e-06\n",
      "VAL: EPOCH 228/1000 | BATCH 6/8 | LOSS: 4.8102788891161e-06\n",
      "VAL: EPOCH 228/1000 | BATCH 7/8 | LOSS: 4.77056858017022e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 0/71 | LOSS: 4.487927981244866e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 1/71 | LOSS: 5.463062279886799e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 2/71 | LOSS: 5.29697202485598e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 3/71 | LOSS: 4.968585585629626e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 4/71 | LOSS: 5.029839030612493e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 5/71 | LOSS: 5.198048484089668e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 6/71 | LOSS: 5.123043917722368e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 7/71 | LOSS: 5.164474657703977e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 8/71 | LOSS: 5.0839203140640166e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 9/71 | LOSS: 5.083969017505296e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 10/71 | LOSS: 5.191202283392406e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 11/71 | LOSS: 5.186897662194194e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 12/71 | LOSS: 5.21796118846396e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 13/71 | LOSS: 5.272484876123988e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 14/71 | LOSS: 5.363205370182793e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 15/71 | LOSS: 5.404816818099789e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 16/71 | LOSS: 5.513821419941344e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 17/71 | LOSS: 5.4441874858134215e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 18/71 | LOSS: 5.543311808172516e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 19/71 | LOSS: 5.453421454149065e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 20/71 | LOSS: 5.429254694414946e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 21/71 | LOSS: 5.480621439346578e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 22/71 | LOSS: 5.4872839427844156e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 23/71 | LOSS: 5.520824307344204e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 24/71 | LOSS: 5.535954242077423e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 25/71 | LOSS: 5.6125201214084745e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 26/71 | LOSS: 5.586401652248309e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 27/71 | LOSS: 5.707399217109403e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 28/71 | LOSS: 5.727332069815522e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 29/71 | LOSS: 5.873778839789642e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 30/71 | LOSS: 5.867695343171832e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 31/71 | LOSS: 5.905839543629554e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 32/71 | LOSS: 5.9559907971230466e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 33/71 | LOSS: 5.967313628014595e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 34/71 | LOSS: 6.019650040148658e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 35/71 | LOSS: 5.997859905922572e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 36/71 | LOSS: 6.025530441351097e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 37/71 | LOSS: 6.028531611503384e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 38/71 | LOSS: 6.039989808087763e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 39/71 | LOSS: 6.104449676058721e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 40/71 | LOSS: 6.076334546200886e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 41/71 | LOSS: 6.1298477313747365e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 42/71 | LOSS: 6.121413820818457e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 43/71 | LOSS: 6.116638939023357e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 44/71 | LOSS: 6.125830158351972e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 45/71 | LOSS: 6.0905845103938505e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 46/71 | LOSS: 6.138050719799023e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 47/71 | LOSS: 6.118016652105022e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 48/71 | LOSS: 6.126764386566237e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 49/71 | LOSS: 6.12229797297914e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 50/71 | LOSS: 6.103486362453801e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 51/71 | LOSS: 6.133268811936432e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 52/71 | LOSS: 6.119860279555586e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 53/71 | LOSS: 6.150031258248082e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 54/71 | LOSS: 6.1789422612822485e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 55/71 | LOSS: 6.223719594800059e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 56/71 | LOSS: 6.217914081692773e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 57/71 | LOSS: 6.236170308731179e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 58/71 | LOSS: 6.263981612068226e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 59/71 | LOSS: 6.2725348622431435e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 60/71 | LOSS: 6.281881992510505e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 61/71 | LOSS: 6.336014540729886e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 62/71 | LOSS: 6.353257684543404e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 63/71 | LOSS: 6.318639108826574e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 64/71 | LOSS: 6.338828979306317e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 65/71 | LOSS: 6.332105179105631e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 66/71 | LOSS: 6.328103480162968e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 67/71 | LOSS: 6.336475761974511e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 68/71 | LOSS: 6.336133247859307e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 69/71 | LOSS: 6.385659974382309e-06\n",
      "TRAIN: EPOCH 229/1000 | BATCH 70/71 | LOSS: 6.361121373168278e-06\n",
      "VAL: EPOCH 229/1000 | BATCH 0/8 | LOSS: 6.501109510281822e-06\n",
      "VAL: EPOCH 229/1000 | BATCH 1/8 | LOSS: 5.739419748351793e-06\n",
      "VAL: EPOCH 229/1000 | BATCH 2/8 | LOSS: 5.729038245287181e-06\n",
      "VAL: EPOCH 229/1000 | BATCH 3/8 | LOSS: 5.942176017015299e-06\n",
      "VAL: EPOCH 229/1000 | BATCH 4/8 | LOSS: 5.92448186580441e-06\n",
      "VAL: EPOCH 229/1000 | BATCH 5/8 | LOSS: 5.5380147614414454e-06\n",
      "VAL: EPOCH 229/1000 | BATCH 6/8 | LOSS: 5.417454336306296e-06\n",
      "VAL: EPOCH 229/1000 | BATCH 7/8 | LOSS: 5.314841217796129e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 0/71 | LOSS: 5.375593445933191e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 1/71 | LOSS: 6.008485570418998e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 2/71 | LOSS: 5.6016613901495775e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 3/71 | LOSS: 5.342065719560196e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 4/71 | LOSS: 5.487442558660405e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 5/71 | LOSS: 5.57240393087947e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 6/71 | LOSS: 5.659263284282393e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 7/71 | LOSS: 5.5712079642944445e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 8/71 | LOSS: 5.7755216100000934e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 9/71 | LOSS: 5.721123670809902e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 10/71 | LOSS: 5.708720726199152e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 11/71 | LOSS: 5.624692410795736e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 12/71 | LOSS: 5.766533726833027e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 13/71 | LOSS: 5.700924670496274e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 14/71 | LOSS: 5.806436638522427e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 15/71 | LOSS: 5.811298080971028e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 16/71 | LOSS: 5.864198957330218e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 17/71 | LOSS: 5.884326457008784e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 18/71 | LOSS: 5.819608615734308e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 19/71 | LOSS: 5.818078398078796e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 20/71 | LOSS: 5.775422431768329e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 21/71 | LOSS: 5.8407259860938545e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 22/71 | LOSS: 5.790794306370157e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 23/71 | LOSS: 5.826926421074556e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 24/71 | LOSS: 5.819005618832307e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 25/71 | LOSS: 5.829145948155201e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 26/71 | LOSS: 5.856636141322303e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 27/71 | LOSS: 5.823990396233317e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 28/71 | LOSS: 5.848179500452684e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 29/71 | LOSS: 5.815856957269716e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 30/71 | LOSS: 5.852195778472026e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 31/71 | LOSS: 5.809486992802704e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 32/71 | LOSS: 5.806332851396732e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 33/71 | LOSS: 5.8689146338592225e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 34/71 | LOSS: 5.869529117522429e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 35/71 | LOSS: 5.912319516533495e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 36/71 | LOSS: 5.9255263246037385e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 37/71 | LOSS: 5.937521001741468e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 38/71 | LOSS: 5.8970604186470155e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 39/71 | LOSS: 5.9076256434309474e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 40/71 | LOSS: 5.897947575271601e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 41/71 | LOSS: 5.8575756048369275e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 42/71 | LOSS: 5.849626716574908e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 43/71 | LOSS: 5.835617819436513e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 44/71 | LOSS: 5.819609143347609e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 45/71 | LOSS: 5.777508615368709e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 46/71 | LOSS: 5.755667972791459e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 47/71 | LOSS: 5.742596120702122e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 48/71 | LOSS: 5.7249928376018495e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 49/71 | LOSS: 5.714492008337402e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 50/71 | LOSS: 5.728954660940765e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 51/71 | LOSS: 5.714852311309816e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 52/71 | LOSS: 5.710788112957984e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 53/71 | LOSS: 5.704402366501637e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 54/71 | LOSS: 5.7053232674000105e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 55/71 | LOSS: 5.67993212793486e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 56/71 | LOSS: 5.691012128201628e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 57/71 | LOSS: 5.672740238376969e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 58/71 | LOSS: 5.65549276809364e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 59/71 | LOSS: 5.672030579262355e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 60/71 | LOSS: 5.665322346505461e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 61/71 | LOSS: 5.669689640384487e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 62/71 | LOSS: 5.681973410294934e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 63/71 | LOSS: 5.6823021807872465e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 64/71 | LOSS: 5.659173089392984e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 65/71 | LOSS: 5.643897841792116e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 66/71 | LOSS: 5.6176051128123115e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 67/71 | LOSS: 5.626013309184225e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 68/71 | LOSS: 5.631275526220502e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 69/71 | LOSS: 5.6192448807580925e-06\n",
      "TRAIN: EPOCH 230/1000 | BATCH 70/71 | LOSS: 5.628302031408579e-06\n",
      "VAL: EPOCH 230/1000 | BATCH 0/8 | LOSS: 6.3006082200445235e-06\n",
      "VAL: EPOCH 230/1000 | BATCH 1/8 | LOSS: 5.79648576604086e-06\n",
      "VAL: EPOCH 230/1000 | BATCH 2/8 | LOSS: 5.397239268252936e-06\n",
      "VAL: EPOCH 230/1000 | BATCH 3/8 | LOSS: 5.78787785343593e-06\n",
      "VAL: EPOCH 230/1000 | BATCH 4/8 | LOSS: 5.634260378428735e-06\n",
      "VAL: EPOCH 230/1000 | BATCH 5/8 | LOSS: 5.349742953815924e-06\n",
      "VAL: EPOCH 230/1000 | BATCH 6/8 | LOSS: 5.221839338836227e-06\n",
      "VAL: EPOCH 230/1000 | BATCH 7/8 | LOSS: 5.069059909601492e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 0/71 | LOSS: 6.028015377523843e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 1/71 | LOSS: 6.43341695649724e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 2/71 | LOSS: 6.497804406535579e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 3/71 | LOSS: 6.275135888245131e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 4/71 | LOSS: 6.352729178615845e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 5/71 | LOSS: 6.1448906762962e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 6/71 | LOSS: 6.362210440004544e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 7/71 | LOSS: 6.223928664894629e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 8/71 | LOSS: 6.204980864923628e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 9/71 | LOSS: 6.0478581417555685e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 10/71 | LOSS: 5.855798049610299e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 11/71 | LOSS: 5.807483792826436e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 12/71 | LOSS: 5.708650965411484e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 13/71 | LOSS: 5.606062943895397e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 14/71 | LOSS: 5.655984720457733e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 15/71 | LOSS: 5.6794451950281655e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 16/71 | LOSS: 5.657190232501735e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 17/71 | LOSS: 5.6424835798275635e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 18/71 | LOSS: 5.693578546733509e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 19/71 | LOSS: 5.730964039685204e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 20/71 | LOSS: 5.757686029114882e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 21/71 | LOSS: 5.710066302940091e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 22/71 | LOSS: 5.75114225522038e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 23/71 | LOSS: 5.80433322738827e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 24/71 | LOSS: 5.842178761668038e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 25/71 | LOSS: 5.881226290791976e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 26/71 | LOSS: 5.884186403245958e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 27/71 | LOSS: 5.984687049931381e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 28/71 | LOSS: 5.934093365042294e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 29/71 | LOSS: 5.999847326165764e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 30/71 | LOSS: 5.9360332641283034e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 31/71 | LOSS: 5.993144512217441e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 32/71 | LOSS: 5.973205051387512e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 33/71 | LOSS: 5.976261384014779e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 34/71 | LOSS: 6.046544019357368e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 35/71 | LOSS: 6.010719390461519e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 36/71 | LOSS: 6.011973955783351e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 37/71 | LOSS: 6.022201815587898e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 38/71 | LOSS: 6.052866993764511e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 39/71 | LOSS: 6.0400199913601685e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 40/71 | LOSS: 6.0566791900073e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 41/71 | LOSS: 6.101270520004965e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 42/71 | LOSS: 6.0963422668485925e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 43/71 | LOSS: 6.095600114845596e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 44/71 | LOSS: 6.086163183580437e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 45/71 | LOSS: 6.092559339737383e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 46/71 | LOSS: 6.089130025391066e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 47/71 | LOSS: 6.0637617214827815e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 48/71 | LOSS: 6.048560974203592e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 49/71 | LOSS: 6.016529332555365e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 50/71 | LOSS: 6.021916992633087e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 51/71 | LOSS: 6.027660552158172e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 52/71 | LOSS: 6.021721365302581e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 53/71 | LOSS: 6.07633728577639e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 54/71 | LOSS: 6.047372947002507e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 55/71 | LOSS: 6.11345954634349e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 56/71 | LOSS: 6.09366757169227e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 57/71 | LOSS: 6.090528374662771e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 58/71 | LOSS: 6.110657684954369e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 59/71 | LOSS: 6.0762108432754754e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 60/71 | LOSS: 6.0641433846598615e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 61/71 | LOSS: 6.0580598528758856e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 62/71 | LOSS: 6.032923465681961e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 63/71 | LOSS: 6.0113454551924406e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 64/71 | LOSS: 6.007219053572044e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 65/71 | LOSS: 5.981871033851128e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 66/71 | LOSS: 5.998722935512874e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 67/71 | LOSS: 5.998340825766825e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 68/71 | LOSS: 6.009627497036675e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 69/71 | LOSS: 6.000672906988517e-06\n",
      "TRAIN: EPOCH 231/1000 | BATCH 70/71 | LOSS: 6.0061520728172775e-06\n",
      "VAL: EPOCH 231/1000 | BATCH 0/8 | LOSS: 7.279691544681555e-06\n",
      "VAL: EPOCH 231/1000 | BATCH 1/8 | LOSS: 7.241642606459209e-06\n",
      "VAL: EPOCH 231/1000 | BATCH 2/8 | LOSS: 6.912606901702627e-06\n",
      "VAL: EPOCH 231/1000 | BATCH 3/8 | LOSS: 7.336178214245592e-06\n",
      "VAL: EPOCH 231/1000 | BATCH 4/8 | LOSS: 7.327725961658871e-06\n",
      "VAL: EPOCH 231/1000 | BATCH 5/8 | LOSS: 7.152818473817509e-06\n",
      "VAL: EPOCH 231/1000 | BATCH 6/8 | LOSS: 7.1172767093230505e-06\n",
      "VAL: EPOCH 231/1000 | BATCH 7/8 | LOSS: 7.035514784092811e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 0/71 | LOSS: 7.291185738722561e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 1/71 | LOSS: 7.909892474344815e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 2/71 | LOSS: 7.253805658062144e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 3/71 | LOSS: 6.829010089859366e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 4/71 | LOSS: 6.806236524425913e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 5/71 | LOSS: 6.523212505271658e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 6/71 | LOSS: 6.582387738619998e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 7/71 | LOSS: 6.603286010431475e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 8/71 | LOSS: 6.361589688215948e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 9/71 | LOSS: 6.1084892422513805e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 10/71 | LOSS: 6.12270650890423e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 11/71 | LOSS: 5.957549319646205e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 12/71 | LOSS: 5.869462746396983e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 13/71 | LOSS: 5.76541093063757e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 14/71 | LOSS: 5.710070854547666e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 15/71 | LOSS: 5.646854475571672e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 16/71 | LOSS: 5.634494032967589e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 17/71 | LOSS: 5.641154024892279e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 18/71 | LOSS: 5.5923855574005035e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 19/71 | LOSS: 5.553416940529132e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 20/71 | LOSS: 5.511054669547039e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 21/71 | LOSS: 5.5042105486791115e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 22/71 | LOSS: 5.496751604685256e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 23/71 | LOSS: 5.443222354036455e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 24/71 | LOSS: 5.4405849550676066e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 25/71 | LOSS: 5.404696804152291e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 26/71 | LOSS: 5.361610975443101e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 27/71 | LOSS: 5.341782509406455e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 28/71 | LOSS: 5.328499528018413e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 29/71 | LOSS: 5.338578542553781e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 30/71 | LOSS: 5.3271017555568e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 31/71 | LOSS: 5.287299643441656e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 32/71 | LOSS: 5.332645769433866e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 33/71 | LOSS: 5.332746749285006e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 34/71 | LOSS: 5.3619882237399e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 35/71 | LOSS: 5.352179515385716e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 36/71 | LOSS: 5.326898251557911e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 37/71 | LOSS: 5.3549659864266256e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 38/71 | LOSS: 5.36572599976819e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 39/71 | LOSS: 5.427125756796158e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 40/71 | LOSS: 5.4135669903721795e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 41/71 | LOSS: 5.456589860841632e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 42/71 | LOSS: 5.498432772075703e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 43/71 | LOSS: 5.4771965096733766e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 44/71 | LOSS: 5.506222947183738e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 45/71 | LOSS: 5.512788942249482e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 46/71 | LOSS: 5.500401710649269e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 47/71 | LOSS: 5.5218595775841095e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 48/71 | LOSS: 5.492223742609839e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 49/71 | LOSS: 5.511520002983161e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 50/71 | LOSS: 5.542823877405288e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 51/71 | LOSS: 5.5384240602926675e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 52/71 | LOSS: 5.539815558805921e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 53/71 | LOSS: 5.529334682958304e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 54/71 | LOSS: 5.558076794510601e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 55/71 | LOSS: 5.541388345266439e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 56/71 | LOSS: 5.573535386167605e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 57/71 | LOSS: 5.5585828704352025e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 58/71 | LOSS: 5.54743039205185e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 59/71 | LOSS: 5.552028983402124e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 60/71 | LOSS: 5.562217874747327e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 61/71 | LOSS: 5.556646893608324e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 62/71 | LOSS: 5.544188131771368e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 63/71 | LOSS: 5.5220240184894465e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 64/71 | LOSS: 5.521308958122973e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 65/71 | LOSS: 5.5120599840434314e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 66/71 | LOSS: 5.502995728461225e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 67/71 | LOSS: 5.51966386678139e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 68/71 | LOSS: 5.536766260827763e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 69/71 | LOSS: 5.542894645519222e-06\n",
      "TRAIN: EPOCH 232/1000 | BATCH 70/71 | LOSS: 5.547847598483688e-06\n",
      "VAL: EPOCH 232/1000 | BATCH 0/8 | LOSS: 8.407890163653065e-06\n",
      "VAL: EPOCH 232/1000 | BATCH 1/8 | LOSS: 7.3848796091624536e-06\n",
      "VAL: EPOCH 232/1000 | BATCH 2/8 | LOSS: 7.953493271391684e-06\n",
      "VAL: EPOCH 232/1000 | BATCH 3/8 | LOSS: 7.693852012380376e-06\n",
      "VAL: EPOCH 232/1000 | BATCH 4/8 | LOSS: 7.837686280254274e-06\n",
      "VAL: EPOCH 232/1000 | BATCH 5/8 | LOSS: 7.643227794081517e-06\n",
      "VAL: EPOCH 232/1000 | BATCH 6/8 | LOSS: 7.493074917874765e-06\n",
      "VAL: EPOCH 232/1000 | BATCH 7/8 | LOSS: 7.636257350895903e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 0/71 | LOSS: 7.382714557024883e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 1/71 | LOSS: 5.979441084491555e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 2/71 | LOSS: 7.132179234758951e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 3/71 | LOSS: 6.58859380564536e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 4/71 | LOSS: 6.858537744847126e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 5/71 | LOSS: 6.75085281424496e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 6/71 | LOSS: 6.988376558833157e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 7/71 | LOSS: 6.742202685927623e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 8/71 | LOSS: 6.590693879439237e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 9/71 | LOSS: 6.640467381657799e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 10/71 | LOSS: 6.531172485582911e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 11/71 | LOSS: 6.502445141146988e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 12/71 | LOSS: 6.414885459749852e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 13/71 | LOSS: 6.38565256849688e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 14/71 | LOSS: 6.328767631202936e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 15/71 | LOSS: 6.193567770651498e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 16/71 | LOSS: 6.176983761714141e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 17/71 | LOSS: 6.108631522591976e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 18/71 | LOSS: 6.046139381061566e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 19/71 | LOSS: 5.982092943668249e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 20/71 | LOSS: 5.916785279883438e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 21/71 | LOSS: 5.908679564857845e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 22/71 | LOSS: 5.8443986587087196e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 23/71 | LOSS: 5.790413410977635e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 24/71 | LOSS: 5.773791945102857e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 25/71 | LOSS: 5.726843937736703e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 26/71 | LOSS: 5.673619330924272e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 27/71 | LOSS: 5.6106368739554976e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 28/71 | LOSS: 5.595482324961581e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 29/71 | LOSS: 5.582562334893737e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 30/71 | LOSS: 5.569944850351256e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 31/71 | LOSS: 5.545652371097276e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 32/71 | LOSS: 5.554745739848868e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 33/71 | LOSS: 5.5165394849823584e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 34/71 | LOSS: 5.503353310944346e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 35/71 | LOSS: 5.49430461028856e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 36/71 | LOSS: 5.457138286146801e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 37/71 | LOSS: 5.4676126015501244e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 38/71 | LOSS: 5.440501692460086e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 39/71 | LOSS: 5.478107595990878e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 40/71 | LOSS: 5.45973482697197e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 41/71 | LOSS: 5.5141523435373705e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 42/71 | LOSS: 5.4981702665235245e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 43/71 | LOSS: 5.533264532625088e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 44/71 | LOSS: 5.55007630585654e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 45/71 | LOSS: 5.542732443918004e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 46/71 | LOSS: 5.518476011778322e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 47/71 | LOSS: 5.513874365684994e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 48/71 | LOSS: 5.540094485806957e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 49/71 | LOSS: 5.556721316679614e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 50/71 | LOSS: 5.655066624789399e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 51/71 | LOSS: 5.6340277559194455e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 52/71 | LOSS: 5.678207421538592e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 53/71 | LOSS: 5.662782895241334e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 54/71 | LOSS: 5.670699473822341e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 55/71 | LOSS: 5.680162912215435e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 56/71 | LOSS: 5.6646444646870355e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 57/71 | LOSS: 5.728595781341749e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 58/71 | LOSS: 5.719422114129036e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 59/71 | LOSS: 5.75489019259597e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 60/71 | LOSS: 5.737156004856768e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 61/71 | LOSS: 5.781705347603307e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 62/71 | LOSS: 5.83546465176845e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 63/71 | LOSS: 5.872741553503147e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 64/71 | LOSS: 5.9174945468280035e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 65/71 | LOSS: 5.910370336178775e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 66/71 | LOSS: 5.9597886958265494e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 67/71 | LOSS: 5.965508775903693e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 68/71 | LOSS: 6.031401610833701e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 69/71 | LOSS: 6.054096944093804e-06\n",
      "TRAIN: EPOCH 233/1000 | BATCH 70/71 | LOSS: 6.080247517339033e-06\n",
      "VAL: EPOCH 233/1000 | BATCH 0/8 | LOSS: 1.5586567315040156e-05\n",
      "VAL: EPOCH 233/1000 | BATCH 1/8 | LOSS: 1.537875641588471e-05\n",
      "VAL: EPOCH 233/1000 | BATCH 2/8 | LOSS: 1.4481201712139105e-05\n",
      "VAL: EPOCH 233/1000 | BATCH 3/8 | LOSS: 1.4958483689042623e-05\n",
      "VAL: EPOCH 233/1000 | BATCH 4/8 | LOSS: 1.4712815027451142e-05\n",
      "VAL: EPOCH 233/1000 | BATCH 5/8 | LOSS: 1.4120365449343808e-05\n",
      "VAL: EPOCH 233/1000 | BATCH 6/8 | LOSS: 1.4307845309044101e-05\n",
      "VAL: EPOCH 233/1000 | BATCH 7/8 | LOSS: 1.3868918017578835e-05\n",
      "TRAIN: EPOCH 234/1000 | BATCH 0/71 | LOSS: 1.4970405572967138e-05\n",
      "TRAIN: EPOCH 234/1000 | BATCH 1/71 | LOSS: 1.0556041615927825e-05\n",
      "TRAIN: EPOCH 234/1000 | BATCH 2/71 | LOSS: 1.3295122395599416e-05\n",
      "TRAIN: EPOCH 234/1000 | BATCH 3/71 | LOSS: 1.1468226716715435e-05\n",
      "TRAIN: EPOCH 234/1000 | BATCH 4/71 | LOSS: 1.1557593552424805e-05\n",
      "TRAIN: EPOCH 234/1000 | BATCH 5/71 | LOSS: 1.1056938774345326e-05\n",
      "TRAIN: EPOCH 234/1000 | BATCH 6/71 | LOSS: 1.0646456952859548e-05\n",
      "TRAIN: EPOCH 234/1000 | BATCH 7/71 | LOSS: 1.054741602501963e-05\n",
      "TRAIN: EPOCH 234/1000 | BATCH 8/71 | LOSS: 9.884715483268439e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 9/71 | LOSS: 9.800305497265071e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 10/71 | LOSS: 9.45468776659585e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 11/71 | LOSS: 9.31210346758841e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 12/71 | LOSS: 9.357536642850251e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 13/71 | LOSS: 9.13758559103631e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 14/71 | LOSS: 8.911184674313214e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 15/71 | LOSS: 8.63753649582577e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 16/71 | LOSS: 8.544518389980486e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 17/71 | LOSS: 8.360462415011069e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 18/71 | LOSS: 8.179615825180295e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 19/71 | LOSS: 8.03856962647842e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 20/71 | LOSS: 7.934287406810437e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 21/71 | LOSS: 7.824412746644652e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 22/71 | LOSS: 7.738295212418407e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 23/71 | LOSS: 7.795258890534265e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 24/71 | LOSS: 7.713851482549217e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 25/71 | LOSS: 7.733725676819002e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 26/71 | LOSS: 7.700625819442965e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 27/71 | LOSS: 7.598988109488605e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 28/71 | LOSS: 7.497787982143124e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 29/71 | LOSS: 7.487996814840395e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 30/71 | LOSS: 7.425817667324502e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 31/71 | LOSS: 7.4368066691477e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 32/71 | LOSS: 7.357456221749696e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 33/71 | LOSS: 7.308655888075865e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 34/71 | LOSS: 7.264220403158106e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 35/71 | LOSS: 7.229650110376598e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 36/71 | LOSS: 7.18345734492539e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 37/71 | LOSS: 7.136904693987545e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 38/71 | LOSS: 7.151317813389678e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 39/71 | LOSS: 7.102438314632309e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 40/71 | LOSS: 7.084710543450655e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 41/71 | LOSS: 7.079845642822745e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 42/71 | LOSS: 7.138397460610492e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 43/71 | LOSS: 7.101735480897663e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 44/71 | LOSS: 7.126647111969456e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 45/71 | LOSS: 7.098847885097949e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 46/71 | LOSS: 7.082351919265459e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 47/71 | LOSS: 7.066195877314385e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 48/71 | LOSS: 7.05194612719981e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 49/71 | LOSS: 7.069403172863531e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 50/71 | LOSS: 7.025725849149132e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 51/71 | LOSS: 6.980407909815347e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 52/71 | LOSS: 6.959080436436639e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 53/71 | LOSS: 6.950876131331488e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 54/71 | LOSS: 6.914444359360707e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 55/71 | LOSS: 6.883826464283109e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 56/71 | LOSS: 6.890405186647529e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 57/71 | LOSS: 6.860586828569749e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 58/71 | LOSS: 6.837458925674891e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 59/71 | LOSS: 6.817754888288619e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 60/71 | LOSS: 6.796086105824826e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 61/71 | LOSS: 6.770423778429939e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 62/71 | LOSS: 6.744561290808691e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 63/71 | LOSS: 6.721687306310287e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 64/71 | LOSS: 6.699504571700415e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 65/71 | LOSS: 6.684526362843109e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 66/71 | LOSS: 6.693340948452294e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 67/71 | LOSS: 6.660960553638099e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 68/71 | LOSS: 6.634255408942514e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 69/71 | LOSS: 6.617101312908094e-06\n",
      "TRAIN: EPOCH 234/1000 | BATCH 70/71 | LOSS: 6.629926721076809e-06\n",
      "VAL: EPOCH 234/1000 | BATCH 0/8 | LOSS: 7.92248647485394e-06\n",
      "VAL: EPOCH 234/1000 | BATCH 1/8 | LOSS: 7.240349532366963e-06\n",
      "VAL: EPOCH 234/1000 | BATCH 2/8 | LOSS: 7.746395264499975e-06\n",
      "VAL: EPOCH 234/1000 | BATCH 3/8 | LOSS: 7.85764655120147e-06\n",
      "VAL: EPOCH 234/1000 | BATCH 4/8 | LOSS: 7.922338045318612e-06\n",
      "VAL: EPOCH 234/1000 | BATCH 5/8 | LOSS: 7.876418446054837e-06\n",
      "VAL: EPOCH 234/1000 | BATCH 6/8 | LOSS: 7.719684357912879e-06\n",
      "VAL: EPOCH 234/1000 | BATCH 7/8 | LOSS: 7.834244684090663e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 0/71 | LOSS: 6.931354164407821e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 1/71 | LOSS: 5.75756530452054e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 2/71 | LOSS: 6.163392299640691e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 3/71 | LOSS: 6.126367566139379e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 4/71 | LOSS: 5.990600129734958e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 5/71 | LOSS: 6.250189092800913e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 6/71 | LOSS: 6.0744880231920566e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 7/71 | LOSS: 6.084407743855991e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 8/71 | LOSS: 6.0759584307056175e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 9/71 | LOSS: 6.052786739019212e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 10/71 | LOSS: 5.8995379235553136e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 11/71 | LOSS: 5.759095453565048e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 12/71 | LOSS: 5.689978374553343e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 13/71 | LOSS: 5.76315259032916e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 14/71 | LOSS: 5.695943036698736e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 15/71 | LOSS: 5.687991404101922e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 16/71 | LOSS: 5.672422102019054e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 17/71 | LOSS: 5.761288270554764e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 18/71 | LOSS: 5.8016756060839584e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 19/71 | LOSS: 5.770907637270284e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 20/71 | LOSS: 5.983424498048234e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 21/71 | LOSS: 5.983024348924201e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 22/71 | LOSS: 6.168854408473556e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 23/71 | LOSS: 6.123644368472014e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 24/71 | LOSS: 6.4440790993103295e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 25/71 | LOSS: 6.442284284975568e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 26/71 | LOSS: 6.37616272393239e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 27/71 | LOSS: 6.4064563827871875e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 28/71 | LOSS: 6.4334572329996795e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 29/71 | LOSS: 6.389603231582441e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 30/71 | LOSS: 6.3690087641119925e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 31/71 | LOSS: 6.438248021822801e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 32/71 | LOSS: 6.430945425315506e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 33/71 | LOSS: 6.4217499317130525e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 34/71 | LOSS: 6.398898105024793e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 35/71 | LOSS: 6.378167452365031e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 36/71 | LOSS: 6.413624932237498e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 37/71 | LOSS: 6.409408534206082e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 38/71 | LOSS: 6.365168141760828e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 39/71 | LOSS: 6.386824395576696e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 40/71 | LOSS: 6.35914032655575e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 41/71 | LOSS: 6.336532611125481e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 42/71 | LOSS: 6.3039522734301455e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 43/71 | LOSS: 6.263622284760376e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 44/71 | LOSS: 6.244248763525522e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 45/71 | LOSS: 6.192977183593979e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 46/71 | LOSS: 6.174469741923298e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 47/71 | LOSS: 6.194203857982454e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 48/71 | LOSS: 6.183005163100981e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 49/71 | LOSS: 6.186261443872354e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 50/71 | LOSS: 6.191908864018521e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 51/71 | LOSS: 6.162829418616849e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 52/71 | LOSS: 6.145263170147477e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 53/71 | LOSS: 6.115577064149504e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 54/71 | LOSS: 6.128948189143557e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 55/71 | LOSS: 6.101549404645214e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 56/71 | LOSS: 6.089040908320162e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 57/71 | LOSS: 6.065261906205078e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 58/71 | LOSS: 6.052285143442085e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 59/71 | LOSS: 6.015107722608567e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 60/71 | LOSS: 6.015916183134293e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 61/71 | LOSS: 5.989065746161355e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 62/71 | LOSS: 5.986138314746934e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 63/71 | LOSS: 5.986735850171954e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 64/71 | LOSS: 5.977493450560499e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 65/71 | LOSS: 5.987286330668157e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 66/71 | LOSS: 5.964957992220471e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 67/71 | LOSS: 5.950788543968349e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 68/71 | LOSS: 5.950080888568124e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 69/71 | LOSS: 5.923175474111174e-06\n",
      "TRAIN: EPOCH 235/1000 | BATCH 70/71 | LOSS: 5.941718113653749e-06\n",
      "VAL: EPOCH 235/1000 | BATCH 0/8 | LOSS: 6.867274805699708e-06\n",
      "VAL: EPOCH 235/1000 | BATCH 1/8 | LOSS: 5.9051128573628375e-06\n",
      "VAL: EPOCH 235/1000 | BATCH 2/8 | LOSS: 5.790689556306461e-06\n",
      "VAL: EPOCH 235/1000 | BATCH 3/8 | LOSS: 5.956608447377221e-06\n",
      "VAL: EPOCH 235/1000 | BATCH 4/8 | LOSS: 5.915481051488314e-06\n",
      "VAL: EPOCH 235/1000 | BATCH 5/8 | LOSS: 5.805084432116321e-06\n",
      "VAL: EPOCH 235/1000 | BATCH 6/8 | LOSS: 5.664755333002956e-06\n",
      "VAL: EPOCH 235/1000 | BATCH 7/8 | LOSS: 5.641987286253425e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 0/71 | LOSS: 5.903318196942564e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 1/71 | LOSS: 5.939042694080854e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 2/71 | LOSS: 5.492805181953979e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 3/71 | LOSS: 5.387078431340342e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 4/71 | LOSS: 5.508651338459458e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 5/71 | LOSS: 5.463321410085579e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 6/71 | LOSS: 5.5219246470577284e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 7/71 | LOSS: 5.501631960669329e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 8/71 | LOSS: 5.462146418722114e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 9/71 | LOSS: 5.327013650457957e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 10/71 | LOSS: 5.19896002515452e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 11/71 | LOSS: 5.372709514025094e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 12/71 | LOSS: 5.325342779262708e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 13/71 | LOSS: 5.248506373131282e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 14/71 | LOSS: 5.22840058086634e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 15/71 | LOSS: 5.154491105940906e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 16/71 | LOSS: 5.113099900079439e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 17/71 | LOSS: 5.128427068888817e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 18/71 | LOSS: 5.055017027189024e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 19/71 | LOSS: 5.027795305068139e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 20/71 | LOSS: 5.020942158548028e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 21/71 | LOSS: 5.0304637707912745e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 22/71 | LOSS: 5.001614602445878e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 23/71 | LOSS: 4.979141484303303e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 24/71 | LOSS: 4.995367380615789e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 25/71 | LOSS: 4.991826647081045e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 26/71 | LOSS: 5.057916984730616e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 27/71 | LOSS: 5.0883068557335975e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 28/71 | LOSS: 5.1410575624372695e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 29/71 | LOSS: 5.1444535074551824e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 30/71 | LOSS: 5.171587329846995e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 31/71 | LOSS: 5.175401170731675e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 32/71 | LOSS: 5.208429174121138e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 33/71 | LOSS: 5.243466509508674e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 34/71 | LOSS: 5.2690880303479e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 35/71 | LOSS: 5.2406191268043285e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 36/71 | LOSS: 5.260572293450791e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 37/71 | LOSS: 5.290235800490145e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 38/71 | LOSS: 5.314790588528968e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 39/71 | LOSS: 5.296793119669019e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 40/71 | LOSS: 5.304290070631484e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 41/71 | LOSS: 5.288755508965031e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 42/71 | LOSS: 5.271565230434008e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 43/71 | LOSS: 5.248855204213627e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 44/71 | LOSS: 5.230084601256142e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 45/71 | LOSS: 5.208705517915128e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 46/71 | LOSS: 5.228311112388953e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 47/71 | LOSS: 5.216770574634211e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 48/71 | LOSS: 5.218985938417787e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 49/71 | LOSS: 5.203258360779728e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 50/71 | LOSS: 5.219391582881251e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 51/71 | LOSS: 5.212184524120507e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 52/71 | LOSS: 5.193615019173726e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 53/71 | LOSS: 5.191913487578859e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 54/71 | LOSS: 5.204537997501162e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 55/71 | LOSS: 5.2035166829357745e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 56/71 | LOSS: 5.225789616241328e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 57/71 | LOSS: 5.23305184601511e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 58/71 | LOSS: 5.25640611528415e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 59/71 | LOSS: 5.2532479685396535e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 60/71 | LOSS: 5.268133046214285e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 61/71 | LOSS: 5.268814198467636e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 62/71 | LOSS: 5.291608279852274e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 63/71 | LOSS: 5.293421004637366e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 64/71 | LOSS: 5.314780992193846e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 65/71 | LOSS: 5.307496435720664e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 66/71 | LOSS: 5.308098683490787e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 67/71 | LOSS: 5.310119858597104e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 68/71 | LOSS: 5.30588980494202e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 69/71 | LOSS: 5.295551428779228e-06\n",
      "TRAIN: EPOCH 236/1000 | BATCH 70/71 | LOSS: 5.302641633354885e-06\n",
      "VAL: EPOCH 236/1000 | BATCH 0/8 | LOSS: 7.090247436281061e-06\n",
      "VAL: EPOCH 236/1000 | BATCH 1/8 | LOSS: 6.8767571974603925e-06\n",
      "VAL: EPOCH 236/1000 | BATCH 2/8 | LOSS: 6.288554686761927e-06\n",
      "VAL: EPOCH 236/1000 | BATCH 3/8 | LOSS: 6.576219448106713e-06\n",
      "VAL: EPOCH 236/1000 | BATCH 4/8 | LOSS: 6.5421192630310545e-06\n",
      "VAL: EPOCH 236/1000 | BATCH 5/8 | LOSS: 6.174673520339032e-06\n",
      "VAL: EPOCH 236/1000 | BATCH 6/8 | LOSS: 6.099774119710284e-06\n",
      "VAL: EPOCH 236/1000 | BATCH 7/8 | LOSS: 5.981420656553382e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 0/71 | LOSS: 5.523696017917246e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 1/71 | LOSS: 5.787446298199939e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 2/71 | LOSS: 5.6731044727105955e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 3/71 | LOSS: 5.821900003866176e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 4/71 | LOSS: 5.533733019547071e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 5/71 | LOSS: 5.4219151479628636e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 6/71 | LOSS: 5.400216715705548e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 7/71 | LOSS: 5.46359012787434e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 8/71 | LOSS: 5.570217581407633e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 9/71 | LOSS: 5.513244695976028e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 10/71 | LOSS: 5.783186969148863e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 11/71 | LOSS: 5.804864523876556e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 12/71 | LOSS: 5.848449046215347e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 13/71 | LOSS: 5.870844526205994e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 14/71 | LOSS: 5.863092186094339e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 15/71 | LOSS: 6.054039687342083e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 16/71 | LOSS: 5.928945537401459e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 17/71 | LOSS: 5.948538702998323e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 18/71 | LOSS: 6.054473846904186e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 19/71 | LOSS: 6.039647564648476e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 20/71 | LOSS: 5.995071638781588e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 21/71 | LOSS: 5.980845220289087e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 22/71 | LOSS: 5.916123684627287e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 23/71 | LOSS: 5.8989720628233044e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 24/71 | LOSS: 5.874650378245861e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 25/71 | LOSS: 5.812077810892683e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 26/71 | LOSS: 5.763572146490871e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 27/71 | LOSS: 5.71549883586288e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 28/71 | LOSS: 5.7083647616623084e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 29/71 | LOSS: 5.685485499877056e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 30/71 | LOSS: 5.660700583487802e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 31/71 | LOSS: 5.614622210714515e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 32/71 | LOSS: 5.68987726375891e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 33/71 | LOSS: 5.6649232379534034e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 34/71 | LOSS: 5.667809429204291e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 35/71 | LOSS: 5.6311412082423985e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 36/71 | LOSS: 5.648216867140601e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 37/71 | LOSS: 5.591869323420963e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 38/71 | LOSS: 5.596373378070078e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 39/71 | LOSS: 5.600487168067048e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 40/71 | LOSS: 5.592801410718338e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 41/71 | LOSS: 5.572762423374418e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 42/71 | LOSS: 5.5531420827698405e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 43/71 | LOSS: 5.526421990576967e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 44/71 | LOSS: 5.542100785128746e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 45/71 | LOSS: 5.533845039636894e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 46/71 | LOSS: 5.553970861504414e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 47/71 | LOSS: 5.569274748040698e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 48/71 | LOSS: 5.534179178669417e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 49/71 | LOSS: 5.525666147150332e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 50/71 | LOSS: 5.523134306856139e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 51/71 | LOSS: 5.544630338450728e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 52/71 | LOSS: 5.55551904018909e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 53/71 | LOSS: 5.561793033040077e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 54/71 | LOSS: 5.531915725110925e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 55/71 | LOSS: 5.529883407819268e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 56/71 | LOSS: 5.553594028721196e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 57/71 | LOSS: 5.569763210399382e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 58/71 | LOSS: 5.574119552613287e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 59/71 | LOSS: 5.589358655318695e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 60/71 | LOSS: 5.578762746333014e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 61/71 | LOSS: 5.585022130793321e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 62/71 | LOSS: 5.585134507176311e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 63/71 | LOSS: 5.595717908590814e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 64/71 | LOSS: 5.5884972075895905e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 65/71 | LOSS: 5.584716628811403e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 66/71 | LOSS: 5.582387007391966e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 67/71 | LOSS: 5.589914100228805e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 68/71 | LOSS: 5.567946316106195e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 69/71 | LOSS: 5.5675803650956365e-06\n",
      "TRAIN: EPOCH 237/1000 | BATCH 70/71 | LOSS: 5.566599982967731e-06\n",
      "VAL: EPOCH 237/1000 | BATCH 0/8 | LOSS: 5.722780315409182e-06\n",
      "VAL: EPOCH 237/1000 | BATCH 1/8 | LOSS: 5.251131142358645e-06\n",
      "VAL: EPOCH 237/1000 | BATCH 2/8 | LOSS: 5.261380313944149e-06\n",
      "VAL: EPOCH 237/1000 | BATCH 3/8 | LOSS: 5.7395088788325666e-06\n",
      "VAL: EPOCH 237/1000 | BATCH 4/8 | LOSS: 5.658672671415843e-06\n",
      "VAL: EPOCH 237/1000 | BATCH 5/8 | LOSS: 5.5406468012127634e-06\n",
      "VAL: EPOCH 237/1000 | BATCH 6/8 | LOSS: 5.461534198860006e-06\n",
      "VAL: EPOCH 237/1000 | BATCH 7/8 | LOSS: 5.3990029300621245e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 0/71 | LOSS: 4.971705038769869e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 1/71 | LOSS: 5.455584414448822e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 2/71 | LOSS: 5.090364538773429e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 3/71 | LOSS: 4.8104031975526595e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 4/71 | LOSS: 4.757552233058959e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 5/71 | LOSS: 4.728819855396675e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 6/71 | LOSS: 4.907753918814706e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 7/71 | LOSS: 4.921344896047231e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 8/71 | LOSS: 4.905385493556322e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 9/71 | LOSS: 5.045476791565307e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 10/71 | LOSS: 5.252722158647058e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 11/71 | LOSS: 5.17752512981436e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 12/71 | LOSS: 5.429816365694233e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 13/71 | LOSS: 5.396878740222226e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 14/71 | LOSS: 5.441211518094254e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 15/71 | LOSS: 5.3955643579683965e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 16/71 | LOSS: 5.404474583937211e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 17/71 | LOSS: 5.3981665991563605e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 18/71 | LOSS: 5.362399765305638e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 19/71 | LOSS: 5.337513835002028e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 20/71 | LOSS: 5.291261350932819e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 21/71 | LOSS: 5.217298084145031e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 22/71 | LOSS: 5.2996702320144875e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 23/71 | LOSS: 5.311104056697029e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 24/71 | LOSS: 5.375905238906853e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 25/71 | LOSS: 5.384822543419432e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 26/71 | LOSS: 5.491608977468719e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 27/71 | LOSS: 5.4912569242203195e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 28/71 | LOSS: 5.579837855701294e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 29/71 | LOSS: 5.655314801818653e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 30/71 | LOSS: 5.660076098020879e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 31/71 | LOSS: 5.662462768896148e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 32/71 | LOSS: 5.67253534951085e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 33/71 | LOSS: 5.6930794551804225e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 34/71 | LOSS: 5.726929936957147e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 35/71 | LOSS: 5.724942411688971e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 36/71 | LOSS: 5.715157188918769e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 37/71 | LOSS: 5.7833606086588635e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 38/71 | LOSS: 5.791405468559018e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 39/71 | LOSS: 5.800929602628457e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 40/71 | LOSS: 5.818196169878697e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 41/71 | LOSS: 5.924030617842661e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 42/71 | LOSS: 5.90850128034173e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 43/71 | LOSS: 5.984345327967524e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 44/71 | LOSS: 5.977361049897607e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 45/71 | LOSS: 6.029643096237281e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 46/71 | LOSS: 6.027472080220716e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 47/71 | LOSS: 6.03746821070672e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 48/71 | LOSS: 6.011175359236979e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 49/71 | LOSS: 6.013029387759162e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 50/71 | LOSS: 6.01984546686735e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 51/71 | LOSS: 5.9851281286263165e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 52/71 | LOSS: 5.991546990940752e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 53/71 | LOSS: 5.995251875127131e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 54/71 | LOSS: 5.978869839137505e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 55/71 | LOSS: 5.978513261847443e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 56/71 | LOSS: 5.964307882478399e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 57/71 | LOSS: 5.938932323762605e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 58/71 | LOSS: 5.917661907252465e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 59/71 | LOSS: 5.9054040624081e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 60/71 | LOSS: 5.900083750497824e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 61/71 | LOSS: 5.888929029923372e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 62/71 | LOSS: 5.87161106782982e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 63/71 | LOSS: 5.8580940347496835e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 64/71 | LOSS: 5.8432324504582745e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 65/71 | LOSS: 5.82756394413247e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 66/71 | LOSS: 5.808717329517692e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 67/71 | LOSS: 5.786654656624117e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 68/71 | LOSS: 5.794295386453007e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 69/71 | LOSS: 5.788916684780685e-06\n",
      "TRAIN: EPOCH 238/1000 | BATCH 70/71 | LOSS: 5.792646129200772e-06\n",
      "VAL: EPOCH 238/1000 | BATCH 0/8 | LOSS: 5.8988025557482615e-06\n",
      "VAL: EPOCH 238/1000 | BATCH 1/8 | LOSS: 5.2061550377402455e-06\n",
      "VAL: EPOCH 238/1000 | BATCH 2/8 | LOSS: 4.999780079136447e-06\n",
      "VAL: EPOCH 238/1000 | BATCH 3/8 | LOSS: 5.318760031514103e-06\n",
      "VAL: EPOCH 238/1000 | BATCH 4/8 | LOSS: 5.235466869635275e-06\n",
      "VAL: EPOCH 238/1000 | BATCH 5/8 | LOSS: 4.959802443712154e-06\n",
      "VAL: EPOCH 238/1000 | BATCH 6/8 | LOSS: 4.861860718717383e-06\n",
      "VAL: EPOCH 238/1000 | BATCH 7/8 | LOSS: 4.782413299153632e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 0/71 | LOSS: 3.249630935897585e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 1/71 | LOSS: 5.712714937544661e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 2/71 | LOSS: 5.371552106225863e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 3/71 | LOSS: 5.634232024931407e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 4/71 | LOSS: 5.977994806016795e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 5/71 | LOSS: 5.6416635440352065e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 6/71 | LOSS: 5.943257617348406e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 7/71 | LOSS: 5.923981461819494e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 8/71 | LOSS: 5.985351082118642e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 9/71 | LOSS: 6.117151497164741e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 10/71 | LOSS: 6.061070135646415e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 11/71 | LOSS: 6.417501026589889e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 12/71 | LOSS: 6.311882316367701e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 13/71 | LOSS: 6.530718694973205e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 14/71 | LOSS: 6.439514193819681e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 15/71 | LOSS: 6.655550578216207e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 16/71 | LOSS: 6.611256166626631e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 17/71 | LOSS: 6.650229200507359e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 18/71 | LOSS: 6.608772046571462e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 19/71 | LOSS: 6.6326455680609795e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 20/71 | LOSS: 6.555682527202104e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 21/71 | LOSS: 6.54277571563646e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 22/71 | LOSS: 6.492635589972411e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 23/71 | LOSS: 6.428922972645523e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 24/71 | LOSS: 6.408934186765691e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 25/71 | LOSS: 6.3402888792132635e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 26/71 | LOSS: 6.289162953449974e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 27/71 | LOSS: 6.2473881468706945e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 28/71 | LOSS: 6.224795266336577e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 29/71 | LOSS: 6.234272662671477e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 30/71 | LOSS: 6.287774442992692e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 31/71 | LOSS: 6.358996884614498e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 32/71 | LOSS: 6.3778891010522125e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 33/71 | LOSS: 6.3501834521208795e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 34/71 | LOSS: 6.427655366029025e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 35/71 | LOSS: 6.400269007321589e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 36/71 | LOSS: 6.463540090850516e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 37/71 | LOSS: 6.434812875292999e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 38/71 | LOSS: 6.515499290589315e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 39/71 | LOSS: 6.5299850120936755e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 40/71 | LOSS: 6.532841802609477e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 41/71 | LOSS: 6.623942519062049e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 42/71 | LOSS: 6.597712050533182e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 43/71 | LOSS: 6.5478767820340265e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 44/71 | LOSS: 6.558695349667687e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 45/71 | LOSS: 6.516556546241349e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 46/71 | LOSS: 6.511605785449996e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 47/71 | LOSS: 6.4699819214789995e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 48/71 | LOSS: 6.486391166803588e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 49/71 | LOSS: 6.455216544054565e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 50/71 | LOSS: 6.434888220218115e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 51/71 | LOSS: 6.427648250232428e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 52/71 | LOSS: 6.425881565519376e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 53/71 | LOSS: 6.426154043395029e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 54/71 | LOSS: 6.41187615738917e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 55/71 | LOSS: 6.384654008278241e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 56/71 | LOSS: 6.347025813930056e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 57/71 | LOSS: 6.314269144084015e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 58/71 | LOSS: 6.2890548626024086e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 59/71 | LOSS: 6.269860114116454e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 60/71 | LOSS: 6.254467645375684e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 61/71 | LOSS: 6.2518556000389755e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 62/71 | LOSS: 6.238156192777087e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 63/71 | LOSS: 6.214258611692003e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 64/71 | LOSS: 6.2047681379996135e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 65/71 | LOSS: 6.197784428381552e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 66/71 | LOSS: 6.181994839567072e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 67/71 | LOSS: 6.155484243208775e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 68/71 | LOSS: 6.143180427529117e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 69/71 | LOSS: 6.12291910978716e-06\n",
      "TRAIN: EPOCH 239/1000 | BATCH 70/71 | LOSS: 6.108898637200621e-06\n",
      "VAL: EPOCH 239/1000 | BATCH 0/8 | LOSS: 7.344836376432795e-06\n",
      "VAL: EPOCH 239/1000 | BATCH 1/8 | LOSS: 6.337077138596214e-06\n",
      "VAL: EPOCH 239/1000 | BATCH 2/8 | LOSS: 5.789993338112254e-06\n",
      "VAL: EPOCH 239/1000 | BATCH 3/8 | LOSS: 6.136929300737393e-06\n",
      "VAL: EPOCH 239/1000 | BATCH 4/8 | LOSS: 5.957439316262025e-06\n",
      "VAL: EPOCH 239/1000 | BATCH 5/8 | LOSS: 5.713820125189765e-06\n",
      "VAL: EPOCH 239/1000 | BATCH 6/8 | LOSS: 5.660845610171756e-06\n",
      "VAL: EPOCH 239/1000 | BATCH 7/8 | LOSS: 5.523771335447236e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 0/71 | LOSS: 5.874589987797663e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 1/71 | LOSS: 5.165484253666364e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 2/71 | LOSS: 5.520715603779536e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 3/71 | LOSS: 5.3381679663289106e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 4/71 | LOSS: 5.444035195978358e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 5/71 | LOSS: 5.50827250359968e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 6/71 | LOSS: 5.498514058542371e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 7/71 | LOSS: 6.027805852681922e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 8/71 | LOSS: 5.924163916562166e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 9/71 | LOSS: 6.1112519688322205e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 10/71 | LOSS: 6.004910078031984e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 11/71 | LOSS: 6.121711900656616e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 12/71 | LOSS: 6.438352102122735e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 13/71 | LOSS: 6.418140628738911e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 14/71 | LOSS: 6.667528668913292e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 15/71 | LOSS: 6.546079873714916e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 16/71 | LOSS: 6.840632201655192e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 17/71 | LOSS: 6.8058519799605065e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 18/71 | LOSS: 6.84277268335695e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 19/71 | LOSS: 6.887773156449839e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 20/71 | LOSS: 6.903331636532953e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 21/71 | LOSS: 7.004871452987903e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 22/71 | LOSS: 6.921711385233642e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 23/71 | LOSS: 7.089543847390208e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 24/71 | LOSS: 7.0713370405428576e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 25/71 | LOSS: 7.013234170909317e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 26/71 | LOSS: 6.925033412025422e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 27/71 | LOSS: 6.877962407868056e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 28/71 | LOSS: 6.856376801116657e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 29/71 | LOSS: 6.885178648493214e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 30/71 | LOSS: 6.8434955386281566e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 31/71 | LOSS: 6.868395772130498e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 32/71 | LOSS: 6.823015899664659e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 33/71 | LOSS: 6.79661161484546e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 34/71 | LOSS: 6.76802506599675e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 35/71 | LOSS: 6.741285915268236e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 36/71 | LOSS: 6.7538716197108244e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 37/71 | LOSS: 6.69030866543165e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 38/71 | LOSS: 6.676899876112703e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 39/71 | LOSS: 6.651472426710825e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 40/71 | LOSS: 6.6484227517787255e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 41/71 | LOSS: 6.625942876867373e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 42/71 | LOSS: 6.61624376651923e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 43/71 | LOSS: 6.575898851224338e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 44/71 | LOSS: 6.5269564705279965e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 45/71 | LOSS: 6.46348353652486e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 46/71 | LOSS: 6.441030283430666e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 47/71 | LOSS: 6.408145992509162e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 48/71 | LOSS: 6.37409506712824e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 49/71 | LOSS: 6.35834080640052e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 50/71 | LOSS: 6.340198025082275e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 51/71 | LOSS: 6.351285019963353e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 52/71 | LOSS: 6.337861740799129e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 53/71 | LOSS: 6.3338932165977355e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 54/71 | LOSS: 6.300589666352607e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 55/71 | LOSS: 6.3304052641147535e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 56/71 | LOSS: 6.31956476226741e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 57/71 | LOSS: 6.318493754814222e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 58/71 | LOSS: 6.335795714514492e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 59/71 | LOSS: 6.337968792043588e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 60/71 | LOSS: 6.34591087426267e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 61/71 | LOSS: 6.337514598213133e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 62/71 | LOSS: 6.344207605930962e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 63/71 | LOSS: 6.324732893858709e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 64/71 | LOSS: 6.314345437903495e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 65/71 | LOSS: 6.304191160641935e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 66/71 | LOSS: 6.283567913737558e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 67/71 | LOSS: 6.315357201374889e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 68/71 | LOSS: 6.301081111155656e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 69/71 | LOSS: 6.327136614865075e-06\n",
      "TRAIN: EPOCH 240/1000 | BATCH 70/71 | LOSS: 6.295471400373802e-06\n",
      "VAL: EPOCH 240/1000 | BATCH 0/8 | LOSS: 5.766644790128339e-06\n",
      "VAL: EPOCH 240/1000 | BATCH 1/8 | LOSS: 5.091679668112192e-06\n",
      "VAL: EPOCH 240/1000 | BATCH 2/8 | LOSS: 5.128584840955834e-06\n",
      "VAL: EPOCH 240/1000 | BATCH 3/8 | LOSS: 5.351121899366262e-06\n",
      "VAL: EPOCH 240/1000 | BATCH 4/8 | LOSS: 5.298419637256302e-06\n",
      "VAL: EPOCH 240/1000 | BATCH 5/8 | LOSS: 5.117067909547283e-06\n",
      "VAL: EPOCH 240/1000 | BATCH 6/8 | LOSS: 5.075482606896133e-06\n",
      "VAL: EPOCH 240/1000 | BATCH 7/8 | LOSS: 5.09754380573213e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 0/71 | LOSS: 5.1766714932455216e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 1/71 | LOSS: 6.21933600086777e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 2/71 | LOSS: 6.000883786327904e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 3/71 | LOSS: 6.13663053172786e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 4/71 | LOSS: 5.975393378321314e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 5/71 | LOSS: 5.916201719931753e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 6/71 | LOSS: 5.969385386249217e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 7/71 | LOSS: 5.768281596374436e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 8/71 | LOSS: 5.656519483131382e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 9/71 | LOSS: 5.617258466372732e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 10/71 | LOSS: 5.548034881609386e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 11/71 | LOSS: 5.5414925175985745e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 12/71 | LOSS: 5.48500723268192e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 13/71 | LOSS: 5.476775705964039e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 14/71 | LOSS: 5.459339505857012e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 15/71 | LOSS: 5.526097197616764e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 16/71 | LOSS: 5.519751191647683e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 17/71 | LOSS: 5.526643968146851e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 18/71 | LOSS: 5.455650041671776e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 19/71 | LOSS: 5.406033028521051e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 20/71 | LOSS: 5.400063552466842e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 21/71 | LOSS: 5.386131926405307e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 22/71 | LOSS: 5.3876130906523345e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 23/71 | LOSS: 5.350190898904354e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 24/71 | LOSS: 5.323539353412343e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 25/71 | LOSS: 5.286724637279314e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 26/71 | LOSS: 5.295757596791696e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 27/71 | LOSS: 5.248916311627129e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 28/71 | LOSS: 5.2945430728994264e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 29/71 | LOSS: 5.247550279818824e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 30/71 | LOSS: 5.268649425899287e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 31/71 | LOSS: 5.280283247088846e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 32/71 | LOSS: 5.315463526626858e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 33/71 | LOSS: 5.318480241519009e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 34/71 | LOSS: 5.316817675650652e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 35/71 | LOSS: 5.499966113752129e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 36/71 | LOSS: 5.501930371718953e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 37/71 | LOSS: 5.554432080220738e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 38/71 | LOSS: 5.5413439930327095e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 39/71 | LOSS: 5.605524256679928e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 40/71 | LOSS: 5.628034181782135e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 41/71 | LOSS: 5.60983599349302e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 42/71 | LOSS: 5.6338893384320745e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 43/71 | LOSS: 5.625009934688436e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 44/71 | LOSS: 5.5990555463520445e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 45/71 | LOSS: 5.607234638575945e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 46/71 | LOSS: 5.613581737286059e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 47/71 | LOSS: 5.590786950430508e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 48/71 | LOSS: 5.61791921427357e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 49/71 | LOSS: 5.60815004973847e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 50/71 | LOSS: 5.583686350862491e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 51/71 | LOSS: 5.585753158881674e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 52/71 | LOSS: 5.601430504337361e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 53/71 | LOSS: 5.5852281093393685e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 54/71 | LOSS: 5.584823016265132e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 55/71 | LOSS: 5.580575873409543e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 56/71 | LOSS: 5.570747362071415e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 57/71 | LOSS: 5.5565955931499425e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 58/71 | LOSS: 5.575496157627825e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 59/71 | LOSS: 5.567714219978371e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 60/71 | LOSS: 5.557472973794306e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 61/71 | LOSS: 5.544889254158461e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 62/71 | LOSS: 5.546543607557569e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 63/71 | LOSS: 5.557314977977512e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 64/71 | LOSS: 5.560061533917458e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 65/71 | LOSS: 5.576004020964099e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 66/71 | LOSS: 5.590008416885524e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 67/71 | LOSS: 5.594695521187318e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 68/71 | LOSS: 5.592092418447609e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 69/71 | LOSS: 5.604552643490024e-06\n",
      "TRAIN: EPOCH 241/1000 | BATCH 70/71 | LOSS: 5.6382220374333385e-06\n",
      "VAL: EPOCH 241/1000 | BATCH 0/8 | LOSS: 7.1081522037275136e-06\n",
      "VAL: EPOCH 241/1000 | BATCH 1/8 | LOSS: 5.983433993606013e-06\n",
      "VAL: EPOCH 241/1000 | BATCH 2/8 | LOSS: 5.8847358559432905e-06\n",
      "VAL: EPOCH 241/1000 | BATCH 3/8 | LOSS: 5.863089313606906e-06\n",
      "VAL: EPOCH 241/1000 | BATCH 4/8 | LOSS: 5.831890848639887e-06\n",
      "VAL: EPOCH 241/1000 | BATCH 5/8 | LOSS: 5.530350563276443e-06\n",
      "VAL: EPOCH 241/1000 | BATCH 6/8 | LOSS: 5.4626718468041095e-06\n",
      "VAL: EPOCH 241/1000 | BATCH 7/8 | LOSS: 5.443492966605845e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 0/71 | LOSS: 5.3386070248961914e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 1/71 | LOSS: 6.216027031769045e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 2/71 | LOSS: 5.696246110649857e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 3/71 | LOSS: 5.7781572877502185e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 4/71 | LOSS: 5.580202650889987e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 5/71 | LOSS: 5.623190569773821e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 6/71 | LOSS: 5.606823508839755e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 7/71 | LOSS: 5.450538253626291e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 8/71 | LOSS: 5.550522524168224e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 9/71 | LOSS: 5.790143086414901e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 10/71 | LOSS: 5.678461465405152e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 11/71 | LOSS: 5.789461207920492e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 12/71 | LOSS: 5.668848294030445e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 13/71 | LOSS: 5.645213669985034e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 14/71 | LOSS: 5.555068188793181e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 15/71 | LOSS: 5.481141670315992e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 16/71 | LOSS: 5.498145957758157e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 17/71 | LOSS: 5.49272555064009e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 18/71 | LOSS: 5.505842253976305e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 19/71 | LOSS: 5.427372457234014e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 20/71 | LOSS: 5.448729877701096e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 21/71 | LOSS: 5.435731405999501e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 22/71 | LOSS: 5.391505194142076e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 23/71 | LOSS: 5.3227332159622165e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 24/71 | LOSS: 5.392899529397255e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 25/71 | LOSS: 5.419554554464412e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 26/71 | LOSS: 5.4225103196114545e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 27/71 | LOSS: 5.499491862792638e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 28/71 | LOSS: 5.475820310901577e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 29/71 | LOSS: 5.48364134071259e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 30/71 | LOSS: 5.482816020723417e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 31/71 | LOSS: 5.507858134023991e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 32/71 | LOSS: 5.53028475582268e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 33/71 | LOSS: 5.522010001989068e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 34/71 | LOSS: 5.535515408285262e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 35/71 | LOSS: 5.54766631921666e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 36/71 | LOSS: 5.528007747941317e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 37/71 | LOSS: 5.571993836527231e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 38/71 | LOSS: 5.602778552808256e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 39/71 | LOSS: 5.610554967461212e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 40/71 | LOSS: 5.591849336136564e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 41/71 | LOSS: 5.5858406323663764e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 42/71 | LOSS: 5.578255008173116e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 43/71 | LOSS: 5.551738147666831e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 44/71 | LOSS: 5.534160754905315e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 45/71 | LOSS: 5.543208149190279e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 46/71 | LOSS: 5.548575148729488e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 47/71 | LOSS: 5.532446190651778e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 48/71 | LOSS: 5.521817076107848e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 49/71 | LOSS: 5.536979197131586e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 50/71 | LOSS: 5.5527766919255434e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 51/71 | LOSS: 5.548230328363738e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 52/71 | LOSS: 5.573353742222952e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 53/71 | LOSS: 5.558335529982557e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 54/71 | LOSS: 5.583249813670673e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 55/71 | LOSS: 5.583699264986665e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 56/71 | LOSS: 5.565840510506971e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 57/71 | LOSS: 5.549171474144557e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 58/71 | LOSS: 5.524886978106471e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 59/71 | LOSS: 5.573007168398666e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 60/71 | LOSS: 5.5603574213312295e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 61/71 | LOSS: 5.6069045146409e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 62/71 | LOSS: 5.581027806560058e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 63/71 | LOSS: 5.646697601946471e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 64/71 | LOSS: 5.636415768934127e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 65/71 | LOSS: 5.642789980783798e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 66/71 | LOSS: 5.655185508549582e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 67/71 | LOSS: 5.681653065306925e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 68/71 | LOSS: 5.6768662877890375e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 69/71 | LOSS: 5.663515003107022e-06\n",
      "TRAIN: EPOCH 242/1000 | BATCH 70/71 | LOSS: 5.664702532901084e-06\n",
      "VAL: EPOCH 242/1000 | BATCH 0/8 | LOSS: 6.7909122662968e-06\n",
      "VAL: EPOCH 242/1000 | BATCH 1/8 | LOSS: 5.978666195005644e-06\n",
      "VAL: EPOCH 242/1000 | BATCH 2/8 | LOSS: 5.816288648929913e-06\n",
      "VAL: EPOCH 242/1000 | BATCH 3/8 | LOSS: 6.187501412568963e-06\n",
      "VAL: EPOCH 242/1000 | BATCH 4/8 | LOSS: 6.014319842506665e-06\n",
      "VAL: EPOCH 242/1000 | BATCH 5/8 | LOSS: 5.923891270261568e-06\n",
      "VAL: EPOCH 242/1000 | BATCH 6/8 | LOSS: 5.9217587866961755e-06\n",
      "VAL: EPOCH 242/1000 | BATCH 7/8 | LOSS: 5.7625918543635635e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 0/71 | LOSS: 4.5687884266953915e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 1/71 | LOSS: 5.71435111851315e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 2/71 | LOSS: 5.835986485180911e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 3/71 | LOSS: 5.696589823855902e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 4/71 | LOSS: 5.71492018934805e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 5/71 | LOSS: 5.545261956285685e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 6/71 | LOSS: 5.6147037607193596e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 7/71 | LOSS: 5.660171780164092e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 8/71 | LOSS: 5.875577749571272e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 9/71 | LOSS: 5.838245533595909e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 10/71 | LOSS: 5.898232178548245e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 11/71 | LOSS: 6.120124415550284e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 12/71 | LOSS: 6.172707904065296e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 13/71 | LOSS: 6.17902359018834e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 14/71 | LOSS: 6.166693431926736e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 15/71 | LOSS: 6.21387667365525e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 16/71 | LOSS: 6.147220520848778e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 17/71 | LOSS: 6.119064715272139e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 18/71 | LOSS: 6.1606801385718615e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 19/71 | LOSS: 6.083087714614521e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 20/71 | LOSS: 6.0361087759831984e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 21/71 | LOSS: 6.036083008935251e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 22/71 | LOSS: 6.041713633116476e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 23/71 | LOSS: 5.98303995502647e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 24/71 | LOSS: 5.981324939057231e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 25/71 | LOSS: 6.003640517239826e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 26/71 | LOSS: 5.993751933096468e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 27/71 | LOSS: 6.01196555700361e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 28/71 | LOSS: 6.04470507572967e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 29/71 | LOSS: 6.019340313893433e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 30/71 | LOSS: 6.096120306474697e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 31/71 | LOSS: 6.092745678643041e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 32/71 | LOSS: 6.057396521183281e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 33/71 | LOSS: 6.055228823242942e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 34/71 | LOSS: 6.062817283236654e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 35/71 | LOSS: 6.043551744975654e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 36/71 | LOSS: 6.012973711689041e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 37/71 | LOSS: 6.040364137573449e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 38/71 | LOSS: 6.030924664511799e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 39/71 | LOSS: 6.066140042548796e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 40/71 | LOSS: 6.081093610324698e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 41/71 | LOSS: 6.115174723423219e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 42/71 | LOSS: 6.095687927712746e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 43/71 | LOSS: 6.056918137305316e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 44/71 | LOSS: 6.0401898837072725e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 45/71 | LOSS: 6.0377750069955765e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 46/71 | LOSS: 6.010116928162925e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 47/71 | LOSS: 6.022910004806666e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 48/71 | LOSS: 6.011777351923971e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 49/71 | LOSS: 5.983530109006096e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 50/71 | LOSS: 5.968073916543896e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 51/71 | LOSS: 5.9498330179289605e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 52/71 | LOSS: 5.925513102378112e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 53/71 | LOSS: 5.912612129611295e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 54/71 | LOSS: 5.89580844131309e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 55/71 | LOSS: 5.898335798195019e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 56/71 | LOSS: 5.902328977849959e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 57/71 | LOSS: 5.891196490759692e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 58/71 | LOSS: 5.884678920033443e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 59/71 | LOSS: 5.862126681677182e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 60/71 | LOSS: 5.863307146906574e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 61/71 | LOSS: 5.857731623344138e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 62/71 | LOSS: 5.837602527338147e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 63/71 | LOSS: 5.832052508480956e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 64/71 | LOSS: 5.817808330623218e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 65/71 | LOSS: 5.808712723426817e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 66/71 | LOSS: 5.7952016002456e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 67/71 | LOSS: 5.792318882627382e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 68/71 | LOSS: 5.8295725505993925e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 69/71 | LOSS: 5.8238773687792545e-06\n",
      "TRAIN: EPOCH 243/1000 | BATCH 70/71 | LOSS: 5.8185298168071724e-06\n",
      "VAL: EPOCH 243/1000 | BATCH 0/8 | LOSS: 6.248821136978222e-06\n",
      "VAL: EPOCH 243/1000 | BATCH 1/8 | LOSS: 5.4670069857820636e-06\n",
      "VAL: EPOCH 243/1000 | BATCH 2/8 | LOSS: 5.607162317270801e-06\n",
      "VAL: EPOCH 243/1000 | BATCH 3/8 | LOSS: 5.8097492683373275e-06\n",
      "VAL: EPOCH 243/1000 | BATCH 4/8 | LOSS: 5.772523309133248e-06\n",
      "VAL: EPOCH 243/1000 | BATCH 5/8 | LOSS: 5.658896270688274e-06\n",
      "VAL: EPOCH 243/1000 | BATCH 6/8 | LOSS: 5.646893246843579e-06\n",
      "VAL: EPOCH 243/1000 | BATCH 7/8 | LOSS: 5.604500302069937e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 0/71 | LOSS: 5.891210093977861e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 1/71 | LOSS: 6.371679319272516e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 2/71 | LOSS: 6.165812616624559e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 3/71 | LOSS: 6.336383080451924e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 4/71 | LOSS: 6.027722611179342e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 5/71 | LOSS: 5.845407713422901e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 6/71 | LOSS: 6.173006438205318e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 7/71 | LOSS: 6.007855574807763e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 8/71 | LOSS: 6.046248371502669e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 9/71 | LOSS: 5.951051252850448e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 10/71 | LOSS: 5.94536835375369e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 11/71 | LOSS: 6.038264928065473e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 12/71 | LOSS: 6.00921248405939e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 13/71 | LOSS: 5.972395553856456e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 14/71 | LOSS: 5.932359999860637e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 15/71 | LOSS: 5.964651933254572e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 16/71 | LOSS: 6.039404524926795e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 17/71 | LOSS: 5.9949926859796205e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 18/71 | LOSS: 6.053990618008691e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 19/71 | LOSS: 6.035744877408433e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 20/71 | LOSS: 6.0637555526184204e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 21/71 | LOSS: 6.034777491301859e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 22/71 | LOSS: 6.087753184186548e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 23/71 | LOSS: 6.205235213959289e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 24/71 | LOSS: 6.169162879814394e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 25/71 | LOSS: 6.310474733394445e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 26/71 | LOSS: 6.299751240240531e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 27/71 | LOSS: 6.346445323807919e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 28/71 | LOSS: 6.303366733155944e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 29/71 | LOSS: 6.335672136022671e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 30/71 | LOSS: 6.317398475080301e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 31/71 | LOSS: 6.270734388635901e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 32/71 | LOSS: 6.282054624589915e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 33/71 | LOSS: 6.234497705543831e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 34/71 | LOSS: 6.1860764877305235e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 35/71 | LOSS: 6.217005749123119e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 36/71 | LOSS: 6.1954171443987335e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 37/71 | LOSS: 6.248518993264665e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 38/71 | LOSS: 6.208677148559871e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 39/71 | LOSS: 6.239157914933457e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 40/71 | LOSS: 6.222086750956184e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 41/71 | LOSS: 6.211490526333191e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 42/71 | LOSS: 6.193429335114562e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 43/71 | LOSS: 6.1934645662569725e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 44/71 | LOSS: 6.140655083678818e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 45/71 | LOSS: 6.111650730852726e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 46/71 | LOSS: 6.103310402358283e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 47/71 | LOSS: 6.078212294369223e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 48/71 | LOSS: 6.0755130794043034e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 49/71 | LOSS: 6.024242034072813e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 50/71 | LOSS: 5.99108729343867e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 51/71 | LOSS: 5.966100192066593e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 52/71 | LOSS: 5.93780691815482e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 53/71 | LOSS: 5.9075458104281876e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 54/71 | LOSS: 5.8957499814741e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 55/71 | LOSS: 5.882185945438323e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 56/71 | LOSS: 5.8536176104034734e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 57/71 | LOSS: 5.854193875228671e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 58/71 | LOSS: 5.846972622378617e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 59/71 | LOSS: 5.8239740193736605e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 60/71 | LOSS: 5.825221008859951e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 61/71 | LOSS: 5.815632848240174e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 62/71 | LOSS: 5.823627733416432e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 63/71 | LOSS: 5.807683674419195e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 64/71 | LOSS: 5.807348788249118e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 65/71 | LOSS: 5.798594632765206e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 66/71 | LOSS: 5.778849934141539e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 67/71 | LOSS: 5.755556461441729e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 68/71 | LOSS: 5.724682190755603e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 69/71 | LOSS: 5.708983323009826e-06\n",
      "TRAIN: EPOCH 244/1000 | BATCH 70/71 | LOSS: 5.688491740675346e-06\n",
      "VAL: EPOCH 244/1000 | BATCH 0/8 | LOSS: 5.766525191575056e-06\n",
      "VAL: EPOCH 244/1000 | BATCH 1/8 | LOSS: 5.151693812877056e-06\n",
      "VAL: EPOCH 244/1000 | BATCH 2/8 | LOSS: 5.137484398195132e-06\n",
      "VAL: EPOCH 244/1000 | BATCH 3/8 | LOSS: 5.534542765417427e-06\n",
      "VAL: EPOCH 244/1000 | BATCH 4/8 | LOSS: 5.456535927805817e-06\n",
      "VAL: EPOCH 244/1000 | BATCH 5/8 | LOSS: 5.266468557844443e-06\n",
      "VAL: EPOCH 244/1000 | BATCH 6/8 | LOSS: 5.139547218797296e-06\n",
      "VAL: EPOCH 244/1000 | BATCH 7/8 | LOSS: 5.069354358511191e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 0/71 | LOSS: 5.396914275479503e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 1/71 | LOSS: 4.9035529627872165e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 2/71 | LOSS: 5.528860735163714e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 3/71 | LOSS: 5.397015684138751e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 4/71 | LOSS: 5.428971508081304e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 5/71 | LOSS: 5.24616052643978e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 6/71 | LOSS: 5.341323490678665e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 7/71 | LOSS: 5.702912744709465e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 8/71 | LOSS: 5.722326123860613e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 9/71 | LOSS: 5.749963838752592e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 10/71 | LOSS: 5.688863687034675e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 11/71 | LOSS: 5.7040327874346985e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 12/71 | LOSS: 5.515705319899448e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 13/71 | LOSS: 5.421552470709555e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 14/71 | LOSS: 5.4840140819578664e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 15/71 | LOSS: 5.453878529237954e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 16/71 | LOSS: 5.465964089607416e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 17/71 | LOSS: 5.5442829989260645e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 18/71 | LOSS: 5.4910789649990256e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 19/71 | LOSS: 5.488614544901793e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 20/71 | LOSS: 5.464563281791579e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 21/71 | LOSS: 5.443031723602458e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 22/71 | LOSS: 5.459509506893126e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 23/71 | LOSS: 5.427957120218707e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 24/71 | LOSS: 5.444908538265736e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 25/71 | LOSS: 5.362023286509681e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 26/71 | LOSS: 5.362183359196664e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 27/71 | LOSS: 5.3562963639056505e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 28/71 | LOSS: 5.378822050198355e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 29/71 | LOSS: 5.387079674316434e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 30/71 | LOSS: 5.36623385796115e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 31/71 | LOSS: 5.326349118206508e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 32/71 | LOSS: 5.313973229986556e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 33/71 | LOSS: 5.280009304442501e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 34/71 | LOSS: 5.257354788617314e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 35/71 | LOSS: 5.2462948221748066e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 36/71 | LOSS: 5.276131246687696e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 37/71 | LOSS: 5.25382629228644e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 38/71 | LOSS: 5.236452445900889e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 39/71 | LOSS: 5.271264325301672e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 40/71 | LOSS: 5.259199114578489e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 41/71 | LOSS: 5.243931793432109e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 42/71 | LOSS: 5.245504088063256e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 43/71 | LOSS: 5.276526306633142e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 44/71 | LOSS: 5.239575885853911e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 45/71 | LOSS: 5.235462703754108e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 46/71 | LOSS: 5.255459064814049e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 47/71 | LOSS: 5.251179904538124e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 48/71 | LOSS: 5.268555311459397e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 49/71 | LOSS: 5.2456346384133215e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 50/71 | LOSS: 5.236593747837355e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 51/71 | LOSS: 5.226569171030021e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 52/71 | LOSS: 5.210263551353424e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 53/71 | LOSS: 5.2178273916878325e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 54/71 | LOSS: 5.2325096310761925e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 55/71 | LOSS: 5.235757742281878e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 56/71 | LOSS: 5.221034706138246e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 57/71 | LOSS: 5.21327734563313e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 58/71 | LOSS: 5.226753913475323e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 59/71 | LOSS: 5.223609976686324e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 60/71 | LOSS: 5.228602410662446e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 61/71 | LOSS: 5.213437112439762e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 62/71 | LOSS: 5.191713077534038e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 63/71 | LOSS: 5.1550799859967356e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 64/71 | LOSS: 5.172652446652557e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 65/71 | LOSS: 5.187172274473238e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 66/71 | LOSS: 5.205705995093067e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 67/71 | LOSS: 5.186525771371046e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 68/71 | LOSS: 5.184172619999077e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 69/71 | LOSS: 5.188020119411314e-06\n",
      "TRAIN: EPOCH 245/1000 | BATCH 70/71 | LOSS: 5.171243785785201e-06\n",
      "VAL: EPOCH 245/1000 | BATCH 0/8 | LOSS: 6.217849659151398e-06\n",
      "VAL: EPOCH 245/1000 | BATCH 1/8 | LOSS: 5.8410578276379965e-06\n",
      "VAL: EPOCH 245/1000 | BATCH 2/8 | LOSS: 5.597454067659176e-06\n",
      "VAL: EPOCH 245/1000 | BATCH 3/8 | LOSS: 6.0085893665018375e-06\n",
      "VAL: EPOCH 245/1000 | BATCH 4/8 | LOSS: 5.860479814145947e-06\n",
      "VAL: EPOCH 245/1000 | BATCH 5/8 | LOSS: 5.558457663331258e-06\n",
      "VAL: EPOCH 245/1000 | BATCH 6/8 | LOSS: 5.45974255926142e-06\n",
      "VAL: EPOCH 245/1000 | BATCH 7/8 | LOSS: 5.290232763854874e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 0/71 | LOSS: 5.087042154627852e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 1/71 | LOSS: 6.147261956357397e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 2/71 | LOSS: 5.822391661543709e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 3/71 | LOSS: 6.008224886500102e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 4/71 | LOSS: 6.31154334769235e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 5/71 | LOSS: 6.4771923765268484e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 6/71 | LOSS: 6.304736156640242e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 7/71 | LOSS: 6.641133268203703e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 8/71 | LOSS: 6.516134110117693e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 9/71 | LOSS: 6.399910353138693e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 10/71 | LOSS: 6.583082929203308e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 11/71 | LOSS: 6.5593949708879036e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 12/71 | LOSS: 6.422632080872203e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 13/71 | LOSS: 6.30698536951968e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 14/71 | LOSS: 6.420529128566462e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 15/71 | LOSS: 6.25710183044248e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 16/71 | LOSS: 6.1897627775204666e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 17/71 | LOSS: 6.278269135388352e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 18/71 | LOSS: 6.239337596382125e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 19/71 | LOSS: 6.1530114180641245e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 20/71 | LOSS: 6.168012814235962e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 21/71 | LOSS: 6.132247976479861e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 22/71 | LOSS: 6.091009274077015e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 23/71 | LOSS: 6.15810946176983e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 24/71 | LOSS: 6.165080831124214e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 25/71 | LOSS: 6.119169099222815e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 26/71 | LOSS: 6.1437229440478535e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 27/71 | LOSS: 6.072494492010654e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 28/71 | LOSS: 6.049005213722482e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 29/71 | LOSS: 6.029901745326545e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 30/71 | LOSS: 6.001449227985558e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 31/71 | LOSS: 5.942984117268679e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 32/71 | LOSS: 5.905750406258669e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 33/71 | LOSS: 5.885997833341689e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 34/71 | LOSS: 5.900499010229915e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 35/71 | LOSS: 5.898906427622326e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 36/71 | LOSS: 5.924166310472395e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 37/71 | LOSS: 5.886294155444423e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 38/71 | LOSS: 5.9015279149230664e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 39/71 | LOSS: 5.88244515711267e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 40/71 | LOSS: 5.902376759012241e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 41/71 | LOSS: 5.8649459983176456e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 42/71 | LOSS: 5.838592547069163e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 43/71 | LOSS: 5.811226380749511e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 44/71 | LOSS: 5.797642552352045e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 45/71 | LOSS: 5.777819791072271e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 46/71 | LOSS: 5.76467097376503e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 47/71 | LOSS: 5.746686071006479e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 48/71 | LOSS: 5.7394076604247615e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 49/71 | LOSS: 5.729654285460128e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 50/71 | LOSS: 5.761194197297229e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 51/71 | LOSS: 5.742420101411811e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 52/71 | LOSS: 5.703423934357728e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 53/71 | LOSS: 5.699351075034227e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 54/71 | LOSS: 5.712684254500676e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 55/71 | LOSS: 5.705972902043348e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 56/71 | LOSS: 5.68858340238078e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 57/71 | LOSS: 5.690218194072729e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 58/71 | LOSS: 5.673347405425344e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 59/71 | LOSS: 5.672721363225719e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 60/71 | LOSS: 5.6645829049482955e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 61/71 | LOSS: 5.658507028959819e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 62/71 | LOSS: 5.644675588377544e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 63/71 | LOSS: 5.637237727285083e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 64/71 | LOSS: 5.646711801651131e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 65/71 | LOSS: 5.645663798008393e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 66/71 | LOSS: 5.639329221811573e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 67/71 | LOSS: 5.647939407454567e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 68/71 | LOSS: 5.660897155736904e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 69/71 | LOSS: 5.685437210039319e-06\n",
      "TRAIN: EPOCH 246/1000 | BATCH 70/71 | LOSS: 5.6848441000882715e-06\n",
      "VAL: EPOCH 246/1000 | BATCH 0/8 | LOSS: 7.647355232620612e-06\n",
      "VAL: EPOCH 246/1000 | BATCH 1/8 | LOSS: 7.457962055923417e-06\n",
      "VAL: EPOCH 246/1000 | BATCH 2/8 | LOSS: 6.9444655916110305e-06\n",
      "VAL: EPOCH 246/1000 | BATCH 3/8 | LOSS: 7.263379529831582e-06\n",
      "VAL: EPOCH 246/1000 | BATCH 4/8 | LOSS: 7.1177912104758436e-06\n",
      "VAL: EPOCH 246/1000 | BATCH 5/8 | LOSS: 6.74510488352098e-06\n",
      "VAL: EPOCH 246/1000 | BATCH 6/8 | LOSS: 6.719237522442459e-06\n",
      "VAL: EPOCH 246/1000 | BATCH 7/8 | LOSS: 6.534577835282107e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 0/71 | LOSS: 6.439255230361596e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 1/71 | LOSS: 6.24961512585287e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 2/71 | LOSS: 6.3469272693813155e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 3/71 | LOSS: 5.916418786000577e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 4/71 | LOSS: 5.750403124693548e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 5/71 | LOSS: 5.810362457244385e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 6/71 | LOSS: 5.590084908492697e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 7/71 | LOSS: 5.720460421798634e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 8/71 | LOSS: 5.713077977513119e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 9/71 | LOSS: 5.820515343657462e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 10/71 | LOSS: 5.693474453106649e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 11/71 | LOSS: 5.718103693652665e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 12/71 | LOSS: 5.726624189824529e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 13/71 | LOSS: 5.628099350880282e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 14/71 | LOSS: 5.596313985734014e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 15/71 | LOSS: 5.518904202972408e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 16/71 | LOSS: 5.494368692512473e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 17/71 | LOSS: 5.407834477308724e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 18/71 | LOSS: 5.331710494111145e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 19/71 | LOSS: 5.253426593299082e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 20/71 | LOSS: 5.216896841461892e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 21/71 | LOSS: 5.2216016683814814e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 22/71 | LOSS: 5.282700504207953e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 23/71 | LOSS: 5.32913226910144e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 24/71 | LOSS: 5.390245769376634e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 25/71 | LOSS: 5.32976392045944e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 26/71 | LOSS: 5.315727237218569e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 27/71 | LOSS: 5.282317845610253e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 28/71 | LOSS: 5.300925138350255e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 29/71 | LOSS: 5.31073034532407e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 30/71 | LOSS: 5.3367869352974416e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 31/71 | LOSS: 5.335332971867501e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 32/71 | LOSS: 5.317591633987225e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 33/71 | LOSS: 5.33396963178615e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 34/71 | LOSS: 5.318028576896592e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 35/71 | LOSS: 5.399000428951695e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 36/71 | LOSS: 5.361647499985584e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 37/71 | LOSS: 5.392918901155721e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 38/71 | LOSS: 5.417559863510691e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 39/71 | LOSS: 5.41124771871182e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 40/71 | LOSS: 5.428260553854648e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 41/71 | LOSS: 5.403553738384896e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 42/71 | LOSS: 5.378162546379784e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 43/71 | LOSS: 5.370909940955409e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 44/71 | LOSS: 5.379013014640401e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 45/71 | LOSS: 5.382520078495398e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 46/71 | LOSS: 5.372857966459049e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 47/71 | LOSS: 5.370090282970826e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 48/71 | LOSS: 5.360123924795201e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 49/71 | LOSS: 5.344830633475794e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 50/71 | LOSS: 5.3298275922904445e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 51/71 | LOSS: 5.334576739047547e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 52/71 | LOSS: 5.32438209017339e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 53/71 | LOSS: 5.303124840725947e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 54/71 | LOSS: 5.294292565271131e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 55/71 | LOSS: 5.3030464560574075e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 56/71 | LOSS: 5.307278370948329e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 57/71 | LOSS: 5.33704363486573e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 58/71 | LOSS: 5.343396177353097e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 59/71 | LOSS: 5.370073389106741e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 60/71 | LOSS: 5.340726296648815e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 61/71 | LOSS: 5.3515198473060966e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 62/71 | LOSS: 5.3453493256667915e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 63/71 | LOSS: 5.3351253157529754e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 64/71 | LOSS: 5.32570649319006e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 65/71 | LOSS: 5.333362327236858e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 66/71 | LOSS: 5.344895264476975e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 67/71 | LOSS: 5.317429795200844e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 68/71 | LOSS: 5.301238214061427e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 69/71 | LOSS: 5.291116466261363e-06\n",
      "TRAIN: EPOCH 247/1000 | BATCH 70/71 | LOSS: 5.278767237311039e-06\n",
      "VAL: EPOCH 247/1000 | BATCH 0/8 | LOSS: 6.0529914662765805e-06\n",
      "VAL: EPOCH 247/1000 | BATCH 1/8 | LOSS: 5.577257979894057e-06\n",
      "VAL: EPOCH 247/1000 | BATCH 2/8 | LOSS: 5.326820883055916e-06\n",
      "VAL: EPOCH 247/1000 | BATCH 3/8 | LOSS: 5.580170181929134e-06\n",
      "VAL: EPOCH 247/1000 | BATCH 4/8 | LOSS: 5.542585859075188e-06\n",
      "VAL: EPOCH 247/1000 | BATCH 5/8 | LOSS: 5.294854872772703e-06\n",
      "VAL: EPOCH 247/1000 | BATCH 6/8 | LOSS: 5.259463250695262e-06\n",
      "VAL: EPOCH 247/1000 | BATCH 7/8 | LOSS: 5.218336582402117e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 0/71 | LOSS: 5.2702880566357635e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 1/71 | LOSS: 4.9740935992304e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 2/71 | LOSS: 4.917867499898421e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 3/71 | LOSS: 5.163915488992643e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 4/71 | LOSS: 5.137146854394814e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 5/71 | LOSS: 5.358747709275728e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 6/71 | LOSS: 5.2399152861783345e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 7/71 | LOSS: 5.08021372525036e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 8/71 | LOSS: 5.209859965462884e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 9/71 | LOSS: 5.179385107112467e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 10/71 | LOSS: 5.153662021422695e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 11/71 | LOSS: 5.20132118708716e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 12/71 | LOSS: 5.176753207846419e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 13/71 | LOSS: 5.1548344280100925e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 14/71 | LOSS: 5.054320490671671e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 15/71 | LOSS: 5.1380946928247795e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 16/71 | LOSS: 5.116016983961077e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 17/71 | LOSS: 5.147558302572482e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 18/71 | LOSS: 5.102692512082285e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 19/71 | LOSS: 5.147192257481947e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 20/71 | LOSS: 5.099482277295292e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 21/71 | LOSS: 5.093090428751814e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 22/71 | LOSS: 5.1018776443727445e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 23/71 | LOSS: 5.1083875121094025e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 24/71 | LOSS: 5.179375739317038e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 25/71 | LOSS: 5.179375114913944e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 26/71 | LOSS: 5.180069037022703e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 27/71 | LOSS: 5.19487021425188e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 28/71 | LOSS: 5.163462470465737e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 29/71 | LOSS: 5.143182859986458e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 30/71 | LOSS: 5.1544440075010626e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 31/71 | LOSS: 5.143916290251127e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 32/71 | LOSS: 5.144898247608305e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 33/71 | LOSS: 5.164311774559096e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 34/71 | LOSS: 5.12323428145984e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 35/71 | LOSS: 5.130498373596816e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 36/71 | LOSS: 5.103342309175295e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 37/71 | LOSS: 5.075679520192161e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 38/71 | LOSS: 5.062847607498556e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 39/71 | LOSS: 5.0337843902070745e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 40/71 | LOSS: 5.001574336569231e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 41/71 | LOSS: 5.032544111470218e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 42/71 | LOSS: 5.057603550804468e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 43/71 | LOSS: 5.057519672391209e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 44/71 | LOSS: 5.101078623839486e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 45/71 | LOSS: 5.127490483217748e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 46/71 | LOSS: 5.143190453460928e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 47/71 | LOSS: 5.1467458632714624e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 48/71 | LOSS: 5.179337152612232e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 49/71 | LOSS: 5.18466503763193e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 50/71 | LOSS: 5.217649245156174e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 51/71 | LOSS: 5.278547768319606e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 52/71 | LOSS: 5.307708225134191e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 53/71 | LOSS: 5.304766853408148e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 54/71 | LOSS: 5.317028886896547e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 55/71 | LOSS: 5.382516710434564e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 56/71 | LOSS: 5.382377099649921e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 57/71 | LOSS: 5.3743395232357156e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 58/71 | LOSS: 5.430411308333209e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 59/71 | LOSS: 5.408729934212412e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 60/71 | LOSS: 5.428478546297706e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 61/71 | LOSS: 5.441127530414839e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 62/71 | LOSS: 5.4179973912617976e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 63/71 | LOSS: 5.394973445760343e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 64/71 | LOSS: 5.4066859460223124e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 65/71 | LOSS: 5.413947461062035e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 66/71 | LOSS: 5.420677997613866e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 67/71 | LOSS: 5.424492946852243e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 68/71 | LOSS: 5.409459855281571e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 69/71 | LOSS: 5.40167593823883e-06\n",
      "TRAIN: EPOCH 248/1000 | BATCH 70/71 | LOSS: 5.405446318933096e-06\n",
      "VAL: EPOCH 248/1000 | BATCH 0/8 | LOSS: 6.1280188674572855e-06\n",
      "VAL: EPOCH 248/1000 | BATCH 1/8 | LOSS: 5.3922401548334165e-06\n",
      "VAL: EPOCH 248/1000 | BATCH 2/8 | LOSS: 5.630954698669181e-06\n",
      "VAL: EPOCH 248/1000 | BATCH 3/8 | LOSS: 5.708483854505175e-06\n",
      "VAL: EPOCH 248/1000 | BATCH 4/8 | LOSS: 5.7010095588339025e-06\n",
      "VAL: EPOCH 248/1000 | BATCH 5/8 | LOSS: 5.489837121785968e-06\n",
      "VAL: EPOCH 248/1000 | BATCH 6/8 | LOSS: 5.355101101096287e-06\n",
      "VAL: EPOCH 248/1000 | BATCH 7/8 | LOSS: 5.371511178964283e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 0/71 | LOSS: 5.525616415980039e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 1/71 | LOSS: 5.42938755643263e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 2/71 | LOSS: 5.21485784095906e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 3/71 | LOSS: 5.029624503549712e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 4/71 | LOSS: 5.5607370995858215e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 5/71 | LOSS: 5.56429101076598e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 6/71 | LOSS: 5.813404056555425e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 7/71 | LOSS: 5.726961092022975e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 8/71 | LOSS: 5.97887108395096e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 9/71 | LOSS: 5.874578391740215e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 10/71 | LOSS: 5.995277495623503e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 11/71 | LOSS: 6.028353216909939e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 12/71 | LOSS: 5.932205602640841e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 13/71 | LOSS: 5.821206319264352e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 14/71 | LOSS: 5.775482016664076e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 15/71 | LOSS: 5.789694313307336e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 16/71 | LOSS: 5.75979931775638e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 17/71 | LOSS: 5.7146384430476855e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 18/71 | LOSS: 5.706610289436005e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 19/71 | LOSS: 5.621518312182161e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 20/71 | LOSS: 5.581209907610346e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 21/71 | LOSS: 5.519175663886761e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 22/71 | LOSS: 5.568712743337054e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 23/71 | LOSS: 5.538531221797409e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 24/71 | LOSS: 5.480533600348281e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 25/71 | LOSS: 5.474206475880167e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 26/71 | LOSS: 5.487414270006681e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 27/71 | LOSS: 5.4558395114716925e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 28/71 | LOSS: 5.446087188515933e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 29/71 | LOSS: 5.487063678325891e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 30/71 | LOSS: 5.450977237013566e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 31/71 | LOSS: 5.434746512378297e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 32/71 | LOSS: 5.436535750711548e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 33/71 | LOSS: 5.415820222718621e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 34/71 | LOSS: 5.383534257167152e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 35/71 | LOSS: 5.386037855714676e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 36/71 | LOSS: 5.3591359918189825e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 37/71 | LOSS: 5.368515076042968e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 38/71 | LOSS: 5.373491067308723e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 39/71 | LOSS: 5.404082310178638e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 40/71 | LOSS: 5.368878957109168e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 41/71 | LOSS: 5.355110921473602e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 42/71 | LOSS: 5.338151981430751e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 43/71 | LOSS: 5.37900105386639e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 44/71 | LOSS: 5.368242192263198e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 45/71 | LOSS: 5.351250573094939e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 46/71 | LOSS: 5.348101549703217e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 47/71 | LOSS: 5.348101770626575e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 48/71 | LOSS: 5.315051580788461e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 49/71 | LOSS: 5.29090182226355e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 50/71 | LOSS: 5.2919720286916025e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 51/71 | LOSS: 5.271716326240914e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 52/71 | LOSS: 5.2699954610360225e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 53/71 | LOSS: 5.2651444977729825e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 54/71 | LOSS: 5.2600690857864475e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 55/71 | LOSS: 5.259194620878459e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 56/71 | LOSS: 5.241832077604557e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 57/71 | LOSS: 5.222982457749383e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 58/71 | LOSS: 5.221954714184996e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 59/71 | LOSS: 5.219157988752461e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 60/71 | LOSS: 5.230004281194672e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 61/71 | LOSS: 5.257443494003617e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 62/71 | LOSS: 5.248534441148328e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 63/71 | LOSS: 5.230600205408109e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 64/71 | LOSS: 5.236048278115609e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 65/71 | LOSS: 5.260483962129828e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 66/71 | LOSS: 5.256641614649387e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 67/71 | LOSS: 5.245362321120945e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 68/71 | LOSS: 5.227287591997478e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 69/71 | LOSS: 5.2216424104959255e-06\n",
      "TRAIN: EPOCH 249/1000 | BATCH 70/71 | LOSS: 5.232609154659194e-06\n",
      "VAL: EPOCH 249/1000 | BATCH 0/8 | LOSS: 6.515315362776164e-06\n",
      "VAL: EPOCH 249/1000 | BATCH 1/8 | LOSS: 5.720446779378108e-06\n",
      "VAL: EPOCH 249/1000 | BATCH 2/8 | LOSS: 5.811717983306153e-06\n",
      "VAL: EPOCH 249/1000 | BATCH 3/8 | LOSS: 6.067633876227774e-06\n",
      "VAL: EPOCH 249/1000 | BATCH 4/8 | LOSS: 5.955583765171468e-06\n",
      "VAL: EPOCH 249/1000 | BATCH 5/8 | LOSS: 5.762332269417432e-06\n",
      "VAL: EPOCH 249/1000 | BATCH 6/8 | LOSS: 5.631276768066787e-06\n",
      "VAL: EPOCH 249/1000 | BATCH 7/8 | LOSS: 5.5941422942851204e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 0/71 | LOSS: 5.574255283136154e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 1/71 | LOSS: 5.655946779370424e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 2/71 | LOSS: 5.537173819902819e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 3/71 | LOSS: 5.225491918281477e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 4/71 | LOSS: 5.400970349001e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 5/71 | LOSS: 5.479605457973473e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 6/71 | LOSS: 5.355673628011053e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 7/71 | LOSS: 5.513639962373418e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 8/71 | LOSS: 5.425237456317215e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 9/71 | LOSS: 5.469314692163607e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 10/71 | LOSS: 5.621635657669024e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 11/71 | LOSS: 5.704543165544844e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 12/71 | LOSS: 5.815975153102325e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 13/71 | LOSS: 5.7684337662067264e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 14/71 | LOSS: 5.956573598571898e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 15/71 | LOSS: 5.915029731795585e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 16/71 | LOSS: 5.91530552436084e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 17/71 | LOSS: 5.92205987636084e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 18/71 | LOSS: 5.926596594164087e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 19/71 | LOSS: 6.02588886522426e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 20/71 | LOSS: 6.075926432372182e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 21/71 | LOSS: 6.0800560649196e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 22/71 | LOSS: 6.022142414345264e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 23/71 | LOSS: 5.985724726542685e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 24/71 | LOSS: 5.935159988439409e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 25/71 | LOSS: 5.9407206593347764e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 26/71 | LOSS: 5.884370306443155e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 27/71 | LOSS: 5.849901299370686e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 28/71 | LOSS: 5.828425197125461e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 29/71 | LOSS: 5.760667621264777e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 30/71 | LOSS: 5.697321761721058e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 31/71 | LOSS: 5.695883295686599e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 32/71 | LOSS: 5.687337396452539e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 33/71 | LOSS: 5.6564700397596905e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 34/71 | LOSS: 5.624639399164672e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 35/71 | LOSS: 5.651098635755463e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 36/71 | LOSS: 5.626209596468126e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 37/71 | LOSS: 5.649255788404461e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 38/71 | LOSS: 5.653448572327481e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 39/71 | LOSS: 5.6212700144442355e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 40/71 | LOSS: 5.597191269904525e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 41/71 | LOSS: 5.617036923207966e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 42/71 | LOSS: 5.656095481754164e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 43/71 | LOSS: 5.6985919369171e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 44/71 | LOSS: 5.687417913375409e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 45/71 | LOSS: 5.6828340970953475e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 46/71 | LOSS: 5.731067713373279e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 47/71 | LOSS: 5.732804514953689e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 48/71 | LOSS: 5.768753012767886e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 49/71 | LOSS: 5.769426056758675e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 50/71 | LOSS: 5.830816520506678e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 51/71 | LOSS: 5.840233217650865e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 52/71 | LOSS: 5.860711437564086e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 53/71 | LOSS: 5.8742888734707746e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 54/71 | LOSS: 5.883145327474763e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 55/71 | LOSS: 5.928767194518514e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 56/71 | LOSS: 5.916370463111027e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 57/71 | LOSS: 5.9264743828768175e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 58/71 | LOSS: 5.923239934603216e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 59/71 | LOSS: 5.912472014794427e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 60/71 | LOSS: 5.965610633390243e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 61/71 | LOSS: 5.99716264376536e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 62/71 | LOSS: 6.039083303669089e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 63/71 | LOSS: 6.029498759829721e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 64/71 | LOSS: 6.0921458743205925e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 65/71 | LOSS: 6.120186144058111e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 66/71 | LOSS: 6.196787238090408e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 67/71 | LOSS: 6.17854391628177e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 68/71 | LOSS: 6.27033217943276e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 69/71 | LOSS: 6.259129619528332e-06\n",
      "TRAIN: EPOCH 250/1000 | BATCH 70/71 | LOSS: 6.249075452832982e-06\n",
      "VAL: EPOCH 250/1000 | BATCH 0/8 | LOSS: 8.853051440382842e-06\n",
      "VAL: EPOCH 250/1000 | BATCH 1/8 | LOSS: 7.519287237300887e-06\n",
      "VAL: EPOCH 250/1000 | BATCH 2/8 | LOSS: 7.912096255798437e-06\n",
      "VAL: EPOCH 250/1000 | BATCH 3/8 | LOSS: 7.550535428890726e-06\n",
      "VAL: EPOCH 250/1000 | BATCH 4/8 | LOSS: 7.646571611985565e-06\n",
      "VAL: EPOCH 250/1000 | BATCH 5/8 | LOSS: 7.431178194868456e-06\n",
      "VAL: EPOCH 250/1000 | BATCH 6/8 | LOSS: 7.260877217569421e-06\n",
      "VAL: EPOCH 250/1000 | BATCH 7/8 | LOSS: 7.400668607715488e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 0/71 | LOSS: 5.9619665080390405e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 1/71 | LOSS: 5.91992943554942e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 2/71 | LOSS: 6.377623473478404e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 3/71 | LOSS: 6.109864216341521e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 4/71 | LOSS: 5.92602627875749e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 5/71 | LOSS: 5.777018638279212e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 6/71 | LOSS: 5.841867083030852e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 7/71 | LOSS: 5.874261944427417e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 8/71 | LOSS: 6.345328327168116e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 9/71 | LOSS: 6.283095035541919e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 10/71 | LOSS: 6.4562588539212644e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 11/71 | LOSS: 6.433794662067764e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 12/71 | LOSS: 6.381733713239485e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 13/71 | LOSS: 6.3006700981661974e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 14/71 | LOSS: 6.2193962548917625e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 15/71 | LOSS: 6.31161790920487e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 16/71 | LOSS: 6.2847279075546845e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 17/71 | LOSS: 6.268902779993368e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 18/71 | LOSS: 6.266094571497218e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 19/71 | LOSS: 6.4124381879082645e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 20/71 | LOSS: 6.370222027250011e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 21/71 | LOSS: 6.259416378187862e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 22/71 | LOSS: 6.224884640212859e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 23/71 | LOSS: 6.209046925202226e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 24/71 | LOSS: 6.174250465846853e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 25/71 | LOSS: 6.2197187931955204e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 26/71 | LOSS: 6.209904296594215e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 27/71 | LOSS: 6.186135578900576e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 28/71 | LOSS: 6.162991933800593e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 29/71 | LOSS: 6.1615094485508354e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 30/71 | LOSS: 6.171633436503227e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 31/71 | LOSS: 6.136314667060105e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 32/71 | LOSS: 6.112381180917675e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 33/71 | LOSS: 6.085532330711003e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 34/71 | LOSS: 6.046199112980893e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 35/71 | LOSS: 6.022966140840759e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 36/71 | LOSS: 6.001247682532365e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 37/71 | LOSS: 5.961582856846355e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 38/71 | LOSS: 5.921694572372451e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 39/71 | LOSS: 5.888591329039628e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 40/71 | LOSS: 5.842062175982325e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 41/71 | LOSS: 5.808260799052992e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 42/71 | LOSS: 5.754867066401815e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 43/71 | LOSS: 5.732057295724437e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 44/71 | LOSS: 5.699715711671161e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 45/71 | LOSS: 5.675116377144308e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 46/71 | LOSS: 5.651146538900533e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 47/71 | LOSS: 5.614970357707231e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 48/71 | LOSS: 5.646020976460672e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 49/71 | LOSS: 5.627937098324765e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 50/71 | LOSS: 5.64861842326982e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 51/71 | LOSS: 5.628523319081489e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 52/71 | LOSS: 5.600387365395261e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 53/71 | LOSS: 5.585694974865031e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 54/71 | LOSS: 5.559168924016624e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 55/71 | LOSS: 5.598212185629693e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 56/71 | LOSS: 5.596704691893058e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 57/71 | LOSS: 5.6153304865739335e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 58/71 | LOSS: 5.619658368540498e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 59/71 | LOSS: 5.618251293526555e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 60/71 | LOSS: 5.6078200583722155e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 61/71 | LOSS: 5.5963350933457294e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 62/71 | LOSS: 5.595662781752975e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 63/71 | LOSS: 5.583743835302357e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 64/71 | LOSS: 5.571716239781101e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 65/71 | LOSS: 5.55817913057884e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 66/71 | LOSS: 5.539810702959666e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 67/71 | LOSS: 5.532474567778135e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 68/71 | LOSS: 5.540050512101279e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 69/71 | LOSS: 5.517809300467239e-06\n",
      "TRAIN: EPOCH 251/1000 | BATCH 70/71 | LOSS: 5.53332745152096e-06\n",
      "VAL: EPOCH 251/1000 | BATCH 0/8 | LOSS: 7.227736659842776e-06\n",
      "VAL: EPOCH 251/1000 | BATCH 1/8 | LOSS: 6.524511945826816e-06\n",
      "VAL: EPOCH 251/1000 | BATCH 2/8 | LOSS: 6.121229641091001e-06\n",
      "VAL: EPOCH 251/1000 | BATCH 3/8 | LOSS: 6.587903385479876e-06\n",
      "VAL: EPOCH 251/1000 | BATCH 4/8 | LOSS: 6.339285664580529e-06\n",
      "VAL: EPOCH 251/1000 | BATCH 5/8 | LOSS: 5.999825513451166e-06\n",
      "VAL: EPOCH 251/1000 | BATCH 6/8 | LOSS: 5.900317449101879e-06\n",
      "VAL: EPOCH 251/1000 | BATCH 7/8 | LOSS: 5.7155434660671744e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 0/71 | LOSS: 4.616686965164263e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 1/71 | LOSS: 4.404667379276361e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 2/71 | LOSS: 4.486944362724898e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 3/71 | LOSS: 4.456730835045164e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 4/71 | LOSS: 4.471419470064575e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 5/71 | LOSS: 4.648515262791382e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 6/71 | LOSS: 4.681353532630185e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 7/71 | LOSS: 4.665858000407752e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 8/71 | LOSS: 4.627363270830958e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 9/71 | LOSS: 4.7089588406379335e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 10/71 | LOSS: 4.747485697655727e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 11/71 | LOSS: 4.896278748371212e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 12/71 | LOSS: 4.986122173879547e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 13/71 | LOSS: 4.9534157499562885e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 14/71 | LOSS: 5.089878474488311e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 15/71 | LOSS: 5.061992141008886e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 16/71 | LOSS: 5.047162818193422e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 17/71 | LOSS: 5.0253711378900334e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 18/71 | LOSS: 5.002656005097761e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 19/71 | LOSS: 4.98030035487318e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 20/71 | LOSS: 4.998969048063459e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 21/71 | LOSS: 5.039666203546486e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 22/71 | LOSS: 5.048053579863019e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 23/71 | LOSS: 5.048086147022938e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 24/71 | LOSS: 5.0406425543769725e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 25/71 | LOSS: 5.046792151775802e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 26/71 | LOSS: 5.1396700318609534e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 27/71 | LOSS: 5.1316553546192675e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 28/71 | LOSS: 5.138483770835968e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 29/71 | LOSS: 5.178984292797395e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 30/71 | LOSS: 5.136509571457282e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 31/71 | LOSS: 5.1704875829727825e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 32/71 | LOSS: 5.1419046802479756e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 33/71 | LOSS: 5.1561783440025895e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 34/71 | LOSS: 5.144549452649828e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 35/71 | LOSS: 5.1813936655283515e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 36/71 | LOSS: 5.182877553247758e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 37/71 | LOSS: 5.1599000399667265e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 38/71 | LOSS: 5.1933805666638554e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 39/71 | LOSS: 5.179169772873138e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 40/71 | LOSS: 5.1637693570405756e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 41/71 | LOSS: 5.150344133566943e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 42/71 | LOSS: 5.144177320353053e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 43/71 | LOSS: 5.164699783480583e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 44/71 | LOSS: 5.166786094276985e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 45/71 | LOSS: 5.221187650575935e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 46/71 | LOSS: 5.234586287189543e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 47/71 | LOSS: 5.2727847048572585e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 48/71 | LOSS: 5.308134293546591e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 49/71 | LOSS: 5.322512879502028e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 50/71 | LOSS: 5.407327633150214e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 51/71 | LOSS: 5.431536972257443e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 52/71 | LOSS: 5.487912103318486e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 53/71 | LOSS: 5.487039541347056e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 54/71 | LOSS: 5.5121736791492864e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 55/71 | LOSS: 5.483501240632904e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 56/71 | LOSS: 5.542527584398876e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 57/71 | LOSS: 5.530789190617787e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 58/71 | LOSS: 5.545033873512678e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 59/71 | LOSS: 5.591447385692542e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 60/71 | LOSS: 5.5829911661185474e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 61/71 | LOSS: 5.585329686695214e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 62/71 | LOSS: 5.595632364207949e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 63/71 | LOSS: 5.639605880958243e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 64/71 | LOSS: 5.6646655209008784e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 65/71 | LOSS: 5.659578729330061e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 66/71 | LOSS: 5.649116667597416e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 67/71 | LOSS: 5.643108218410977e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 68/71 | LOSS: 5.667024639030611e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 69/71 | LOSS: 5.652171837157636e-06\n",
      "TRAIN: EPOCH 252/1000 | BATCH 70/71 | LOSS: 5.654230508426595e-06\n",
      "VAL: EPOCH 252/1000 | BATCH 0/8 | LOSS: 1.0183633094129618e-05\n",
      "VAL: EPOCH 252/1000 | BATCH 1/8 | LOSS: 9.240103736374294e-06\n",
      "VAL: EPOCH 252/1000 | BATCH 2/8 | LOSS: 8.555052318115486e-06\n",
      "VAL: EPOCH 252/1000 | BATCH 3/8 | LOSS: 9.010611051962769e-06\n",
      "VAL: EPOCH 252/1000 | BATCH 4/8 | LOSS: 8.6001514318923e-06\n",
      "VAL: EPOCH 252/1000 | BATCH 5/8 | LOSS: 8.122822843385316e-06\n",
      "VAL: EPOCH 252/1000 | BATCH 6/8 | LOSS: 8.02667195135395e-06\n",
      "VAL: EPOCH 252/1000 | BATCH 7/8 | LOSS: 7.816404718141712e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 0/71 | LOSS: 1.1576125871215481e-05\n",
      "TRAIN: EPOCH 253/1000 | BATCH 1/71 | LOSS: 8.509661256539403e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 2/71 | LOSS: 8.835099227629447e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 3/71 | LOSS: 8.093702263067826e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 4/71 | LOSS: 7.988137804204598e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 5/71 | LOSS: 7.747377821942791e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 6/71 | LOSS: 7.784996601653152e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 7/71 | LOSS: 7.474640369764529e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 8/71 | LOSS: 7.9478763331685e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 9/71 | LOSS: 7.761112146909e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 10/71 | LOSS: 7.808386767878387e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 11/71 | LOSS: 7.605321305466835e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 12/71 | LOSS: 7.446460970711017e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 13/71 | LOSS: 7.322371435033606e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 14/71 | LOSS: 7.206278405647027e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 15/71 | LOSS: 7.1280371116699826e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 16/71 | LOSS: 7.0465274107204205e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 17/71 | LOSS: 7.039231528930638e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 18/71 | LOSS: 6.916143775015371e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 19/71 | LOSS: 6.81537537730037e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 20/71 | LOSS: 6.718834919454475e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 21/71 | LOSS: 6.6776911518734945e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 22/71 | LOSS: 6.636338758139375e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 23/71 | LOSS: 6.593582061971877e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 24/71 | LOSS: 6.574840044777375e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 25/71 | LOSS: 6.50070034260772e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 26/71 | LOSS: 6.45194776788466e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 27/71 | LOSS: 6.430904347455778e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 28/71 | LOSS: 6.386763994543861e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 29/71 | LOSS: 6.345712426991667e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 30/71 | LOSS: 6.318771181301394e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 31/71 | LOSS: 6.312076507697384e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 32/71 | LOSS: 6.295512307450267e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 33/71 | LOSS: 6.301521339350181e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 34/71 | LOSS: 6.3347376323820625e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 35/71 | LOSS: 6.301994428920605e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 36/71 | LOSS: 6.3104396734136545e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 37/71 | LOSS: 6.2710672218121254e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 38/71 | LOSS: 6.245386441897165e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 39/71 | LOSS: 6.2456599152938e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 40/71 | LOSS: 6.2366538392181126e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 41/71 | LOSS: 6.239066979538793e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 42/71 | LOSS: 6.219097549335349e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 43/71 | LOSS: 6.197860185156142e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 44/71 | LOSS: 6.144920290454239e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 45/71 | LOSS: 6.170939168046259e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 46/71 | LOSS: 6.13601439018818e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 47/71 | LOSS: 6.131466686080482e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 48/71 | LOSS: 6.095158833162788e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 49/71 | LOSS: 6.074227649150998e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 50/71 | LOSS: 6.069329745015221e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 51/71 | LOSS: 6.034226019507444e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 52/71 | LOSS: 6.026100522230647e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 53/71 | LOSS: 5.982852018051845e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 54/71 | LOSS: 5.956783835162324e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 55/71 | LOSS: 5.9256827691050214e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 56/71 | LOSS: 5.925681698340269e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 57/71 | LOSS: 5.9677456255454234e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 58/71 | LOSS: 5.984310102915407e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 59/71 | LOSS: 5.972748742048376e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 60/71 | LOSS: 5.971288709454219e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 61/71 | LOSS: 5.986663689972991e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 62/71 | LOSS: 5.966297673391334e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 63/71 | LOSS: 5.975803787805489e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 64/71 | LOSS: 5.9612284390963256e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 65/71 | LOSS: 5.968416079474881e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 66/71 | LOSS: 5.949150098745325e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 67/71 | LOSS: 5.942913811154808e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 68/71 | LOSS: 5.930095869479229e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 69/71 | LOSS: 5.9218387962443685e-06\n",
      "TRAIN: EPOCH 253/1000 | BATCH 70/71 | LOSS: 5.895809312611343e-06\n",
      "VAL: EPOCH 253/1000 | BATCH 0/8 | LOSS: 6.020090495439945e-06\n",
      "VAL: EPOCH 253/1000 | BATCH 1/8 | LOSS: 5.278573098621564e-06\n",
      "VAL: EPOCH 253/1000 | BATCH 2/8 | LOSS: 5.272103559642953e-06\n",
      "VAL: EPOCH 253/1000 | BATCH 3/8 | LOSS: 5.4383754104492255e-06\n",
      "VAL: EPOCH 253/1000 | BATCH 4/8 | LOSS: 5.402957503974903e-06\n",
      "VAL: EPOCH 253/1000 | BATCH 5/8 | LOSS: 5.241277146221061e-06\n",
      "VAL: EPOCH 253/1000 | BATCH 6/8 | LOSS: 5.137037338239939e-06\n",
      "VAL: EPOCH 253/1000 | BATCH 7/8 | LOSS: 5.101941951579647e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 0/71 | LOSS: 5.4993083722365554e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 1/71 | LOSS: 5.147753881828976e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 2/71 | LOSS: 5.249597961665131e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 3/71 | LOSS: 5.841961865371559e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 4/71 | LOSS: 5.813026473333594e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 5/71 | LOSS: 5.9728164008750655e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 6/71 | LOSS: 5.767639127693006e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 7/71 | LOSS: 6.1229541188367875e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 8/71 | LOSS: 6.065318100607126e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 9/71 | LOSS: 6.2417922890745105e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 10/71 | LOSS: 6.176675139034739e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 11/71 | LOSS: 6.27905842520704e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 12/71 | LOSS: 6.16749209187289e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 13/71 | LOSS: 6.204757580365237e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 14/71 | LOSS: 6.130514793767361e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 15/71 | LOSS: 6.097257056580929e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 16/71 | LOSS: 6.2764071812767615e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 17/71 | LOSS: 6.207460021161953e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 18/71 | LOSS: 6.2836330997877095e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 19/71 | LOSS: 6.290876149250834e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 20/71 | LOSS: 6.214570779169056e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 21/71 | LOSS: 6.167386655678539e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 22/71 | LOSS: 6.146461403303597e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 23/71 | LOSS: 6.093620129377086e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 24/71 | LOSS: 6.031677348801168e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 25/71 | LOSS: 6.056260872355779e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 26/71 | LOSS: 6.097680549525346e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 27/71 | LOSS: 6.195397353232173e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 28/71 | LOSS: 6.144615577670328e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 29/71 | LOSS: 6.162921332967623e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 30/71 | LOSS: 6.137847087243685e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 31/71 | LOSS: 6.093976281817959e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 32/71 | LOSS: 6.181608803286288e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 33/71 | LOSS: 6.1251180073561365e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 34/71 | LOSS: 6.19900285008563e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 35/71 | LOSS: 6.163453033675776e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 36/71 | LOSS: 6.172538608528133e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 37/71 | LOSS: 6.219329694240035e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 38/71 | LOSS: 6.24072366078438e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 39/71 | LOSS: 6.278607020249183e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 40/71 | LOSS: 6.264604397118801e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 41/71 | LOSS: 6.335197548694504e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 42/71 | LOSS: 6.317487066891857e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 43/71 | LOSS: 6.375030042446187e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 44/71 | LOSS: 6.329396415417755e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 45/71 | LOSS: 6.303616057424654e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 46/71 | LOSS: 6.288808213247784e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 47/71 | LOSS: 6.254107546510568e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 48/71 | LOSS: 6.250297386087572e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 49/71 | LOSS: 6.246125694815419e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 50/71 | LOSS: 6.246360436646273e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 51/71 | LOSS: 6.247511053586025e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 52/71 | LOSS: 6.212820496656141e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 53/71 | LOSS: 6.227225816878166e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 54/71 | LOSS: 6.209882609585872e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 55/71 | LOSS: 6.257554681659323e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 56/71 | LOSS: 6.237514027592975e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 57/71 | LOSS: 6.21837753458059e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 58/71 | LOSS: 6.195204905915011e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 59/71 | LOSS: 6.177889705819931e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 60/71 | LOSS: 6.180409102063229e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 61/71 | LOSS: 6.158355796698391e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 62/71 | LOSS: 6.159142909411227e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 63/71 | LOSS: 6.156458546513477e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 64/71 | LOSS: 6.136467100777037e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 65/71 | LOSS: 6.113735122253826e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 66/71 | LOSS: 6.08727941653342e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 67/71 | LOSS: 6.08271137908185e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 68/71 | LOSS: 6.0642882076997235e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 69/71 | LOSS: 6.0455655134969025e-06\n",
      "TRAIN: EPOCH 254/1000 | BATCH 70/71 | LOSS: 6.010621806946263e-06\n",
      "VAL: EPOCH 254/1000 | BATCH 0/8 | LOSS: 6.537216904689558e-06\n",
      "VAL: EPOCH 254/1000 | BATCH 1/8 | LOSS: 5.585373855865328e-06\n",
      "VAL: EPOCH 254/1000 | BATCH 2/8 | LOSS: 5.364798198570497e-06\n",
      "VAL: EPOCH 254/1000 | BATCH 3/8 | LOSS: 5.588422709479346e-06\n",
      "VAL: EPOCH 254/1000 | BATCH 4/8 | LOSS: 5.4589419050898865e-06\n",
      "VAL: EPOCH 254/1000 | BATCH 5/8 | LOSS: 5.264605230574186e-06\n",
      "VAL: EPOCH 254/1000 | BATCH 6/8 | LOSS: 5.2531744846159456e-06\n",
      "VAL: EPOCH 254/1000 | BATCH 7/8 | LOSS: 5.180369100798998e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 0/71 | LOSS: 4.7458315748372115e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 1/71 | LOSS: 5.121114554640371e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 2/71 | LOSS: 5.100739751166354e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 3/71 | LOSS: 5.1688314215425635e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 4/71 | LOSS: 4.915801400784403e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 5/71 | LOSS: 4.797758568505135e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 6/71 | LOSS: 4.795691893377807e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 7/71 | LOSS: 4.685271562721027e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 8/71 | LOSS: 4.759637855992575e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 9/71 | LOSS: 4.702681326307356e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 10/71 | LOSS: 4.6838519913514824e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 11/71 | LOSS: 4.760524461744353e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 12/71 | LOSS: 4.874307565982095e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 13/71 | LOSS: 4.824754179415842e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 14/71 | LOSS: 4.969558115893354e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 15/71 | LOSS: 4.926625877033075e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 16/71 | LOSS: 4.956661755653947e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 17/71 | LOSS: 4.963145329384133e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 18/71 | LOSS: 5.041120426827356e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 19/71 | LOSS: 5.020710091230285e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 20/71 | LOSS: 4.973804628943548e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 21/71 | LOSS: 5.026222817667241e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 22/71 | LOSS: 5.068568162127833e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 23/71 | LOSS: 5.0364047297080106e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 24/71 | LOSS: 5.063266162323998e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 25/71 | LOSS: 5.089996945501368e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 26/71 | LOSS: 5.049479855019685e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 27/71 | LOSS: 5.112626662854122e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 28/71 | LOSS: 5.137939391214127e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 29/71 | LOSS: 5.134419325258932e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 30/71 | LOSS: 5.266081599632255e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 31/71 | LOSS: 5.2704829585081825e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 32/71 | LOSS: 5.312276050862045e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 33/71 | LOSS: 5.313760017516495e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 34/71 | LOSS: 5.29287755333436e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 35/71 | LOSS: 5.292103044970038e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 36/71 | LOSS: 5.304875968077623e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 37/71 | LOSS: 5.279021371806218e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 38/71 | LOSS: 5.296429522926561e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 39/71 | LOSS: 5.359098622648162e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 40/71 | LOSS: 5.328198667706513e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 41/71 | LOSS: 5.364994043096279e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 42/71 | LOSS: 5.383640898291433e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 43/71 | LOSS: 5.387852690381061e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 44/71 | LOSS: 5.3854364624486256e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 45/71 | LOSS: 5.381389596352718e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 46/71 | LOSS: 5.3969613273171064e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 47/71 | LOSS: 5.376946035084984e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 48/71 | LOSS: 5.374072910000199e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 49/71 | LOSS: 5.400007767093484e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 50/71 | LOSS: 5.4142591597708675e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 51/71 | LOSS: 5.407507055329356e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 52/71 | LOSS: 5.447827720243642e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 53/71 | LOSS: 5.44535112533561e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 54/71 | LOSS: 5.426586854313924e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 55/71 | LOSS: 5.417991441650624e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 56/71 | LOSS: 5.4190635669408695e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 57/71 | LOSS: 5.396685725733325e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 58/71 | LOSS: 5.398520874747438e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 59/71 | LOSS: 5.41735568428218e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 60/71 | LOSS: 5.454447687714223e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 61/71 | LOSS: 5.472203024363543e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 62/71 | LOSS: 5.465240476388274e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 63/71 | LOSS: 5.499680789000649e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 64/71 | LOSS: 5.5308761567665404e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 65/71 | LOSS: 5.519234105811462e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 66/71 | LOSS: 5.516146736783358e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 67/71 | LOSS: 5.519913255578585e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 68/71 | LOSS: 5.530021718331609e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 69/71 | LOSS: 5.5462640375480985e-06\n",
      "TRAIN: EPOCH 255/1000 | BATCH 70/71 | LOSS: 5.544191436187453e-06\n",
      "VAL: EPOCH 255/1000 | BATCH 0/8 | LOSS: 1.1559022823348641e-05\n",
      "VAL: EPOCH 255/1000 | BATCH 1/8 | LOSS: 1.0639093488862272e-05\n",
      "VAL: EPOCH 255/1000 | BATCH 2/8 | LOSS: 1.0545808133125925e-05\n",
      "VAL: EPOCH 255/1000 | BATCH 3/8 | LOSS: 1.0352097433496965e-05\n",
      "VAL: EPOCH 255/1000 | BATCH 4/8 | LOSS: 1.0460673547640908e-05\n",
      "VAL: EPOCH 255/1000 | BATCH 5/8 | LOSS: 9.962860455440628e-06\n",
      "VAL: EPOCH 255/1000 | BATCH 6/8 | LOSS: 1.0209990835262164e-05\n",
      "VAL: EPOCH 255/1000 | BATCH 7/8 | LOSS: 1.0172465579216805e-05\n",
      "TRAIN: EPOCH 256/1000 | BATCH 0/71 | LOSS: 1.1194156286364887e-05\n",
      "TRAIN: EPOCH 256/1000 | BATCH 1/71 | LOSS: 8.112779141811188e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 2/71 | LOSS: 7.531646891341855e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 3/71 | LOSS: 7.537659485024051e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 4/71 | LOSS: 7.875759001763072e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 5/71 | LOSS: 7.3439296102151275e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 6/71 | LOSS: 7.166510191122402e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 7/71 | LOSS: 7.099705214841379e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 8/71 | LOSS: 6.924389456091578e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 9/71 | LOSS: 6.64034691908455e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 10/71 | LOSS: 6.706428600657753e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 11/71 | LOSS: 6.669933516908107e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 12/71 | LOSS: 6.540225548143588e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 13/71 | LOSS: 6.520896412049686e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 14/71 | LOSS: 6.609835751684538e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 15/71 | LOSS: 6.503029283067008e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 16/71 | LOSS: 6.490595724779482e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 17/71 | LOSS: 6.535929110719331e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 18/71 | LOSS: 6.479234511253277e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 19/71 | LOSS: 6.34320055041826e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 20/71 | LOSS: 6.271399700251225e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 21/71 | LOSS: 6.244806947754776e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 22/71 | LOSS: 6.221405299030543e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 23/71 | LOSS: 6.186449307203172e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 24/71 | LOSS: 6.212217767824768e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 25/71 | LOSS: 6.194378862836367e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 26/71 | LOSS: 6.130112397942058e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 27/71 | LOSS: 6.089573982340621e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 28/71 | LOSS: 6.062863772528764e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 29/71 | LOSS: 6.022909724379133e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 30/71 | LOSS: 6.030467059850531e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 31/71 | LOSS: 6.020647283833114e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 32/71 | LOSS: 5.947243311531951e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 33/71 | LOSS: 5.920528585246596e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 34/71 | LOSS: 5.905094550533769e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 35/71 | LOSS: 5.897433772538028e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 36/71 | LOSS: 5.9088118337672925e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 37/71 | LOSS: 5.880593412736121e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 38/71 | LOSS: 5.859368118949649e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 39/71 | LOSS: 5.837241627659751e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 40/71 | LOSS: 5.821046199077788e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 41/71 | LOSS: 5.852392085952965e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 42/71 | LOSS: 5.825359573654443e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 43/71 | LOSS: 5.810890999410648e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 44/71 | LOSS: 5.81065551664829e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 45/71 | LOSS: 5.7745121318436325e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 46/71 | LOSS: 5.7616976082112585e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 47/71 | LOSS: 5.744699881423306e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 48/71 | LOSS: 5.706398575725592e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 49/71 | LOSS: 5.668419426001492e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 50/71 | LOSS: 5.638860410031425e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 51/71 | LOSS: 5.609541613711582e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 52/71 | LOSS: 5.599236777376269e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 53/71 | LOSS: 5.564741134852132e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 54/71 | LOSS: 5.541079388587439e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 55/71 | LOSS: 5.527869819031496e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 56/71 | LOSS: 5.515255938996888e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 57/71 | LOSS: 5.534999484646913e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 58/71 | LOSS: 5.5357389147363805e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 59/71 | LOSS: 5.529905168562739e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 60/71 | LOSS: 5.510424559405856e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 61/71 | LOSS: 5.529484166888118e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 62/71 | LOSS: 5.539193064159629e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 63/71 | LOSS: 5.5122417066399976e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 64/71 | LOSS: 5.51849575808424e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 65/71 | LOSS: 5.524274766602585e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 66/71 | LOSS: 5.521433636272038e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 67/71 | LOSS: 5.5040401053595226e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 68/71 | LOSS: 5.5670786186174554e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 69/71 | LOSS: 5.5523163674869075e-06\n",
      "TRAIN: EPOCH 256/1000 | BATCH 70/71 | LOSS: 5.585385692106235e-06\n",
      "VAL: EPOCH 256/1000 | BATCH 0/8 | LOSS: 6.718260465277126e-06\n",
      "VAL: EPOCH 256/1000 | BATCH 1/8 | LOSS: 5.715093038816121e-06\n",
      "VAL: EPOCH 256/1000 | BATCH 2/8 | LOSS: 5.417343193888276e-06\n",
      "VAL: EPOCH 256/1000 | BATCH 3/8 | LOSS: 5.615350005427899e-06\n",
      "VAL: EPOCH 256/1000 | BATCH 4/8 | LOSS: 5.469171719596489e-06\n",
      "VAL: EPOCH 256/1000 | BATCH 5/8 | LOSS: 5.176224552390825e-06\n",
      "VAL: EPOCH 256/1000 | BATCH 6/8 | LOSS: 5.075076322620069e-06\n",
      "VAL: EPOCH 256/1000 | BATCH 7/8 | LOSS: 4.9519057370162045e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 0/71 | LOSS: 4.57590294900001e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 1/71 | LOSS: 6.411015647245222e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 2/71 | LOSS: 6.0920298589432304e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 3/71 | LOSS: 6.490391342595103e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 4/71 | LOSS: 6.355878576869145e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 5/71 | LOSS: 6.017557704277958e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 6/71 | LOSS: 6.160924255839616e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 7/71 | LOSS: 6.05894632599302e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 8/71 | LOSS: 6.162938461784506e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 9/71 | LOSS: 6.073417307561613e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 10/71 | LOSS: 6.025214753952406e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 11/71 | LOSS: 6.087620363359747e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 12/71 | LOSS: 5.994574131853789e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 13/71 | LOSS: 5.938743145504434e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 14/71 | LOSS: 6.026263023765447e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 15/71 | LOSS: 6.1092123075923155e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 16/71 | LOSS: 6.031004217862467e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 17/71 | LOSS: 6.172908974298884e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 18/71 | LOSS: 6.197145610617315e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 19/71 | LOSS: 6.18612480138836e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 20/71 | LOSS: 6.265619525774604e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 21/71 | LOSS: 6.284573973144606e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 22/71 | LOSS: 6.3188383097914755e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 23/71 | LOSS: 6.329691890035368e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 24/71 | LOSS: 6.325755166471936e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 25/71 | LOSS: 6.308312619624373e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 26/71 | LOSS: 6.262511861778977e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 27/71 | LOSS: 6.252043720321256e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 28/71 | LOSS: 6.2430908342478156e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 29/71 | LOSS: 6.205972901322335e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 30/71 | LOSS: 6.2729283942439175e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 31/71 | LOSS: 6.2173471917503775e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 32/71 | LOSS: 6.200348114802954e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 33/71 | LOSS: 6.192207071281326e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 34/71 | LOSS: 6.195439345901832e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 35/71 | LOSS: 6.152217035075107e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 36/71 | LOSS: 6.175493003743091e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 37/71 | LOSS: 6.1754873106685025e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 38/71 | LOSS: 6.160087557360291e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 39/71 | LOSS: 6.139415461348108e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 40/71 | LOSS: 6.091241042357557e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 41/71 | LOSS: 6.076129498718988e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 42/71 | LOSS: 6.048929525456705e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 43/71 | LOSS: 6.079549848436727e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 44/71 | LOSS: 6.0482652366368306e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 45/71 | LOSS: 5.991507400129036e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 46/71 | LOSS: 5.979735770450404e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 47/71 | LOSS: 5.924758132399195e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 48/71 | LOSS: 5.875109782580842e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 49/71 | LOSS: 5.872962292414741e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 50/71 | LOSS: 5.865427533089864e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 51/71 | LOSS: 5.82194634437026e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 52/71 | LOSS: 5.847845275055525e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 53/71 | LOSS: 5.832942145355412e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 54/71 | LOSS: 5.844828303286869e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 55/71 | LOSS: 5.818625506240746e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 56/71 | LOSS: 5.820216150217068e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 57/71 | LOSS: 5.803621881004174e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 58/71 | LOSS: 5.8273582939271645e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 59/71 | LOSS: 5.786469235620946e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 60/71 | LOSS: 5.7595844200328485e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 61/71 | LOSS: 5.7841935857492984e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 62/71 | LOSS: 5.774552319630785e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 63/71 | LOSS: 5.76979194732985e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 64/71 | LOSS: 5.781316024065465e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 65/71 | LOSS: 5.770659768269539e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 66/71 | LOSS: 5.754031591815005e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 67/71 | LOSS: 5.7422682723828235e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 68/71 | LOSS: 5.731285468293894e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 69/71 | LOSS: 5.724240262290031e-06\n",
      "TRAIN: EPOCH 257/1000 | BATCH 70/71 | LOSS: 5.775092279396682e-06\n",
      "VAL: EPOCH 257/1000 | BATCH 0/8 | LOSS: 8.576144864491653e-06\n",
      "VAL: EPOCH 257/1000 | BATCH 1/8 | LOSS: 7.658497906959383e-06\n",
      "VAL: EPOCH 257/1000 | BATCH 2/8 | LOSS: 7.582401243174293e-06\n",
      "VAL: EPOCH 257/1000 | BATCH 3/8 | LOSS: 7.590468044327281e-06\n",
      "VAL: EPOCH 257/1000 | BATCH 4/8 | LOSS: 7.598282081744401e-06\n",
      "VAL: EPOCH 257/1000 | BATCH 5/8 | LOSS: 7.15387560982587e-06\n",
      "VAL: EPOCH 257/1000 | BATCH 6/8 | LOSS: 7.192689281509956e-06\n",
      "VAL: EPOCH 257/1000 | BATCH 7/8 | LOSS: 7.084486355779518e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 0/71 | LOSS: 5.60248827241594e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 1/71 | LOSS: 6.2800650084682275e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 2/71 | LOSS: 5.684420405790054e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 3/71 | LOSS: 5.870674726793368e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 4/71 | LOSS: 6.097412097005872e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 5/71 | LOSS: 5.979365444848857e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 6/71 | LOSS: 5.961348246533557e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 7/71 | LOSS: 5.967557342501095e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 8/71 | LOSS: 5.903721861007701e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 9/71 | LOSS: 5.8494410950515885e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 10/71 | LOSS: 5.823540215549821e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 11/71 | LOSS: 5.760801400356286e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 12/71 | LOSS: 5.834935564053012e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 13/71 | LOSS: 5.733525150779835e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 14/71 | LOSS: 5.808733961506126e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 15/71 | LOSS: 5.807436735949523e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 16/71 | LOSS: 5.709209209967144e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 17/71 | LOSS: 5.765226205767249e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 18/71 | LOSS: 5.720660905545198e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 19/71 | LOSS: 5.67304912237887e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 20/71 | LOSS: 5.609098458253789e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 21/71 | LOSS: 5.567587922169795e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 22/71 | LOSS: 5.558883221174651e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 23/71 | LOSS: 5.511957112958044e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 24/71 | LOSS: 5.591050230577821e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 25/71 | LOSS: 5.540842039408744e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 26/71 | LOSS: 5.608881178044679e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 27/71 | LOSS: 5.5574337726024425e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 28/71 | LOSS: 5.569336845555567e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 29/71 | LOSS: 5.599655651167268e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 30/71 | LOSS: 5.595492145784515e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 31/71 | LOSS: 5.6617026586991415e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 32/71 | LOSS: 5.631592874565148e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 33/71 | LOSS: 5.640888676840832e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 34/71 | LOSS: 5.5932033059694475e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 35/71 | LOSS: 5.638185168916405e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 36/71 | LOSS: 5.594694382150034e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 37/71 | LOSS: 5.583959483900751e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 38/71 | LOSS: 5.590985881262429e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 39/71 | LOSS: 5.545704300402576e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 40/71 | LOSS: 5.526965595913126e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 41/71 | LOSS: 5.499525589887829e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 42/71 | LOSS: 5.475901214721975e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 43/71 | LOSS: 5.463057070962598e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 44/71 | LOSS: 5.454058130756796e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 45/71 | LOSS: 5.433088438661324e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 46/71 | LOSS: 5.4129813024507924e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 47/71 | LOSS: 5.43201336237568e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 48/71 | LOSS: 5.458805807560269e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 49/71 | LOSS: 5.452561117635923e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 50/71 | LOSS: 5.451405482459699e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 51/71 | LOSS: 5.44446976828812e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 52/71 | LOSS: 5.441738327170002e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 53/71 | LOSS: 5.447694455277222e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 54/71 | LOSS: 5.459553573349364e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 55/71 | LOSS: 5.440334137217308e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 56/71 | LOSS: 5.433094852298637e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 57/71 | LOSS: 5.425084940850803e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 58/71 | LOSS: 5.4268171047393264e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 59/71 | LOSS: 5.412221154680689e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 60/71 | LOSS: 5.403623819176096e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 61/71 | LOSS: 5.416202670418105e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 62/71 | LOSS: 5.4139273916440465e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 63/71 | LOSS: 5.4223997878466434e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 64/71 | LOSS: 5.427593910448754e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 65/71 | LOSS: 5.421833727516721e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 66/71 | LOSS: 5.421815996072185e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 67/71 | LOSS: 5.41997268127863e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 68/71 | LOSS: 5.436949540244709e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 69/71 | LOSS: 5.428935849392604e-06\n",
      "TRAIN: EPOCH 258/1000 | BATCH 70/71 | LOSS: 5.433976517631147e-06\n",
      "VAL: EPOCH 258/1000 | BATCH 0/8 | LOSS: 1.0550716069701593e-05\n",
      "VAL: EPOCH 258/1000 | BATCH 1/8 | LOSS: 1.005775175144663e-05\n",
      "VAL: EPOCH 258/1000 | BATCH 2/8 | LOSS: 9.426534840410264e-06\n",
      "VAL: EPOCH 258/1000 | BATCH 3/8 | LOSS: 9.586274700268405e-06\n",
      "VAL: EPOCH 258/1000 | BATCH 4/8 | LOSS: 9.468806092627347e-06\n",
      "VAL: EPOCH 258/1000 | BATCH 5/8 | LOSS: 8.947747422401639e-06\n",
      "VAL: EPOCH 258/1000 | BATCH 6/8 | LOSS: 8.927474092550775e-06\n",
      "VAL: EPOCH 258/1000 | BATCH 7/8 | LOSS: 8.7502875771861e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 0/71 | LOSS: 9.547569788992405e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 1/71 | LOSS: 7.188167728600092e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 2/71 | LOSS: 8.598626664024778e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 3/71 | LOSS: 7.327855996663857e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 4/71 | LOSS: 8.074301422311692e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 5/71 | LOSS: 7.598372955423353e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 6/71 | LOSS: 7.5286128386713765e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 7/71 | LOSS: 7.383080571798928e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 8/71 | LOSS: 7.072806662714963e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 9/71 | LOSS: 6.9633242674171925e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 10/71 | LOSS: 6.717650773101592e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 11/71 | LOSS: 6.713665811730607e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 12/71 | LOSS: 6.541449098361996e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 13/71 | LOSS: 6.587310183801622e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 14/71 | LOSS: 6.551205054468786e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 15/71 | LOSS: 6.529596817017591e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 16/71 | LOSS: 6.526298420794774e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 17/71 | LOSS: 6.377189619241916e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 18/71 | LOSS: 6.400475788388193e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 19/71 | LOSS: 6.271647566791216e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 20/71 | LOSS: 6.1766158112385755e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 21/71 | LOSS: 6.117586095073917e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 22/71 | LOSS: 6.106493895893673e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 23/71 | LOSS: 6.106582873144362e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 24/71 | LOSS: 6.054693349142326e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 25/71 | LOSS: 6.081790458595676e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 26/71 | LOSS: 6.057763416551838e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 27/71 | LOSS: 6.0441834161143305e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 28/71 | LOSS: 6.016528459440451e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 29/71 | LOSS: 5.990428568717713e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 30/71 | LOSS: 5.949400034413914e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 31/71 | LOSS: 5.917476613603867e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 32/71 | LOSS: 5.883450671028571e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 33/71 | LOSS: 5.848588631017585e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 34/71 | LOSS: 5.852590376369855e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 35/71 | LOSS: 5.808944517108709e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 36/71 | LOSS: 5.816857058537583e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 37/71 | LOSS: 5.791415883562111e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 38/71 | LOSS: 5.7554310469407565e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 39/71 | LOSS: 5.752909191869548e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 40/71 | LOSS: 5.7613232257142776e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 41/71 | LOSS: 5.752528728581161e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 42/71 | LOSS: 5.738485046898605e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 43/71 | LOSS: 5.717649545741469e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 44/71 | LOSS: 5.705218498203774e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 45/71 | LOSS: 5.705169077491914e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 46/71 | LOSS: 5.681322528154e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 47/71 | LOSS: 5.668985949114358e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 48/71 | LOSS: 5.649402319992375e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 49/71 | LOSS: 5.643204158332082e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 50/71 | LOSS: 5.649522915750733e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 51/71 | LOSS: 5.631194059018386e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 52/71 | LOSS: 5.644490316489333e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 53/71 | LOSS: 5.669624409131723e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 54/71 | LOSS: 5.6554735816792926e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 55/71 | LOSS: 5.677957574334869e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 56/71 | LOSS: 5.664352987591165e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 57/71 | LOSS: 5.7060463734223015e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 58/71 | LOSS: 5.707228518662367e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 59/71 | LOSS: 5.703697866010771e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 60/71 | LOSS: 5.717794361085818e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 61/71 | LOSS: 5.750106545805935e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 62/71 | LOSS: 5.732524569461858e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 63/71 | LOSS: 5.728882655375855e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 64/71 | LOSS: 5.7773738192806304e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 65/71 | LOSS: 5.760243821538152e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 66/71 | LOSS: 5.8019072435963065e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 67/71 | LOSS: 5.847950994982111e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 68/71 | LOSS: 5.861850866864003e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 69/71 | LOSS: 5.9130914451088755e-06\n",
      "TRAIN: EPOCH 259/1000 | BATCH 70/71 | LOSS: 5.944094725787757e-06\n",
      "VAL: EPOCH 259/1000 | BATCH 0/8 | LOSS: 1.6365629562642425e-05\n",
      "VAL: EPOCH 259/1000 | BATCH 1/8 | LOSS: 1.5092983630893286e-05\n",
      "VAL: EPOCH 259/1000 | BATCH 2/8 | LOSS: 1.4586471176395813e-05\n",
      "VAL: EPOCH 259/1000 | BATCH 3/8 | LOSS: 1.5058226381370332e-05\n",
      "VAL: EPOCH 259/1000 | BATCH 4/8 | LOSS: 1.4655063205282204e-05\n",
      "VAL: EPOCH 259/1000 | BATCH 5/8 | LOSS: 1.4067621426268792e-05\n",
      "VAL: EPOCH 259/1000 | BATCH 6/8 | LOSS: 1.4131358901587581e-05\n",
      "VAL: EPOCH 259/1000 | BATCH 7/8 | LOSS: 1.3830440821038792e-05\n",
      "TRAIN: EPOCH 260/1000 | BATCH 0/71 | LOSS: 1.1058566997235175e-05\n",
      "TRAIN: EPOCH 260/1000 | BATCH 1/71 | LOSS: 7.5270368142810185e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 2/71 | LOSS: 8.335651424810445e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 3/71 | LOSS: 7.605571681779111e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 4/71 | LOSS: 7.469934644177556e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 5/71 | LOSS: 7.147398567515968e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 6/71 | LOSS: 6.785154580159412e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 7/71 | LOSS: 6.675844872461312e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 8/71 | LOSS: 6.39322676458202e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 9/71 | LOSS: 6.560731935678632e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 10/71 | LOSS: 6.448797772794602e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 11/71 | LOSS: 6.535813781738398e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 12/71 | LOSS: 6.5370657536662365e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 13/71 | LOSS: 6.497468478171088e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 14/71 | LOSS: 6.545494670717744e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 15/71 | LOSS: 6.4074573344896635e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 16/71 | LOSS: 6.500467942019254e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 17/71 | LOSS: 6.386251874977865e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 18/71 | LOSS: 6.420752685555367e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 19/71 | LOSS: 6.361238661156676e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 20/71 | LOSS: 6.372050782894283e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 21/71 | LOSS: 6.432860635124433e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 22/71 | LOSS: 6.376044102976068e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 23/71 | LOSS: 6.412456760547987e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 24/71 | LOSS: 6.460801923822146e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 25/71 | LOSS: 6.45233876639395e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 26/71 | LOSS: 6.513630372258679e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 27/71 | LOSS: 6.471014574214808e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 28/71 | LOSS: 6.511096702963876e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 29/71 | LOSS: 6.494627920498412e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 30/71 | LOSS: 6.505569628291508e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 31/71 | LOSS: 6.582941438182388e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 32/71 | LOSS: 6.508382422466776e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 33/71 | LOSS: 6.826373116970953e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 34/71 | LOSS: 6.75691939509956e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 35/71 | LOSS: 6.832202504685685e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 36/71 | LOSS: 6.824686057075883e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 37/71 | LOSS: 6.857928838144289e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 38/71 | LOSS: 6.8086149160360665e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 39/71 | LOSS: 6.815165829721082e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 40/71 | LOSS: 6.88992811153251e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 41/71 | LOSS: 6.829153400238803e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 42/71 | LOSS: 6.942642865523826e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 43/71 | LOSS: 6.923406172890364e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 44/71 | LOSS: 6.924530175132935e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 45/71 | LOSS: 6.8948375566991595e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 46/71 | LOSS: 6.9338753127475625e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 47/71 | LOSS: 6.958249571198394e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 48/71 | LOSS: 6.918976980939208e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 49/71 | LOSS: 6.911301088621258e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 50/71 | LOSS: 6.877136064419984e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 51/71 | LOSS: 6.838693681087394e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 52/71 | LOSS: 6.7938142493866904e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 53/71 | LOSS: 6.7627524829192595e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 54/71 | LOSS: 6.725903313054005e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 55/71 | LOSS: 6.692084403831748e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 56/71 | LOSS: 6.664039654169117e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 57/71 | LOSS: 6.680952553543993e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 58/71 | LOSS: 6.686569538603288e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 59/71 | LOSS: 6.683686077243086e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 60/71 | LOSS: 6.69024519561017e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 61/71 | LOSS: 6.678473937427043e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 62/71 | LOSS: 6.672401288221682e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 63/71 | LOSS: 6.6684710802178415e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 64/71 | LOSS: 6.639665431206455e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 65/71 | LOSS: 6.621477943939901e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 66/71 | LOSS: 6.628393382029504e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 67/71 | LOSS: 6.605366492936703e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 68/71 | LOSS: 6.603525342644138e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 69/71 | LOSS: 6.583473467409411e-06\n",
      "TRAIN: EPOCH 260/1000 | BATCH 70/71 | LOSS: 6.648110107741725e-06\n",
      "VAL: EPOCH 260/1000 | BATCH 0/8 | LOSS: 6.406634838640457e-06\n",
      "VAL: EPOCH 260/1000 | BATCH 1/8 | LOSS: 6.396948037945549e-06\n",
      "VAL: EPOCH 260/1000 | BATCH 2/8 | LOSS: 5.9471867643878795e-06\n",
      "VAL: EPOCH 260/1000 | BATCH 3/8 | LOSS: 6.2660632238475955e-06\n",
      "VAL: EPOCH 260/1000 | BATCH 4/8 | LOSS: 6.200990355864633e-06\n",
      "VAL: EPOCH 260/1000 | BATCH 5/8 | LOSS: 5.876771865587216e-06\n",
      "VAL: EPOCH 260/1000 | BATCH 6/8 | LOSS: 5.860506332412894e-06\n",
      "VAL: EPOCH 260/1000 | BATCH 7/8 | LOSS: 5.729710778723529e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 0/71 | LOSS: 5.441936991701368e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 1/71 | LOSS: 6.415559710148955e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 2/71 | LOSS: 6.55569571487528e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 3/71 | LOSS: 6.30317765626387e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 4/71 | LOSS: 6.678445515717613e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 5/71 | LOSS: 6.300435567633637e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 6/71 | LOSS: 6.246936079280983e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 7/71 | LOSS: 6.226880032045301e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 8/71 | LOSS: 6.187310090252949e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 9/71 | LOSS: 6.222513320608414e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 10/71 | LOSS: 6.035864234118807e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 11/71 | LOSS: 6.028372846837253e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 12/71 | LOSS: 6.1189431247699895e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 13/71 | LOSS: 6.087331550069004e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 14/71 | LOSS: 6.115563852896836e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 15/71 | LOSS: 6.211658813981558e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 16/71 | LOSS: 6.2670103622675764e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 17/71 | LOSS: 6.380176930381114e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 18/71 | LOSS: 6.243193183763651e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 19/71 | LOSS: 6.511350898108504e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 20/71 | LOSS: 6.479925689615941e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 21/71 | LOSS: 6.57015543352331e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 22/71 | LOSS: 6.55877544847064e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 23/71 | LOSS: 6.582356074128863e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 24/71 | LOSS: 6.712009198963642e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 25/71 | LOSS: 6.679632546225688e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 26/71 | LOSS: 6.7710678329738715e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 27/71 | LOSS: 6.781933328576477e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 28/71 | LOSS: 6.7609731590421465e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 29/71 | LOSS: 6.7822311848431125e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 30/71 | LOSS: 6.745345334850079e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 31/71 | LOSS: 6.802655335036434e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 32/71 | LOSS: 6.838194952830566e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 33/71 | LOSS: 6.805538412342739e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 34/71 | LOSS: 6.776268868894217e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 35/71 | LOSS: 6.745082234576532e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 36/71 | LOSS: 6.701076042215415e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 37/71 | LOSS: 6.669754288611186e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 38/71 | LOSS: 6.679956081475561e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 39/71 | LOSS: 6.6503481320978605e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 40/71 | LOSS: 6.634381639830791e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 41/71 | LOSS: 6.617835641095769e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 42/71 | LOSS: 6.5624506025288124e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 43/71 | LOSS: 6.513341841954653e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 44/71 | LOSS: 6.534565192042565e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 45/71 | LOSS: 6.556558485934384e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 46/71 | LOSS: 6.5538806182380635e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 47/71 | LOSS: 6.52024978838502e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 48/71 | LOSS: 6.595702314148277e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 49/71 | LOSS: 6.561145219166065e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 50/71 | LOSS: 6.599518593586753e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 51/71 | LOSS: 6.598013031249978e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 52/71 | LOSS: 6.561586188873439e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 53/71 | LOSS: 6.6243438612690185e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 54/71 | LOSS: 6.6095188892425295e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 55/71 | LOSS: 6.665517503279261e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 56/71 | LOSS: 6.705589776015187e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 57/71 | LOSS: 6.714109100232298e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 58/71 | LOSS: 6.735629393900172e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 59/71 | LOSS: 6.6989295646635584e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 60/71 | LOSS: 6.687769439208825e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 61/71 | LOSS: 6.694887566306035e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 62/71 | LOSS: 6.6784695451450904e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 63/71 | LOSS: 6.680404929682027e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 64/71 | LOSS: 6.646211820155328e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 65/71 | LOSS: 6.638741255797371e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 66/71 | LOSS: 6.6094545937671475e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 67/71 | LOSS: 6.5869959973283315e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 68/71 | LOSS: 6.57729697854518e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 69/71 | LOSS: 6.561282251433502e-06\n",
      "TRAIN: EPOCH 261/1000 | BATCH 70/71 | LOSS: 6.532696543089655e-06\n",
      "VAL: EPOCH 261/1000 | BATCH 0/8 | LOSS: 7.455274499079678e-06\n",
      "VAL: EPOCH 261/1000 | BATCH 1/8 | LOSS: 6.7416438014333835e-06\n",
      "VAL: EPOCH 261/1000 | BATCH 2/8 | LOSS: 6.690221804698619e-06\n",
      "VAL: EPOCH 261/1000 | BATCH 3/8 | LOSS: 6.957049436095986e-06\n",
      "VAL: EPOCH 261/1000 | BATCH 4/8 | LOSS: 6.86482871969929e-06\n",
      "VAL: EPOCH 261/1000 | BATCH 5/8 | LOSS: 6.722538425189366e-06\n",
      "VAL: EPOCH 261/1000 | BATCH 6/8 | LOSS: 6.84899948412619e-06\n",
      "VAL: EPOCH 261/1000 | BATCH 7/8 | LOSS: 6.759751158824656e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 0/71 | LOSS: 4.918712875223719e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 1/71 | LOSS: 5.716655095966416e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 2/71 | LOSS: 5.9253939070913475e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 3/71 | LOSS: 5.8154270163868205e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 4/71 | LOSS: 6.100531572883483e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 5/71 | LOSS: 6.364098377768339e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 6/71 | LOSS: 6.228303910964834e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 7/71 | LOSS: 6.167273681967345e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 8/71 | LOSS: 6.303604701921965e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 9/71 | LOSS: 6.097704090279876e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 10/71 | LOSS: 6.302572439215675e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 11/71 | LOSS: 6.195779368075212e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 12/71 | LOSS: 6.213932570412558e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 13/71 | LOSS: 6.098396787430309e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 14/71 | LOSS: 6.0684560291216865e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 15/71 | LOSS: 6.212751344492062e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 16/71 | LOSS: 6.291755064868245e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 17/71 | LOSS: 6.314572172211936e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 18/71 | LOSS: 6.348195998534241e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 19/71 | LOSS: 6.315097880360554e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 20/71 | LOSS: 6.261105790188248e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 21/71 | LOSS: 6.237364583615669e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 22/71 | LOSS: 6.165006870224201e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 23/71 | LOSS: 6.191414703001404e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 24/71 | LOSS: 6.1468692911148535e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 25/71 | LOSS: 6.160162036814234e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 26/71 | LOSS: 6.160793435372869e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 27/71 | LOSS: 6.128525439765196e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 28/71 | LOSS: 6.1620239491809305e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 29/71 | LOSS: 6.125582785898587e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 30/71 | LOSS: 6.137454977673066e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 31/71 | LOSS: 6.0797340069029815e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 32/71 | LOSS: 6.129677882254879e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 33/71 | LOSS: 6.110192169431043e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 34/71 | LOSS: 6.123180719441735e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 35/71 | LOSS: 6.090137655216192e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 36/71 | LOSS: 6.109790676320601e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 37/71 | LOSS: 6.10976316669334e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 38/71 | LOSS: 6.0772060397795985e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 39/71 | LOSS: 6.065323623261065e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 40/71 | LOSS: 6.06660077897775e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 41/71 | LOSS: 6.0173496790195625e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 42/71 | LOSS: 5.989368398945243e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 43/71 | LOSS: 5.98035176773754e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 44/71 | LOSS: 5.978105218673591e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 45/71 | LOSS: 5.984378652140984e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 46/71 | LOSS: 5.963112722925638e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 47/71 | LOSS: 5.949301443782436e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 48/71 | LOSS: 5.908010168742312e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 49/71 | LOSS: 5.868023863513372e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 50/71 | LOSS: 5.890841712957184e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 51/71 | LOSS: 5.888932413164674e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 52/71 | LOSS: 5.89910766548029e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 53/71 | LOSS: 5.901859670605821e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 54/71 | LOSS: 5.886895004633433e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 55/71 | LOSS: 5.890388397996763e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 56/71 | LOSS: 5.93764525547158e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 57/71 | LOSS: 5.930116811151524e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 58/71 | LOSS: 5.912312197755979e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 59/71 | LOSS: 5.925837172071624e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 60/71 | LOSS: 5.905483256535009e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 61/71 | LOSS: 5.910371900456319e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 62/71 | LOSS: 5.898944090447566e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 63/71 | LOSS: 5.939668952237298e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 64/71 | LOSS: 5.940358460065909e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 65/71 | LOSS: 5.957403366416904e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 66/71 | LOSS: 5.983104569873939e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 67/71 | LOSS: 5.971884146514212e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 68/71 | LOSS: 5.978950893209464e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 69/71 | LOSS: 5.974771238470567e-06\n",
      "TRAIN: EPOCH 262/1000 | BATCH 70/71 | LOSS: 5.982237665411445e-06\n",
      "VAL: EPOCH 262/1000 | BATCH 0/8 | LOSS: 9.090319508686662e-06\n",
      "VAL: EPOCH 262/1000 | BATCH 1/8 | LOSS: 8.881144822225906e-06\n",
      "VAL: EPOCH 262/1000 | BATCH 2/8 | LOSS: 8.355532221078951e-06\n",
      "VAL: EPOCH 262/1000 | BATCH 3/8 | LOSS: 8.681746408001345e-06\n",
      "VAL: EPOCH 262/1000 | BATCH 4/8 | LOSS: 8.486901879223297e-06\n",
      "VAL: EPOCH 262/1000 | BATCH 5/8 | LOSS: 8.037998062112214e-06\n",
      "VAL: EPOCH 262/1000 | BATCH 6/8 | LOSS: 8.083631005969697e-06\n",
      "VAL: EPOCH 262/1000 | BATCH 7/8 | LOSS: 7.797419243615877e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 0/71 | LOSS: 8.589866411057301e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 1/71 | LOSS: 7.254599495354341e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 2/71 | LOSS: 7.32395013377148e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 3/71 | LOSS: 7.346689926635008e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 4/71 | LOSS: 6.774546818633098e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 5/71 | LOSS: 6.976798128259058e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 6/71 | LOSS: 7.0704706688827305e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 7/71 | LOSS: 6.834117186826916e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 8/71 | LOSS: 6.9954916297218815e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 9/71 | LOSS: 6.976295526328613e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 10/71 | LOSS: 7.008354921047364e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 11/71 | LOSS: 6.852998656844041e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 12/71 | LOSS: 6.878056655505833e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 13/71 | LOSS: 6.901014558025054e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 14/71 | LOSS: 6.864349810105826e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 15/71 | LOSS: 6.791268219785707e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 16/71 | LOSS: 6.7051132646156475e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 17/71 | LOSS: 6.72196031498768e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 18/71 | LOSS: 6.636158200547614e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 19/71 | LOSS: 6.573855034730513e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 20/71 | LOSS: 6.6959191393661535e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 21/71 | LOSS: 6.685798408315432e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 22/71 | LOSS: 6.726524391168546e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 23/71 | LOSS: 6.6854478291134e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 24/71 | LOSS: 6.7465257234289315e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 25/71 | LOSS: 6.722610053727224e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 26/71 | LOSS: 6.680839558410096e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 27/71 | LOSS: 6.659273172512517e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 28/71 | LOSS: 6.625259943418847e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 29/71 | LOSS: 6.590092471014941e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 30/71 | LOSS: 6.6125534460532495e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 31/71 | LOSS: 6.59530456914581e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 32/71 | LOSS: 6.589771215437919e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 33/71 | LOSS: 6.561247078827685e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 34/71 | LOSS: 6.517560749281464e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 35/71 | LOSS: 6.458429942944753e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 36/71 | LOSS: 6.404062959690408e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 37/71 | LOSS: 6.341443036274684e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 38/71 | LOSS: 6.323918510950021e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 39/71 | LOSS: 6.3041954035725215e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 40/71 | LOSS: 6.2927390722396055e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 41/71 | LOSS: 6.263018890638791e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 42/71 | LOSS: 6.23831822870903e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 43/71 | LOSS: 6.205724182770857e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 44/71 | LOSS: 6.173006467078166e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 45/71 | LOSS: 6.131046229316069e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 46/71 | LOSS: 6.1084209498423e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 47/71 | LOSS: 6.104751406610376e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 48/71 | LOSS: 6.091047722219053e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 49/71 | LOSS: 6.0387901430658535e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 50/71 | LOSS: 6.011736349690039e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 51/71 | LOSS: 5.989781610738786e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 52/71 | LOSS: 5.993540508945512e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 53/71 | LOSS: 5.9791091189254075e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 54/71 | LOSS: 5.952385105377868e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 55/71 | LOSS: 5.915947797672873e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 56/71 | LOSS: 5.899998110467813e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 57/71 | LOSS: 5.8846859591822385e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 58/71 | LOSS: 5.8633797565007476e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 59/71 | LOSS: 5.848346957767111e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 60/71 | LOSS: 5.83573219195327e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 61/71 | LOSS: 5.831105997253138e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 62/71 | LOSS: 5.8129481975999036e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 63/71 | LOSS: 5.828032549004547e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 64/71 | LOSS: 5.803923312879096e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 65/71 | LOSS: 5.805617836978689e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 66/71 | LOSS: 5.809708115398891e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 67/71 | LOSS: 5.7996922163618045e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 68/71 | LOSS: 5.787518991870667e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 69/71 | LOSS: 5.77782722887475e-06\n",
      "TRAIN: EPOCH 263/1000 | BATCH 70/71 | LOSS: 5.747424464215341e-06\n",
      "VAL: EPOCH 263/1000 | BATCH 0/8 | LOSS: 9.553720701660495e-06\n",
      "VAL: EPOCH 263/1000 | BATCH 1/8 | LOSS: 8.82915492184111e-06\n",
      "VAL: EPOCH 263/1000 | BATCH 2/8 | LOSS: 8.25089288506812e-06\n",
      "VAL: EPOCH 263/1000 | BATCH 3/8 | LOSS: 8.69717814566684e-06\n",
      "VAL: EPOCH 263/1000 | BATCH 4/8 | LOSS: 8.378266920772149e-06\n",
      "VAL: EPOCH 263/1000 | BATCH 5/8 | LOSS: 7.868905186114716e-06\n",
      "VAL: EPOCH 263/1000 | BATCH 6/8 | LOSS: 7.751419876253099e-06\n",
      "VAL: EPOCH 263/1000 | BATCH 7/8 | LOSS: 7.4733616202138364e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 0/71 | LOSS: 6.543981271533994e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 1/71 | LOSS: 5.757586905019707e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 2/71 | LOSS: 6.8558709547990775e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 3/71 | LOSS: 6.604493137274403e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 4/71 | LOSS: 7.096026092767716e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 5/71 | LOSS: 6.909358792957694e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 6/71 | LOSS: 6.794166568267558e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 7/71 | LOSS: 7.092085866133857e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 8/71 | LOSS: 6.740291812295456e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 9/71 | LOSS: 6.830065649410244e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 10/71 | LOSS: 6.814069282964655e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 11/71 | LOSS: 6.725184069485597e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 12/71 | LOSS: 6.6906579657090615e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 13/71 | LOSS: 6.793255124648567e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 14/71 | LOSS: 6.644724332242428e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 15/71 | LOSS: 6.5847387418216385e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 16/71 | LOSS: 6.4358079779434345e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 17/71 | LOSS: 6.716441829565964e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 18/71 | LOSS: 6.640259303630476e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 19/71 | LOSS: 6.608089461224154e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 20/71 | LOSS: 6.563154567543063e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 21/71 | LOSS: 6.636353067402855e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 22/71 | LOSS: 6.661714826844653e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 23/71 | LOSS: 6.61047873033264e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 24/71 | LOSS: 6.801466824981617e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 25/71 | LOSS: 6.853754939584178e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 26/71 | LOSS: 6.760275095142855e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 27/71 | LOSS: 6.741143092118104e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 28/71 | LOSS: 6.775436774879726e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 29/71 | LOSS: 6.8320496211526915e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 30/71 | LOSS: 6.748956190178081e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 31/71 | LOSS: 6.838256268792975e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 32/71 | LOSS: 6.792651606012827e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 33/71 | LOSS: 6.803370925969348e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 34/71 | LOSS: 6.745815673119588e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 35/71 | LOSS: 6.712381857849121e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 36/71 | LOSS: 6.692632735861038e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 37/71 | LOSS: 6.668476520357408e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 38/71 | LOSS: 6.6868946398757635e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 39/71 | LOSS: 6.676205862277129e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 40/71 | LOSS: 6.7112736271555175e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 41/71 | LOSS: 6.660456706629789e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 42/71 | LOSS: 6.699391591144715e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 43/71 | LOSS: 6.686382489946978e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 44/71 | LOSS: 6.711063603385507e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 45/71 | LOSS: 6.6652976230492955e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 46/71 | LOSS: 6.617487146131486e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 47/71 | LOSS: 6.650729555227978e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 48/71 | LOSS: 6.648223340877793e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 49/71 | LOSS: 6.6517193863546705e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 50/71 | LOSS: 6.606897315185622e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 51/71 | LOSS: 6.607288128179789e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 52/71 | LOSS: 6.5777166951793675e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 53/71 | LOSS: 6.552052321206213e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 54/71 | LOSS: 6.567065429408103e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 55/71 | LOSS: 6.559110423560404e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 56/71 | LOSS: 6.5864136271748544e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 57/71 | LOSS: 6.574434776447216e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 58/71 | LOSS: 6.56907694342485e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 59/71 | LOSS: 6.571742771181259e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 60/71 | LOSS: 6.532973821165678e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 61/71 | LOSS: 6.536904280555093e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 62/71 | LOSS: 6.537653223945415e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 63/71 | LOSS: 6.53271835204805e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 64/71 | LOSS: 6.530315183156815e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 65/71 | LOSS: 6.507705269210749e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 66/71 | LOSS: 6.551850477610141e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 67/71 | LOSS: 6.5294245881459035e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 68/71 | LOSS: 6.528900597175749e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 69/71 | LOSS: 6.526916318502377e-06\n",
      "TRAIN: EPOCH 264/1000 | BATCH 70/71 | LOSS: 6.487316145659195e-06\n",
      "VAL: EPOCH 264/1000 | BATCH 0/8 | LOSS: 8.311140845762566e-06\n",
      "VAL: EPOCH 264/1000 | BATCH 1/8 | LOSS: 7.388615586023661e-06\n",
      "VAL: EPOCH 264/1000 | BATCH 2/8 | LOSS: 7.141538693152445e-06\n",
      "VAL: EPOCH 264/1000 | BATCH 3/8 | LOSS: 7.372015147666389e-06\n",
      "VAL: EPOCH 264/1000 | BATCH 4/8 | LOSS: 7.197128888947191e-06\n",
      "VAL: EPOCH 264/1000 | BATCH 5/8 | LOSS: 6.804358387550262e-06\n",
      "VAL: EPOCH 264/1000 | BATCH 6/8 | LOSS: 6.877160753252351e-06\n",
      "VAL: EPOCH 264/1000 | BATCH 7/8 | LOSS: 6.710347349780932e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 0/71 | LOSS: 5.263367711449973e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 1/71 | LOSS: 5.417837201093789e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 2/71 | LOSS: 6.591854495733666e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 3/71 | LOSS: 6.23796790932829e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 4/71 | LOSS: 6.844287236162927e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 5/71 | LOSS: 6.4355206935336655e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 6/71 | LOSS: 6.310730376363998e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 7/71 | LOSS: 6.383495133377437e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 8/71 | LOSS: 6.3016553516111644e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 9/71 | LOSS: 6.687209861411247e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 10/71 | LOSS: 6.533006274945695e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 11/71 | LOSS: 6.570393679794506e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 12/71 | LOSS: 6.716220713525007e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 13/71 | LOSS: 6.8151256138142865e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 14/71 | LOSS: 6.878186589650189e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 15/71 | LOSS: 6.762929302794873e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 16/71 | LOSS: 6.984956358150264e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 17/71 | LOSS: 6.977883685976849e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 18/71 | LOSS: 6.968696150005072e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 19/71 | LOSS: 6.825647619734809e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 20/71 | LOSS: 6.729319033010619e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 21/71 | LOSS: 6.740341198317103e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 22/71 | LOSS: 6.797028598394599e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 23/71 | LOSS: 6.810884372043802e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 24/71 | LOSS: 6.783823337173089e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 25/71 | LOSS: 6.766464296630987e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 26/71 | LOSS: 6.86815017750767e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 27/71 | LOSS: 6.951180190688839e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 28/71 | LOSS: 6.869746310508947e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 29/71 | LOSS: 6.911430151982737e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 30/71 | LOSS: 6.9903147989187405e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 31/71 | LOSS: 6.976359827604028e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 32/71 | LOSS: 6.956805395241121e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 33/71 | LOSS: 7.027278276355526e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 34/71 | LOSS: 7.041039004564351e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 35/71 | LOSS: 7.019664394647407e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 36/71 | LOSS: 7.028092076743141e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 37/71 | LOSS: 7.016006161632647e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 38/71 | LOSS: 7.070651953164577e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 39/71 | LOSS: 7.045995619137102e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 40/71 | LOSS: 7.1634333879756456e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 41/71 | LOSS: 7.166272877969147e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 42/71 | LOSS: 7.2128321039070765e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 43/71 | LOSS: 7.159506243293766e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 44/71 | LOSS: 7.2763457258891625e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 45/71 | LOSS: 7.2612921702903295e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 46/71 | LOSS: 7.267024760407599e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 47/71 | LOSS: 7.346296030164012e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 48/71 | LOSS: 7.338357284666296e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 49/71 | LOSS: 7.331750521188951e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 50/71 | LOSS: 7.292306940607709e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 51/71 | LOSS: 7.313288506338722e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 52/71 | LOSS: 7.28300472233831e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 53/71 | LOSS: 7.243717938628699e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 54/71 | LOSS: 7.247502608978803e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 55/71 | LOSS: 7.218442200545334e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 56/71 | LOSS: 7.193087297134314e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 57/71 | LOSS: 7.1624946532869345e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 58/71 | LOSS: 7.158369907440412e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 59/71 | LOSS: 7.131207469986596e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 60/71 | LOSS: 7.087510975459915e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 61/71 | LOSS: 7.068788766944987e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 62/71 | LOSS: 7.023991021548956e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 63/71 | LOSS: 6.9748458386698076e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 64/71 | LOSS: 6.945523707075224e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 65/71 | LOSS: 6.908781146690175e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 66/71 | LOSS: 6.876814196986553e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 67/71 | LOSS: 6.836899963084707e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 68/71 | LOSS: 6.805839104239619e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 69/71 | LOSS: 6.7937404180286515e-06\n",
      "TRAIN: EPOCH 265/1000 | BATCH 70/71 | LOSS: 6.749589448178009e-06\n",
      "VAL: EPOCH 265/1000 | BATCH 0/8 | LOSS: 6.030422355252085e-06\n",
      "VAL: EPOCH 265/1000 | BATCH 1/8 | LOSS: 5.490494913829025e-06\n",
      "VAL: EPOCH 265/1000 | BATCH 2/8 | LOSS: 5.786469349307784e-06\n",
      "VAL: EPOCH 265/1000 | BATCH 3/8 | LOSS: 6.030628583175712e-06\n",
      "VAL: EPOCH 265/1000 | BATCH 4/8 | LOSS: 5.963427247479558e-06\n",
      "VAL: EPOCH 265/1000 | BATCH 5/8 | LOSS: 5.829628359303267e-06\n",
      "VAL: EPOCH 265/1000 | BATCH 6/8 | LOSS: 5.757885446655564e-06\n",
      "VAL: EPOCH 265/1000 | BATCH 7/8 | LOSS: 5.776602108653606e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 0/71 | LOSS: 5.4618058129563e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 1/71 | LOSS: 5.0724831908155465e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 2/71 | LOSS: 5.002832040190697e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 3/71 | LOSS: 5.356433462111454e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 4/71 | LOSS: 5.218529531703098e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 5/71 | LOSS: 5.108677896714653e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 6/71 | LOSS: 5.1514567270974765e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 7/71 | LOSS: 5.206501157317689e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 8/71 | LOSS: 5.1578933582479495e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 9/71 | LOSS: 5.14704820488987e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 10/71 | LOSS: 5.080790990839225e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 11/71 | LOSS: 5.271784592271918e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 12/71 | LOSS: 5.2001429629364375e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 13/71 | LOSS: 5.1206053350532395e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 14/71 | LOSS: 5.067058464192087e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 15/71 | LOSS: 5.108976466772219e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 16/71 | LOSS: 5.18751007535577e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 17/71 | LOSS: 5.205536530815556e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 18/71 | LOSS: 5.224948788389539e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 19/71 | LOSS: 5.304843944031745e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 20/71 | LOSS: 5.3362774841454145e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 21/71 | LOSS: 5.2955393708874574e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 22/71 | LOSS: 5.377868784189164e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 23/71 | LOSS: 5.403507998380519e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 24/71 | LOSS: 5.3912418661639095e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 25/71 | LOSS: 5.384284262478244e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 26/71 | LOSS: 5.373779222955582e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 27/71 | LOSS: 5.388004296946747e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 28/71 | LOSS: 5.399896069540209e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 29/71 | LOSS: 5.435483186981098e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 30/71 | LOSS: 5.428189465859642e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 31/71 | LOSS: 5.425689508342657e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 32/71 | LOSS: 5.470261688363583e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 33/71 | LOSS: 5.440792793866849e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 34/71 | LOSS: 5.417864304035902e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 35/71 | LOSS: 5.39880718659131e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 36/71 | LOSS: 5.362583394614646e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 37/71 | LOSS: 5.374777461647203e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 38/71 | LOSS: 5.348590583218118e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 39/71 | LOSS: 5.338623805073439e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 40/71 | LOSS: 5.3147125471718384e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 41/71 | LOSS: 5.3047020801819775e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 42/71 | LOSS: 5.284494385028496e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 43/71 | LOSS: 5.2746817531938594e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 44/71 | LOSS: 5.2493098640601e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 45/71 | LOSS: 5.233897433714921e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 46/71 | LOSS: 5.236516621965727e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 47/71 | LOSS: 5.200238571016295e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 48/71 | LOSS: 5.1902120178835635e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 49/71 | LOSS: 5.181204928703665e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 50/71 | LOSS: 5.179934336696283e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 51/71 | LOSS: 5.176172412400494e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 52/71 | LOSS: 5.156223010690499e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 53/71 | LOSS: 5.1653642382613425e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 54/71 | LOSS: 5.145543354956317e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 55/71 | LOSS: 5.144415147079988e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 56/71 | LOSS: 5.126995718360137e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 57/71 | LOSS: 5.1304209238941405e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 58/71 | LOSS: 5.145169282868511e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 59/71 | LOSS: 5.150621317776919e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 60/71 | LOSS: 5.138079258671734e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 61/71 | LOSS: 5.141996872049687e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 62/71 | LOSS: 5.133469993245579e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 63/71 | LOSS: 5.162557012994284e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 64/71 | LOSS: 5.168193152153086e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 65/71 | LOSS: 5.159303909887527e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 66/71 | LOSS: 5.14052221117695e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 67/71 | LOSS: 5.152237787676902e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 68/71 | LOSS: 5.147549930937374e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 69/71 | LOSS: 5.134109781985379e-06\n",
      "TRAIN: EPOCH 266/1000 | BATCH 70/71 | LOSS: 5.111410893066573e-06\n",
      "VAL: EPOCH 266/1000 | BATCH 0/8 | LOSS: 7.0820815381011926e-06\n",
      "VAL: EPOCH 266/1000 | BATCH 1/8 | LOSS: 6.2832132243784145e-06\n",
      "VAL: EPOCH 266/1000 | BATCH 2/8 | LOSS: 6.073175124280776e-06\n",
      "VAL: EPOCH 266/1000 | BATCH 3/8 | LOSS: 6.168439767861855e-06\n",
      "VAL: EPOCH 266/1000 | BATCH 4/8 | LOSS: 5.981238791719079e-06\n",
      "VAL: EPOCH 266/1000 | BATCH 5/8 | LOSS: 5.569391987592098e-06\n",
      "VAL: EPOCH 266/1000 | BATCH 6/8 | LOSS: 5.443139018877576e-06\n",
      "VAL: EPOCH 266/1000 | BATCH 7/8 | LOSS: 5.2580990654860216e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 0/71 | LOSS: 5.45846660315874e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 1/71 | LOSS: 5.727490588469664e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 2/71 | LOSS: 5.7199188934949534e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 3/71 | LOSS: 5.663958859258855e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 4/71 | LOSS: 5.65349009775673e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 5/71 | LOSS: 5.700352100272236e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 6/71 | LOSS: 5.406103452644727e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 7/71 | LOSS: 5.514567902764611e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 8/71 | LOSS: 5.483629870973851e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 9/71 | LOSS: 5.550692753786279e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 10/71 | LOSS: 5.388372115207445e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 11/71 | LOSS: 5.301061927790822e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 12/71 | LOSS: 5.252132985753213e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 13/71 | LOSS: 5.298439824790486e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 14/71 | LOSS: 5.212417408984038e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 15/71 | LOSS: 5.11611881393037e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 16/71 | LOSS: 5.024587019707885e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 17/71 | LOSS: 4.991901303381181e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 18/71 | LOSS: 4.99017606240446e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 19/71 | LOSS: 4.974823377779103e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 20/71 | LOSS: 4.964035378222997e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 21/71 | LOSS: 4.89433381806686e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 22/71 | LOSS: 4.86229194630932e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 23/71 | LOSS: 4.86000810913841e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 24/71 | LOSS: 4.874909254795057e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 25/71 | LOSS: 4.931716944156506e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 26/71 | LOSS: 4.930070316630817e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 27/71 | LOSS: 4.91002948267903e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 28/71 | LOSS: 4.937131122993566e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 29/71 | LOSS: 4.931772256592618e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 30/71 | LOSS: 4.936349305059975e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 31/71 | LOSS: 4.934912972487382e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 32/71 | LOSS: 4.928002833063753e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 33/71 | LOSS: 4.943844922255427e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 34/71 | LOSS: 4.968244106099259e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 35/71 | LOSS: 4.992074164217936e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 36/71 | LOSS: 5.000487479608981e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 37/71 | LOSS: 5.021263961536629e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 38/71 | LOSS: 5.028374983176503e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 39/71 | LOSS: 5.037698377918786e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 40/71 | LOSS: 5.027981648331993e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 41/71 | LOSS: 5.0186010704627345e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 42/71 | LOSS: 5.034820105720428e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 43/71 | LOSS: 5.021189730831049e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 44/71 | LOSS: 5.037672159839227e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 45/71 | LOSS: 5.03899683656567e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 46/71 | LOSS: 5.043029311328861e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 47/71 | LOSS: 5.026683249790646e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 48/71 | LOSS: 5.007812168681697e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 49/71 | LOSS: 5.0181300457552424e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 50/71 | LOSS: 5.041158392130313e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 51/71 | LOSS: 5.009582222415842e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 52/71 | LOSS: 5.02457567323153e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 53/71 | LOSS: 5.009022330610875e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 54/71 | LOSS: 5.023991558449449e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 55/71 | LOSS: 5.0268724131845895e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 56/71 | LOSS: 5.018275826666484e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 57/71 | LOSS: 5.0241110408211986e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 58/71 | LOSS: 5.0298423602880116e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 59/71 | LOSS: 5.036406120476992e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 60/71 | LOSS: 5.020525402303109e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 61/71 | LOSS: 5.031920343225788e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 62/71 | LOSS: 5.028714252740241e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 63/71 | LOSS: 5.034179828555807e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 64/71 | LOSS: 5.04403407411211e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 65/71 | LOSS: 5.042162187733497e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 66/71 | LOSS: 5.032429943463258e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 67/71 | LOSS: 5.0710449854437574e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 68/71 | LOSS: 5.080563520702223e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 69/71 | LOSS: 5.07574040316935e-06\n",
      "TRAIN: EPOCH 267/1000 | BATCH 70/71 | LOSS: 5.061421836191431e-06\n",
      "VAL: EPOCH 267/1000 | BATCH 0/8 | LOSS: 6.151150046207476e-06\n",
      "VAL: EPOCH 267/1000 | BATCH 1/8 | LOSS: 5.660603619617177e-06\n",
      "VAL: EPOCH 267/1000 | BATCH 2/8 | LOSS: 5.63326284463983e-06\n",
      "VAL: EPOCH 267/1000 | BATCH 3/8 | LOSS: 5.777248361482634e-06\n",
      "VAL: EPOCH 267/1000 | BATCH 4/8 | LOSS: 5.7044913774007e-06\n",
      "VAL: EPOCH 267/1000 | BATCH 5/8 | LOSS: 5.429278189694742e-06\n",
      "VAL: EPOCH 267/1000 | BATCH 6/8 | LOSS: 5.332366656927791e-06\n",
      "VAL: EPOCH 267/1000 | BATCH 7/8 | LOSS: 5.185879786040459e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 0/71 | LOSS: 5.605831574939657e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 1/71 | LOSS: 5.360687282518484e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 2/71 | LOSS: 5.1909028115915135e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 3/71 | LOSS: 5.760467729487573e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 4/71 | LOSS: 6.176952047098893e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 5/71 | LOSS: 6.043164072859024e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 6/71 | LOSS: 6.028589658464105e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 7/71 | LOSS: 6.06991790164102e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 8/71 | LOSS: 5.900116219790006e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 9/71 | LOSS: 5.816378097733832e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 10/71 | LOSS: 5.8878602596285585e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 11/71 | LOSS: 5.792639437155837e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 12/71 | LOSS: 5.652202127328985e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 13/71 | LOSS: 5.630262551546496e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 14/71 | LOSS: 5.5242216452218905e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 15/71 | LOSS: 5.5314936844297335e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 16/71 | LOSS: 5.50961254595998e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 17/71 | LOSS: 5.592869936461082e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 18/71 | LOSS: 5.559424583847622e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 19/71 | LOSS: 5.509139919013251e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 20/71 | LOSS: 5.50414422052979e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 21/71 | LOSS: 5.480877916852478e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 22/71 | LOSS: 5.3954186707573095e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 23/71 | LOSS: 5.417237446181389e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 24/71 | LOSS: 5.337242464520387e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 25/71 | LOSS: 5.321637471612261e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 26/71 | LOSS: 5.327066707684059e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 27/71 | LOSS: 5.284035369511132e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 28/71 | LOSS: 5.269647027520917e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 29/71 | LOSS: 5.318162773922571e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 30/71 | LOSS: 5.276281722070698e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 31/71 | LOSS: 5.289809401176626e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 32/71 | LOSS: 5.311240487792405e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 33/71 | LOSS: 5.311996594226751e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 34/71 | LOSS: 5.328472272075097e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 35/71 | LOSS: 5.33149091729178e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 36/71 | LOSS: 5.3739068433158635e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 37/71 | LOSS: 5.395159866183301e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 38/71 | LOSS: 5.430734261561809e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 39/71 | LOSS: 5.41284044288659e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 40/71 | LOSS: 5.380847768742015e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 41/71 | LOSS: 5.406708596854601e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 42/71 | LOSS: 5.371002321966401e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 43/71 | LOSS: 5.381642221312756e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 44/71 | LOSS: 5.382571619823769e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 45/71 | LOSS: 5.3776911806341125e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 46/71 | LOSS: 5.433309723307599e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 47/71 | LOSS: 5.404783872601608e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 48/71 | LOSS: 5.415164126561092e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 49/71 | LOSS: 5.422308427114331e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 50/71 | LOSS: 5.457481803187322e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 51/71 | LOSS: 5.414049286838025e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 52/71 | LOSS: 5.419301144358096e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 53/71 | LOSS: 5.437467650524384e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 54/71 | LOSS: 5.430935363454575e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 55/71 | LOSS: 5.434795298242534e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 56/71 | LOSS: 5.453755715790433e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 57/71 | LOSS: 5.459070617407319e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 58/71 | LOSS: 5.451177373087977e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 59/71 | LOSS: 5.449924113539358e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 60/71 | LOSS: 5.437497039848927e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 61/71 | LOSS: 5.433543125740647e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 62/71 | LOSS: 5.425118016564594e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 63/71 | LOSS: 5.437408468367266e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 64/71 | LOSS: 5.475123313002629e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 65/71 | LOSS: 5.446490935024816e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 66/71 | LOSS: 5.4387322920978365e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 67/71 | LOSS: 5.449778287209697e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 68/71 | LOSS: 5.4323214907777e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 69/71 | LOSS: 5.428136221650805e-06\n",
      "TRAIN: EPOCH 268/1000 | BATCH 70/71 | LOSS: 5.423413697403895e-06\n",
      "VAL: EPOCH 268/1000 | BATCH 0/8 | LOSS: 6.414632025553146e-06\n",
      "VAL: EPOCH 268/1000 | BATCH 1/8 | LOSS: 5.702758471670677e-06\n",
      "VAL: EPOCH 268/1000 | BATCH 2/8 | LOSS: 5.852553992250857e-06\n",
      "VAL: EPOCH 268/1000 | BATCH 3/8 | LOSS: 5.982165930618066e-06\n",
      "VAL: EPOCH 268/1000 | BATCH 4/8 | LOSS: 5.863646947545931e-06\n",
      "VAL: EPOCH 268/1000 | BATCH 5/8 | LOSS: 5.566168359412889e-06\n",
      "VAL: EPOCH 268/1000 | BATCH 6/8 | LOSS: 5.4432270449719255e-06\n",
      "VAL: EPOCH 268/1000 | BATCH 7/8 | LOSS: 5.373284352572227e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 0/71 | LOSS: 5.2706127462442964e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 1/71 | LOSS: 5.114333816891303e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 2/71 | LOSS: 5.234485949282923e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 3/71 | LOSS: 5.182347535992449e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 4/71 | LOSS: 5.04772042404511e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 5/71 | LOSS: 4.990645341725515e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 6/71 | LOSS: 5.022576325635393e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 7/71 | LOSS: 4.930745774345269e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 8/71 | LOSS: 5.138952878446111e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 9/71 | LOSS: 5.223830339673441e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 10/71 | LOSS: 5.261115761227715e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 11/71 | LOSS: 5.5384484388317406e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 12/71 | LOSS: 5.4429728012249916e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 13/71 | LOSS: 5.917214658828536e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 14/71 | LOSS: 5.824581937001009e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 15/71 | LOSS: 6.0070648544297e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 16/71 | LOSS: 6.060310143391903e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 17/71 | LOSS: 6.0911155136030475e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 18/71 | LOSS: 6.218528812352911e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 19/71 | LOSS: 6.220929162736866e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 20/71 | LOSS: 6.378732645576487e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 21/71 | LOSS: 6.2635207624142906e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 22/71 | LOSS: 6.346783819659006e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 23/71 | LOSS: 6.242709635747208e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 24/71 | LOSS: 6.2498126135324125e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 25/71 | LOSS: 6.26320087786343e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 26/71 | LOSS: 6.206925617233436e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 27/71 | LOSS: 6.166177838297569e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 28/71 | LOSS: 6.12967798964558e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 29/71 | LOSS: 6.083134682436745e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 30/71 | LOSS: 6.071308498869785e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 31/71 | LOSS: 6.080355831272755e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 32/71 | LOSS: 6.091044173389849e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 33/71 | LOSS: 6.081379965775448e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 34/71 | LOSS: 6.066707893686336e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 35/71 | LOSS: 6.0576180026651454e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 36/71 | LOSS: 6.028099800755012e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 37/71 | LOSS: 5.999030918246717e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 38/71 | LOSS: 5.956983969577451e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 39/71 | LOSS: 5.9439401525196445e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 40/71 | LOSS: 5.91786752787501e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 41/71 | LOSS: 5.877152976349212e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 42/71 | LOSS: 5.9027506828241775e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 43/71 | LOSS: 5.8812657202493446e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 44/71 | LOSS: 5.871643224963918e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 45/71 | LOSS: 5.838985286928896e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 46/71 | LOSS: 5.88023930446225e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 47/71 | LOSS: 5.849726733231364e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 48/71 | LOSS: 5.8415013083912504e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 49/71 | LOSS: 5.8243232160748445e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 50/71 | LOSS: 5.824803106495928e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 51/71 | LOSS: 5.81574623153686e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 52/71 | LOSS: 5.795692017410096e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 53/71 | LOSS: 5.787571429512657e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 54/71 | LOSS: 5.798868584282569e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 55/71 | LOSS: 5.783859113047843e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 56/71 | LOSS: 5.799072185478157e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 57/71 | LOSS: 5.787008243213018e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 58/71 | LOSS: 5.791913250597367e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 59/71 | LOSS: 5.7921778534364424e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 60/71 | LOSS: 5.815722898496162e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 61/71 | LOSS: 5.7951143213769334e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 62/71 | LOSS: 5.76531643702534e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 63/71 | LOSS: 5.762773199080584e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 64/71 | LOSS: 5.750305074168584e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 65/71 | LOSS: 5.733171678223174e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 66/71 | LOSS: 5.7362931193662254e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 67/71 | LOSS: 5.720337566451824e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 68/71 | LOSS: 5.686972026128585e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 69/71 | LOSS: 5.657445499959327e-06\n",
      "TRAIN: EPOCH 269/1000 | BATCH 70/71 | LOSS: 5.6781485634185715e-06\n",
      "VAL: EPOCH 269/1000 | BATCH 0/8 | LOSS: 6.1100190578144975e-06\n",
      "VAL: EPOCH 269/1000 | BATCH 1/8 | LOSS: 5.54790130991023e-06\n",
      "VAL: EPOCH 269/1000 | BATCH 2/8 | LOSS: 5.340220620079587e-06\n",
      "VAL: EPOCH 269/1000 | BATCH 3/8 | LOSS: 5.621253194476594e-06\n",
      "VAL: EPOCH 269/1000 | BATCH 4/8 | LOSS: 5.455305017676437e-06\n",
      "VAL: EPOCH 269/1000 | BATCH 5/8 | LOSS: 5.218910700932611e-06\n",
      "VAL: EPOCH 269/1000 | BATCH 6/8 | LOSS: 5.104700773829661e-06\n",
      "VAL: EPOCH 269/1000 | BATCH 7/8 | LOSS: 4.943578687743866e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 0/71 | LOSS: 5.236929155216785e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 1/71 | LOSS: 5.354075938157621e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 2/71 | LOSS: 5.470002633956028e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 3/71 | LOSS: 5.02840759963874e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 4/71 | LOSS: 5.104134379507741e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 5/71 | LOSS: 5.539757921724231e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 6/71 | LOSS: 5.569264495924082e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 7/71 | LOSS: 5.596280800546083e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 8/71 | LOSS: 5.607205417214169e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 9/71 | LOSS: 5.587376108451281e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 10/71 | LOSS: 5.7094255019116895e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 11/71 | LOSS: 5.690605348718236e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 12/71 | LOSS: 5.60699831671977e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 13/71 | LOSS: 5.8139941886890615e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 14/71 | LOSS: 5.87095646551461e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 15/71 | LOSS: 5.818072253305218e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 16/71 | LOSS: 5.9896693956569135e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 17/71 | LOSS: 6.081973045487151e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 18/71 | LOSS: 6.05961134313379e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 19/71 | LOSS: 6.048464888408489e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 20/71 | LOSS: 6.088623822939726e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 21/71 | LOSS: 6.0968110243596705e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 22/71 | LOSS: 6.139781303120443e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 23/71 | LOSS: 6.153456240554078e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 24/71 | LOSS: 6.224596018000739e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 25/71 | LOSS: 6.162031887959949e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 26/71 | LOSS: 6.21869200282769e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 27/71 | LOSS: 6.234417875410665e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 28/71 | LOSS: 6.239067998053385e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 29/71 | LOSS: 6.2901609908294635e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 30/71 | LOSS: 6.2023406474421085e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 31/71 | LOSS: 6.316774019410332e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 32/71 | LOSS: 6.2684091448538695e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 33/71 | LOSS: 6.328149872976664e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 34/71 | LOSS: 6.308643930554224e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 35/71 | LOSS: 6.28308285083929e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 36/71 | LOSS: 6.353146570315427e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 37/71 | LOSS: 6.340666423535654e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 38/71 | LOSS: 6.352663184281062e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 39/71 | LOSS: 6.340442030250415e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 40/71 | LOSS: 6.310390046298668e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 41/71 | LOSS: 6.337690387053521e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 42/71 | LOSS: 6.287559636105069e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 43/71 | LOSS: 6.286299284593869e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 44/71 | LOSS: 6.3168045825376695e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 45/71 | LOSS: 6.261661432606912e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 46/71 | LOSS: 6.321605656496026e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 47/71 | LOSS: 6.288874942583789e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 48/71 | LOSS: 6.3220456226821515e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 49/71 | LOSS: 6.3080559266381896e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 50/71 | LOSS: 6.296159346461268e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 51/71 | LOSS: 6.313822093663755e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 52/71 | LOSS: 6.287537682922293e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 53/71 | LOSS: 6.2773988079074855e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 54/71 | LOSS: 6.277571744497188e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 55/71 | LOSS: 6.257939292351823e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 56/71 | LOSS: 6.243036734718899e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 57/71 | LOSS: 6.233787722709982e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 58/71 | LOSS: 6.2102102427901224e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 59/71 | LOSS: 6.199610447765736e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 60/71 | LOSS: 6.170831981210943e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 61/71 | LOSS: 6.149565728440892e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 62/71 | LOSS: 6.1386232364239976e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 63/71 | LOSS: 6.109152970168452e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 64/71 | LOSS: 6.105916194288651e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 65/71 | LOSS: 6.086314160160272e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 66/71 | LOSS: 6.06606070152284e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 67/71 | LOSS: 6.04941687791271e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 68/71 | LOSS: 6.039178034383888e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 69/71 | LOSS: 6.031167725138533e-06\n",
      "TRAIN: EPOCH 270/1000 | BATCH 70/71 | LOSS: 6.0104305024216166e-06\n",
      "VAL: EPOCH 270/1000 | BATCH 0/8 | LOSS: 7.094567990861833e-06\n",
      "VAL: EPOCH 270/1000 | BATCH 1/8 | LOSS: 6.04210572419106e-06\n",
      "VAL: EPOCH 270/1000 | BATCH 2/8 | LOSS: 5.8480056092472905e-06\n",
      "VAL: EPOCH 270/1000 | BATCH 3/8 | LOSS: 5.89832359310094e-06\n",
      "VAL: EPOCH 270/1000 | BATCH 4/8 | LOSS: 5.772012173110852e-06\n",
      "VAL: EPOCH 270/1000 | BATCH 5/8 | LOSS: 5.4396357048365944e-06\n",
      "VAL: EPOCH 270/1000 | BATCH 6/8 | LOSS: 5.38837720601545e-06\n",
      "VAL: EPOCH 270/1000 | BATCH 7/8 | LOSS: 5.274001949828744e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 0/71 | LOSS: 5.002932539355243e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 1/71 | LOSS: 4.621707148544374e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 2/71 | LOSS: 4.73192888724346e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 3/71 | LOSS: 4.724712425741018e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 4/71 | LOSS: 4.86311537315487e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 5/71 | LOSS: 4.986226410134502e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 6/71 | LOSS: 4.84334389381859e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 7/71 | LOSS: 5.087586316676607e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 8/71 | LOSS: 5.01991098644794e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 9/71 | LOSS: 5.124688459545723e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 10/71 | LOSS: 5.149827343716011e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 11/71 | LOSS: 5.214805620804934e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 12/71 | LOSS: 5.216621362845986e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 13/71 | LOSS: 5.175149973573363e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 14/71 | LOSS: 5.2008824241056574e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 15/71 | LOSS: 5.168330091009921e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 16/71 | LOSS: 5.089951654928668e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 17/71 | LOSS: 5.089736128461987e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 18/71 | LOSS: 5.108264543339733e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 19/71 | LOSS: 5.099696704746748e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 20/71 | LOSS: 5.156044499883483e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 21/71 | LOSS: 5.147117400900408e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 22/71 | LOSS: 5.123054487067629e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 23/71 | LOSS: 5.114764064728661e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 24/71 | LOSS: 5.201456388022052e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 25/71 | LOSS: 5.25649096953902e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 26/71 | LOSS: 5.231313800188083e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 27/71 | LOSS: 5.215446029589137e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 28/71 | LOSS: 5.1986186353415495e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 29/71 | LOSS: 5.174085360219275e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 30/71 | LOSS: 5.256543288616681e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 31/71 | LOSS: 5.255713105611903e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 32/71 | LOSS: 5.203829611370263e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 33/71 | LOSS: 5.222545830881299e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 34/71 | LOSS: 5.209334098513604e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 35/71 | LOSS: 5.190322844504812e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 36/71 | LOSS: 5.21132149613648e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 37/71 | LOSS: 5.232416792990223e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 38/71 | LOSS: 5.21994184638084e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 39/71 | LOSS: 5.229797602623876e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 40/71 | LOSS: 5.258942498430104e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 41/71 | LOSS: 5.2833529371994135e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 42/71 | LOSS: 5.298999207467247e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 43/71 | LOSS: 5.350641965213237e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 44/71 | LOSS: 5.4079889802475615e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 45/71 | LOSS: 5.426021595627864e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 46/71 | LOSS: 5.517744526475732e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 47/71 | LOSS: 5.548089878478398e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 48/71 | LOSS: 5.626791548542203e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 49/71 | LOSS: 5.622998742182972e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 50/71 | LOSS: 5.6218028547329025e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 51/71 | LOSS: 5.6283584381862265e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 52/71 | LOSS: 5.656826797005708e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 53/71 | LOSS: 5.679533867603621e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 54/71 | LOSS: 5.653062635246897e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 55/71 | LOSS: 5.6874895010748465e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 56/71 | LOSS: 5.7218732540046375e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 57/71 | LOSS: 5.692305123751606e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 58/71 | LOSS: 5.727453800179735e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 59/71 | LOSS: 5.748106642992449e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 60/71 | LOSS: 5.7617277188463225e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 61/71 | LOSS: 5.7393828698085625e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 62/71 | LOSS: 5.72956455399103e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 63/71 | LOSS: 5.746440493226146e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 64/71 | LOSS: 5.7237348440909185e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 65/71 | LOSS: 5.739204080975579e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 66/71 | LOSS: 5.725482838191857e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 67/71 | LOSS: 5.7126083795404106e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 68/71 | LOSS: 5.698228288446958e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 69/71 | LOSS: 5.701647816230044e-06\n",
      "TRAIN: EPOCH 271/1000 | BATCH 70/71 | LOSS: 5.683975173974111e-06\n",
      "VAL: EPOCH 271/1000 | BATCH 0/8 | LOSS: 6.451866283896379e-06\n",
      "VAL: EPOCH 271/1000 | BATCH 1/8 | LOSS: 6.063981345505454e-06\n",
      "VAL: EPOCH 271/1000 | BATCH 2/8 | LOSS: 5.737832301141073e-06\n",
      "VAL: EPOCH 271/1000 | BATCH 3/8 | LOSS: 6.005768113936938e-06\n",
      "VAL: EPOCH 271/1000 | BATCH 4/8 | LOSS: 5.867039635631954e-06\n",
      "VAL: EPOCH 271/1000 | BATCH 5/8 | LOSS: 5.564177248137033e-06\n",
      "VAL: EPOCH 271/1000 | BATCH 6/8 | LOSS: 5.51591334182636e-06\n",
      "VAL: EPOCH 271/1000 | BATCH 7/8 | LOSS: 5.33036745764548e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 0/71 | LOSS: 4.300747605157085e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 1/71 | LOSS: 5.394861091190251e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 2/71 | LOSS: 5.239009624347091e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 3/71 | LOSS: 5.4754143548052525e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 4/71 | LOSS: 5.795849938294851e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 5/71 | LOSS: 6.086967763015612e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 6/71 | LOSS: 5.971195475597467e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 7/71 | LOSS: 6.061969088477781e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 8/71 | LOSS: 5.899527170388157e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 9/71 | LOSS: 5.953805748504238e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 10/71 | LOSS: 5.885510042637841e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 11/71 | LOSS: 5.917170862327718e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 12/71 | LOSS: 6.190312678182426e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 13/71 | LOSS: 6.0378391870991825e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 14/71 | LOSS: 6.329126669394706e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 15/71 | LOSS: 6.195622290761094e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 16/71 | LOSS: 6.165566660724032e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 17/71 | LOSS: 6.228476498411813e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 18/71 | LOSS: 6.177716625786975e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 19/71 | LOSS: 6.058103315353946e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 20/71 | LOSS: 6.020905803303495e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 21/71 | LOSS: 5.923178491684666e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 22/71 | LOSS: 5.911531161473249e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 23/71 | LOSS: 5.834300424112371e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 24/71 | LOSS: 5.854911314600031e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 25/71 | LOSS: 5.824922928136052e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 26/71 | LOSS: 5.813266423299663e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 27/71 | LOSS: 5.772277851130119e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 28/71 | LOSS: 5.734060299926506e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 29/71 | LOSS: 5.777904584647331e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 30/71 | LOSS: 5.724622885663262e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 31/71 | LOSS: 5.694323398586221e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 32/71 | LOSS: 5.757397909264634e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 33/71 | LOSS: 5.750437868728648e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 34/71 | LOSS: 5.732459957081508e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 35/71 | LOSS: 5.692966984346033e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 36/71 | LOSS: 5.710769571394209e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 37/71 | LOSS: 5.6598899774192095e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 38/71 | LOSS: 5.651384899698687e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 39/71 | LOSS: 5.627785435535771e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 40/71 | LOSS: 5.636483265556219e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 41/71 | LOSS: 5.623787100852323e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 42/71 | LOSS: 5.617280199065897e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 43/71 | LOSS: 5.623967302364525e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 44/71 | LOSS: 5.620597433638371e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 45/71 | LOSS: 5.6081972299738405e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 46/71 | LOSS: 5.584437337560342e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 47/71 | LOSS: 5.595007394276763e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 48/71 | LOSS: 5.577754684648481e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 49/71 | LOSS: 5.575288241743692e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 50/71 | LOSS: 5.561298336283025e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 51/71 | LOSS: 5.529251412693926e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 52/71 | LOSS: 5.525069011711375e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 53/71 | LOSS: 5.5106288106832866e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 54/71 | LOSS: 5.502949112600287e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 55/71 | LOSS: 5.5002706095105424e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 56/71 | LOSS: 5.498389032653537e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 57/71 | LOSS: 5.484923215997947e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 58/71 | LOSS: 5.46052453531864e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 59/71 | LOSS: 5.438287257675256e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 60/71 | LOSS: 5.442408594523736e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 61/71 | LOSS: 5.445789266530248e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 62/71 | LOSS: 5.426111588998797e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 63/71 | LOSS: 5.410866528166025e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 64/71 | LOSS: 5.3981162147045745e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 65/71 | LOSS: 5.3961633429108504e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 66/71 | LOSS: 5.404713755890939e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 67/71 | LOSS: 5.382937328089183e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 68/71 | LOSS: 5.375797152974764e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 69/71 | LOSS: 5.390308595971354e-06\n",
      "TRAIN: EPOCH 272/1000 | BATCH 70/71 | LOSS: 5.389310208044317e-06\n",
      "VAL: EPOCH 272/1000 | BATCH 0/8 | LOSS: 6.5159379118995275e-06\n",
      "VAL: EPOCH 272/1000 | BATCH 1/8 | LOSS: 6.331225222311332e-06\n",
      "VAL: EPOCH 272/1000 | BATCH 2/8 | LOSS: 6.820627428775576e-06\n",
      "VAL: EPOCH 272/1000 | BATCH 3/8 | LOSS: 7.151537261051999e-06\n",
      "VAL: EPOCH 272/1000 | BATCH 4/8 | LOSS: 7.134237330319593e-06\n",
      "VAL: EPOCH 272/1000 | BATCH 5/8 | LOSS: 7.115851379542922e-06\n",
      "VAL: EPOCH 272/1000 | BATCH 6/8 | LOSS: 7.04166034535904e-06\n",
      "VAL: EPOCH 272/1000 | BATCH 7/8 | LOSS: 7.064848205118324e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 0/71 | LOSS: 7.037035175017081e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 1/71 | LOSS: 6.059763563825982e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 2/71 | LOSS: 5.631755357171642e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 3/71 | LOSS: 6.089142402743164e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 4/71 | LOSS: 5.971654445602326e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 5/71 | LOSS: 6.147629771173039e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 6/71 | LOSS: 6.0280402587003275e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 7/71 | LOSS: 6.022997183663392e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 8/71 | LOSS: 5.994286463343694e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 9/71 | LOSS: 6.008073114571744e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 10/71 | LOSS: 6.119501408234513e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 11/71 | LOSS: 6.035157980477379e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 12/71 | LOSS: 6.075756398506481e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 13/71 | LOSS: 5.952498700675538e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 14/71 | LOSS: 5.9282305301167074e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 15/71 | LOSS: 5.972435928924824e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 16/71 | LOSS: 5.916674394136541e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 17/71 | LOSS: 6.023355783529243e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 18/71 | LOSS: 5.928742163967782e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 19/71 | LOSS: 5.936381921856082e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 20/71 | LOSS: 5.85873877276927e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 21/71 | LOSS: 5.76951087664797e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 22/71 | LOSS: 5.709120238822399e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 23/71 | LOSS: 5.609279298823822e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 24/71 | LOSS: 5.545399271795759e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 25/71 | LOSS: 5.53366890716671e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 26/71 | LOSS: 5.47900599148879e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 27/71 | LOSS: 5.465592689038853e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 28/71 | LOSS: 5.458773212639839e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 29/71 | LOSS: 5.424389716305692e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 30/71 | LOSS: 5.4125307982641036e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 31/71 | LOSS: 5.378988362281234e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 32/71 | LOSS: 5.361591706098283e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 33/71 | LOSS: 5.3620403030115136e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 34/71 | LOSS: 5.335511390772548e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 35/71 | LOSS: 5.3129283767500765e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 36/71 | LOSS: 5.307962633319343e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 37/71 | LOSS: 5.3025590228858885e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 38/71 | LOSS: 5.310226499251514e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 39/71 | LOSS: 5.297735742715304e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 40/71 | LOSS: 5.316015631251503e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 41/71 | LOSS: 5.302261277912683e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 42/71 | LOSS: 5.3156966841502145e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 43/71 | LOSS: 5.313453605346942e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 44/71 | LOSS: 5.307703769277497e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 45/71 | LOSS: 5.316472672110768e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 46/71 | LOSS: 5.321848201423021e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 47/71 | LOSS: 5.317849532578596e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 48/71 | LOSS: 5.319435544783007e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 49/71 | LOSS: 5.307482215357595e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 50/71 | LOSS: 5.311853400081757e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 51/71 | LOSS: 5.3103542978323485e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 52/71 | LOSS: 5.296406187589351e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 53/71 | LOSS: 5.3260615307711605e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 54/71 | LOSS: 5.32392382824169e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 55/71 | LOSS: 5.307223042499183e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 56/71 | LOSS: 5.31057654977e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 57/71 | LOSS: 5.299849660865408e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 58/71 | LOSS: 5.286454918443608e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 59/71 | LOSS: 5.3055534484277205e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 60/71 | LOSS: 5.304309293843871e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 61/71 | LOSS: 5.295224927096804e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 62/71 | LOSS: 5.302221007508389e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 63/71 | LOSS: 5.3098877685897605e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 64/71 | LOSS: 5.29215340765614e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 65/71 | LOSS: 5.323097556439197e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 66/71 | LOSS: 5.326039532085258e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 67/71 | LOSS: 5.363319019124901e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 68/71 | LOSS: 5.355306342792481e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 69/71 | LOSS: 5.368669306855216e-06\n",
      "TRAIN: EPOCH 273/1000 | BATCH 70/71 | LOSS: 5.395132460070669e-06\n",
      "VAL: EPOCH 273/1000 | BATCH 0/8 | LOSS: 6.9878888098173775e-06\n",
      "VAL: EPOCH 273/1000 | BATCH 1/8 | LOSS: 6.075235887692543e-06\n",
      "VAL: EPOCH 273/1000 | BATCH 2/8 | LOSS: 6.246715353578717e-06\n",
      "VAL: EPOCH 273/1000 | BATCH 3/8 | LOSS: 6.071499115023471e-06\n",
      "VAL: EPOCH 273/1000 | BATCH 4/8 | LOSS: 6.090520673751598e-06\n",
      "VAL: EPOCH 273/1000 | BATCH 5/8 | LOSS: 5.850314892086317e-06\n",
      "VAL: EPOCH 273/1000 | BATCH 6/8 | LOSS: 5.853784192108183e-06\n",
      "VAL: EPOCH 273/1000 | BATCH 7/8 | LOSS: 5.879270190689567e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 0/71 | LOSS: 4.425112820172217e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 1/71 | LOSS: 4.382061888463795e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 2/71 | LOSS: 4.730733053293079e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 3/71 | LOSS: 5.115086196383345e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 4/71 | LOSS: 5.50586664758157e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 5/71 | LOSS: 5.5444099871237995e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 6/71 | LOSS: 5.571351396481207e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 7/71 | LOSS: 5.626971187666641e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 8/71 | LOSS: 5.628363901956214e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 9/71 | LOSS: 5.608231822407106e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 10/71 | LOSS: 5.568017927130726e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 11/71 | LOSS: 5.401877918605654e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 12/71 | LOSS: 5.365800986440333e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 13/71 | LOSS: 5.315642640977915e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 14/71 | LOSS: 5.379978544321299e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 15/71 | LOSS: 5.426944795772215e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 16/71 | LOSS: 5.410007280956693e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 17/71 | LOSS: 5.334456419505942e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 18/71 | LOSS: 5.535593764331057e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 19/71 | LOSS: 5.513951805369288e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 20/71 | LOSS: 5.5199380507734275e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 21/71 | LOSS: 5.5270648069629465e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 22/71 | LOSS: 5.555815357144203e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 23/71 | LOSS: 5.534447363212773e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 24/71 | LOSS: 5.51214250663179e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 25/71 | LOSS: 5.507431926441943e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 26/71 | LOSS: 5.468441486300435e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 27/71 | LOSS: 5.456733301148883e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 28/71 | LOSS: 5.423840179744353e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 29/71 | LOSS: 5.408516775181245e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 30/71 | LOSS: 5.391198790734521e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 31/71 | LOSS: 5.3997808890926535e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 32/71 | LOSS: 5.45669062183041e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 33/71 | LOSS: 5.470279869341004e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 34/71 | LOSS: 5.484614614813057e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 35/71 | LOSS: 5.4864071646281646e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 36/71 | LOSS: 5.510863785103366e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 37/71 | LOSS: 5.505051376530901e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 38/71 | LOSS: 5.572137687965714e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 39/71 | LOSS: 5.581604591498035e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 40/71 | LOSS: 5.598880772772756e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 41/71 | LOSS: 5.6152374608722135e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 42/71 | LOSS: 5.62475024919475e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 43/71 | LOSS: 5.642222773035015e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 44/71 | LOSS: 5.6251889974292786e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 45/71 | LOSS: 5.6222747250854885e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 46/71 | LOSS: 5.6411374626282425e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 47/71 | LOSS: 5.635975753648381e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 48/71 | LOSS: 5.614486753427404e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 49/71 | LOSS: 5.609496247416245e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 50/71 | LOSS: 5.660342113140573e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 51/71 | LOSS: 5.643332909900122e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 52/71 | LOSS: 5.640729478676255e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 53/71 | LOSS: 5.618199507066661e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 54/71 | LOSS: 5.621088679287244e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 55/71 | LOSS: 5.619852199093397e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 56/71 | LOSS: 5.606595957368355e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 57/71 | LOSS: 5.591602671458908e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 58/71 | LOSS: 5.612271090316333e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 59/71 | LOSS: 5.616416175750298e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 60/71 | LOSS: 5.606600351428392e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 61/71 | LOSS: 5.59754216151158e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 62/71 | LOSS: 5.614304045016142e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 63/71 | LOSS: 5.594140560560845e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 64/71 | LOSS: 5.576712898730945e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 65/71 | LOSS: 5.582295657266954e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 66/71 | LOSS: 5.56380155782125e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 67/71 | LOSS: 5.562866657355164e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 68/71 | LOSS: 5.542211352882947e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 69/71 | LOSS: 5.532555828072613e-06\n",
      "TRAIN: EPOCH 274/1000 | BATCH 70/71 | LOSS: 5.515475870299884e-06\n",
      "VAL: EPOCH 274/1000 | BATCH 0/8 | LOSS: 5.806295121146832e-06\n",
      "VAL: EPOCH 274/1000 | BATCH 1/8 | LOSS: 5.242129645921523e-06\n",
      "VAL: EPOCH 274/1000 | BATCH 2/8 | LOSS: 5.613317474247499e-06\n",
      "VAL: EPOCH 274/1000 | BATCH 3/8 | LOSS: 5.6692248335821205e-06\n",
      "VAL: EPOCH 274/1000 | BATCH 4/8 | LOSS: 5.697892993339337e-06\n",
      "VAL: EPOCH 274/1000 | BATCH 5/8 | LOSS: 5.538473033084301e-06\n",
      "VAL: EPOCH 274/1000 | BATCH 6/8 | LOSS: 5.4750961940694e-06\n",
      "VAL: EPOCH 274/1000 | BATCH 7/8 | LOSS: 5.479943183672731e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 0/71 | LOSS: 5.542077815334778e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 1/71 | LOSS: 5.108431651024148e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 2/71 | LOSS: 5.181090424836536e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 3/71 | LOSS: 5.310659275892249e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 4/71 | LOSS: 5.455158589029452e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 5/71 | LOSS: 5.511318477147142e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 6/71 | LOSS: 5.307427987385641e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 7/71 | LOSS: 5.428272345398e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 8/71 | LOSS: 5.369147149596958e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 9/71 | LOSS: 5.49382434655854e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 10/71 | LOSS: 5.389104320453374e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 11/71 | LOSS: 5.340827177254444e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 12/71 | LOSS: 5.339254760026681e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 13/71 | LOSS: 5.363532766620795e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 14/71 | LOSS: 5.454747588373721e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 15/71 | LOSS: 5.416588294337998e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 16/71 | LOSS: 5.5339649085962105e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 17/71 | LOSS: 5.436681261825369e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 18/71 | LOSS: 5.4040260333989095e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 19/71 | LOSS: 5.482444373683393e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 20/71 | LOSS: 5.453507724052456e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 21/71 | LOSS: 5.499546380798909e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 22/71 | LOSS: 5.502092681522101e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 23/71 | LOSS: 5.544901654275236e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 24/71 | LOSS: 5.5799477649998154e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 25/71 | LOSS: 5.5039717276136235e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 26/71 | LOSS: 5.505969806175993e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 27/71 | LOSS: 5.51962532264432e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 28/71 | LOSS: 5.544630210289174e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 29/71 | LOSS: 5.4801360571824866e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 30/71 | LOSS: 5.456032164558213e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 31/71 | LOSS: 5.450291865827239e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 32/71 | LOSS: 5.4475288682927685e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 33/71 | LOSS: 5.4303803100243815e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 34/71 | LOSS: 5.40173933976413e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 35/71 | LOSS: 5.433263393367977e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 36/71 | LOSS: 5.411730657018583e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 37/71 | LOSS: 5.41767991834556e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 38/71 | LOSS: 5.394388095644386e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 39/71 | LOSS: 5.368925894799759e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 40/71 | LOSS: 5.338443659683235e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 41/71 | LOSS: 5.333928692423451e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 42/71 | LOSS: 5.35027625436521e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 43/71 | LOSS: 5.364613374777863e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 44/71 | LOSS: 5.365717121700678e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 45/71 | LOSS: 5.371621326681318e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 46/71 | LOSS: 5.3337194390707655e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 47/71 | LOSS: 5.32153097765331e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 48/71 | LOSS: 5.294643294607284e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 49/71 | LOSS: 5.292943997119437e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 50/71 | LOSS: 5.281283499832224e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 51/71 | LOSS: 5.286036875636805e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 52/71 | LOSS: 5.290250186899013e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 53/71 | LOSS: 5.261572655231079e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 54/71 | LOSS: 5.259624313938812e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 55/71 | LOSS: 5.271167180710888e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 56/71 | LOSS: 5.274886461847927e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 57/71 | LOSS: 5.2868560079990986e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 58/71 | LOSS: 5.297001758138985e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 59/71 | LOSS: 5.2848207284720654e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 60/71 | LOSS: 5.286822454122632e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 61/71 | LOSS: 5.290601364087648e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 62/71 | LOSS: 5.2982871036672604e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 63/71 | LOSS: 5.2945342474686186e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 64/71 | LOSS: 5.279153041416206e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 65/71 | LOSS: 5.256398163978839e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 66/71 | LOSS: 5.257350012606597e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 67/71 | LOSS: 5.2472586744175605e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 68/71 | LOSS: 5.234317613676598e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 69/71 | LOSS: 5.217740382639541e-06\n",
      "TRAIN: EPOCH 275/1000 | BATCH 70/71 | LOSS: 5.209387830252805e-06\n",
      "VAL: EPOCH 275/1000 | BATCH 0/8 | LOSS: 5.529446752916556e-06\n",
      "VAL: EPOCH 275/1000 | BATCH 1/8 | LOSS: 4.849327069678111e-06\n",
      "VAL: EPOCH 275/1000 | BATCH 2/8 | LOSS: 4.9624697264031665e-06\n",
      "VAL: EPOCH 275/1000 | BATCH 3/8 | LOSS: 5.15102647113963e-06\n",
      "VAL: EPOCH 275/1000 | BATCH 4/8 | LOSS: 5.046004298492335e-06\n",
      "VAL: EPOCH 275/1000 | BATCH 5/8 | LOSS: 4.879977420083985e-06\n",
      "VAL: EPOCH 275/1000 | BATCH 6/8 | LOSS: 4.8711641673954934e-06\n",
      "VAL: EPOCH 275/1000 | BATCH 7/8 | LOSS: 4.78707084994312e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 0/71 | LOSS: 4.654122676583938e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 1/71 | LOSS: 5.522665787793812e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 2/71 | LOSS: 5.026781006260232e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 3/71 | LOSS: 4.924572635900404e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 4/71 | LOSS: 4.841538884647889e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 5/71 | LOSS: 4.814172446761707e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 6/71 | LOSS: 4.626200052371132e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 7/71 | LOSS: 4.576339421191733e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 8/71 | LOSS: 4.592539400821099e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 9/71 | LOSS: 4.721289155895647e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 10/71 | LOSS: 4.63692311892704e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 11/71 | LOSS: 4.686038645710748e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 12/71 | LOSS: 4.803960546968693e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 13/71 | LOSS: 4.797093116134679e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 14/71 | LOSS: 4.855277696454626e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 15/71 | LOSS: 4.848639733268101e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 16/71 | LOSS: 4.8919339940980805e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 17/71 | LOSS: 4.873397617504654e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 18/71 | LOSS: 4.865202431334949e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 19/71 | LOSS: 4.876769150996552e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 20/71 | LOSS: 4.841780699799918e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 21/71 | LOSS: 4.816091477137392e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 22/71 | LOSS: 4.827489577940509e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 23/71 | LOSS: 4.817314040413597e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 24/71 | LOSS: 4.766401716551627e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 25/71 | LOSS: 4.8180045881123915e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 26/71 | LOSS: 4.848375645586337e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 27/71 | LOSS: 4.970607140616526e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 28/71 | LOSS: 4.98003736663969e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 29/71 | LOSS: 4.950877572203656e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 30/71 | LOSS: 4.934893702797595e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 31/71 | LOSS: 4.935030013086816e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 32/71 | LOSS: 4.969378971619528e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 33/71 | LOSS: 5.0093893565537975e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 34/71 | LOSS: 5.03705353886679e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 35/71 | LOSS: 5.097413192187459e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 36/71 | LOSS: 5.090767807495148e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 37/71 | LOSS: 5.135786086372575e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 38/71 | LOSS: 5.155944604550127e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 39/71 | LOSS: 5.2335739098907655e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 40/71 | LOSS: 5.290197611052795e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 41/71 | LOSS: 5.330978145442107e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 42/71 | LOSS: 5.447976126413058e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 43/71 | LOSS: 5.4700162091979685e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 44/71 | LOSS: 5.611085720779051e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 45/71 | LOSS: 5.612378557721652e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 46/71 | LOSS: 5.630374023391422e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 47/71 | LOSS: 5.614107706719551e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 48/71 | LOSS: 5.601542513933073e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 49/71 | LOSS: 5.648953979289217e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 50/71 | LOSS: 5.620736414237078e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 51/71 | LOSS: 5.609999155135133e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 52/71 | LOSS: 5.5917247415359356e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 53/71 | LOSS: 5.599118590335247e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 54/71 | LOSS: 5.5888505853958615e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 55/71 | LOSS: 5.617914386643211e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 56/71 | LOSS: 5.601084056475533e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 57/71 | LOSS: 5.614004996051994e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 58/71 | LOSS: 5.587188555999369e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 59/71 | LOSS: 5.570806285959406e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 60/71 | LOSS: 5.554798049235585e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 61/71 | LOSS: 5.546137766677637e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 62/71 | LOSS: 5.534151579836145e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 63/71 | LOSS: 5.530193018188356e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 64/71 | LOSS: 5.533104807070161e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 65/71 | LOSS: 5.528937528900067e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 66/71 | LOSS: 5.503804700242993e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 67/71 | LOSS: 5.507795706575159e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 68/71 | LOSS: 5.498319036421996e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 69/71 | LOSS: 5.478705923321416e-06\n",
      "TRAIN: EPOCH 276/1000 | BATCH 70/71 | LOSS: 5.484386411155074e-06\n",
      "VAL: EPOCH 276/1000 | BATCH 0/8 | LOSS: 5.947387762716971e-06\n",
      "VAL: EPOCH 276/1000 | BATCH 1/8 | LOSS: 5.536414846574189e-06\n",
      "VAL: EPOCH 276/1000 | BATCH 2/8 | LOSS: 5.590674845734611e-06\n",
      "VAL: EPOCH 276/1000 | BATCH 3/8 | LOSS: 5.718560259992955e-06\n",
      "VAL: EPOCH 276/1000 | BATCH 4/8 | LOSS: 5.664686614181846e-06\n",
      "VAL: EPOCH 276/1000 | BATCH 5/8 | LOSS: 5.388200103576916e-06\n",
      "VAL: EPOCH 276/1000 | BATCH 6/8 | LOSS: 5.376658139409431e-06\n",
      "VAL: EPOCH 276/1000 | BATCH 7/8 | LOSS: 5.2480492627182684e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 0/71 | LOSS: 5.517867521120934e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 1/71 | LOSS: 5.103702051201253e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 2/71 | LOSS: 5.081470741667242e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 3/71 | LOSS: 4.850657319366292e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 4/71 | LOSS: 5.034479363530408e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 5/71 | LOSS: 4.9941140029356274e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 6/71 | LOSS: 4.952090876031434e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 7/71 | LOSS: 4.853472546528792e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 8/71 | LOSS: 4.83608088567659e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 9/71 | LOSS: 4.835067238673219e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 10/71 | LOSS: 4.811053630956766e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 11/71 | LOSS: 4.857295152760344e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 12/71 | LOSS: 5.020681153557514e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 13/71 | LOSS: 4.983517036245237e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 14/71 | LOSS: 4.919239684871476e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 15/71 | LOSS: 4.901392543388283e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 16/71 | LOSS: 4.9510461876172275e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 17/71 | LOSS: 4.936138869500913e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 18/71 | LOSS: 4.931264572262176e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 19/71 | LOSS: 4.895936126558809e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 20/71 | LOSS: 4.960671893578754e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 21/71 | LOSS: 4.949919168046273e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 22/71 | LOSS: 4.960857515233448e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 23/71 | LOSS: 4.920872584079916e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 24/71 | LOSS: 4.92595605464885e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 25/71 | LOSS: 4.929473491826614e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 26/71 | LOSS: 4.9592814927261015e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 27/71 | LOSS: 4.975674934420178e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 28/71 | LOSS: 4.9811779592409004e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 29/71 | LOSS: 5.000379527094386e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 30/71 | LOSS: 4.985115948887053e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 31/71 | LOSS: 5.0423202395677436e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 32/71 | LOSS: 5.026761741508821e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 33/71 | LOSS: 5.039219344967826e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 34/71 | LOSS: 5.035297259122932e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 35/71 | LOSS: 5.035798380959022e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 36/71 | LOSS: 5.027602779788336e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 37/71 | LOSS: 5.039164067822735e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 38/71 | LOSS: 5.0362878144556025e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 39/71 | LOSS: 5.044625402206293e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 40/71 | LOSS: 5.045065424893415e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 41/71 | LOSS: 5.054538806212977e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 42/71 | LOSS: 5.072638127468201e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 43/71 | LOSS: 5.084686956086609e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 44/71 | LOSS: 5.056409842533564e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 45/71 | LOSS: 5.061489955353507e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 46/71 | LOSS: 5.114410025774479e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 47/71 | LOSS: 5.097841641562202e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 48/71 | LOSS: 5.1101825814542706e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 49/71 | LOSS: 5.120525624988659e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 50/71 | LOSS: 5.139005443376007e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 51/71 | LOSS: 5.114847528357831e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 52/71 | LOSS: 5.125662109660554e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 53/71 | LOSS: 5.153051658992438e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 54/71 | LOSS: 5.160820724499486e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 55/71 | LOSS: 5.1662783846495586e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 56/71 | LOSS: 5.17493449798055e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 57/71 | LOSS: 5.218051561659477e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 58/71 | LOSS: 5.224024747248978e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 59/71 | LOSS: 5.235574299907361e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 60/71 | LOSS: 5.258573485319131e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 61/71 | LOSS: 5.242765701774671e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 62/71 | LOSS: 5.241450978809596e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 63/71 | LOSS: 5.285993733394889e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 64/71 | LOSS: 5.299058057062212e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 65/71 | LOSS: 5.293884782986424e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 66/71 | LOSS: 5.267170471692659e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 67/71 | LOSS: 5.281609633932342e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 68/71 | LOSS: 5.26017573434544e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 69/71 | LOSS: 5.25664698086205e-06\n",
      "TRAIN: EPOCH 277/1000 | BATCH 70/71 | LOSS: 5.242146702149627e-06\n",
      "VAL: EPOCH 277/1000 | BATCH 0/8 | LOSS: 7.64135256758891e-06\n",
      "VAL: EPOCH 277/1000 | BATCH 1/8 | LOSS: 7.081884405124583e-06\n",
      "VAL: EPOCH 277/1000 | BATCH 2/8 | LOSS: 6.9193080586652895e-06\n",
      "VAL: EPOCH 277/1000 | BATCH 3/8 | LOSS: 7.007109616097296e-06\n",
      "VAL: EPOCH 277/1000 | BATCH 4/8 | LOSS: 6.961197323107627e-06\n",
      "VAL: EPOCH 277/1000 | BATCH 5/8 | LOSS: 6.633930221748112e-06\n",
      "VAL: EPOCH 277/1000 | BATCH 6/8 | LOSS: 6.673980936154424e-06\n",
      "VAL: EPOCH 277/1000 | BATCH 7/8 | LOSS: 6.520923932384903e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 0/71 | LOSS: 5.122406037116889e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 1/71 | LOSS: 4.999843895348022e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 2/71 | LOSS: 5.2690265874844044e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 3/71 | LOSS: 5.212138717070047e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 4/71 | LOSS: 5.2706944188685155e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 5/71 | LOSS: 5.446999618167562e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 6/71 | LOSS: 5.317286260313787e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 7/71 | LOSS: 5.345443526039162e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 8/71 | LOSS: 5.273565825013469e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 9/71 | LOSS: 5.26804951732629e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 10/71 | LOSS: 5.214882052810439e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 11/71 | LOSS: 5.273733070983629e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 12/71 | LOSS: 5.412806785898283e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 13/71 | LOSS: 5.366170918413056e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 14/71 | LOSS: 5.353371337453912e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 15/71 | LOSS: 5.449320411798908e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 16/71 | LOSS: 5.4035167522670235e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 17/71 | LOSS: 5.415215405769737e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 18/71 | LOSS: 5.375866629420718e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 19/71 | LOSS: 5.425983613349672e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 20/71 | LOSS: 5.3434126974544986e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 21/71 | LOSS: 5.335533625260393e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 22/71 | LOSS: 5.39252551868191e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 23/71 | LOSS: 5.3515022860513755e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 24/71 | LOSS: 5.358275038815918e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 25/71 | LOSS: 5.420403471571962e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 26/71 | LOSS: 5.458722499598581e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 27/71 | LOSS: 5.472817284563851e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 28/71 | LOSS: 5.504791566824584e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 29/71 | LOSS: 5.483852805809874e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 30/71 | LOSS: 5.471234449179329e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 31/71 | LOSS: 5.471847281057762e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 32/71 | LOSS: 5.488655329882573e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 33/71 | LOSS: 5.44969093559356e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 34/71 | LOSS: 5.410436844093575e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 35/71 | LOSS: 5.428866775976833e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 36/71 | LOSS: 5.397763860290414e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 37/71 | LOSS: 5.390890351009148e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 38/71 | LOSS: 5.397218810452241e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 39/71 | LOSS: 5.388636390080137e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 40/71 | LOSS: 5.413557246578332e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 41/71 | LOSS: 5.436718175761121e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 42/71 | LOSS: 5.4638826705465975e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 43/71 | LOSS: 5.480265118959241e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 44/71 | LOSS: 5.458572069174908e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 45/71 | LOSS: 5.460725130591448e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 46/71 | LOSS: 5.46484354944891e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 47/71 | LOSS: 5.461260689306376e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 48/71 | LOSS: 5.458571106884141e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 49/71 | LOSS: 5.443777095024416e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 50/71 | LOSS: 5.429425246963065e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 51/71 | LOSS: 5.451647032609449e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 52/71 | LOSS: 5.432045425996481e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 53/71 | LOSS: 5.431655663583284e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 54/71 | LOSS: 5.419434931106728e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 55/71 | LOSS: 5.4024565209991645e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 56/71 | LOSS: 5.390824422615799e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 57/71 | LOSS: 5.365571061294508e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 58/71 | LOSS: 5.371555354971768e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 59/71 | LOSS: 5.344123561220234e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 60/71 | LOSS: 5.363290476273759e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 61/71 | LOSS: 5.3473954098550285e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 62/71 | LOSS: 5.349522054876572e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 63/71 | LOSS: 5.342607696690038e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 64/71 | LOSS: 5.3297347186834e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 65/71 | LOSS: 5.332437974009901e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 66/71 | LOSS: 5.313872403593138e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 67/71 | LOSS: 5.322917174640572e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 68/71 | LOSS: 5.319309277341666e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 69/71 | LOSS: 5.320768036654045e-06\n",
      "TRAIN: EPOCH 278/1000 | BATCH 70/71 | LOSS: 5.347880735659511e-06\n",
      "VAL: EPOCH 278/1000 | BATCH 0/8 | LOSS: 5.979772140563e-06\n",
      "VAL: EPOCH 278/1000 | BATCH 1/8 | LOSS: 5.674646217812551e-06\n",
      "VAL: EPOCH 278/1000 | BATCH 2/8 | LOSS: 6.11644812427888e-06\n",
      "VAL: EPOCH 278/1000 | BATCH 3/8 | LOSS: 6.404096779988322e-06\n",
      "VAL: EPOCH 278/1000 | BATCH 4/8 | LOSS: 6.314044094324345e-06\n",
      "VAL: EPOCH 278/1000 | BATCH 5/8 | LOSS: 6.2298778023735695e-06\n",
      "VAL: EPOCH 278/1000 | BATCH 6/8 | LOSS: 6.164522021988107e-06\n",
      "VAL: EPOCH 278/1000 | BATCH 7/8 | LOSS: 6.155258006401709e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 0/71 | LOSS: 6.108210982347373e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 1/71 | LOSS: 5.8474906836636364e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 2/71 | LOSS: 6.087808287702501e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 3/71 | LOSS: 5.7966284430222e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 4/71 | LOSS: 5.589206466538599e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 5/71 | LOSS: 5.439873802970396e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 6/71 | LOSS: 5.416487965703709e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 7/71 | LOSS: 5.429101975096273e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 8/71 | LOSS: 5.298288215271896e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 9/71 | LOSS: 5.397498580350657e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 10/71 | LOSS: 5.37239012291221e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 11/71 | LOSS: 5.239168880658933e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 12/71 | LOSS: 5.238714720690041e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 13/71 | LOSS: 5.124278459334164e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 14/71 | LOSS: 5.144760295176335e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 15/71 | LOSS: 5.141671550745741e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 16/71 | LOSS: 5.1377346340707256e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 17/71 | LOSS: 5.129013490861527e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 18/71 | LOSS: 5.0716393113696925e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 19/71 | LOSS: 5.069123767498241e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 20/71 | LOSS: 5.081759809399955e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 21/71 | LOSS: 5.094308810591676e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 22/71 | LOSS: 5.131857031538738e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 23/71 | LOSS: 5.096152373577449e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 24/71 | LOSS: 5.172587607376044e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 25/71 | LOSS: 5.172517533607718e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 26/71 | LOSS: 5.26279993411865e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 27/71 | LOSS: 5.253720571220453e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 28/71 | LOSS: 5.300717444258445e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 29/71 | LOSS: 5.2743236816847155e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 30/71 | LOSS: 5.251864261099986e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 31/71 | LOSS: 5.2522698865686834e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 32/71 | LOSS: 5.231818235959949e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 33/71 | LOSS: 5.2691236492939744e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 34/71 | LOSS: 5.282856402897908e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 35/71 | LOSS: 5.278489778801385e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 36/71 | LOSS: 5.302765375007938e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 37/71 | LOSS: 5.282842171015402e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 38/71 | LOSS: 5.2742807850173e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 39/71 | LOSS: 5.259139823010628e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 40/71 | LOSS: 5.299151481023462e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 41/71 | LOSS: 5.289609708554399e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 42/71 | LOSS: 5.326421191861046e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 43/71 | LOSS: 5.377733817691809e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 44/71 | LOSS: 5.3756051480983535e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 45/71 | LOSS: 5.376046848813649e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 46/71 | LOSS: 5.388445051141118e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 47/71 | LOSS: 5.36783904673636e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 48/71 | LOSS: 5.344213224482385e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 49/71 | LOSS: 5.34533225618361e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 50/71 | LOSS: 5.344358812056039e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 51/71 | LOSS: 5.3464056533742296e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 52/71 | LOSS: 5.328345436419971e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 53/71 | LOSS: 5.352829778833741e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 54/71 | LOSS: 5.330016202523521e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 55/71 | LOSS: 5.32993180968333e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 56/71 | LOSS: 5.304764511636581e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 57/71 | LOSS: 5.2914799417866844e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 58/71 | LOSS: 5.296204323975086e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 59/71 | LOSS: 5.299286188649906e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 60/71 | LOSS: 5.287692795822981e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 61/71 | LOSS: 5.262589202321843e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 62/71 | LOSS: 5.272275566023873e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 63/71 | LOSS: 5.267089843385975e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 64/71 | LOSS: 5.286290143675825e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 65/71 | LOSS: 5.295940716430116e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 66/71 | LOSS: 5.325444990647618e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 67/71 | LOSS: 5.315873044550886e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 68/71 | LOSS: 5.304475596700365e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 69/71 | LOSS: 5.300053586714577e-06\n",
      "TRAIN: EPOCH 279/1000 | BATCH 70/71 | LOSS: 5.275653716702257e-06\n",
      "VAL: EPOCH 279/1000 | BATCH 0/8 | LOSS: 1.0070511052617803e-05\n",
      "VAL: EPOCH 279/1000 | BATCH 1/8 | LOSS: 9.491840501141269e-06\n",
      "VAL: EPOCH 279/1000 | BATCH 2/8 | LOSS: 9.243297123854669e-06\n",
      "VAL: EPOCH 279/1000 | BATCH 3/8 | LOSS: 9.368537575937808e-06\n",
      "VAL: EPOCH 279/1000 | BATCH 4/8 | LOSS: 9.234890967491083e-06\n",
      "VAL: EPOCH 279/1000 | BATCH 5/8 | LOSS: 8.765929048119384e-06\n",
      "VAL: EPOCH 279/1000 | BATCH 6/8 | LOSS: 8.903830673391764e-06\n",
      "VAL: EPOCH 279/1000 | BATCH 7/8 | LOSS: 8.654523583118134e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 0/71 | LOSS: 8.424283805652522e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 1/71 | LOSS: 7.069922048685839e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 2/71 | LOSS: 8.035286555241328e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 3/71 | LOSS: 6.998169510552543e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 4/71 | LOSS: 8.453832197119483e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 5/71 | LOSS: 8.016123729248648e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 6/71 | LOSS: 7.811866713122331e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 7/71 | LOSS: 7.844961601222167e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 8/71 | LOSS: 7.591448037500312e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 9/71 | LOSS: 7.645482810403337e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 10/71 | LOSS: 7.400683609210484e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 11/71 | LOSS: 7.504414384129632e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 12/71 | LOSS: 7.360926002337902e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 13/71 | LOSS: 7.385130792109911e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 14/71 | LOSS: 7.300876162238031e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 15/71 | LOSS: 7.154063808911815e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 16/71 | LOSS: 7.087625443305382e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 17/71 | LOSS: 6.9754049844858755e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 18/71 | LOSS: 7.058636823019601e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 19/71 | LOSS: 7.015936853349558e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 20/71 | LOSS: 7.070203190821866e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 21/71 | LOSS: 6.98745287247055e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 22/71 | LOSS: 7.038511443091051e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 23/71 | LOSS: 7.003969206683299e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 24/71 | LOSS: 6.934646316949511e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 25/71 | LOSS: 6.881366079351909e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 26/71 | LOSS: 6.829486850350534e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 27/71 | LOSS: 6.775543283765728e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 28/71 | LOSS: 6.681088005530785e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 29/71 | LOSS: 6.679656068323917e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 30/71 | LOSS: 6.624539706066454e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 31/71 | LOSS: 6.617610793568929e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 32/71 | LOSS: 6.5619632909092065e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 33/71 | LOSS: 6.549726870563224e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 34/71 | LOSS: 6.544615111384441e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 35/71 | LOSS: 6.495360569008173e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 36/71 | LOSS: 6.494809919810929e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 37/71 | LOSS: 6.508749792326278e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 38/71 | LOSS: 6.493499989655669e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 39/71 | LOSS: 6.528839844577306e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 40/71 | LOSS: 6.496965475312985e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 41/71 | LOSS: 6.539709147546091e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 42/71 | LOSS: 6.474889317381217e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 43/71 | LOSS: 6.457084251368625e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 44/71 | LOSS: 6.432387039644204e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 45/71 | LOSS: 6.429848079515859e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 46/71 | LOSS: 6.410441519039252e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 47/71 | LOSS: 6.4367170106531075e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 48/71 | LOSS: 6.404798327543065e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 49/71 | LOSS: 6.387436433215043e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 50/71 | LOSS: 6.378855428635029e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 51/71 | LOSS: 6.3642623025281446e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 52/71 | LOSS: 6.335432248786948e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 53/71 | LOSS: 6.32884749157475e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 54/71 | LOSS: 6.327785153685413e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 55/71 | LOSS: 6.323720136996209e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 56/71 | LOSS: 6.308991511392187e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 57/71 | LOSS: 6.289501949015238e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 58/71 | LOSS: 6.266403912469918e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 59/71 | LOSS: 6.241890120387931e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 60/71 | LOSS: 6.292772192611039e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 61/71 | LOSS: 6.275800277792094e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 62/71 | LOSS: 6.280546513730697e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 63/71 | LOSS: 6.285517052617706e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 64/71 | LOSS: 6.319561872749966e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 65/71 | LOSS: 6.302967731795489e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 66/71 | LOSS: 6.315206784206363e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 67/71 | LOSS: 6.295179773855232e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 68/71 | LOSS: 6.2746145517124114e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 69/71 | LOSS: 6.268987077159441e-06\n",
      "TRAIN: EPOCH 280/1000 | BATCH 70/71 | LOSS: 6.247891921613162e-06\n",
      "VAL: EPOCH 280/1000 | BATCH 0/8 | LOSS: 5.7173856475856155e-06\n",
      "VAL: EPOCH 280/1000 | BATCH 1/8 | LOSS: 5.146379862708272e-06\n",
      "VAL: EPOCH 280/1000 | BATCH 2/8 | LOSS: 5.20627554578823e-06\n",
      "VAL: EPOCH 280/1000 | BATCH 3/8 | LOSS: 5.375507384997036e-06\n",
      "VAL: EPOCH 280/1000 | BATCH 4/8 | LOSS: 5.3524217037193015e-06\n",
      "VAL: EPOCH 280/1000 | BATCH 5/8 | LOSS: 5.223954000636392e-06\n",
      "VAL: EPOCH 280/1000 | BATCH 6/8 | LOSS: 5.178995382136366e-06\n",
      "VAL: EPOCH 280/1000 | BATCH 7/8 | LOSS: 5.159255351827596e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 0/71 | LOSS: 3.839300916297361e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 1/71 | LOSS: 4.075922788615571e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 2/71 | LOSS: 4.5904359164220905e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 3/71 | LOSS: 4.517017600846884e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 4/71 | LOSS: 4.491014351515332e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 5/71 | LOSS: 4.666007877555482e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 6/71 | LOSS: 4.635886658174318e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 7/71 | LOSS: 4.709954168902186e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 8/71 | LOSS: 4.837861322610277e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 9/71 | LOSS: 4.8949369556794405e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 10/71 | LOSS: 4.977288550766058e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 11/71 | LOSS: 5.0349709302584715e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 12/71 | LOSS: 5.016877156986783e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 13/71 | LOSS: 4.968335230971986e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 14/71 | LOSS: 4.967443722610673e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 15/71 | LOSS: 5.035264450725663e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 16/71 | LOSS: 5.027472926646142e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 17/71 | LOSS: 4.998364197995721e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 18/71 | LOSS: 5.033234294004567e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 19/71 | LOSS: 5.0613388793863125e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 20/71 | LOSS: 5.055659866706091e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 21/71 | LOSS: 5.149345890293427e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 22/71 | LOSS: 5.133289327521039e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 23/71 | LOSS: 5.159961991315261e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 24/71 | LOSS: 5.169273936189711e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 25/71 | LOSS: 5.201644643630761e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 26/71 | LOSS: 5.297098478304515e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 27/71 | LOSS: 5.244272512884761e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 28/71 | LOSS: 5.226208737314727e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 29/71 | LOSS: 5.244093805837716e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 30/71 | LOSS: 5.219739558660583e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 31/71 | LOSS: 5.22819850345968e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 32/71 | LOSS: 5.250574745194612e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 33/71 | LOSS: 5.316702553874861e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 34/71 | LOSS: 5.265124316013368e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 35/71 | LOSS: 5.271868385787255e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 36/71 | LOSS: 5.268997157956521e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 37/71 | LOSS: 5.2746434053723964e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 38/71 | LOSS: 5.263364813893135e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 39/71 | LOSS: 5.2652827264410005e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 40/71 | LOSS: 5.279274635141497e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 41/71 | LOSS: 5.289938723676425e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 42/71 | LOSS: 5.273933052300235e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 43/71 | LOSS: 5.308812944804231e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 44/71 | LOSS: 5.310566600908513e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 45/71 | LOSS: 5.3046421281460026e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 46/71 | LOSS: 5.301082971547e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 47/71 | LOSS: 5.281650833429315e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 48/71 | LOSS: 5.250774583313317e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 49/71 | LOSS: 5.248434667919355e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 50/71 | LOSS: 5.245207817364751e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 51/71 | LOSS: 5.272907834261367e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 52/71 | LOSS: 5.293488181090446e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 53/71 | LOSS: 5.3002026805660095e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 54/71 | LOSS: 5.3152476828637935e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 55/71 | LOSS: 5.309848408582443e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 56/71 | LOSS: 5.28604414683829e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 57/71 | LOSS: 5.279723715265673e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 58/71 | LOSS: 5.273319460283639e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 59/71 | LOSS: 5.275008337927526e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 60/71 | LOSS: 5.293132412812976e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 61/71 | LOSS: 5.2819872573837206e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 62/71 | LOSS: 5.2831783214348855e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 63/71 | LOSS: 5.309519647056504e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 64/71 | LOSS: 5.3202832147620095e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 65/71 | LOSS: 5.332825528994638e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 66/71 | LOSS: 5.349006502924499e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 67/71 | LOSS: 5.357596749303518e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 68/71 | LOSS: 5.34066384386898e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 69/71 | LOSS: 5.32803552297472e-06\n",
      "TRAIN: EPOCH 281/1000 | BATCH 70/71 | LOSS: 5.30589559723883e-06\n",
      "VAL: EPOCH 281/1000 | BATCH 0/8 | LOSS: 7.088598977134097e-06\n",
      "VAL: EPOCH 281/1000 | BATCH 1/8 | LOSS: 6.646288284173352e-06\n",
      "VAL: EPOCH 281/1000 | BATCH 2/8 | LOSS: 6.394114734575851e-06\n",
      "VAL: EPOCH 281/1000 | BATCH 3/8 | LOSS: 6.519706971630512e-06\n",
      "VAL: EPOCH 281/1000 | BATCH 4/8 | LOSS: 6.397588458639803e-06\n",
      "VAL: EPOCH 281/1000 | BATCH 5/8 | LOSS: 5.969565184689903e-06\n",
      "VAL: EPOCH 281/1000 | BATCH 6/8 | LOSS: 5.966380350790652e-06\n",
      "VAL: EPOCH 281/1000 | BATCH 7/8 | LOSS: 5.775415786501981e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 0/71 | LOSS: 6.067687991162529e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 1/71 | LOSS: 5.024945721743279e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 2/71 | LOSS: 4.88245026038688e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 3/71 | LOSS: 5.1534281055864994e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 4/71 | LOSS: 4.975752472091699e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 5/71 | LOSS: 4.965033061428888e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 6/71 | LOSS: 5.091966613690602e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 7/71 | LOSS: 4.985016119007923e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 8/71 | LOSS: 5.176355696474073e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 9/71 | LOSS: 5.24329520885658e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 10/71 | LOSS: 5.175905946750638e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 11/71 | LOSS: 5.166066595544787e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 12/71 | LOSS: 5.154385619859498e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 13/71 | LOSS: 5.075770137149707e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 14/71 | LOSS: 5.054874539685746e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 15/71 | LOSS: 5.07010835804067e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 16/71 | LOSS: 5.056478799173526e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 17/71 | LOSS: 5.008146546161798e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 18/71 | LOSS: 5.002680417850283e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 19/71 | LOSS: 4.957874784849991e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 20/71 | LOSS: 4.91105018746956e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 21/71 | LOSS: 4.937774623282497e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 22/71 | LOSS: 4.989040680020875e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 23/71 | LOSS: 4.995831792105794e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 24/71 | LOSS: 5.026052222092403e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 25/71 | LOSS: 5.018670348223308e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 26/71 | LOSS: 5.019030477679162e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 27/71 | LOSS: 5.021017386752646e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 28/71 | LOSS: 5.0067026614701245e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 29/71 | LOSS: 5.028654474396414e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 30/71 | LOSS: 5.056120420293507e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 31/71 | LOSS: 5.064081790351338e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 32/71 | LOSS: 5.0530228911542725e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 33/71 | LOSS: 5.111745689218344e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 34/71 | LOSS: 5.081683048047125e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 35/71 | LOSS: 5.1671889499023864e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 36/71 | LOSS: 5.1587620308399324e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 37/71 | LOSS: 5.1584919147947066e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 38/71 | LOSS: 5.159934050588606e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 39/71 | LOSS: 5.141692940924258e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 40/71 | LOSS: 5.160362811360889e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 41/71 | LOSS: 5.172568037215526e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 42/71 | LOSS: 5.16626384925858e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 43/71 | LOSS: 5.1715833871474555e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 44/71 | LOSS: 5.1506755880836865e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 45/71 | LOSS: 5.164774788484109e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 46/71 | LOSS: 5.155423355547022e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 47/71 | LOSS: 5.132257314244271e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 48/71 | LOSS: 5.126256754159172e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 49/71 | LOSS: 5.130772160555353e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 50/71 | LOSS: 5.161117778519767e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 51/71 | LOSS: 5.164603451773403e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 52/71 | LOSS: 5.171500947224833e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 53/71 | LOSS: 5.1847879418616045e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 54/71 | LOSS: 5.222500592274379e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 55/71 | LOSS: 5.2888726713717915e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 56/71 | LOSS: 5.275261764038812e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 57/71 | LOSS: 5.271513381213199e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 58/71 | LOSS: 5.309209944058794e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 59/71 | LOSS: 5.30470034997658e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 60/71 | LOSS: 5.341680736789579e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 61/71 | LOSS: 5.335648482639133e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 62/71 | LOSS: 5.384354500122191e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 63/71 | LOSS: 5.3879632417874745e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 64/71 | LOSS: 5.412831762022017e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 65/71 | LOSS: 5.431420890311005e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 66/71 | LOSS: 5.405626759613315e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 67/71 | LOSS: 5.42054959516784e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 68/71 | LOSS: 5.421191325286349e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 69/71 | LOSS: 5.410297756368112e-06\n",
      "TRAIN: EPOCH 282/1000 | BATCH 70/71 | LOSS: 5.434820925817713e-06\n",
      "VAL: EPOCH 282/1000 | BATCH 0/8 | LOSS: 7.141732567106374e-06\n",
      "VAL: EPOCH 282/1000 | BATCH 1/8 | LOSS: 6.021925628374447e-06\n",
      "VAL: EPOCH 282/1000 | BATCH 2/8 | LOSS: 5.88808552492992e-06\n",
      "VAL: EPOCH 282/1000 | BATCH 3/8 | LOSS: 5.924162110204634e-06\n",
      "VAL: EPOCH 282/1000 | BATCH 4/8 | LOSS: 5.800524468213552e-06\n",
      "VAL: EPOCH 282/1000 | BATCH 5/8 | LOSS: 5.532518647063019e-06\n",
      "VAL: EPOCH 282/1000 | BATCH 6/8 | LOSS: 5.437535557056046e-06\n",
      "VAL: EPOCH 282/1000 | BATCH 7/8 | LOSS: 5.403094348821469e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 0/71 | LOSS: 4.909822564513888e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 1/71 | LOSS: 7.518165602959925e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 2/71 | LOSS: 7.061818602475493e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 3/71 | LOSS: 7.508338512707269e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 4/71 | LOSS: 7.032618850644212e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 5/71 | LOSS: 6.91970634155344e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 6/71 | LOSS: 6.771588817043396e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 7/71 | LOSS: 6.497276729078294e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 8/71 | LOSS: 6.381046912590844e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 9/71 | LOSS: 6.167023047964903e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 10/71 | LOSS: 6.279350310995307e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 11/71 | LOSS: 6.1668710789793595e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 12/71 | LOSS: 6.228637595575912e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 13/71 | LOSS: 6.144630528329539e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 14/71 | LOSS: 6.205653971846914e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 15/71 | LOSS: 6.149019384338317e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 16/71 | LOSS: 6.224669023675447e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 17/71 | LOSS: 6.153246583077514e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 18/71 | LOSS: 6.153143850066331e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 19/71 | LOSS: 6.138927642496128e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 20/71 | LOSS: 6.121790539466365e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 21/71 | LOSS: 6.093063368759679e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 22/71 | LOSS: 6.100988467432731e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 23/71 | LOSS: 6.119552210748225e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 24/71 | LOSS: 6.114750121923862e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 25/71 | LOSS: 6.048838800919699e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 26/71 | LOSS: 6.073512681555089e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 27/71 | LOSS: 6.069023519168175e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 28/71 | LOSS: 6.011327090602898e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 29/71 | LOSS: 5.942967284984965e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 30/71 | LOSS: 5.990654714086334e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 31/71 | LOSS: 6.017399115876287e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 32/71 | LOSS: 5.990391193997559e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 33/71 | LOSS: 6.049651226610021e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 34/71 | LOSS: 6.0313281470111435e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 35/71 | LOSS: 5.987155898563085e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 36/71 | LOSS: 5.989247772565766e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 37/71 | LOSS: 5.966565750943693e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 38/71 | LOSS: 5.979302281607838e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 39/71 | LOSS: 5.938656738635473e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 40/71 | LOSS: 5.902362040726031e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 41/71 | LOSS: 5.9288047395966714e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 42/71 | LOSS: 5.881185505893162e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 43/71 | LOSS: 5.961802623294881e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 44/71 | LOSS: 5.936550233956142e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 45/71 | LOSS: 5.985554005984222e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 46/71 | LOSS: 6.021958597557386e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 47/71 | LOSS: 6.067548696364611e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 48/71 | LOSS: 6.128490551826853e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 49/71 | LOSS: 6.1347578684944895e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 50/71 | LOSS: 6.172506310380698e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 51/71 | LOSS: 6.200610283275847e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 52/71 | LOSS: 6.2174480829200104e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 53/71 | LOSS: 6.187128772378654e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 54/71 | LOSS: 6.189242082076486e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 55/71 | LOSS: 6.244546284506214e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 56/71 | LOSS: 6.225098539353144e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 57/71 | LOSS: 6.242980456089095e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 58/71 | LOSS: 6.250985356696913e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 59/71 | LOSS: 6.2199892151208285e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 60/71 | LOSS: 6.233080392896644e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 61/71 | LOSS: 6.217862634120168e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 62/71 | LOSS: 6.260227673053686e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 63/71 | LOSS: 6.237127955444066e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 64/71 | LOSS: 6.241849419334008e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 65/71 | LOSS: 6.256625662596586e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 66/71 | LOSS: 6.248759352423966e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 67/71 | LOSS: 6.259895625964204e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 68/71 | LOSS: 6.230181221010001e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 69/71 | LOSS: 6.238882425740095e-06\n",
      "TRAIN: EPOCH 283/1000 | BATCH 70/71 | LOSS: 6.2388421354908e-06\n",
      "VAL: EPOCH 283/1000 | BATCH 0/8 | LOSS: 7.71677696320694e-06\n",
      "VAL: EPOCH 283/1000 | BATCH 1/8 | LOSS: 7.103340749381459e-06\n",
      "VAL: EPOCH 283/1000 | BATCH 2/8 | LOSS: 7.699532185749073e-06\n",
      "VAL: EPOCH 283/1000 | BATCH 3/8 | LOSS: 7.982918873494782e-06\n",
      "VAL: EPOCH 283/1000 | BATCH 4/8 | LOSS: 7.964070482557872e-06\n",
      "VAL: EPOCH 283/1000 | BATCH 5/8 | LOSS: 8.104665766950347e-06\n",
      "VAL: EPOCH 283/1000 | BATCH 6/8 | LOSS: 8.057697219295992e-06\n",
      "VAL: EPOCH 283/1000 | BATCH 7/8 | LOSS: 8.170170588073233e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 0/71 | LOSS: 8.577075277571566e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 1/71 | LOSS: 8.08434219834453e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 2/71 | LOSS: 7.3274918577226344e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 3/71 | LOSS: 7.719691325291933e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 4/71 | LOSS: 7.212947639345657e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 5/71 | LOSS: 7.704442396061495e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 6/71 | LOSS: 7.339186140598031e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 7/71 | LOSS: 7.31352810134922e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 8/71 | LOSS: 7.510079689786329e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 9/71 | LOSS: 7.535064605690423e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 10/71 | LOSS: 7.706238110668808e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 11/71 | LOSS: 7.504820662992036e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 12/71 | LOSS: 7.723668484406797e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 13/71 | LOSS: 7.6119914800593894e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 14/71 | LOSS: 7.66718130762456e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 15/71 | LOSS: 7.765031625694974e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 16/71 | LOSS: 7.827144519764902e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 17/71 | LOSS: 8.159270035119132e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 18/71 | LOSS: 8.0742105350701e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 19/71 | LOSS: 8.313093121614657e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 20/71 | LOSS: 8.355058309375419e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 21/71 | LOSS: 8.475340729835972e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 22/71 | LOSS: 8.393081869776918e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 23/71 | LOSS: 8.386225488266064e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 24/71 | LOSS: 8.468890282529173e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 25/71 | LOSS: 8.264917683985774e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 26/71 | LOSS: 8.51373528752851e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 27/71 | LOSS: 8.479475111146581e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 28/71 | LOSS: 8.611110968496208e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 29/71 | LOSS: 8.658134159607774e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 30/71 | LOSS: 8.558706301095476e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 31/71 | LOSS: 8.77772100693619e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 32/71 | LOSS: 8.637936444398347e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 33/71 | LOSS: 8.762773565168199e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 34/71 | LOSS: 8.83907982175255e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 35/71 | LOSS: 8.844025021264517e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 36/71 | LOSS: 8.906749765112574e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 37/71 | LOSS: 8.850490494775218e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 38/71 | LOSS: 8.894871149468832e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 39/71 | LOSS: 8.898659098122153e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 40/71 | LOSS: 8.915529090189943e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 41/71 | LOSS: 9.089366921259734e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 42/71 | LOSS: 9.029178981172452e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 43/71 | LOSS: 9.25526011418366e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 44/71 | LOSS: 9.145009466414599e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 45/71 | LOSS: 9.251979028466968e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 46/71 | LOSS: 9.280847018786479e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 47/71 | LOSS: 9.241258643063096e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 48/71 | LOSS: 9.370393130740588e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 49/71 | LOSS: 9.322267087554792e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 50/71 | LOSS: 9.371266826871993e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 51/71 | LOSS: 9.354686898614791e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 52/71 | LOSS: 9.328404955323214e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 53/71 | LOSS: 9.326261264235385e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 54/71 | LOSS: 9.290406127216888e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 55/71 | LOSS: 9.276127538474352e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 56/71 | LOSS: 9.249083041938496e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 57/71 | LOSS: 9.203930850507558e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 58/71 | LOSS: 9.176959515493994e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 59/71 | LOSS: 9.112601113277681e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 60/71 | LOSS: 9.084879910333853e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 61/71 | LOSS: 9.051553221187561e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 62/71 | LOSS: 8.99827333948148e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 63/71 | LOSS: 8.978754870270222e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 64/71 | LOSS: 8.942913035962892e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 65/71 | LOSS: 8.959365617777097e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 66/71 | LOSS: 8.886655335240292e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 67/71 | LOSS: 8.835081931626037e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 68/71 | LOSS: 8.79455831017222e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 69/71 | LOSS: 8.782028394957057e-06\n",
      "TRAIN: EPOCH 284/1000 | BATCH 70/71 | LOSS: 8.744403068806311e-06\n",
      "VAL: EPOCH 284/1000 | BATCH 0/8 | LOSS: 6.368294634739868e-06\n",
      "VAL: EPOCH 284/1000 | BATCH 1/8 | LOSS: 5.690199031960219e-06\n",
      "VAL: EPOCH 284/1000 | BATCH 2/8 | LOSS: 5.951617519410017e-06\n",
      "VAL: EPOCH 284/1000 | BATCH 3/8 | LOSS: 5.979411639600585e-06\n",
      "VAL: EPOCH 284/1000 | BATCH 4/8 | LOSS: 5.884229813091224e-06\n",
      "VAL: EPOCH 284/1000 | BATCH 5/8 | LOSS: 5.650747046577938e-06\n",
      "VAL: EPOCH 284/1000 | BATCH 6/8 | LOSS: 5.573680482484633e-06\n",
      "VAL: EPOCH 284/1000 | BATCH 7/8 | LOSS: 5.590826390289294e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 0/71 | LOSS: 5.1802403504552785e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 1/71 | LOSS: 5.753877530878526e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 2/71 | LOSS: 5.488877377501922e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 3/71 | LOSS: 5.130859449309355e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 4/71 | LOSS: 5.185854934097733e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 5/71 | LOSS: 5.142965619597817e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 6/71 | LOSS: 5.230224035455779e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 7/71 | LOSS: 5.363976811167959e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 8/71 | LOSS: 5.38146120258413e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 9/71 | LOSS: 5.327150302036898e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 10/71 | LOSS: 5.287018542400222e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 11/71 | LOSS: 5.314780764820171e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 12/71 | LOSS: 5.262355793652555e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 13/71 | LOSS: 5.285999902428427e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 14/71 | LOSS: 5.334411980584264e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 15/71 | LOSS: 5.388403621964244e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 16/71 | LOSS: 5.465666243467508e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 17/71 | LOSS: 5.482694758837978e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 18/71 | LOSS: 5.50395904946e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 19/71 | LOSS: 5.4173523267309065e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 20/71 | LOSS: 5.4355734738194204e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 21/71 | LOSS: 5.350842601811482e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 22/71 | LOSS: 5.277032315738641e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 23/71 | LOSS: 5.2629180042155594e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 24/71 | LOSS: 5.246196706139017e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 25/71 | LOSS: 5.208562889982624e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 26/71 | LOSS: 5.18428094909285e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 27/71 | LOSS: 5.180253180826964e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 28/71 | LOSS: 5.197391225198677e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 29/71 | LOSS: 5.192371872908552e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 30/71 | LOSS: 5.140849240096061e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 31/71 | LOSS: 5.143827479514584e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 32/71 | LOSS: 5.196128561444559e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 33/71 | LOSS: 5.1971810133930856e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 34/71 | LOSS: 5.251600123301614e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 35/71 | LOSS: 5.257420108743342e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 36/71 | LOSS: 5.291418605141902e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 37/71 | LOSS: 5.247024211329469e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 38/71 | LOSS: 5.2377838820132474e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 39/71 | LOSS: 5.227089377513039e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 40/71 | LOSS: 5.208309896882317e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 41/71 | LOSS: 5.220880319982479e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 42/71 | LOSS: 5.205315785640905e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 43/71 | LOSS: 5.183566207216989e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 44/71 | LOSS: 5.168692041479517e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 45/71 | LOSS: 5.153703331534811e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 46/71 | LOSS: 5.130929405416456e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 47/71 | LOSS: 5.132779297885766e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 48/71 | LOSS: 5.12946961843765e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 49/71 | LOSS: 5.1140136201865975e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 50/71 | LOSS: 5.130029065042561e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 51/71 | LOSS: 5.139192895652601e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 52/71 | LOSS: 5.134863969496504e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 53/71 | LOSS: 5.1467799056370965e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 54/71 | LOSS: 5.150225198039235e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 55/71 | LOSS: 5.155971418194635e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 56/71 | LOSS: 5.147683655275359e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 57/71 | LOSS: 5.161773201664268e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 58/71 | LOSS: 5.262251406902116e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 59/71 | LOSS: 5.27276113947058e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 60/71 | LOSS: 5.330879368974835e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 61/71 | LOSS: 5.35624519304087e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 62/71 | LOSS: 5.440702518662586e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 63/71 | LOSS: 5.469434938731865e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 64/71 | LOSS: 5.532932818123999e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 65/71 | LOSS: 5.611938694059835e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 66/71 | LOSS: 5.602088501983593e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 67/71 | LOSS: 5.6238980719439606e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 68/71 | LOSS: 5.61276059327782e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 69/71 | LOSS: 5.6217212529320805e-06\n",
      "TRAIN: EPOCH 285/1000 | BATCH 70/71 | LOSS: 5.60318399102855e-06\n",
      "VAL: EPOCH 285/1000 | BATCH 0/8 | LOSS: 7.690254278713837e-06\n",
      "VAL: EPOCH 285/1000 | BATCH 1/8 | LOSS: 7.116752840374829e-06\n",
      "VAL: EPOCH 285/1000 | BATCH 2/8 | LOSS: 6.605104772461345e-06\n",
      "VAL: EPOCH 285/1000 | BATCH 3/8 | LOSS: 6.716227858305501e-06\n",
      "VAL: EPOCH 285/1000 | BATCH 4/8 | LOSS: 6.499670689663617e-06\n",
      "VAL: EPOCH 285/1000 | BATCH 5/8 | LOSS: 6.10600333554127e-06\n",
      "VAL: EPOCH 285/1000 | BATCH 6/8 | LOSS: 6.0311648277482684e-06\n",
      "VAL: EPOCH 285/1000 | BATCH 7/8 | LOSS: 5.792649744762457e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 0/71 | LOSS: 6.812350875406992e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 1/71 | LOSS: 6.580928811672493e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 2/71 | LOSS: 6.412437414837768e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 3/71 | LOSS: 6.076830914025777e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 4/71 | LOSS: 5.873489863006398e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 5/71 | LOSS: 5.94382178557377e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 6/71 | LOSS: 6.147408904715641e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 7/71 | LOSS: 6.00005967044126e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 8/71 | LOSS: 5.883039092522166e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 9/71 | LOSS: 5.924894639974809e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 10/71 | LOSS: 5.843524087884527e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 11/71 | LOSS: 5.722193880804601e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 12/71 | LOSS: 5.697868346032919e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 13/71 | LOSS: 5.755273735173562e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 14/71 | LOSS: 5.717250132875051e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 15/71 | LOSS: 5.748923001647199e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 16/71 | LOSS: 5.809319244529915e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 17/71 | LOSS: 5.716179556556098e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 18/71 | LOSS: 5.664612121778957e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 19/71 | LOSS: 5.640514473270741e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 20/71 | LOSS: 5.596831453571906e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 21/71 | LOSS: 5.5475701918277824e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 22/71 | LOSS: 5.486408198244728e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 23/71 | LOSS: 5.4553040248113876e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 24/71 | LOSS: 5.423965103545925e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 25/71 | LOSS: 5.3790646374042035e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 26/71 | LOSS: 5.3722267828696334e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 27/71 | LOSS: 5.342293954007411e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 28/71 | LOSS: 5.35151896436783e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 29/71 | LOSS: 5.362051024349057e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 30/71 | LOSS: 5.3252767957606155e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 31/71 | LOSS: 5.333616712732692e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 32/71 | LOSS: 5.304078210125833e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 33/71 | LOSS: 5.303695000774871e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 34/71 | LOSS: 5.261575357248408e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 35/71 | LOSS: 5.2988943303312735e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 36/71 | LOSS: 5.3071590332979825e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 37/71 | LOSS: 5.262552540922402e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 38/71 | LOSS: 5.332257667480288e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 39/71 | LOSS: 5.328442568952596e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 40/71 | LOSS: 5.322726116080558e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 41/71 | LOSS: 5.348936156224227e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 42/71 | LOSS: 5.316483936418758e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 43/71 | LOSS: 5.347136214533583e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 44/71 | LOSS: 5.365401001553335e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 45/71 | LOSS: 5.361767969826241e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 46/71 | LOSS: 5.4103395493083106e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 47/71 | LOSS: 5.428230328637558e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 48/71 | LOSS: 5.460177047153797e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 49/71 | LOSS: 5.510147248060093e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 50/71 | LOSS: 5.515114534158356e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 51/71 | LOSS: 5.529524523458699e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 52/71 | LOSS: 5.499700467414684e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 53/71 | LOSS: 5.561061302446264e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 54/71 | LOSS: 5.533669214359675e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 55/71 | LOSS: 5.527383921487074e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 56/71 | LOSS: 5.53170986294342e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 57/71 | LOSS: 5.524435123490925e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 58/71 | LOSS: 5.5260107744242445e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 59/71 | LOSS: 5.522284088025723e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 60/71 | LOSS: 5.523370806467491e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 61/71 | LOSS: 5.514385080577115e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 62/71 | LOSS: 5.486065757640828e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 63/71 | LOSS: 5.512082356773362e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 64/71 | LOSS: 5.510716832409693e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 65/71 | LOSS: 5.509924142533044e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 66/71 | LOSS: 5.523562373100639e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 67/71 | LOSS: 5.540326954440602e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 68/71 | LOSS: 5.554429560726166e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 69/71 | LOSS: 5.5467793734871e-06\n",
      "TRAIN: EPOCH 286/1000 | BATCH 70/71 | LOSS: 5.533047330355261e-06\n",
      "VAL: EPOCH 286/1000 | BATCH 0/8 | LOSS: 6.957000550755765e-06\n",
      "VAL: EPOCH 286/1000 | BATCH 1/8 | LOSS: 5.984355539112585e-06\n",
      "VAL: EPOCH 286/1000 | BATCH 2/8 | LOSS: 5.932376097916858e-06\n",
      "VAL: EPOCH 286/1000 | BATCH 3/8 | LOSS: 5.8757008218890405e-06\n",
      "VAL: EPOCH 286/1000 | BATCH 4/8 | LOSS: 5.816478824272053e-06\n",
      "VAL: EPOCH 286/1000 | BATCH 5/8 | LOSS: 5.593563476698667e-06\n",
      "VAL: EPOCH 286/1000 | BATCH 6/8 | LOSS: 5.5489215680738975e-06\n",
      "VAL: EPOCH 286/1000 | BATCH 7/8 | LOSS: 5.516154999440914e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 0/71 | LOSS: 5.063567186880391e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 1/71 | LOSS: 4.834064384340309e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 2/71 | LOSS: 4.698739758168813e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 3/71 | LOSS: 4.909534027319751e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 4/71 | LOSS: 5.35651506652357e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 5/71 | LOSS: 5.54443810566833e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 6/71 | LOSS: 5.277789049224728e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 7/71 | LOSS: 5.303761099639814e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 8/71 | LOSS: 5.295755979912226e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 9/71 | LOSS: 5.298746918924735e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 10/71 | LOSS: 5.445847116127632e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 11/71 | LOSS: 5.412066381419815e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 12/71 | LOSS: 5.36797968099843e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 13/71 | LOSS: 5.299671784086968e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 14/71 | LOSS: 5.226794686071419e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 15/71 | LOSS: 5.228998361417325e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 16/71 | LOSS: 5.160311796798316e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 17/71 | LOSS: 5.116066404298181e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 18/71 | LOSS: 5.082865156605533e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 19/71 | LOSS: 5.10007519096689e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 20/71 | LOSS: 5.136628433752949e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 21/71 | LOSS: 5.118315812069341e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 22/71 | LOSS: 5.0933881934527205e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 23/71 | LOSS: 5.089830627487875e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 24/71 | LOSS: 5.1324548985576256e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 25/71 | LOSS: 5.1236172042151266e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 26/71 | LOSS: 5.1827480126155054e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 27/71 | LOSS: 5.215803396043027e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 28/71 | LOSS: 5.303331551564544e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 29/71 | LOSS: 5.291442736658306e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 30/71 | LOSS: 5.323566138911467e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 31/71 | LOSS: 5.369668400589944e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 32/71 | LOSS: 5.435398937455223e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 33/71 | LOSS: 5.428162266445511e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 34/71 | LOSS: 5.472796133939742e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 35/71 | LOSS: 5.502826891440944e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 36/71 | LOSS: 5.469725274943394e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 37/71 | LOSS: 5.408079962427636e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 38/71 | LOSS: 5.435008729247597e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 39/71 | LOSS: 5.409830703229091e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 40/71 | LOSS: 5.382011483850031e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 41/71 | LOSS: 5.402664783105138e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 42/71 | LOSS: 5.4004532356948195e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 43/71 | LOSS: 5.393735136749456e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 44/71 | LOSS: 5.399365020113894e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 45/71 | LOSS: 5.378768758854004e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 46/71 | LOSS: 5.3938177169332815e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 47/71 | LOSS: 5.380595032041431e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 48/71 | LOSS: 5.401118785960776e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 49/71 | LOSS: 5.390537071434665e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 50/71 | LOSS: 5.397038822753957e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 51/71 | LOSS: 5.384624597144676e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 52/71 | LOSS: 5.3957696849774606e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 53/71 | LOSS: 5.394191652562262e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 54/71 | LOSS: 5.417105322439139e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 55/71 | LOSS: 5.423102314645283e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 56/71 | LOSS: 5.4165919113744935e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 57/71 | LOSS: 5.409846098387036e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 58/71 | LOSS: 5.402318462167109e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 59/71 | LOSS: 5.384327641877462e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 60/71 | LOSS: 5.374432088292898e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 61/71 | LOSS: 5.360947999443235e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 62/71 | LOSS: 5.361474183109027e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 63/71 | LOSS: 5.351765757666271e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 64/71 | LOSS: 5.351388706633141e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 65/71 | LOSS: 5.341171414108954e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 66/71 | LOSS: 5.339023743151451e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 67/71 | LOSS: 5.329697374049026e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 68/71 | LOSS: 5.33604954556643e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 69/71 | LOSS: 5.348186277842615e-06\n",
      "TRAIN: EPOCH 287/1000 | BATCH 70/71 | LOSS: 5.319964064794066e-06\n",
      "VAL: EPOCH 287/1000 | BATCH 0/8 | LOSS: 9.050148946698755e-06\n",
      "VAL: EPOCH 287/1000 | BATCH 1/8 | LOSS: 8.102543915811111e-06\n",
      "VAL: EPOCH 287/1000 | BATCH 2/8 | LOSS: 7.62201292066796e-06\n",
      "VAL: EPOCH 287/1000 | BATCH 3/8 | LOSS: 7.812006856511289e-06\n",
      "VAL: EPOCH 287/1000 | BATCH 4/8 | LOSS: 7.497091155528324e-06\n",
      "VAL: EPOCH 287/1000 | BATCH 5/8 | LOSS: 7.041676857018804e-06\n",
      "VAL: EPOCH 287/1000 | BATCH 6/8 | LOSS: 6.971655693632783e-06\n",
      "VAL: EPOCH 287/1000 | BATCH 7/8 | LOSS: 6.681752097392746e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 0/71 | LOSS: 6.359848612191854e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 1/71 | LOSS: 6.566508545802208e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 2/71 | LOSS: 6.290092642302625e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 3/71 | LOSS: 6.557831284226268e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 4/71 | LOSS: 6.143598511698656e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 5/71 | LOSS: 6.671472268256669e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 6/71 | LOSS: 6.708544333378086e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 7/71 | LOSS: 6.897496575675177e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 8/71 | LOSS: 6.825992740131268e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 9/71 | LOSS: 6.677216470052372e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 10/71 | LOSS: 6.772483133814107e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 11/71 | LOSS: 6.503700736478398e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 12/71 | LOSS: 6.544516264302029e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 13/71 | LOSS: 6.4711186626352306e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 14/71 | LOSS: 6.3531399670561465e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 15/71 | LOSS: 6.3994873755746084e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 16/71 | LOSS: 6.316228711297941e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 17/71 | LOSS: 6.240812909810403e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 18/71 | LOSS: 6.229804799467705e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 19/71 | LOSS: 6.112252390266804e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 20/71 | LOSS: 6.139674540117128e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 21/71 | LOSS: 6.0320270863071945e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 22/71 | LOSS: 5.960389038907875e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 23/71 | LOSS: 5.939947404461539e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 24/71 | LOSS: 5.877986814084579e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 25/71 | LOSS: 5.937519019356002e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 26/71 | LOSS: 5.913383717968288e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 27/71 | LOSS: 5.924996238653486e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 28/71 | LOSS: 5.9041271532774334e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 29/71 | LOSS: 5.8845789908446026e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 30/71 | LOSS: 5.930384634578537e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 31/71 | LOSS: 5.861295726106164e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 32/71 | LOSS: 5.863415005726645e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 33/71 | LOSS: 5.830412319215504e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 34/71 | LOSS: 5.819884394700888e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 35/71 | LOSS: 5.7760096612784964e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 36/71 | LOSS: 5.791721789776332e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 37/71 | LOSS: 5.758442972891269e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 38/71 | LOSS: 5.78991278370011e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 39/71 | LOSS: 5.768624924940014e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 40/71 | LOSS: 5.7906364673260945e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 41/71 | LOSS: 5.807637519085672e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 42/71 | LOSS: 5.782392801932495e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 43/71 | LOSS: 5.764204507324228e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 44/71 | LOSS: 5.737915161161153e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 45/71 | LOSS: 5.736922794905853e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 46/71 | LOSS: 5.779045397232155e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 47/71 | LOSS: 5.784697743630811e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 48/71 | LOSS: 5.843560306950204e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 49/71 | LOSS: 5.812408694509941e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 50/71 | LOSS: 5.819192562971604e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 51/71 | LOSS: 5.799046067960052e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 52/71 | LOSS: 5.788664021514157e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 53/71 | LOSS: 5.785641342804916e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 54/71 | LOSS: 5.780577260917529e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 55/71 | LOSS: 5.783917137997767e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 56/71 | LOSS: 5.775611558228296e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 57/71 | LOSS: 5.768687558351115e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 58/71 | LOSS: 5.781787094593263e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 59/71 | LOSS: 5.771120265762875e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 60/71 | LOSS: 5.748332684606049e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 61/71 | LOSS: 5.759803515971843e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 62/71 | LOSS: 5.7512517547106e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 63/71 | LOSS: 5.7575792347108745e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 64/71 | LOSS: 5.756678564676594e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 65/71 | LOSS: 5.773443252688924e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 66/71 | LOSS: 5.775580633265086e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 67/71 | LOSS: 5.764326531602017e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 68/71 | LOSS: 5.766273245066311e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 69/71 | LOSS: 5.746354330377861e-06\n",
      "TRAIN: EPOCH 288/1000 | BATCH 70/71 | LOSS: 5.779261918703437e-06\n",
      "VAL: EPOCH 288/1000 | BATCH 0/8 | LOSS: 7.1360973379341885e-06\n",
      "VAL: EPOCH 288/1000 | BATCH 1/8 | LOSS: 6.099491201894125e-06\n",
      "VAL: EPOCH 288/1000 | BATCH 2/8 | LOSS: 6.131019593643335e-06\n",
      "VAL: EPOCH 288/1000 | BATCH 3/8 | LOSS: 6.018748308633803e-06\n",
      "VAL: EPOCH 288/1000 | BATCH 4/8 | LOSS: 5.927414349571336e-06\n",
      "VAL: EPOCH 288/1000 | BATCH 5/8 | LOSS: 5.56845399538967e-06\n",
      "VAL: EPOCH 288/1000 | BATCH 6/8 | LOSS: 5.502629197248357e-06\n",
      "VAL: EPOCH 288/1000 | BATCH 7/8 | LOSS: 5.430509304460429e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 0/71 | LOSS: 5.134418643137906e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 1/71 | LOSS: 5.299241593093029e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 2/71 | LOSS: 5.6584452371074194e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 3/71 | LOSS: 5.480906338561908e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 4/71 | LOSS: 5.391644936025841e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 5/71 | LOSS: 5.629934700361143e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 6/71 | LOSS: 5.587997033476963e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 7/71 | LOSS: 5.755967606546619e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 8/71 | LOSS: 5.670469110757242e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 9/71 | LOSS: 5.798810616397532e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 10/71 | LOSS: 5.660862040134486e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 11/71 | LOSS: 5.665525804943172e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 12/71 | LOSS: 5.679258370386938e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 13/71 | LOSS: 5.589728451533509e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 14/71 | LOSS: 5.562233188053748e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 15/71 | LOSS: 5.51514276025955e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 16/71 | LOSS: 5.483155478834626e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 17/71 | LOSS: 5.426331148959838e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 18/71 | LOSS: 5.385782562945296e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 19/71 | LOSS: 5.32170186033909e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 20/71 | LOSS: 5.343275786019928e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 21/71 | LOSS: 5.317919741091001e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 22/71 | LOSS: 5.304149256161232e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 23/71 | LOSS: 5.279709588042654e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 24/71 | LOSS: 5.284933286020532e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 25/71 | LOSS: 5.32497943612935e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 26/71 | LOSS: 5.325007064298175e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 27/71 | LOSS: 5.357332416419272e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 28/71 | LOSS: 5.389719262893777e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 29/71 | LOSS: 5.371722863856121e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 30/71 | LOSS: 5.384014163991519e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 31/71 | LOSS: 5.431753024254249e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 32/71 | LOSS: 5.47896730274784e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 33/71 | LOSS: 5.458787307040492e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 34/71 | LOSS: 5.511369636224117e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 35/71 | LOSS: 5.536128128369455e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 36/71 | LOSS: 5.505064613365952e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 37/71 | LOSS: 5.501130365366025e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 38/71 | LOSS: 5.540891857564044e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 39/71 | LOSS: 5.553727862661617e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 40/71 | LOSS: 5.523670363511817e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 41/71 | LOSS: 5.533004901908931e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 42/71 | LOSS: 5.571656899075267e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 43/71 | LOSS: 5.5888138438769435e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 44/71 | LOSS: 5.579265972402128e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 45/71 | LOSS: 5.566825057426088e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 46/71 | LOSS: 5.566496867609106e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 47/71 | LOSS: 5.556259774645393e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 48/71 | LOSS: 5.55005207926876e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 49/71 | LOSS: 5.535912505365559e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 50/71 | LOSS: 5.526911857433494e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 51/71 | LOSS: 5.5285936556314345e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 52/71 | LOSS: 5.511896036313253e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 53/71 | LOSS: 5.491638729735213e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 54/71 | LOSS: 5.4875260916146955e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 55/71 | LOSS: 5.4828114863474705e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 56/71 | LOSS: 5.440898268971707e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 57/71 | LOSS: 5.435683533505653e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 58/71 | LOSS: 5.406718038734576e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 59/71 | LOSS: 5.379694512915497e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 60/71 | LOSS: 5.372072251464219e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 61/71 | LOSS: 5.362263352488357e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 62/71 | LOSS: 5.334575686021477e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 63/71 | LOSS: 5.319957335103709e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 64/71 | LOSS: 5.312206586985177e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 65/71 | LOSS: 5.289233299801268e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 66/71 | LOSS: 5.289148682651958e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 67/71 | LOSS: 5.264425139477466e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 68/71 | LOSS: 5.2480061745829545e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 69/71 | LOSS: 5.220664992780907e-06\n",
      "TRAIN: EPOCH 289/1000 | BATCH 70/71 | LOSS: 5.27573637183695e-06\n",
      "VAL: EPOCH 289/1000 | BATCH 0/8 | LOSS: 6.200044026627438e-06\n",
      "VAL: EPOCH 289/1000 | BATCH 1/8 | LOSS: 5.348470267563243e-06\n",
      "VAL: EPOCH 289/1000 | BATCH 2/8 | LOSS: 5.46432496169776e-06\n",
      "VAL: EPOCH 289/1000 | BATCH 3/8 | LOSS: 5.516969736163446e-06\n",
      "VAL: EPOCH 289/1000 | BATCH 4/8 | LOSS: 5.431572117231554e-06\n",
      "VAL: EPOCH 289/1000 | BATCH 5/8 | LOSS: 5.315901717040106e-06\n",
      "VAL: EPOCH 289/1000 | BATCH 6/8 | LOSS: 5.2606949663770915e-06\n",
      "VAL: EPOCH 289/1000 | BATCH 7/8 | LOSS: 5.2489117479126435e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 0/71 | LOSS: 4.911937594442861e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 1/71 | LOSS: 6.104527074057842e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 2/71 | LOSS: 5.900425700626026e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 3/71 | LOSS: 5.908816547162132e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 4/71 | LOSS: 5.717600197385764e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 5/71 | LOSS: 5.468716608447721e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 6/71 | LOSS: 5.2451944481747755e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 7/71 | LOSS: 5.180267407922656e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 8/71 | LOSS: 5.118284712403289e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 9/71 | LOSS: 5.122551829117583e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 10/71 | LOSS: 5.001885380228156e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 11/71 | LOSS: 4.9543319467678275e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 12/71 | LOSS: 5.024526846962264e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 13/71 | LOSS: 5.022259561623546e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 14/71 | LOSS: 5.0586108955030795e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 15/71 | LOSS: 5.143091556192303e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 16/71 | LOSS: 5.246465849161134e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 17/71 | LOSS: 5.323290856217176e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 18/71 | LOSS: 5.309277017467569e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 19/71 | LOSS: 5.337669722393912e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 20/71 | LOSS: 5.288012419311729e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 21/71 | LOSS: 5.241652615950443e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 22/71 | LOSS: 5.23515174553841e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 23/71 | LOSS: 5.239207742609627e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 24/71 | LOSS: 5.220473467488773e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 25/71 | LOSS: 5.166401164248013e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 26/71 | LOSS: 5.1867881733482206e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 27/71 | LOSS: 5.157177497494558e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 28/71 | LOSS: 5.138796354756401e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 29/71 | LOSS: 5.118566514283885e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 30/71 | LOSS: 5.105428044771543e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 31/71 | LOSS: 5.084739612470912e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 32/71 | LOSS: 5.096370965805942e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 33/71 | LOSS: 5.074645407469635e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 34/71 | LOSS: 5.082674020481396e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 35/71 | LOSS: 5.059318166584288e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 36/71 | LOSS: 5.060722988009975e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 37/71 | LOSS: 5.0446219455280745e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 38/71 | LOSS: 5.0428342057365135e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 39/71 | LOSS: 5.02648633755598e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 40/71 | LOSS: 4.994832535175658e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 41/71 | LOSS: 5.002115282574475e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 42/71 | LOSS: 4.989630618839265e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 43/71 | LOSS: 4.987754633392879e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 44/71 | LOSS: 4.9671951071811945e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 45/71 | LOSS: 4.95761057960487e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 46/71 | LOSS: 4.9761865835883305e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 47/71 | LOSS: 4.946491534004356e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 48/71 | LOSS: 4.931070504556244e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 49/71 | LOSS: 4.935239740007092e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 50/71 | LOSS: 4.934867373616298e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 51/71 | LOSS: 4.9184157940264695e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 52/71 | LOSS: 4.921524681055914e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 53/71 | LOSS: 4.9165307869029845e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 54/71 | LOSS: 4.910559379244329e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 55/71 | LOSS: 4.9128160851198895e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 56/71 | LOSS: 4.923565252842341e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 57/71 | LOSS: 4.929274617662204e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 58/71 | LOSS: 4.930859562345007e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 59/71 | LOSS: 4.91197185965575e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 60/71 | LOSS: 4.921096504284699e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 61/71 | LOSS: 4.940404067148789e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 62/71 | LOSS: 4.9272801204226975e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 63/71 | LOSS: 4.927722173420079e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 64/71 | LOSS: 4.939471630048222e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 65/71 | LOSS: 4.93991912221783e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 66/71 | LOSS: 4.955729207776769e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 67/71 | LOSS: 4.940309241539901e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 68/71 | LOSS: 4.9416929987669196e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 69/71 | LOSS: 4.951646276042863e-06\n",
      "TRAIN: EPOCH 290/1000 | BATCH 70/71 | LOSS: 4.944446278437989e-06\n",
      "VAL: EPOCH 290/1000 | BATCH 0/8 | LOSS: 7.1660451794741675e-06\n",
      "VAL: EPOCH 290/1000 | BATCH 1/8 | LOSS: 6.403857469194918e-06\n",
      "VAL: EPOCH 290/1000 | BATCH 2/8 | LOSS: 6.948082500457531e-06\n",
      "VAL: EPOCH 290/1000 | BATCH 3/8 | LOSS: 6.94531320277747e-06\n",
      "VAL: EPOCH 290/1000 | BATCH 4/8 | LOSS: 6.962728730286472e-06\n",
      "VAL: EPOCH 290/1000 | BATCH 5/8 | LOSS: 7.02110363211735e-06\n",
      "VAL: EPOCH 290/1000 | BATCH 6/8 | LOSS: 7.004321365197289e-06\n",
      "VAL: EPOCH 290/1000 | BATCH 7/8 | LOSS: 7.095255398326117e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 0/71 | LOSS: 7.055342848616419e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 1/71 | LOSS: 7.346965730903321e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 2/71 | LOSS: 7.33140359443496e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 3/71 | LOSS: 7.715730021118361e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 4/71 | LOSS: 7.212843956949655e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 5/71 | LOSS: 7.015645223873435e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 6/71 | LOSS: 6.670707859614465e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 7/71 | LOSS: 6.455630114032829e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 8/71 | LOSS: 6.634113863886644e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 9/71 | LOSS: 6.451298031606712e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 10/71 | LOSS: 6.314544035293776e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 11/71 | LOSS: 6.2112731787541025e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 12/71 | LOSS: 6.24309072568744e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 13/71 | LOSS: 6.148196949392773e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 14/71 | LOSS: 6.142926789228417e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 15/71 | LOSS: 6.198457327855067e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 16/71 | LOSS: 6.117528942815946e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 17/71 | LOSS: 6.2256183355202666e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 18/71 | LOSS: 6.127414005612464e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 19/71 | LOSS: 6.285457129706629e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 20/71 | LOSS: 6.1887295010993035e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 21/71 | LOSS: 6.449183977317509e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 22/71 | LOSS: 6.423208876936848e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 23/71 | LOSS: 6.550395653448504e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 24/71 | LOSS: 6.669358535873471e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 25/71 | LOSS: 6.7722048173051844e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 26/71 | LOSS: 6.747288959256063e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 27/71 | LOSS: 6.775380873997554e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 28/71 | LOSS: 6.7700333570898915e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 29/71 | LOSS: 6.745903723034037e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 30/71 | LOSS: 6.70222845010651e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 31/71 | LOSS: 6.699012217836753e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 32/71 | LOSS: 6.6877460636588335e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 33/71 | LOSS: 6.608261427947368e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 34/71 | LOSS: 6.5852180731391335e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 35/71 | LOSS: 6.565174304442836e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 36/71 | LOSS: 6.5289894312602856e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 37/71 | LOSS: 6.467006319086833e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 38/71 | LOSS: 6.453618192235575e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 39/71 | LOSS: 6.436999592551729e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 40/71 | LOSS: 6.403586800244251e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 41/71 | LOSS: 6.364179582652425e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 42/71 | LOSS: 6.374599727910949e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 43/71 | LOSS: 6.353062593172416e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 44/71 | LOSS: 6.336502024674297e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 45/71 | LOSS: 6.337209341559767e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 46/71 | LOSS: 6.373360394447561e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 47/71 | LOSS: 6.389329617680535e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 48/71 | LOSS: 6.364270482626292e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 49/71 | LOSS: 6.400495485650026e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 50/71 | LOSS: 6.385617682241775e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 51/71 | LOSS: 6.455203456076001e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 52/71 | LOSS: 6.46974289990777e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 53/71 | LOSS: 6.45657472954621e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 54/71 | LOSS: 6.520028240298746e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 55/71 | LOSS: 6.497978631614387e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 56/71 | LOSS: 6.5281858057954974e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 57/71 | LOSS: 6.545347857887652e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 58/71 | LOSS: 6.506750288913204e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 59/71 | LOSS: 6.495159815737376e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 60/71 | LOSS: 6.459489109371044e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 61/71 | LOSS: 6.458627357489839e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 62/71 | LOSS: 6.434451885676633e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 63/71 | LOSS: 6.4224159075365606e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 64/71 | LOSS: 6.424825201415493e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 65/71 | LOSS: 6.436208733165668e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 66/71 | LOSS: 6.442919523429574e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 67/71 | LOSS: 6.411767097180179e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 68/71 | LOSS: 6.40670245100383e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 69/71 | LOSS: 6.408516882012399e-06\n",
      "TRAIN: EPOCH 291/1000 | BATCH 70/71 | LOSS: 6.4282592088410395e-06\n",
      "VAL: EPOCH 291/1000 | BATCH 0/8 | LOSS: 6.690155260002939e-06\n",
      "VAL: EPOCH 291/1000 | BATCH 1/8 | LOSS: 6.453127525674063e-06\n",
      "VAL: EPOCH 291/1000 | BATCH 2/8 | LOSS: 6.859789057974315e-06\n",
      "VAL: EPOCH 291/1000 | BATCH 3/8 | LOSS: 6.965208626752428e-06\n",
      "VAL: EPOCH 291/1000 | BATCH 4/8 | LOSS: 6.984955234656809e-06\n",
      "VAL: EPOCH 291/1000 | BATCH 5/8 | LOSS: 6.7609488117644405e-06\n",
      "VAL: EPOCH 291/1000 | BATCH 6/8 | LOSS: 6.625901992915065e-06\n",
      "VAL: EPOCH 291/1000 | BATCH 7/8 | LOSS: 6.573483631200361e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 0/71 | LOSS: 6.05424293098622e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 1/71 | LOSS: 5.533998091777903e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 2/71 | LOSS: 5.58077363166376e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 3/71 | LOSS: 5.855811764376995e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 4/71 | LOSS: 5.751937351305969e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 5/71 | LOSS: 5.823943562669835e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 6/71 | LOSS: 5.697924864632244e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 7/71 | LOSS: 5.382464195236025e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 8/71 | LOSS: 5.333103546137055e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 9/71 | LOSS: 5.2035299859198855e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 10/71 | LOSS: 5.2784155700051505e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 11/71 | LOSS: 5.2457680226325465e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 12/71 | LOSS: 5.213195033950167e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 13/71 | LOSS: 5.448688966680493e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 14/71 | LOSS: 5.3868290554722385e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 15/71 | LOSS: 5.365342005347884e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 16/71 | LOSS: 5.373942619425018e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 17/71 | LOSS: 5.29134158316285e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 18/71 | LOSS: 5.397075291942824e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 19/71 | LOSS: 5.43555851209021e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 20/71 | LOSS: 5.520961329758782e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 21/71 | LOSS: 5.540450594956135e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 22/71 | LOSS: 5.559615631326501e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 23/71 | LOSS: 5.6136553420553055e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 24/71 | LOSS: 5.6199086884589635e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 25/71 | LOSS: 5.717833250156008e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 26/71 | LOSS: 5.7351170468151665e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 27/71 | LOSS: 5.807422789010681e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 28/71 | LOSS: 5.883651338394727e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 29/71 | LOSS: 5.894510218240612e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 30/71 | LOSS: 6.0203449559213045e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 31/71 | LOSS: 5.971685204997357e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 32/71 | LOSS: 6.053441631703401e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 33/71 | LOSS: 6.074496568811917e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 34/71 | LOSS: 6.065678365173101e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 35/71 | LOSS: 6.106629819492406e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 36/71 | LOSS: 6.090661513922355e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 37/71 | LOSS: 6.108354748328948e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 38/71 | LOSS: 6.117295009598587e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 39/71 | LOSS: 6.151120334152438e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 40/71 | LOSS: 6.165788036236247e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 41/71 | LOSS: 6.134640903794553e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 42/71 | LOSS: 6.096036766520715e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 43/71 | LOSS: 6.159241991099407e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 44/71 | LOSS: 6.169145010264603e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 45/71 | LOSS: 6.176038353892775e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 46/71 | LOSS: 6.1549856224140035e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 47/71 | LOSS: 6.178515173852854e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 48/71 | LOSS: 6.12801567958555e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 49/71 | LOSS: 6.115595774645044e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 50/71 | LOSS: 6.109495817487614e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 51/71 | LOSS: 6.068762292425862e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 52/71 | LOSS: 6.060730326663481e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 53/71 | LOSS: 6.026470840778428e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 54/71 | LOSS: 6.007402783904003e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 55/71 | LOSS: 5.98306661865016e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 56/71 | LOSS: 5.976692579535761e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 57/71 | LOSS: 5.936682429882208e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 58/71 | LOSS: 5.904414800910272e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 59/71 | LOSS: 5.904919908061856e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 60/71 | LOSS: 5.873756659549726e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 61/71 | LOSS: 5.8791736944019045e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 62/71 | LOSS: 5.8535774192095565e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 63/71 | LOSS: 5.828177620514907e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 64/71 | LOSS: 5.824470519235967e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 65/71 | LOSS: 5.830779366189028e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 66/71 | LOSS: 5.827973117407258e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 67/71 | LOSS: 5.823729780144383e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 68/71 | LOSS: 5.852250603271861e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 69/71 | LOSS: 5.850256450555338e-06\n",
      "TRAIN: EPOCH 292/1000 | BATCH 70/71 | LOSS: 5.851650142394134e-06\n",
      "VAL: EPOCH 292/1000 | BATCH 0/8 | LOSS: 6.699426194245461e-06\n",
      "VAL: EPOCH 292/1000 | BATCH 1/8 | LOSS: 5.822111461384338e-06\n",
      "VAL: EPOCH 292/1000 | BATCH 2/8 | LOSS: 5.614176340410874e-06\n",
      "VAL: EPOCH 292/1000 | BATCH 3/8 | LOSS: 5.724414791075105e-06\n",
      "VAL: EPOCH 292/1000 | BATCH 4/8 | LOSS: 5.486242753249826e-06\n",
      "VAL: EPOCH 292/1000 | BATCH 5/8 | LOSS: 5.157865264967161e-06\n",
      "VAL: EPOCH 292/1000 | BATCH 6/8 | LOSS: 5.081190985427904e-06\n",
      "VAL: EPOCH 292/1000 | BATCH 7/8 | LOSS: 4.902909864767935e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 0/71 | LOSS: 4.208913196634967e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 1/71 | LOSS: 5.128750672156457e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 2/71 | LOSS: 5.135485632005536e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 3/71 | LOSS: 4.785913176874601e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 4/71 | LOSS: 4.896415839539259e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 5/71 | LOSS: 5.04640831877623e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 6/71 | LOSS: 5.165683660379727e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 7/71 | LOSS: 5.137499300644777e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 8/71 | LOSS: 5.12238883250878e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 9/71 | LOSS: 5.296183257996745e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 10/71 | LOSS: 5.198849831603266e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 11/71 | LOSS: 5.173969933972937e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 12/71 | LOSS: 5.159003246892378e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 13/71 | LOSS: 5.138263515748674e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 14/71 | LOSS: 5.189932304953497e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 15/71 | LOSS: 5.2372472367778755e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 16/71 | LOSS: 5.337715064717272e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 17/71 | LOSS: 5.367052141183295e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 18/71 | LOSS: 5.381775652399244e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 19/71 | LOSS: 5.343396162516001e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 20/71 | LOSS: 5.315783829203047e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 21/71 | LOSS: 5.29813305340874e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 22/71 | LOSS: 5.265527711937624e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 23/71 | LOSS: 5.281098755934484e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 24/71 | LOSS: 5.26960021488776e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 25/71 | LOSS: 5.232249673799043e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 26/71 | LOSS: 5.248415357494162e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 27/71 | LOSS: 5.2249731246385115e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 28/71 | LOSS: 5.2132967743216735e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 29/71 | LOSS: 5.21464379896012e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 30/71 | LOSS: 5.2161490569438505e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 31/71 | LOSS: 5.206059093154636e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 32/71 | LOSS: 5.245781425621702e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 33/71 | LOSS: 5.204474726216337e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 34/71 | LOSS: 5.269996894899772e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 35/71 | LOSS: 5.281093567393529e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 36/71 | LOSS: 5.286236986452226e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 37/71 | LOSS: 5.3201762045175295e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 38/71 | LOSS: 5.314562556052876e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 39/71 | LOSS: 5.301986669792314e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 40/71 | LOSS: 5.303666429005339e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 41/71 | LOSS: 5.328358670773187e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 42/71 | LOSS: 5.31138326275754e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 43/71 | LOSS: 5.291579157074011e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 44/71 | LOSS: 5.285234546034998e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 45/71 | LOSS: 5.2670482140772785e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 46/71 | LOSS: 5.2741358045905335e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 47/71 | LOSS: 5.269668723902517e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 48/71 | LOSS: 5.277071267653587e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 49/71 | LOSS: 5.273498022688727e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 50/71 | LOSS: 5.255239160510873e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 51/71 | LOSS: 5.24992583013255e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 52/71 | LOSS: 5.2530495584094e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 53/71 | LOSS: 5.251318460371597e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 54/71 | LOSS: 5.231774068968e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 55/71 | LOSS: 5.208732387375546e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 56/71 | LOSS: 5.219840679210233e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 57/71 | LOSS: 5.202800738955155e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 58/71 | LOSS: 5.213820085666188e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 59/71 | LOSS: 5.223405404800967e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 60/71 | LOSS: 5.211492880782137e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 61/71 | LOSS: 5.206254836447178e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 62/71 | LOSS: 5.189838388798221e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 63/71 | LOSS: 5.199439730319e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 64/71 | LOSS: 5.1884289195396165e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 65/71 | LOSS: 5.217722738934478e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 66/71 | LOSS: 5.2282480113201776e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 67/71 | LOSS: 5.212196660576249e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 68/71 | LOSS: 5.215220363570024e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 69/71 | LOSS: 5.205279821244143e-06\n",
      "TRAIN: EPOCH 293/1000 | BATCH 70/71 | LOSS: 5.183832220442947e-06\n",
      "VAL: EPOCH 293/1000 | BATCH 0/8 | LOSS: 7.021158126008231e-06\n",
      "VAL: EPOCH 293/1000 | BATCH 1/8 | LOSS: 6.055069434296456e-06\n",
      "VAL: EPOCH 293/1000 | BATCH 2/8 | LOSS: 5.895414839566608e-06\n",
      "VAL: EPOCH 293/1000 | BATCH 3/8 | LOSS: 6.038068249836215e-06\n",
      "VAL: EPOCH 293/1000 | BATCH 4/8 | LOSS: 5.798124038847163e-06\n",
      "VAL: EPOCH 293/1000 | BATCH 5/8 | LOSS: 5.522113951883512e-06\n",
      "VAL: EPOCH 293/1000 | BATCH 6/8 | LOSS: 5.505043710789843e-06\n",
      "VAL: EPOCH 293/1000 | BATCH 7/8 | LOSS: 5.3349046424955304e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 0/71 | LOSS: 5.484984285430983e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 1/71 | LOSS: 5.7667630244395696e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 2/71 | LOSS: 5.757025822579938e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 3/71 | LOSS: 5.097723715152824e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 4/71 | LOSS: 5.5646252803853715e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 5/71 | LOSS: 5.4373593532848945e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 6/71 | LOSS: 5.469183049301916e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 7/71 | LOSS: 5.373783949380595e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 8/71 | LOSS: 5.427952752749358e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 9/71 | LOSS: 5.3303335789678385e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 10/71 | LOSS: 5.3123862099627415e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 11/71 | LOSS: 5.33127808921563e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 12/71 | LOSS: 5.301655269599555e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 13/71 | LOSS: 5.347554049097068e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 14/71 | LOSS: 5.411218383718127e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 15/71 | LOSS: 5.4900045824979316e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 16/71 | LOSS: 5.618172508548014e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 17/71 | LOSS: 5.570904502544889e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 18/71 | LOSS: 5.560648500441427e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 19/71 | LOSS: 5.558139173444943e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 20/71 | LOSS: 5.526844515337698e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 21/71 | LOSS: 5.5155093667219095e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 22/71 | LOSS: 5.4757810096856225e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 23/71 | LOSS: 5.510710423095588e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 24/71 | LOSS: 5.4583596829616e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 25/71 | LOSS: 5.448069976829664e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 26/71 | LOSS: 5.419091141650117e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 27/71 | LOSS: 5.415814858419513e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 28/71 | LOSS: 5.381319938194416e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 29/71 | LOSS: 5.3696549457527e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 30/71 | LOSS: 5.361997292282233e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 31/71 | LOSS: 5.326076276901404e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 32/71 | LOSS: 5.325436549438481e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 33/71 | LOSS: 5.320005143135752e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 34/71 | LOSS: 5.2959092110021236e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 35/71 | LOSS: 5.289693262966466e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 36/71 | LOSS: 5.268334240787914e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 37/71 | LOSS: 5.230654982082533e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 38/71 | LOSS: 5.210823112578603e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 39/71 | LOSS: 5.224867993547377e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 40/71 | LOSS: 5.25142201983793e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 41/71 | LOSS: 5.253634353288189e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 42/71 | LOSS: 5.263684924166016e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 43/71 | LOSS: 5.283668594364082e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 44/71 | LOSS: 5.325064689208779e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 45/71 | LOSS: 5.330265544809821e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 46/71 | LOSS: 5.3425514453649275e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 47/71 | LOSS: 5.3795278347479325e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 48/71 | LOSS: 5.400340871012898e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 49/71 | LOSS: 5.399342444434296e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 50/71 | LOSS: 5.426908964913238e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 51/71 | LOSS: 5.432742280110968e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 52/71 | LOSS: 5.436139520686581e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 53/71 | LOSS: 5.419640333288784e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 54/71 | LOSS: 5.417509807805552e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 55/71 | LOSS: 5.4474257597446145e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 56/71 | LOSS: 5.438039495761347e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 57/71 | LOSS: 5.450757671183212e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 58/71 | LOSS: 5.471244949765574e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 59/71 | LOSS: 5.526141179264717e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 60/71 | LOSS: 5.522915209260899e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 61/71 | LOSS: 5.531536419678765e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 62/71 | LOSS: 5.527870410924874e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 63/71 | LOSS: 5.518052844877275e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 64/71 | LOSS: 5.513782035940005e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 65/71 | LOSS: 5.491522236325457e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 66/71 | LOSS: 5.499686124752391e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 67/71 | LOSS: 5.511270534738929e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 68/71 | LOSS: 5.5085531881638374e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 69/71 | LOSS: 5.506284423972829e-06\n",
      "TRAIN: EPOCH 294/1000 | BATCH 70/71 | LOSS: 5.47627793283437e-06\n",
      "VAL: EPOCH 294/1000 | BATCH 0/8 | LOSS: 7.991106940608006e-06\n",
      "VAL: EPOCH 294/1000 | BATCH 1/8 | LOSS: 7.244652351801051e-06\n",
      "VAL: EPOCH 294/1000 | BATCH 2/8 | LOSS: 6.884069686445097e-06\n",
      "VAL: EPOCH 294/1000 | BATCH 3/8 | LOSS: 7.085161541908747e-06\n",
      "VAL: EPOCH 294/1000 | BATCH 4/8 | LOSS: 6.8630086389021015e-06\n",
      "VAL: EPOCH 294/1000 | BATCH 5/8 | LOSS: 6.457798917836044e-06\n",
      "VAL: EPOCH 294/1000 | BATCH 6/8 | LOSS: 6.439221319202001e-06\n",
      "VAL: EPOCH 294/1000 | BATCH 7/8 | LOSS: 6.2360280139728275e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 0/71 | LOSS: 4.2713609218481e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 1/71 | LOSS: 4.170783086010488e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 2/71 | LOSS: 4.837150451445875e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 3/71 | LOSS: 5.509143761628366e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 4/71 | LOSS: 5.377960405894555e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 5/71 | LOSS: 5.4681155840323e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 6/71 | LOSS: 5.692820780365894e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 7/71 | LOSS: 5.515063833172462e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 8/71 | LOSS: 5.656544544763165e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 9/71 | LOSS: 5.548781973629957e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 10/71 | LOSS: 5.57917552545074e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 11/71 | LOSS: 5.590737752451484e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 12/71 | LOSS: 5.555123817220402e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 13/71 | LOSS: 5.666776100302481e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 14/71 | LOSS: 5.657484204372547e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 15/71 | LOSS: 5.800533074307168e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 16/71 | LOSS: 5.820084318068385e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 17/71 | LOSS: 5.79513079578141e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 18/71 | LOSS: 5.841445798930516e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 19/71 | LOSS: 5.892967919862713e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 20/71 | LOSS: 5.898409610727824e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 21/71 | LOSS: 5.864872160931224e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 22/71 | LOSS: 5.861920687058718e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 23/71 | LOSS: 6.030941032501384e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 24/71 | LOSS: 5.939896473137196e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 25/71 | LOSS: 5.963234693458967e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 26/71 | LOSS: 5.9719684402268655e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 27/71 | LOSS: 5.971603351489259e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 28/71 | LOSS: 5.9945611830134395e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 29/71 | LOSS: 5.93687582295388e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 30/71 | LOSS: 5.954533662588801e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 31/71 | LOSS: 6.034258717591001e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 32/71 | LOSS: 6.015197372603738e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 33/71 | LOSS: 6.1102119776906504e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 34/71 | LOSS: 6.105265635726807e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 35/71 | LOSS: 6.104649615837035e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 36/71 | LOSS: 6.099750579950695e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 37/71 | LOSS: 6.073645073755822e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 38/71 | LOSS: 6.12393112019979e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 39/71 | LOSS: 6.0483134461719604e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 40/71 | LOSS: 6.064331955712987e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 41/71 | LOSS: 6.065142131427863e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 42/71 | LOSS: 6.046811026757242e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 43/71 | LOSS: 6.0457162424592585e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 44/71 | LOSS: 5.994352320865599e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 45/71 | LOSS: 6.011113337871907e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 46/71 | LOSS: 6.055601812825425e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 47/71 | LOSS: 6.045218592968619e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 48/71 | LOSS: 6.073146555629174e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 49/71 | LOSS: 6.104384037826094e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 50/71 | LOSS: 6.110845093002461e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 51/71 | LOSS: 6.08623118016392e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 52/71 | LOSS: 6.109626078882083e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 53/71 | LOSS: 6.0866851206945415e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 54/71 | LOSS: 6.066815554235786e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 55/71 | LOSS: 6.0917859221782626e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 56/71 | LOSS: 6.074854761544357e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 57/71 | LOSS: 6.060952688946192e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 58/71 | LOSS: 6.034914850576986e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 59/71 | LOSS: 6.029814403518685e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 60/71 | LOSS: 5.9940153792534845e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 61/71 | LOSS: 5.966781930422463e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 62/71 | LOSS: 5.9542173894153654e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 63/71 | LOSS: 5.936134726880482e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 64/71 | LOSS: 5.920312273351906e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 65/71 | LOSS: 5.880054889971551e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 66/71 | LOSS: 5.845934711304388e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 67/71 | LOSS: 5.826240256363449e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 68/71 | LOSS: 5.819961198421484e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 69/71 | LOSS: 5.818423684494129e-06\n",
      "TRAIN: EPOCH 295/1000 | BATCH 70/71 | LOSS: 5.8003064924365316e-06\n",
      "VAL: EPOCH 295/1000 | BATCH 0/8 | LOSS: 6.601148470508633e-06\n",
      "VAL: EPOCH 295/1000 | BATCH 1/8 | LOSS: 5.912636652283254e-06\n",
      "VAL: EPOCH 295/1000 | BATCH 2/8 | LOSS: 5.893150349341643e-06\n",
      "VAL: EPOCH 295/1000 | BATCH 3/8 | LOSS: 5.961195370218775e-06\n",
      "VAL: EPOCH 295/1000 | BATCH 4/8 | LOSS: 5.8037011513079054e-06\n",
      "VAL: EPOCH 295/1000 | BATCH 5/8 | LOSS: 5.541966705398711e-06\n",
      "VAL: EPOCH 295/1000 | BATCH 6/8 | LOSS: 5.559472811000887e-06\n",
      "VAL: EPOCH 295/1000 | BATCH 7/8 | LOSS: 5.530466125946987e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 0/71 | LOSS: 5.4607489801128395e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 1/71 | LOSS: 4.9428592774347635e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 2/71 | LOSS: 4.836279761851377e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 3/71 | LOSS: 5.888649525331857e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 4/71 | LOSS: 5.79611132707214e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 5/71 | LOSS: 5.598918581502706e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 6/71 | LOSS: 5.5436824238443346e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 7/71 | LOSS: 5.495406753652787e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 8/71 | LOSS: 5.490869120371321e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 9/71 | LOSS: 5.507830792339519e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 10/71 | LOSS: 5.514888719269286e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 11/71 | LOSS: 5.459708669756462e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 12/71 | LOSS: 5.459366862991234e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 13/71 | LOSS: 5.435689770040751e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 14/71 | LOSS: 5.456958903475121e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 15/71 | LOSS: 5.457503846173495e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 16/71 | LOSS: 5.379030961408075e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 17/71 | LOSS: 5.4226032817597215e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 18/71 | LOSS: 5.351178635081803e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 19/71 | LOSS: 5.323083701114228e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 20/71 | LOSS: 5.2973453724310575e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 21/71 | LOSS: 5.300505604695487e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 22/71 | LOSS: 5.287978301962341e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 23/71 | LOSS: 5.2420554273642965e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 24/71 | LOSS: 5.215928304096451e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 25/71 | LOSS: 5.211436893240227e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 26/71 | LOSS: 5.1905141373464615e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 27/71 | LOSS: 5.1444161174783535e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 28/71 | LOSS: 5.112168780434676e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 29/71 | LOSS: 5.137866128279711e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 30/71 | LOSS: 5.156028222582761e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 31/71 | LOSS: 5.216024092646876e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 32/71 | LOSS: 5.243397398854458e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 33/71 | LOSS: 5.248260242082966e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 34/71 | LOSS: 5.2645428435685715e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 35/71 | LOSS: 5.249135104653154e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 36/71 | LOSS: 5.339544062538866e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 37/71 | LOSS: 5.334693708535174e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 38/71 | LOSS: 5.431216614408932e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 39/71 | LOSS: 5.48428126876388e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 40/71 | LOSS: 5.5585317723609214e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 41/71 | LOSS: 5.574973480785654e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 42/71 | LOSS: 5.6272763178513256e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 43/71 | LOSS: 5.664904185984844e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 44/71 | LOSS: 5.698571041850503e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 45/71 | LOSS: 5.769377693191611e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 46/71 | LOSS: 5.746755834208786e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 47/71 | LOSS: 5.774177784208708e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 48/71 | LOSS: 5.806674621731984e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 49/71 | LOSS: 5.796213445137255e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 50/71 | LOSS: 5.835817426165758e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 51/71 | LOSS: 5.804164857181604e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 52/71 | LOSS: 5.802486552022187e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 53/71 | LOSS: 5.791378717495482e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 54/71 | LOSS: 5.789709757664241e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 55/71 | LOSS: 5.834173407492926e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 56/71 | LOSS: 5.843173503786889e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 57/71 | LOSS: 5.83970900779291e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 58/71 | LOSS: 5.9015457844583645e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 59/71 | LOSS: 5.907625328897362e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 60/71 | LOSS: 5.9214722181214245e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 61/71 | LOSS: 5.95017262082453e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 62/71 | LOSS: 5.955524383829297e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 63/71 | LOSS: 5.967706513843041e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 64/71 | LOSS: 5.960408256774267e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 65/71 | LOSS: 5.966614053525546e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 66/71 | LOSS: 5.952828210702951e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 67/71 | LOSS: 5.9638659074741604e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 68/71 | LOSS: 5.956779639983205e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 69/71 | LOSS: 5.9865126526606865e-06\n",
      "TRAIN: EPOCH 296/1000 | BATCH 70/71 | LOSS: 5.950349192754005e-06\n",
      "VAL: EPOCH 296/1000 | BATCH 0/8 | LOSS: 9.798645805858541e-06\n",
      "VAL: EPOCH 296/1000 | BATCH 1/8 | LOSS: 9.62773219725932e-06\n",
      "VAL: EPOCH 296/1000 | BATCH 2/8 | LOSS: 9.031825356942136e-06\n",
      "VAL: EPOCH 296/1000 | BATCH 3/8 | LOSS: 9.297794122176128e-06\n",
      "VAL: EPOCH 296/1000 | BATCH 4/8 | LOSS: 9.013664566737134e-06\n",
      "VAL: EPOCH 296/1000 | BATCH 5/8 | LOSS: 8.467367176005306e-06\n",
      "VAL: EPOCH 296/1000 | BATCH 6/8 | LOSS: 8.296501229259385e-06\n",
      "VAL: EPOCH 296/1000 | BATCH 7/8 | LOSS: 7.95450108626028e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 0/71 | LOSS: 8.251801773440093e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 1/71 | LOSS: 6.9450618411792675e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 2/71 | LOSS: 6.459213636844652e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 3/71 | LOSS: 6.580583431059495e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 4/71 | LOSS: 6.697816661471734e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 5/71 | LOSS: 6.716651038611114e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 6/71 | LOSS: 6.790364750486333e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 7/71 | LOSS: 6.820881822022784e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 8/71 | LOSS: 7.058451552034563e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 9/71 | LOSS: 6.7388035404292165e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 10/71 | LOSS: 6.698524636951995e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 11/71 | LOSS: 6.846396824281935e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 12/71 | LOSS: 6.784263125434965e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 13/71 | LOSS: 6.886594064781093e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 14/71 | LOSS: 6.818102701799944e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 15/71 | LOSS: 6.9065670231793774e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 16/71 | LOSS: 6.961891973760965e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 17/71 | LOSS: 6.856293844571661e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 18/71 | LOSS: 6.8486153730574235e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 19/71 | LOSS: 6.78251565204846e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 20/71 | LOSS: 6.828587196477678e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 21/71 | LOSS: 6.656582896043388e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 22/71 | LOSS: 6.612661605765385e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 23/71 | LOSS: 6.548934057567142e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 24/71 | LOSS: 6.465767382906051e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 25/71 | LOSS: 6.416342417791244e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 26/71 | LOSS: 6.362449699668947e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 27/71 | LOSS: 6.3098083111461686e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 28/71 | LOSS: 6.258142704986124e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 29/71 | LOSS: 6.2065289512247546e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 30/71 | LOSS: 6.1568933878950175e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 31/71 | LOSS: 6.08907400589942e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 32/71 | LOSS: 6.059518165619176e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 33/71 | LOSS: 6.055812451343038e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 34/71 | LOSS: 6.044508102474667e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 35/71 | LOSS: 5.986414433007465e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 36/71 | LOSS: 6.007598942134693e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 37/71 | LOSS: 5.9817288466116515e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 38/71 | LOSS: 5.954396830276938e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 39/71 | LOSS: 5.971258462977857e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 40/71 | LOSS: 5.9499952174785614e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 41/71 | LOSS: 5.911403669805233e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 42/71 | LOSS: 5.883564712306441e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 43/71 | LOSS: 5.862193843726842e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 44/71 | LOSS: 5.8727674210482896e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 45/71 | LOSS: 5.862467688806962e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 46/71 | LOSS: 5.9043682376021875e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 47/71 | LOSS: 5.898665364156841e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 48/71 | LOSS: 5.840692767601374e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 49/71 | LOSS: 5.831452072015964e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 50/71 | LOSS: 5.800406588783588e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 51/71 | LOSS: 5.818569993957331e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 52/71 | LOSS: 5.800586874994344e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 53/71 | LOSS: 5.79463339481326e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 54/71 | LOSS: 5.773991696597394e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 55/71 | LOSS: 5.760459665842583e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 56/71 | LOSS: 5.7656556986841135e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 57/71 | LOSS: 5.754221696218432e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 58/71 | LOSS: 5.752965843369612e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 59/71 | LOSS: 5.751801040787541e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 60/71 | LOSS: 5.7413522344937575e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 61/71 | LOSS: 5.737016173894048e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 62/71 | LOSS: 5.710428460646191e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 63/71 | LOSS: 5.691886144632008e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 64/71 | LOSS: 5.689368180565697e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 65/71 | LOSS: 5.693150101113574e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 66/71 | LOSS: 5.663534181228137e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 67/71 | LOSS: 5.6638281795826246e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 68/71 | LOSS: 5.665906613011021e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 69/71 | LOSS: 5.657338130861588e-06\n",
      "TRAIN: EPOCH 297/1000 | BATCH 70/71 | LOSS: 5.651584525738554e-06\n",
      "VAL: EPOCH 297/1000 | BATCH 0/8 | LOSS: 6.992914222792024e-06\n",
      "VAL: EPOCH 297/1000 | BATCH 1/8 | LOSS: 6.883411288072239e-06\n",
      "VAL: EPOCH 297/1000 | BATCH 2/8 | LOSS: 6.939721515664132e-06\n",
      "VAL: EPOCH 297/1000 | BATCH 3/8 | LOSS: 7.333355483751802e-06\n",
      "VAL: EPOCH 297/1000 | BATCH 4/8 | LOSS: 7.168115098465933e-06\n",
      "VAL: EPOCH 297/1000 | BATCH 5/8 | LOSS: 6.972387306329135e-06\n",
      "VAL: EPOCH 297/1000 | BATCH 6/8 | LOSS: 6.863851012894884e-06\n",
      "VAL: EPOCH 297/1000 | BATCH 7/8 | LOSS: 6.711299192829756e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 0/71 | LOSS: 5.829121619171929e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 1/71 | LOSS: 5.683493782271398e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 2/71 | LOSS: 5.362848241929896e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 3/71 | LOSS: 5.670914333677501e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 4/71 | LOSS: 5.462940589495702e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 5/71 | LOSS: 5.498462314790231e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 6/71 | LOSS: 5.390891406672641e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 7/71 | LOSS: 5.259184320038912e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 8/71 | LOSS: 5.1497841013770085e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 9/71 | LOSS: 5.1158888709323945e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 10/71 | LOSS: 5.162046983803686e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 11/71 | LOSS: 5.165845133584905e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 12/71 | LOSS: 5.044381291204795e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 13/71 | LOSS: 5.070891120340093e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 14/71 | LOSS: 5.051999930098343e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 15/71 | LOSS: 5.0125295274483506e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 16/71 | LOSS: 5.0158714924798595e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 17/71 | LOSS: 5.011356734030414e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 18/71 | LOSS: 5.0659836185667175e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 19/71 | LOSS: 5.073646434539114e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 20/71 | LOSS: 5.070098788045081e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 21/71 | LOSS: 5.044582973244939e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 22/71 | LOSS: 5.053871459297269e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 23/71 | LOSS: 5.0207127060275525e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 24/71 | LOSS: 5.049996871093754e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 25/71 | LOSS: 5.030851228254435e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 26/71 | LOSS: 4.994926628030414e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 27/71 | LOSS: 4.995891020241418e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 28/71 | LOSS: 5.01949114487647e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 29/71 | LOSS: 5.04089781922327e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 30/71 | LOSS: 5.063803802195546e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 31/71 | LOSS: 5.036521557144624e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 32/71 | LOSS: 5.017365200535895e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 33/71 | LOSS: 5.009482867321554e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 34/71 | LOSS: 5.024093206884573e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 35/71 | LOSS: 5.036847306251326e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 36/71 | LOSS: 5.0290909589298155e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 37/71 | LOSS: 5.049242405986756e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 38/71 | LOSS: 5.109823690966154e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 39/71 | LOSS: 5.117812156640866e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 40/71 | LOSS: 5.10351680820416e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 41/71 | LOSS: 5.1759031651142455e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 42/71 | LOSS: 5.2322698561616814e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 43/71 | LOSS: 5.2403012825214485e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 44/71 | LOSS: 5.294440325087635e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 45/71 | LOSS: 5.320900458094042e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 46/71 | LOSS: 5.310979130181875e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 47/71 | LOSS: 5.355408423686943e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 48/71 | LOSS: 5.3717564455559e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 49/71 | LOSS: 5.4251829715212805e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 50/71 | LOSS: 5.4226543486983445e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 51/71 | LOSS: 5.476089199873968e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 52/71 | LOSS: 5.50383860824678e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 53/71 | LOSS: 5.505885677916079e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 54/71 | LOSS: 5.529442230247448e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 55/71 | LOSS: 5.5569275981984645e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 56/71 | LOSS: 5.573847558256953e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 57/71 | LOSS: 5.587045005337006e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 58/71 | LOSS: 5.612726623840607e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 59/71 | LOSS: 5.604382401240097e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 60/71 | LOSS: 5.591953326909818e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 61/71 | LOSS: 5.608585561569799e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 62/71 | LOSS: 5.586096817534694e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 63/71 | LOSS: 5.564923249323783e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 64/71 | LOSS: 5.568357461855228e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 65/71 | LOSS: 5.586109506166946e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 66/71 | LOSS: 5.5637881868916785e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 67/71 | LOSS: 5.585937254395771e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 68/71 | LOSS: 5.5883704563725125e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 69/71 | LOSS: 5.569650959712038e-06\n",
      "TRAIN: EPOCH 298/1000 | BATCH 70/71 | LOSS: 5.578400811225973e-06\n",
      "VAL: EPOCH 298/1000 | BATCH 0/8 | LOSS: 6.454640242736787e-06\n",
      "VAL: EPOCH 298/1000 | BATCH 1/8 | LOSS: 6.106206910772016e-06\n",
      "VAL: EPOCH 298/1000 | BATCH 2/8 | LOSS: 6.008237202574189e-06\n",
      "VAL: EPOCH 298/1000 | BATCH 3/8 | LOSS: 6.249039188332972e-06\n",
      "VAL: EPOCH 298/1000 | BATCH 4/8 | LOSS: 6.138767184893368e-06\n",
      "VAL: EPOCH 298/1000 | BATCH 5/8 | LOSS: 5.950272149372419e-06\n",
      "VAL: EPOCH 298/1000 | BATCH 6/8 | LOSS: 5.883227134160864e-06\n",
      "VAL: EPOCH 298/1000 | BATCH 7/8 | LOSS: 5.764623040249717e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 0/71 | LOSS: 6.655126071564155e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 1/71 | LOSS: 5.681867833118304e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 2/71 | LOSS: 4.914692453894531e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 3/71 | LOSS: 5.227950509834045e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 4/71 | LOSS: 5.485707879415713e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 5/71 | LOSS: 5.326778894717184e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 6/71 | LOSS: 5.2927042426225464e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 7/71 | LOSS: 5.248785612366191e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 8/71 | LOSS: 5.099426491344477e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 9/71 | LOSS: 5.109879157316754e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 10/71 | LOSS: 5.212723367135781e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 11/71 | LOSS: 5.196880654996979e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 12/71 | LOSS: 5.2901587318606635e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 13/71 | LOSS: 5.28145785340582e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 14/71 | LOSS: 5.26033921535903e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 15/71 | LOSS: 5.189248554415826e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 16/71 | LOSS: 5.191216828012223e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 17/71 | LOSS: 5.1570406817012954e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 18/71 | LOSS: 5.0850064900786445e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 19/71 | LOSS: 5.007154186387197e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 20/71 | LOSS: 4.970624537409527e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 21/71 | LOSS: 4.96033046395248e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 22/71 | LOSS: 4.932754069633792e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 23/71 | LOSS: 4.969117583186744e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 24/71 | LOSS: 5.005641396564897e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 25/71 | LOSS: 4.986277627512419e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 26/71 | LOSS: 4.942730828150633e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 27/71 | LOSS: 5.021748669215802e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 28/71 | LOSS: 5.0394156648414125e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 29/71 | LOSS: 5.011096800444648e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 30/71 | LOSS: 5.040695698206095e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 31/71 | LOSS: 5.0421755872775975e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 32/71 | LOSS: 5.011269532780826e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 33/71 | LOSS: 4.9853732728960865e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 34/71 | LOSS: 4.9718943955667784e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 35/71 | LOSS: 4.946975107284541e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 36/71 | LOSS: 4.917476589243223e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 37/71 | LOSS: 4.885157798190975e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 38/71 | LOSS: 4.864587224755012e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 39/71 | LOSS: 4.84526950685904e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 40/71 | LOSS: 4.8750968478377285e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 41/71 | LOSS: 4.8468873338996284e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 42/71 | LOSS: 4.847821343033967e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 43/71 | LOSS: 4.87261588556397e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 44/71 | LOSS: 4.87421233527938e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 45/71 | LOSS: 4.922977817169221e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 46/71 | LOSS: 4.918614906196029e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 47/71 | LOSS: 4.952708323457955e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 48/71 | LOSS: 4.95882947540608e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 49/71 | LOSS: 4.968726202605467e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 50/71 | LOSS: 4.971314923500804e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 51/71 | LOSS: 4.957101225500082e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 52/71 | LOSS: 4.965216519094439e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 53/71 | LOSS: 4.997090648565198e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 54/71 | LOSS: 5.020290398699051e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 55/71 | LOSS: 5.0349419171068026e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 56/71 | LOSS: 5.05392059542493e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 57/71 | LOSS: 5.0651891883560515e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 58/71 | LOSS: 5.0652223092321825e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 59/71 | LOSS: 5.084006954803044e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 60/71 | LOSS: 5.101396191392149e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 61/71 | LOSS: 5.095011708304341e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 62/71 | LOSS: 5.110042875024928e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 63/71 | LOSS: 5.0997923857210026e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 64/71 | LOSS: 5.117631011531697e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 65/71 | LOSS: 5.112139891742007e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 66/71 | LOSS: 5.138547107686572e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 67/71 | LOSS: 5.1521905407646365e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 68/71 | LOSS: 5.1575265539819766e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 69/71 | LOSS: 5.176596612597808e-06\n",
      "TRAIN: EPOCH 299/1000 | BATCH 70/71 | LOSS: 5.168181292953103e-06\n",
      "VAL: EPOCH 299/1000 | BATCH 0/8 | LOSS: 6.064095032343175e-06\n",
      "VAL: EPOCH 299/1000 | BATCH 1/8 | LOSS: 5.441511802928289e-06\n",
      "VAL: EPOCH 299/1000 | BATCH 2/8 | LOSS: 5.318525230298595e-06\n",
      "VAL: EPOCH 299/1000 | BATCH 3/8 | LOSS: 5.37445680492965e-06\n",
      "VAL: EPOCH 299/1000 | BATCH 4/8 | LOSS: 5.253949257166823e-06\n",
      "VAL: EPOCH 299/1000 | BATCH 5/8 | LOSS: 4.981340301431676e-06\n",
      "VAL: EPOCH 299/1000 | BATCH 6/8 | LOSS: 4.936936095743606e-06\n",
      "VAL: EPOCH 299/1000 | BATCH 7/8 | LOSS: 4.861930960942118e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 0/71 | LOSS: 5.549999968934571e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 1/71 | LOSS: 5.6667806802579435e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 2/71 | LOSS: 6.107210841340323e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 3/71 | LOSS: 5.8632795116864145e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 4/71 | LOSS: 5.693536331818905e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 5/71 | LOSS: 5.756390085783399e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 6/71 | LOSS: 5.962667143778942e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 7/71 | LOSS: 5.9187082683820336e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 8/71 | LOSS: 5.767055528041156e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 9/71 | LOSS: 5.6459226016158935e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 10/71 | LOSS: 5.5428264534963425e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 11/71 | LOSS: 5.577764341069269e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 12/71 | LOSS: 5.633597188884428e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 13/71 | LOSS: 5.5466347410921505e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 14/71 | LOSS: 5.500160265607216e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 15/71 | LOSS: 5.648885320397312e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 16/71 | LOSS: 5.643642827421632e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 17/71 | LOSS: 5.58838713813101e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 18/71 | LOSS: 5.564083973072947e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 19/71 | LOSS: 5.548577405534161e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 20/71 | LOSS: 5.46501780176879e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 21/71 | LOSS: 5.492778655025177e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 22/71 | LOSS: 5.454163131261062e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 23/71 | LOSS: 5.380572588364885e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 24/71 | LOSS: 5.366328841773793e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 25/71 | LOSS: 5.296449199494628e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 26/71 | LOSS: 5.328659493965528e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 27/71 | LOSS: 5.365896478386796e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 28/71 | LOSS: 5.314330306207231e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 29/71 | LOSS: 5.310228425514652e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 30/71 | LOSS: 5.29463320544305e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 31/71 | LOSS: 5.2930540732631925e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 32/71 | LOSS: 5.283609728697532e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 33/71 | LOSS: 5.320036039205768e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 34/71 | LOSS: 5.335027305721139e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 35/71 | LOSS: 5.333916545977344e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 36/71 | LOSS: 5.312204590351226e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 37/71 | LOSS: 5.330023106197539e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 38/71 | LOSS: 5.3430927227264665e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 39/71 | LOSS: 5.308782249358046e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 40/71 | LOSS: 5.294841696190926e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 41/71 | LOSS: 5.275345854118184e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 42/71 | LOSS: 5.267234618533917e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 43/71 | LOSS: 5.258644963842843e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 44/71 | LOSS: 5.23562390905378e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 45/71 | LOSS: 5.213136042268319e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 46/71 | LOSS: 5.209592714852485e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 47/71 | LOSS: 5.2167128880379705e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 48/71 | LOSS: 5.194985737480052e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 49/71 | LOSS: 5.189068160689203e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 50/71 | LOSS: 5.186844806731267e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 51/71 | LOSS: 5.218560702884973e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 52/71 | LOSS: 5.220049353978676e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 53/71 | LOSS: 5.210146742616349e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 54/71 | LOSS: 5.196327806929317e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 55/71 | LOSS: 5.180279012100593e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 56/71 | LOSS: 5.198005781717815e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 57/71 | LOSS: 5.2095610875579135e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 58/71 | LOSS: 5.210704661664549e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 59/71 | LOSS: 5.194673667574534e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 60/71 | LOSS: 5.190403096437011e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 61/71 | LOSS: 5.176582157394977e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 62/71 | LOSS: 5.175550021314328e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 63/71 | LOSS: 5.163152394516146e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 64/71 | LOSS: 5.169309910203223e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 65/71 | LOSS: 5.1496835333113484e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 66/71 | LOSS: 5.130429674035943e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 67/71 | LOSS: 5.131283313965148e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 68/71 | LOSS: 5.1150734469811665e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 69/71 | LOSS: 5.115162723866108e-06\n",
      "TRAIN: EPOCH 300/1000 | BATCH 70/71 | LOSS: 5.1169489215937294e-06\n",
      "VAL: EPOCH 300/1000 | BATCH 0/8 | LOSS: 5.467134087666636e-06\n",
      "VAL: EPOCH 300/1000 | BATCH 1/8 | LOSS: 4.986011163055082e-06\n",
      "VAL: EPOCH 300/1000 | BATCH 2/8 | LOSS: 5.018418050894979e-06\n",
      "VAL: EPOCH 300/1000 | BATCH 3/8 | LOSS: 5.160051955499512e-06\n",
      "VAL: EPOCH 300/1000 | BATCH 4/8 | LOSS: 5.0549640945973804e-06\n",
      "VAL: EPOCH 300/1000 | BATCH 5/8 | LOSS: 4.824049483431736e-06\n",
      "VAL: EPOCH 300/1000 | BATCH 6/8 | LOSS: 4.834884878489122e-06\n",
      "VAL: EPOCH 300/1000 | BATCH 7/8 | LOSS: 4.715776924513193e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 0/71 | LOSS: 5.219480954110622e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 1/71 | LOSS: 4.85666214444791e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 2/71 | LOSS: 5.027268495420382e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 3/71 | LOSS: 5.135492870067537e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 4/71 | LOSS: 4.944211741531035e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 5/71 | LOSS: 4.900950292115643e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 6/71 | LOSS: 4.789736067323247e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 7/71 | LOSS: 4.719654441487364e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 8/71 | LOSS: 4.886156956571439e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 9/71 | LOSS: 4.960942305842764e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 10/71 | LOSS: 4.852463420816506e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 11/71 | LOSS: 5.0146629367494216e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 12/71 | LOSS: 5.002038226218876e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 13/71 | LOSS: 5.086059218228911e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 14/71 | LOSS: 5.113841386143273e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 15/71 | LOSS: 5.208891082020273e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 16/71 | LOSS: 5.257272304708535e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 17/71 | LOSS: 5.300934945909022e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 18/71 | LOSS: 5.309664964826018e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 19/71 | LOSS: 5.3072866194270315e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 20/71 | LOSS: 5.3841449049027034e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 21/71 | LOSS: 5.368768452998748e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 22/71 | LOSS: 5.334528198189851e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 23/71 | LOSS: 5.295665933620815e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 24/71 | LOSS: 5.362619867810281e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 25/71 | LOSS: 5.375487787135241e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 26/71 | LOSS: 5.430613748979306e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 27/71 | LOSS: 5.440355656511591e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 28/71 | LOSS: 5.40930058973729e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 29/71 | LOSS: 5.451766310216044e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 30/71 | LOSS: 5.434002163852828e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 31/71 | LOSS: 5.413642981011435e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 32/71 | LOSS: 5.375330822447943e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 33/71 | LOSS: 5.35061951054291e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 34/71 | LOSS: 5.332997807272477e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 35/71 | LOSS: 5.349146034758532e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 36/71 | LOSS: 5.336253621322071e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 37/71 | LOSS: 5.322095220387717e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 38/71 | LOSS: 5.310327138338322e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 39/71 | LOSS: 5.328097984147462e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 40/71 | LOSS: 5.309318788842974e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 41/71 | LOSS: 5.263842797917494e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 42/71 | LOSS: 5.2414204568522956e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 43/71 | LOSS: 5.229248177908051e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 44/71 | LOSS: 5.234685194308339e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 45/71 | LOSS: 5.228552911672602e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 46/71 | LOSS: 5.208625384995607e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 47/71 | LOSS: 5.169690789822805e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 48/71 | LOSS: 5.1526881086792115e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 49/71 | LOSS: 5.152174244358321e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 50/71 | LOSS: 5.135346399066632e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 51/71 | LOSS: 5.112797820262271e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 52/71 | LOSS: 5.111047340626169e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 53/71 | LOSS: 5.113139296655491e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 54/71 | LOSS: 5.07930361147208e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 55/71 | LOSS: 5.0604305751481405e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 56/71 | LOSS: 5.0432714216871665e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 57/71 | LOSS: 5.042722571896116e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 58/71 | LOSS: 5.025928457745082e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 59/71 | LOSS: 5.013753207094851e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 60/71 | LOSS: 5.009033944201083e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 61/71 | LOSS: 5.018411552408642e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 62/71 | LOSS: 4.993482117055147e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 63/71 | LOSS: 4.992139622572722e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 64/71 | LOSS: 5.00133286410486e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 65/71 | LOSS: 5.006845640309621e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 66/71 | LOSS: 5.023748341425663e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 67/71 | LOSS: 5.025099747346009e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 68/71 | LOSS: 5.026799215925891e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 69/71 | LOSS: 5.024989955667739e-06\n",
      "TRAIN: EPOCH 301/1000 | BATCH 70/71 | LOSS: 5.039187777891155e-06\n",
      "VAL: EPOCH 301/1000 | BATCH 0/8 | LOSS: 6.255995685933158e-06\n",
      "VAL: EPOCH 301/1000 | BATCH 1/8 | LOSS: 5.494749075296568e-06\n",
      "VAL: EPOCH 301/1000 | BATCH 2/8 | LOSS: 5.331736701918999e-06\n",
      "VAL: EPOCH 301/1000 | BATCH 3/8 | LOSS: 5.399295673669258e-06\n",
      "VAL: EPOCH 301/1000 | BATCH 4/8 | LOSS: 5.2567028433259114e-06\n",
      "VAL: EPOCH 301/1000 | BATCH 5/8 | LOSS: 4.998175957856195e-06\n",
      "VAL: EPOCH 301/1000 | BATCH 6/8 | LOSS: 5.00344913234585e-06\n",
      "VAL: EPOCH 301/1000 | BATCH 7/8 | LOSS: 4.899074781405943e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 0/71 | LOSS: 4.139535121794324e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 1/71 | LOSS: 5.288452484819572e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 2/71 | LOSS: 4.976972377335187e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 3/71 | LOSS: 5.145818022356252e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 4/71 | LOSS: 4.993603397451807e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 5/71 | LOSS: 5.097635645749203e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 6/71 | LOSS: 4.907777565676952e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 7/71 | LOSS: 4.7989715312724e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 8/71 | LOSS: 4.7004563788909254e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 9/71 | LOSS: 4.648003232432529e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 10/71 | LOSS: 4.602859751850536e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 11/71 | LOSS: 4.590249470008227e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 12/71 | LOSS: 4.544351003670062e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 13/71 | LOSS: 4.612552142394374e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 14/71 | LOSS: 4.697583851035839e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 15/71 | LOSS: 4.693861569649016e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 16/71 | LOSS: 4.686016649536802e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 17/71 | LOSS: 4.775260170693703e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 18/71 | LOSS: 4.791084465020838e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 19/71 | LOSS: 4.76536292808305e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 20/71 | LOSS: 4.77187209721056e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 21/71 | LOSS: 4.784194102698662e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 22/71 | LOSS: 4.830886906426673e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 23/71 | LOSS: 4.850645893839101e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 24/71 | LOSS: 4.804550335393287e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 25/71 | LOSS: 4.8143022538100085e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 26/71 | LOSS: 4.814833253611276e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 27/71 | LOSS: 4.840789090719357e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 28/71 | LOSS: 4.828502541981991e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 29/71 | LOSS: 4.820874846700463e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 30/71 | LOSS: 4.814344089437885e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 31/71 | LOSS: 4.802587639574085e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 32/71 | LOSS: 4.773128197980854e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 33/71 | LOSS: 4.774511594137841e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 34/71 | LOSS: 4.761118686604147e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 35/71 | LOSS: 4.750732045977202e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 36/71 | LOSS: 4.739558507198298e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 37/71 | LOSS: 4.740731236252989e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 38/71 | LOSS: 4.75747514755289e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 39/71 | LOSS: 4.779332596172026e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 40/71 | LOSS: 4.778395267327674e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 41/71 | LOSS: 4.779126865763655e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 42/71 | LOSS: 4.778297331308009e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 43/71 | LOSS: 4.775879533140555e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 44/71 | LOSS: 4.764832566353208e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 45/71 | LOSS: 4.821425314414386e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 46/71 | LOSS: 4.80766780032006e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 47/71 | LOSS: 4.842843755644329e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 48/71 | LOSS: 4.847545518046269e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 49/71 | LOSS: 4.889296997134807e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 50/71 | LOSS: 4.873847376752654e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 51/71 | LOSS: 4.8711292055696294e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 52/71 | LOSS: 4.892890038323492e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 53/71 | LOSS: 4.900523149613205e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 54/71 | LOSS: 4.9354456264567984e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 55/71 | LOSS: 4.921026710949913e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 56/71 | LOSS: 4.913180498733071e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 57/71 | LOSS: 4.942838006235402e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 58/71 | LOSS: 4.9584243581995325e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 59/71 | LOSS: 5.061609014470984e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 60/71 | LOSS: 5.067174809438092e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 61/71 | LOSS: 5.120016765196793e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 62/71 | LOSS: 5.1011600315553274e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 63/71 | LOSS: 5.111245322098057e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 64/71 | LOSS: 5.10961532339794e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 65/71 | LOSS: 5.13963127421889e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 66/71 | LOSS: 5.12674970236527e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 67/71 | LOSS: 5.1130768417137646e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 68/71 | LOSS: 5.121275581336374e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 69/71 | LOSS: 5.1151026062663214e-06\n",
      "TRAIN: EPOCH 302/1000 | BATCH 70/71 | LOSS: 5.096302911496564e-06\n",
      "VAL: EPOCH 302/1000 | BATCH 0/8 | LOSS: 1.0156465577892959e-05\n",
      "VAL: EPOCH 302/1000 | BATCH 1/8 | LOSS: 9.39163328439463e-06\n",
      "VAL: EPOCH 302/1000 | BATCH 2/8 | LOSS: 8.856359575778091e-06\n",
      "VAL: EPOCH 302/1000 | BATCH 3/8 | LOSS: 8.904264177544974e-06\n",
      "VAL: EPOCH 302/1000 | BATCH 4/8 | LOSS: 8.63255208969349e-06\n",
      "VAL: EPOCH 302/1000 | BATCH 5/8 | LOSS: 8.124121147072097e-06\n",
      "VAL: EPOCH 302/1000 | BATCH 6/8 | LOSS: 8.043373652201677e-06\n",
      "VAL: EPOCH 302/1000 | BATCH 7/8 | LOSS: 7.800555920312036e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 0/71 | LOSS: 7.797302714607213e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 1/71 | LOSS: 6.372675215970958e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 2/71 | LOSS: 6.588603355339728e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 3/71 | LOSS: 6.966432920307852e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 4/71 | LOSS: 6.657412632193882e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 5/71 | LOSS: 6.409541886872224e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 6/71 | LOSS: 6.1438117882062215e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 7/71 | LOSS: 6.2001367382436e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 8/71 | LOSS: 6.080499941971438e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 9/71 | LOSS: 6.0681828472297635e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 10/71 | LOSS: 5.9586031552912156e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 11/71 | LOSS: 5.902924158363021e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 12/71 | LOSS: 5.9394443759139485e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 13/71 | LOSS: 5.83000080821096e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 14/71 | LOSS: 5.797198112607778e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 15/71 | LOSS: 5.801689553663891e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 16/71 | LOSS: 5.844587202077759e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 17/71 | LOSS: 5.739993487926161e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 18/71 | LOSS: 5.706149295292562e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 19/71 | LOSS: 5.634258536701964e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 20/71 | LOSS: 5.5986973469164996e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 21/71 | LOSS: 5.600087475117603e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 22/71 | LOSS: 5.547833236208965e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 23/71 | LOSS: 5.558030996629289e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 24/71 | LOSS: 5.578808795689838e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 25/71 | LOSS: 5.608327018517929e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 26/71 | LOSS: 5.624121008117476e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 27/71 | LOSS: 5.603352990744627e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 28/71 | LOSS: 5.6408600890978865e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 29/71 | LOSS: 5.593276273430092e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 30/71 | LOSS: 5.647358413566948e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 31/71 | LOSS: 5.601386675380127e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 32/71 | LOSS: 5.633373610070328e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 33/71 | LOSS: 5.6392285547618114e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 34/71 | LOSS: 5.6206590540698795e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 35/71 | LOSS: 5.633182480677432e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 36/71 | LOSS: 5.6397296715216284e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 37/71 | LOSS: 5.7003596913617865e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 38/71 | LOSS: 5.6848733574719145e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 39/71 | LOSS: 5.78593130740046e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 40/71 | LOSS: 5.755089992684278e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 41/71 | LOSS: 5.873651497538612e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 42/71 | LOSS: 5.886741831112209e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 43/71 | LOSS: 5.932522051146159e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 44/71 | LOSS: 5.949348269496113e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 45/71 | LOSS: 5.949857258320148e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 46/71 | LOSS: 5.973004493732352e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 47/71 | LOSS: 5.923277029751262e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 48/71 | LOSS: 5.9252585779919465e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 49/71 | LOSS: 5.902673829041305e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 50/71 | LOSS: 5.893216787037946e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 51/71 | LOSS: 5.8581416274486964e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 52/71 | LOSS: 5.826288277191607e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 53/71 | LOSS: 5.8028970215673326e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 54/71 | LOSS: 5.792453926419098e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 55/71 | LOSS: 5.771704406600163e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 56/71 | LOSS: 5.747590748477817e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 57/71 | LOSS: 5.725350853214783e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 58/71 | LOSS: 5.70718982659963e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 59/71 | LOSS: 5.674664438023077e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 60/71 | LOSS: 5.676647739820793e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 61/71 | LOSS: 5.6589184872967554e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 62/71 | LOSS: 5.656777108332977e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 63/71 | LOSS: 5.637480043674259e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 64/71 | LOSS: 5.670528914309286e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 65/71 | LOSS: 5.708112784111908e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 66/71 | LOSS: 5.725968331893482e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 67/71 | LOSS: 5.780036751863972e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 68/71 | LOSS: 5.76774175277788e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 69/71 | LOSS: 5.814946306210394e-06\n",
      "TRAIN: EPOCH 303/1000 | BATCH 70/71 | LOSS: 5.7801276295616795e-06\n",
      "VAL: EPOCH 303/1000 | BATCH 0/8 | LOSS: 6.159274562378414e-06\n",
      "VAL: EPOCH 303/1000 | BATCH 1/8 | LOSS: 5.7181007377948845e-06\n",
      "VAL: EPOCH 303/1000 | BATCH 2/8 | LOSS: 6.077314083086094e-06\n",
      "VAL: EPOCH 303/1000 | BATCH 3/8 | LOSS: 6.184849098644918e-06\n",
      "VAL: EPOCH 303/1000 | BATCH 4/8 | LOSS: 6.118723831605166e-06\n",
      "VAL: EPOCH 303/1000 | BATCH 5/8 | LOSS: 6.132291294610089e-06\n",
      "VAL: EPOCH 303/1000 | BATCH 6/8 | LOSS: 6.050130845583876e-06\n",
      "VAL: EPOCH 303/1000 | BATCH 7/8 | LOSS: 6.113410904617922e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 0/71 | LOSS: 5.19588229508372e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 1/71 | LOSS: 6.6328252614766825e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 2/71 | LOSS: 5.93839437594094e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 3/71 | LOSS: 6.400672418749309e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 4/71 | LOSS: 6.155475693958578e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 5/71 | LOSS: 6.4236612615786726e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 6/71 | LOSS: 6.499350027817334e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 7/71 | LOSS: 6.548994576860423e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 8/71 | LOSS: 6.468947049143026e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 9/71 | LOSS: 6.50717192911543e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 10/71 | LOSS: 6.497781421124025e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 11/71 | LOSS: 6.425055554852103e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 12/71 | LOSS: 6.367045303844721e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 13/71 | LOSS: 6.324259142534824e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 14/71 | LOSS: 6.135977294737435e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 15/71 | LOSS: 6.07411855924056e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 16/71 | LOSS: 5.944584900669938e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 17/71 | LOSS: 5.9885676996095245e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 18/71 | LOSS: 5.907660399890646e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 19/71 | LOSS: 5.833618808992469e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 20/71 | LOSS: 5.764721977573659e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 21/71 | LOSS: 5.712594129776831e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 22/71 | LOSS: 5.800190543279472e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 23/71 | LOSS: 5.7502348435415724e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 24/71 | LOSS: 5.813307352582342e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 25/71 | LOSS: 5.792369384275494e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 26/71 | LOSS: 5.783386238059999e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 27/71 | LOSS: 5.770754090203159e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 28/71 | LOSS: 5.781211782583529e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 29/71 | LOSS: 5.809241800610228e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 30/71 | LOSS: 5.840722686175028e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 31/71 | LOSS: 5.89130606698518e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 32/71 | LOSS: 5.884772573346819e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 33/71 | LOSS: 5.898826931543761e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 34/71 | LOSS: 5.890177862966084e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 35/71 | LOSS: 5.8682310976918316e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 36/71 | LOSS: 5.874400782173521e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 37/71 | LOSS: 5.83339702811109e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 38/71 | LOSS: 5.839634660822832e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 39/71 | LOSS: 5.793702786149879e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 40/71 | LOSS: 5.746266768300102e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 41/71 | LOSS: 5.790439493986403e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 42/71 | LOSS: 5.817713980472628e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 43/71 | LOSS: 5.827920140497802e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 44/71 | LOSS: 5.791665691706132e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 45/71 | LOSS: 5.801039433010927e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 46/71 | LOSS: 5.760402942503285e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 47/71 | LOSS: 5.7280336657565085e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 48/71 | LOSS: 5.740900201553999e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 49/71 | LOSS: 5.7315718504469256e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 50/71 | LOSS: 5.7023813523675716e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 51/71 | LOSS: 5.673857209295909e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 52/71 | LOSS: 5.663241244343775e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 53/71 | LOSS: 5.690235997024973e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 54/71 | LOSS: 5.695211344077093e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 55/71 | LOSS: 5.677923115042306e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 56/71 | LOSS: 5.688277935820352e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 57/71 | LOSS: 5.668995680119631e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 58/71 | LOSS: 5.680431432680916e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 59/71 | LOSS: 5.655226213245139e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 60/71 | LOSS: 5.6636982387063935e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 61/71 | LOSS: 5.648877537433075e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 62/71 | LOSS: 5.6203749041049466e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 63/71 | LOSS: 5.643351773443328e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 64/71 | LOSS: 5.631259021426488e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 65/71 | LOSS: 5.627427723001429e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 66/71 | LOSS: 5.611041924159725e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 67/71 | LOSS: 5.619160109451032e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 68/71 | LOSS: 5.5954481803896865e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 69/71 | LOSS: 5.5669401100593465e-06\n",
      "TRAIN: EPOCH 304/1000 | BATCH 70/71 | LOSS: 5.532566220467807e-06\n",
      "VAL: EPOCH 304/1000 | BATCH 0/8 | LOSS: 6.9911052378301974e-06\n",
      "VAL: EPOCH 304/1000 | BATCH 1/8 | LOSS: 6.746875442331657e-06\n",
      "VAL: EPOCH 304/1000 | BATCH 2/8 | LOSS: 6.3742242370305275e-06\n",
      "VAL: EPOCH 304/1000 | BATCH 3/8 | LOSS: 6.741249080732814e-06\n",
      "VAL: EPOCH 304/1000 | BATCH 4/8 | LOSS: 6.47806027700426e-06\n",
      "VAL: EPOCH 304/1000 | BATCH 5/8 | LOSS: 6.155233677418437e-06\n",
      "VAL: EPOCH 304/1000 | BATCH 6/8 | LOSS: 6.021403286078046e-06\n",
      "VAL: EPOCH 304/1000 | BATCH 7/8 | LOSS: 5.747250781951152e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 0/71 | LOSS: 5.3735329856863245e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 1/71 | LOSS: 6.874487553432118e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 2/71 | LOSS: 6.8527545712034526e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 3/71 | LOSS: 6.49897992843762e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 4/71 | LOSS: 7.0939298893790695e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 5/71 | LOSS: 6.902978687624757e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 6/71 | LOSS: 6.877031669968606e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 7/71 | LOSS: 6.896051331750641e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 8/71 | LOSS: 7.231350031765436e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 9/71 | LOSS: 7.1585289333597755e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 10/71 | LOSS: 7.104525345560185e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 11/71 | LOSS: 7.248652120021386e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 12/71 | LOSS: 7.1047518129209775e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 13/71 | LOSS: 7.139723136008667e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 14/71 | LOSS: 7.036282628784344e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 15/71 | LOSS: 6.999264229534674e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 16/71 | LOSS: 6.8863728034450425e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 17/71 | LOSS: 6.793930247618442e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 18/71 | LOSS: 6.764433931637445e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 19/71 | LOSS: 6.6622136273508655e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 20/71 | LOSS: 6.6879211512319415e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 21/71 | LOSS: 6.61358707144576e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 22/71 | LOSS: 6.498351981660918e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 23/71 | LOSS: 6.441843879656517e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 24/71 | LOSS: 6.366852430801373e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 25/71 | LOSS: 6.3672426816852885e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 26/71 | LOSS: 6.301863541685398e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 27/71 | LOSS: 6.259049184466546e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 28/71 | LOSS: 6.186759947013343e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 29/71 | LOSS: 6.117808212972401e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 30/71 | LOSS: 6.053484104350284e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 31/71 | LOSS: 6.005265731801046e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 32/71 | LOSS: 5.972515103194658e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 33/71 | LOSS: 5.918382064063574e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 34/71 | LOSS: 5.910129987439307e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 35/71 | LOSS: 5.866553616821572e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 36/71 | LOSS: 5.854642802512751e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 37/71 | LOSS: 5.802847346402766e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 38/71 | LOSS: 5.797928912588992e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 39/71 | LOSS: 5.7445583081516816e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 40/71 | LOSS: 5.704667212673329e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 41/71 | LOSS: 5.686020667131483e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 42/71 | LOSS: 5.677066695191802e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 43/71 | LOSS: 5.683212939609141e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 44/71 | LOSS: 5.663117174966222e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 45/71 | LOSS: 5.682780634624183e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 46/71 | LOSS: 5.670589992386088e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 47/71 | LOSS: 5.6537548734543934e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 48/71 | LOSS: 5.657513792338788e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 49/71 | LOSS: 5.666254478455812e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 50/71 | LOSS: 5.657485350157538e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 51/71 | LOSS: 5.656416013047979e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 52/71 | LOSS: 5.657757360238048e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 53/71 | LOSS: 5.6338708292160945e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 54/71 | LOSS: 5.6188506319317225e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 55/71 | LOSS: 5.6200374639761975e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 56/71 | LOSS: 5.612814128765246e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 57/71 | LOSS: 5.603487763970594e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 58/71 | LOSS: 5.590440184540181e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 59/71 | LOSS: 5.6389396718259375e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 60/71 | LOSS: 5.62953351752677e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 61/71 | LOSS: 5.6234826850472486e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 62/71 | LOSS: 5.645957878461184e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 63/71 | LOSS: 5.641388678867543e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 64/71 | LOSS: 5.642133527754385e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 65/71 | LOSS: 5.64050944004438e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 66/71 | LOSS: 5.652211586439351e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 67/71 | LOSS: 5.6631909615134646e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 68/71 | LOSS: 5.644668886895448e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 69/71 | LOSS: 5.6587828534507675e-06\n",
      "TRAIN: EPOCH 305/1000 | BATCH 70/71 | LOSS: 5.635467591097219e-06\n",
      "VAL: EPOCH 305/1000 | BATCH 0/8 | LOSS: 7.054223260638537e-06\n",
      "VAL: EPOCH 305/1000 | BATCH 1/8 | LOSS: 6.884660479045124e-06\n",
      "VAL: EPOCH 305/1000 | BATCH 2/8 | LOSS: 7.059411776329701e-06\n",
      "VAL: EPOCH 305/1000 | BATCH 3/8 | LOSS: 7.1102733727457235e-06\n",
      "VAL: EPOCH 305/1000 | BATCH 4/8 | LOSS: 7.05873917468125e-06\n",
      "VAL: EPOCH 305/1000 | BATCH 5/8 | LOSS: 6.852777384362223e-06\n",
      "VAL: EPOCH 305/1000 | BATCH 6/8 | LOSS: 6.691241456532485e-06\n",
      "VAL: EPOCH 305/1000 | BATCH 7/8 | LOSS: 6.665817011253239e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 0/71 | LOSS: 6.7701116677199025e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 1/71 | LOSS: 5.98438668930612e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 2/71 | LOSS: 5.509448328666622e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 3/71 | LOSS: 5.431525892163336e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 4/71 | LOSS: 5.532674640562618e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 5/71 | LOSS: 5.708429777466033e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 6/71 | LOSS: 5.572173709619424e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 7/71 | LOSS: 5.628466226426099e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 8/71 | LOSS: 5.5419262328844825e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 9/71 | LOSS: 5.639882147079334e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 10/71 | LOSS: 5.649362926553956e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 11/71 | LOSS: 5.647266524041091e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 12/71 | LOSS: 5.633711540352893e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 13/71 | LOSS: 5.619402242830672e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 14/71 | LOSS: 5.532433957948039e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 15/71 | LOSS: 5.585376641192852e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 16/71 | LOSS: 5.518382161372917e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 17/71 | LOSS: 5.422374366996034e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 18/71 | LOSS: 5.291694965068784e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 19/71 | LOSS: 5.336926790278085e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 20/71 | LOSS: 5.430404458127755e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 21/71 | LOSS: 5.382683050150015e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 22/71 | LOSS: 5.393004565358632e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 23/71 | LOSS: 5.402958445680876e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 24/71 | LOSS: 5.394905010689399e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 25/71 | LOSS: 5.4307339671420496e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 26/71 | LOSS: 5.420531250719149e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 27/71 | LOSS: 5.427977669114625e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 28/71 | LOSS: 5.561558964473079e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 29/71 | LOSS: 5.558097253318314e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 30/71 | LOSS: 5.667346415300985e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 31/71 | LOSS: 5.673094854330429e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 32/71 | LOSS: 5.693177633998637e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 33/71 | LOSS: 5.695580438548538e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 34/71 | LOSS: 5.667704895781104e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 35/71 | LOSS: 5.6798264280486264e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 36/71 | LOSS: 5.629554300105303e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 37/71 | LOSS: 5.675829943117462e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 38/71 | LOSS: 5.632974237484562e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 39/71 | LOSS: 5.64732953876046e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 40/71 | LOSS: 5.62414447813353e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 41/71 | LOSS: 5.579785460256625e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 42/71 | LOSS: 5.591395519818329e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 43/71 | LOSS: 5.596302669759594e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 44/71 | LOSS: 5.601416175219735e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 45/71 | LOSS: 5.593675085868239e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 46/71 | LOSS: 5.551522975865543e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 47/71 | LOSS: 5.585854353284958e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 48/71 | LOSS: 5.558190506996411e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 49/71 | LOSS: 5.5545968916703715e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 50/71 | LOSS: 5.545739770918861e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 51/71 | LOSS: 5.550192003493766e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 52/71 | LOSS: 5.589592332536034e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 53/71 | LOSS: 5.577921692073923e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 54/71 | LOSS: 5.613277517113602e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 55/71 | LOSS: 5.614616408625547e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 56/71 | LOSS: 5.605590702448114e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 57/71 | LOSS: 5.623932009194536e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 58/71 | LOSS: 5.620597861152265e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 59/71 | LOSS: 5.6384553545285595e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 60/71 | LOSS: 5.623512330859113e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 61/71 | LOSS: 5.643258807521629e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 62/71 | LOSS: 5.634817156648736e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 63/71 | LOSS: 5.628853884331875e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 64/71 | LOSS: 5.631542038687397e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 65/71 | LOSS: 5.610913266344031e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 66/71 | LOSS: 5.621495055588163e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 67/71 | LOSS: 5.595542448003578e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 68/71 | LOSS: 5.592433413055361e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 69/71 | LOSS: 5.581376555190738e-06\n",
      "TRAIN: EPOCH 306/1000 | BATCH 70/71 | LOSS: 5.563106287168716e-06\n",
      "VAL: EPOCH 306/1000 | BATCH 0/8 | LOSS: 7.826032742741518e-06\n",
      "VAL: EPOCH 306/1000 | BATCH 1/8 | LOSS: 7.03744490238023e-06\n",
      "VAL: EPOCH 306/1000 | BATCH 2/8 | LOSS: 6.797856864674638e-06\n",
      "VAL: EPOCH 306/1000 | BATCH 3/8 | LOSS: 6.980155376368202e-06\n",
      "VAL: EPOCH 306/1000 | BATCH 4/8 | LOSS: 6.835580734332325e-06\n",
      "VAL: EPOCH 306/1000 | BATCH 5/8 | LOSS: 6.535533505787801e-06\n",
      "VAL: EPOCH 306/1000 | BATCH 6/8 | LOSS: 6.586136610816798e-06\n",
      "VAL: EPOCH 306/1000 | BATCH 7/8 | LOSS: 6.376287615239562e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 0/71 | LOSS: 5.811533355881693e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 1/71 | LOSS: 5.103170678921742e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 2/71 | LOSS: 5.211463151984693e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 3/71 | LOSS: 5.052756023360416e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 4/71 | LOSS: 5.060470175521914e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 5/71 | LOSS: 4.91059040541586e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 6/71 | LOSS: 4.936368181266257e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 7/71 | LOSS: 5.1069526421088085e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 8/71 | LOSS: 5.1950000852230005e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 9/71 | LOSS: 5.107028482598253e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 10/71 | LOSS: 5.163780852771279e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 11/71 | LOSS: 5.250360042433992e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 12/71 | LOSS: 5.259908553293476e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 13/71 | LOSS: 5.222402998177651e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 14/71 | LOSS: 5.277426407701569e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 15/71 | LOSS: 5.225004827025259e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 16/71 | LOSS: 5.165879348864751e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 17/71 | LOSS: 5.1668793174839166e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 18/71 | LOSS: 5.130871135119569e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 19/71 | LOSS: 5.1291739055159266e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 20/71 | LOSS: 5.067787545461518e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 21/71 | LOSS: 5.022915460854578e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 22/71 | LOSS: 4.970862174440813e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 23/71 | LOSS: 4.941706398161235e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 24/71 | LOSS: 4.978065135219367e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 25/71 | LOSS: 4.988459995029888e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 26/71 | LOSS: 4.967212623375451e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 27/71 | LOSS: 4.9714552850283714e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 28/71 | LOSS: 4.986712830377028e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 29/71 | LOSS: 4.978075397351252e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 30/71 | LOSS: 4.99144183660035e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 31/71 | LOSS: 4.9853606611804935e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 32/71 | LOSS: 4.988901447645868e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 33/71 | LOSS: 4.972984189568224e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 34/71 | LOSS: 4.9801130444393495e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 35/71 | LOSS: 4.9681613568787324e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 36/71 | LOSS: 4.982524338580284e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 37/71 | LOSS: 4.9773424100414174e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 38/71 | LOSS: 4.97730129959214e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 39/71 | LOSS: 4.984320287348964e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 40/71 | LOSS: 4.982617355846919e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 41/71 | LOSS: 4.959883664837218e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 42/71 | LOSS: 4.9941169341171955e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 43/71 | LOSS: 4.995250254647875e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 44/71 | LOSS: 5.029117467832597e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 45/71 | LOSS: 5.0055260821270915e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 46/71 | LOSS: 4.987419568815892e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 47/71 | LOSS: 4.999254902789592e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 48/71 | LOSS: 4.986559639403321e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 49/71 | LOSS: 5.038221233917284e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 50/71 | LOSS: 5.05647497394581e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 51/71 | LOSS: 5.098965061733907e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 52/71 | LOSS: 5.130468767709077e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 53/71 | LOSS: 5.135941566752283e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 54/71 | LOSS: 5.196239453653107e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 55/71 | LOSS: 5.179362225200228e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 56/71 | LOSS: 5.2534537644532975e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 57/71 | LOSS: 5.269296197780179e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 58/71 | LOSS: 5.276628791638729e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 59/71 | LOSS: 5.308213477898486e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 60/71 | LOSS: 5.309901657487079e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 61/71 | LOSS: 5.332514642612941e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 62/71 | LOSS: 5.3226349914367015e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 63/71 | LOSS: 5.340454364954894e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 64/71 | LOSS: 5.349672923889906e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 65/71 | LOSS: 5.363422863438521e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 66/71 | LOSS: 5.3875834435448676e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 67/71 | LOSS: 5.386388820357645e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 68/71 | LOSS: 5.422745460605981e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 69/71 | LOSS: 5.427808426767505e-06\n",
      "TRAIN: EPOCH 307/1000 | BATCH 70/71 | LOSS: 5.3974478830664145e-06\n",
      "VAL: EPOCH 307/1000 | BATCH 0/8 | LOSS: 9.531957402941771e-06\n",
      "VAL: EPOCH 307/1000 | BATCH 1/8 | LOSS: 9.74120075625251e-06\n",
      "VAL: EPOCH 307/1000 | BATCH 2/8 | LOSS: 9.313896953244694e-06\n",
      "VAL: EPOCH 307/1000 | BATCH 3/8 | LOSS: 9.485362397754216e-06\n",
      "VAL: EPOCH 307/1000 | BATCH 4/8 | LOSS: 9.303227670898195e-06\n",
      "VAL: EPOCH 307/1000 | BATCH 5/8 | LOSS: 8.75624641594186e-06\n",
      "VAL: EPOCH 307/1000 | BATCH 6/8 | LOSS: 8.79040551028863e-06\n",
      "VAL: EPOCH 307/1000 | BATCH 7/8 | LOSS: 8.529444301075273e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 0/71 | LOSS: 8.135727512126323e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 1/71 | LOSS: 6.26603605269338e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 2/71 | LOSS: 6.6933800250505255e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 3/71 | LOSS: 6.560391398124921e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 4/71 | LOSS: 6.49040257485467e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 5/71 | LOSS: 6.863115459054825e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 6/71 | LOSS: 6.814926759294135e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 7/71 | LOSS: 6.925267769020138e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 8/71 | LOSS: 6.719668868350305e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 9/71 | LOSS: 6.7806850893248335e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 10/71 | LOSS: 6.551511763642669e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 11/71 | LOSS: 6.436290391320654e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 12/71 | LOSS: 6.260844936780184e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 13/71 | LOSS: 6.089403054180106e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 14/71 | LOSS: 5.995725799342229e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 15/71 | LOSS: 5.8446266848477535e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 16/71 | LOSS: 5.742577928578933e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 17/71 | LOSS: 5.658431619950635e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 18/71 | LOSS: 5.513653341519162e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 19/71 | LOSS: 5.452407992834196e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 20/71 | LOSS: 5.410386111612925e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 21/71 | LOSS: 5.37366483141258e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 22/71 | LOSS: 5.322826945817012e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 23/71 | LOSS: 5.2774271637190395e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 24/71 | LOSS: 5.2723744829563655e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 25/71 | LOSS: 5.326165810751822e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 26/71 | LOSS: 5.361951269622767e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 27/71 | LOSS: 5.4392414686585e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 28/71 | LOSS: 5.488665661248198e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 29/71 | LOSS: 5.498291792112772e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 30/71 | LOSS: 5.55303762989605e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 31/71 | LOSS: 5.502642743238084e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 32/71 | LOSS: 5.5588197731346804e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 33/71 | LOSS: 5.507384324065403e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 34/71 | LOSS: 5.49042436302573e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 35/71 | LOSS: 5.496032978650571e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 36/71 | LOSS: 5.640352724554218e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 37/71 | LOSS: 5.617574695287131e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 38/71 | LOSS: 5.5808310814124224e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 39/71 | LOSS: 5.614074484583398e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 40/71 | LOSS: 5.6065159142457475e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 41/71 | LOSS: 5.583496279844742e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 42/71 | LOSS: 5.5925188145158965e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 43/71 | LOSS: 5.631275040617435e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 44/71 | LOSS: 5.6459107023935456e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 45/71 | LOSS: 5.644114836563265e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 46/71 | LOSS: 5.673153882139075e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 47/71 | LOSS: 5.6518359391096356e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 48/71 | LOSS: 5.673204223243448e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 49/71 | LOSS: 5.634721978822199e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 50/71 | LOSS: 5.679463441208331e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 51/71 | LOSS: 5.665322563485321e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 52/71 | LOSS: 5.730985060166996e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 53/71 | LOSS: 5.713676833457376e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 54/71 | LOSS: 5.744020567858983e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 55/71 | LOSS: 5.733891193975589e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 56/71 | LOSS: 5.738372911972028e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 57/71 | LOSS: 5.723516053500322e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 58/71 | LOSS: 5.73472913733355e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 59/71 | LOSS: 5.748349284810198e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 60/71 | LOSS: 5.720957035725301e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 61/71 | LOSS: 5.725341683530367e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 62/71 | LOSS: 5.713110484730495e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 63/71 | LOSS: 5.708248128399873e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 64/71 | LOSS: 5.710544851703722e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 65/71 | LOSS: 5.700595338429125e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 66/71 | LOSS: 5.694575041962341e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 67/71 | LOSS: 5.67536753070432e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 68/71 | LOSS: 5.687194947893369e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 69/71 | LOSS: 5.657033024622901e-06\n",
      "TRAIN: EPOCH 308/1000 | BATCH 70/71 | LOSS: 5.6537441497299295e-06\n",
      "VAL: EPOCH 308/1000 | BATCH 0/8 | LOSS: 5.8772338888957165e-06\n",
      "VAL: EPOCH 308/1000 | BATCH 1/8 | LOSS: 5.71499731449876e-06\n",
      "VAL: EPOCH 308/1000 | BATCH 2/8 | LOSS: 5.454559262337473e-06\n",
      "VAL: EPOCH 308/1000 | BATCH 3/8 | LOSS: 5.598745246970793e-06\n",
      "VAL: EPOCH 308/1000 | BATCH 4/8 | LOSS: 5.4773689043940975e-06\n",
      "VAL: EPOCH 308/1000 | BATCH 5/8 | LOSS: 5.1954974651380326e-06\n",
      "VAL: EPOCH 308/1000 | BATCH 6/8 | LOSS: 5.076520697652345e-06\n",
      "VAL: EPOCH 308/1000 | BATCH 7/8 | LOSS: 4.894607286587416e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 0/71 | LOSS: 4.4659159357252065e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 1/71 | LOSS: 4.427343128554639e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 2/71 | LOSS: 4.720879436111621e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 3/71 | LOSS: 4.8131643097804044e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 4/71 | LOSS: 5.025374684919371e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 5/71 | LOSS: 4.974646268844178e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 6/71 | LOSS: 5.07244041208261e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 7/71 | LOSS: 5.0516051146587415e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 8/71 | LOSS: 5.0777476745780505e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 9/71 | LOSS: 4.956571956427069e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 10/71 | LOSS: 4.961590341488111e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 11/71 | LOSS: 5.032907248884537e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 12/71 | LOSS: 5.1283717976520375e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 13/71 | LOSS: 5.183415169085492e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 14/71 | LOSS: 5.048481458895063e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 15/71 | LOSS: 5.044640573714787e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 16/71 | LOSS: 5.0561310511993186e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 17/71 | LOSS: 4.983115243501541e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 18/71 | LOSS: 4.894297023240238e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 19/71 | LOSS: 4.861593072291725e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 20/71 | LOSS: 4.870166527498873e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 21/71 | LOSS: 4.854717200363336e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 22/71 | LOSS: 4.930228116158421e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 23/71 | LOSS: 4.926720057104224e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 24/71 | LOSS: 4.936504037686973e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 25/71 | LOSS: 4.943132379454395e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 26/71 | LOSS: 4.9694194196355835e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 27/71 | LOSS: 4.948495050679672e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 28/71 | LOSS: 4.9340177187771364e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 29/71 | LOSS: 4.928599711699159e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 30/71 | LOSS: 4.900387048403518e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 31/71 | LOSS: 4.855170047335378e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 32/71 | LOSS: 4.88429223520198e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 33/71 | LOSS: 4.878107775392196e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 34/71 | LOSS: 4.902383079752326e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 35/71 | LOSS: 4.891300919654087e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 36/71 | LOSS: 4.869450131573595e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 37/71 | LOSS: 4.887765846050522e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 38/71 | LOSS: 4.868232549480849e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 39/71 | LOSS: 4.928574708173983e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 40/71 | LOSS: 4.933592652200676e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 41/71 | LOSS: 4.958571913621632e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 42/71 | LOSS: 4.984234206054934e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 43/71 | LOSS: 4.995772087568184e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 44/71 | LOSS: 5.010905937928732e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 45/71 | LOSS: 5.0059287609063015e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 46/71 | LOSS: 5.046383900294808e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 47/71 | LOSS: 5.03852740507682e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 48/71 | LOSS: 5.077750580423935e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 49/71 | LOSS: 5.07491489770473e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 50/71 | LOSS: 5.117451591478825e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 51/71 | LOSS: 5.09191111265094e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 52/71 | LOSS: 5.079902925813859e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 53/71 | LOSS: 5.061547228622119e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 54/71 | LOSS: 5.031820368954372e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 55/71 | LOSS: 5.0411064721954195e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 56/71 | LOSS: 5.023332246230239e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 57/71 | LOSS: 5.037705073877517e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 58/71 | LOSS: 5.049014173321558e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 59/71 | LOSS: 5.055929322376566e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 60/71 | LOSS: 5.082636961128255e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 61/71 | LOSS: 5.08092964243969e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 62/71 | LOSS: 5.1068675763836215e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 63/71 | LOSS: 5.098215414278684e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 64/71 | LOSS: 5.103080425564818e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 65/71 | LOSS: 5.1298666531610335e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 66/71 | LOSS: 5.130957686614876e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 67/71 | LOSS: 5.142027592843506e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 68/71 | LOSS: 5.108469777305668e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 69/71 | LOSS: 5.124071484325603e-06\n",
      "TRAIN: EPOCH 309/1000 | BATCH 70/71 | LOSS: 5.134752901655484e-06\n",
      "VAL: EPOCH 309/1000 | BATCH 0/8 | LOSS: 6.218561338755535e-06\n",
      "VAL: EPOCH 309/1000 | BATCH 1/8 | LOSS: 5.742649818785139e-06\n",
      "VAL: EPOCH 309/1000 | BATCH 2/8 | LOSS: 5.997273698691667e-06\n",
      "VAL: EPOCH 309/1000 | BATCH 3/8 | LOSS: 6.108314551056537e-06\n",
      "VAL: EPOCH 309/1000 | BATCH 4/8 | LOSS: 6.037066032149596e-06\n",
      "VAL: EPOCH 309/1000 | BATCH 5/8 | LOSS: 5.961573909492775e-06\n",
      "VAL: EPOCH 309/1000 | BATCH 6/8 | LOSS: 5.823010204559458e-06\n",
      "VAL: EPOCH 309/1000 | BATCH 7/8 | LOSS: 5.775770148375159e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 0/71 | LOSS: 6.442538051487645e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 1/71 | LOSS: 7.095887895047781e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 2/71 | LOSS: 6.6546422203828115e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 3/71 | LOSS: 6.3030867067936924e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 4/71 | LOSS: 6.07620395385311e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 5/71 | LOSS: 5.896059140771588e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 6/71 | LOSS: 5.930350783143824e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 7/71 | LOSS: 5.6218478619030066e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 8/71 | LOSS: 5.812962803651721e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 9/71 | LOSS: 5.733421471632028e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 10/71 | LOSS: 5.8399841898128875e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 11/71 | LOSS: 5.760546798683208e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 12/71 | LOSS: 5.780376078781466e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 13/71 | LOSS: 5.717606573593262e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 14/71 | LOSS: 5.7421654219069754e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 15/71 | LOSS: 5.644651579927995e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 16/71 | LOSS: 5.588610787633704e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 17/71 | LOSS: 5.579951309502778e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 18/71 | LOSS: 5.491618127818679e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 19/71 | LOSS: 5.415373937012191e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 20/71 | LOSS: 5.3460781016058585e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 21/71 | LOSS: 5.313553246692622e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 22/71 | LOSS: 5.276301032569355e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 23/71 | LOSS: 5.26614802727939e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 24/71 | LOSS: 5.313841311362921e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 25/71 | LOSS: 5.265173259571244e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 26/71 | LOSS: 5.246759251854706e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 27/71 | LOSS: 5.224565817180908e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 28/71 | LOSS: 5.255289336454208e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 29/71 | LOSS: 5.269210252360305e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 30/71 | LOSS: 5.2386000216727314e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 31/71 | LOSS: 5.1986333815534636e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 32/71 | LOSS: 5.196716060571347e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 33/71 | LOSS: 5.1848465198097124e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 34/71 | LOSS: 5.161097455389349e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 35/71 | LOSS: 5.153156034036026e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 36/71 | LOSS: 5.1427486006424576e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 37/71 | LOSS: 5.180936658402252e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 38/71 | LOSS: 5.226382497242482e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 39/71 | LOSS: 5.300585024770044e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 40/71 | LOSS: 5.311654032949463e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 41/71 | LOSS: 5.361643513337878e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 42/71 | LOSS: 5.362840347304374e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 43/71 | LOSS: 5.387149253072659e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 44/71 | LOSS: 5.4963385537121215e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 45/71 | LOSS: 5.465447553314848e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 46/71 | LOSS: 5.553982196324022e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 47/71 | LOSS: 5.569343789109856e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 48/71 | LOSS: 5.613856051333679e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 49/71 | LOSS: 5.6064457612592375e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 50/71 | LOSS: 5.593216923550833e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 51/71 | LOSS: 5.6263664305333805e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 52/71 | LOSS: 5.593161820385985e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 53/71 | LOSS: 5.578176910603361e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 54/71 | LOSS: 5.55830276673739e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 55/71 | LOSS: 5.600854913317173e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 56/71 | LOSS: 5.587084499718019e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 57/71 | LOSS: 5.582734584565734e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 58/71 | LOSS: 5.5765584975387805e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 59/71 | LOSS: 5.5996782862166585e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 60/71 | LOSS: 5.5695553760788096e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 61/71 | LOSS: 5.576716665517221e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 62/71 | LOSS: 5.567268209190279e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 63/71 | LOSS: 5.5650669033013855e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 64/71 | LOSS: 5.551617186938529e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 65/71 | LOSS: 5.534508135717646e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 66/71 | LOSS: 5.528997839227581e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 67/71 | LOSS: 5.512060935790728e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 68/71 | LOSS: 5.511806513322222e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 69/71 | LOSS: 5.487952385660069e-06\n",
      "TRAIN: EPOCH 310/1000 | BATCH 70/71 | LOSS: 5.482458243957962e-06\n",
      "VAL: EPOCH 310/1000 | BATCH 0/8 | LOSS: 5.317700924933888e-06\n",
      "VAL: EPOCH 310/1000 | BATCH 1/8 | LOSS: 4.881235099674086e-06\n",
      "VAL: EPOCH 310/1000 | BATCH 2/8 | LOSS: 5.0491053116275e-06\n",
      "VAL: EPOCH 310/1000 | BATCH 3/8 | LOSS: 5.195034191274317e-06\n",
      "VAL: EPOCH 310/1000 | BATCH 4/8 | LOSS: 5.118699573358754e-06\n",
      "VAL: EPOCH 310/1000 | BATCH 5/8 | LOSS: 4.9730455581690576e-06\n",
      "VAL: EPOCH 310/1000 | BATCH 6/8 | LOSS: 4.915520613784403e-06\n",
      "VAL: EPOCH 310/1000 | BATCH 7/8 | LOSS: 4.853386371905799e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 0/71 | LOSS: 4.510911367106019e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 1/71 | LOSS: 4.602266244546627e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 2/71 | LOSS: 4.382033087798239e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 3/71 | LOSS: 4.597372480930062e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 4/71 | LOSS: 4.667833854909986e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 5/71 | LOSS: 4.542227998172166e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 6/71 | LOSS: 4.570115379465278e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 7/71 | LOSS: 4.557180091069313e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 8/71 | LOSS: 4.54314148454513e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 9/71 | LOSS: 4.523261895883479e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 10/71 | LOSS: 4.4636230169007005e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 11/71 | LOSS: 4.569088522051364e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 12/71 | LOSS: 4.626569989341078e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 13/71 | LOSS: 4.646776005756692e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 14/71 | LOSS: 4.648320646083448e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 15/71 | LOSS: 4.706504427076652e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 16/71 | LOSS: 4.717920386182788e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 17/71 | LOSS: 4.76419136753571e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 18/71 | LOSS: 4.796124262372216e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 19/71 | LOSS: 4.770206396642607e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 20/71 | LOSS: 4.715205618378518e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 21/71 | LOSS: 4.768723763184178e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 22/71 | LOSS: 4.7198974915891485e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 23/71 | LOSS: 4.679268954532745e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 24/71 | LOSS: 4.772441989189247e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 25/71 | LOSS: 4.7948265966204836e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 26/71 | LOSS: 4.773606195629799e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 27/71 | LOSS: 4.7751579391582965e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 28/71 | LOSS: 4.780199984013272e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 29/71 | LOSS: 4.77014426299623e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 30/71 | LOSS: 4.7876105928744734e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 31/71 | LOSS: 4.879119671841181e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 32/71 | LOSS: 4.885271609413339e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 33/71 | LOSS: 4.89529065816896e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 34/71 | LOSS: 4.869695372430475e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 35/71 | LOSS: 4.895289571828067e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 36/71 | LOSS: 4.876177147682914e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 37/71 | LOSS: 4.873767789161464e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 38/71 | LOSS: 4.877970870718575e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 39/71 | LOSS: 4.87033375975443e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 40/71 | LOSS: 4.8478948928077885e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 41/71 | LOSS: 4.834816125020595e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 42/71 | LOSS: 4.818622496819698e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 43/71 | LOSS: 4.851777248404687e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 44/71 | LOSS: 4.8395836832545076e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 45/71 | LOSS: 4.890605692787136e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 46/71 | LOSS: 4.902371325984627e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 47/71 | LOSS: 4.901483729706039e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 48/71 | LOSS: 4.924236635611588e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 49/71 | LOSS: 4.937861704092939e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 50/71 | LOSS: 4.924219580311283e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 51/71 | LOSS: 4.927341426362265e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 52/71 | LOSS: 4.913682811813899e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 53/71 | LOSS: 4.9404160546584065e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 54/71 | LOSS: 4.96573110550261e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 55/71 | LOSS: 4.986250067824065e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 56/71 | LOSS: 4.9771687803226274e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 57/71 | LOSS: 4.984527408694326e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 58/71 | LOSS: 4.9943313631771715e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 59/71 | LOSS: 4.985846574830551e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 60/71 | LOSS: 4.97923366444903e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 61/71 | LOSS: 5.001371164325974e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 62/71 | LOSS: 4.998512777669858e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 63/71 | LOSS: 5.022322739023366e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 64/71 | LOSS: 5.06734048450688e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 65/71 | LOSS: 5.081940991768726e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 66/71 | LOSS: 5.099109608609205e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 67/71 | LOSS: 5.109610924591834e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 68/71 | LOSS: 5.16981893836772e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 69/71 | LOSS: 5.165257256781583e-06\n",
      "TRAIN: EPOCH 311/1000 | BATCH 70/71 | LOSS: 5.1490309372856645e-06\n",
      "VAL: EPOCH 311/1000 | BATCH 0/8 | LOSS: 1.3689038496522699e-05\n",
      "VAL: EPOCH 311/1000 | BATCH 1/8 | LOSS: 1.4223297057469608e-05\n",
      "VAL: EPOCH 311/1000 | BATCH 2/8 | LOSS: 1.339344422982928e-05\n",
      "VAL: EPOCH 311/1000 | BATCH 3/8 | LOSS: 1.3530083151636063e-05\n",
      "VAL: EPOCH 311/1000 | BATCH 4/8 | LOSS: 1.335362085228553e-05\n",
      "VAL: EPOCH 311/1000 | BATCH 5/8 | LOSS: 1.2663004023731142e-05\n",
      "VAL: EPOCH 311/1000 | BATCH 6/8 | LOSS: 1.2510898938801671e-05\n",
      "VAL: EPOCH 311/1000 | BATCH 7/8 | LOSS: 1.2218902270433318e-05\n",
      "TRAIN: EPOCH 312/1000 | BATCH 0/71 | LOSS: 1.14244958240306e-05\n",
      "TRAIN: EPOCH 312/1000 | BATCH 1/71 | LOSS: 8.957163345257868e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 2/71 | LOSS: 8.136932137858821e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 3/71 | LOSS: 8.023238819987455e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 4/71 | LOSS: 8.125269141601166e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 5/71 | LOSS: 7.82504427358314e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 6/71 | LOSS: 7.531475083461763e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 7/71 | LOSS: 7.4992635745729785e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 8/71 | LOSS: 7.356680220659149e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 9/71 | LOSS: 7.450368957506725e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 10/71 | LOSS: 7.346290493098257e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 11/71 | LOSS: 7.3052525901099825e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 12/71 | LOSS: 7.212166845140298e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 13/71 | LOSS: 7.528535921405169e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 14/71 | LOSS: 7.397065564873628e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 15/71 | LOSS: 7.285184835836844e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 16/71 | LOSS: 7.308330703595215e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 17/71 | LOSS: 7.251054790281665e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 18/71 | LOSS: 7.204337156221491e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 19/71 | LOSS: 7.187686014731298e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 20/71 | LOSS: 7.116031286256787e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 21/71 | LOSS: 7.100887511445432e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 22/71 | LOSS: 7.013606611271035e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 23/71 | LOSS: 7.014409258469338e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 24/71 | LOSS: 6.8968782943557016e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 25/71 | LOSS: 6.856114011428032e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 26/71 | LOSS: 6.824163089428718e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 27/71 | LOSS: 6.7644330264166844e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 28/71 | LOSS: 6.7067324317608364e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 29/71 | LOSS: 6.631521364397485e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 30/71 | LOSS: 6.5834975841074e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 31/71 | LOSS: 6.5057869136353474e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 32/71 | LOSS: 6.446555454948049e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 33/71 | LOSS: 6.441015878355008e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 34/71 | LOSS: 6.4116314336258385e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 35/71 | LOSS: 6.396036812677468e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 36/71 | LOSS: 6.411390426659932e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 37/71 | LOSS: 6.3894769817508e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 38/71 | LOSS: 6.461710864431762e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 39/71 | LOSS: 6.457515735291963e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 40/71 | LOSS: 6.472377219502161e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 41/71 | LOSS: 6.40588217763815e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 42/71 | LOSS: 6.449897830671669e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 43/71 | LOSS: 6.404071283131171e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 44/71 | LOSS: 6.388452877419897e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 45/71 | LOSS: 6.3690741606715955e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 46/71 | LOSS: 6.346745251110096e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 47/71 | LOSS: 6.340456356686748e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 48/71 | LOSS: 6.300612117878959e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 49/71 | LOSS: 6.2750086090090915e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 50/71 | LOSS: 6.232404721944761e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 51/71 | LOSS: 6.187242066656696e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 52/71 | LOSS: 6.188276220543556e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 53/71 | LOSS: 6.162939253381747e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 54/71 | LOSS: 6.142973986491349e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 55/71 | LOSS: 6.132611369074377e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 56/71 | LOSS: 6.121597292357157e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 57/71 | LOSS: 6.093432610778736e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 58/71 | LOSS: 6.080692385740665e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 59/71 | LOSS: 6.045473378435418e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 60/71 | LOSS: 6.027233368632685e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 61/71 | LOSS: 6.024648024732972e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 62/71 | LOSS: 6.016770132393705e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 63/71 | LOSS: 6.009185426592012e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 64/71 | LOSS: 5.978572545035814e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 65/71 | LOSS: 5.991274340913426e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 66/71 | LOSS: 5.957221799744458e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 67/71 | LOSS: 5.934109203601966e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 68/71 | LOSS: 5.9213962725587645e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 69/71 | LOSS: 5.921767754963574e-06\n",
      "TRAIN: EPOCH 312/1000 | BATCH 70/71 | LOSS: 5.892542052171054e-06\n",
      "VAL: EPOCH 312/1000 | BATCH 0/8 | LOSS: 5.807991328765638e-06\n",
      "VAL: EPOCH 312/1000 | BATCH 1/8 | LOSS: 5.337867150956299e-06\n",
      "VAL: EPOCH 312/1000 | BATCH 2/8 | LOSS: 5.530970308124476e-06\n",
      "VAL: EPOCH 312/1000 | BATCH 3/8 | LOSS: 5.46073249552137e-06\n",
      "VAL: EPOCH 312/1000 | BATCH 4/8 | LOSS: 5.452453206089558e-06\n",
      "VAL: EPOCH 312/1000 | BATCH 5/8 | LOSS: 5.24897897472935e-06\n",
      "VAL: EPOCH 312/1000 | BATCH 6/8 | LOSS: 5.157957470406213e-06\n",
      "VAL: EPOCH 312/1000 | BATCH 7/8 | LOSS: 5.166467758499493e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 0/71 | LOSS: 4.64629511043313e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 1/71 | LOSS: 4.9178058816323755e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 2/71 | LOSS: 4.657639237848343e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 3/71 | LOSS: 4.8297780494976905e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 4/71 | LOSS: 4.832572722079931e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 5/71 | LOSS: 5.119662773722666e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 6/71 | LOSS: 5.098913269258836e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 7/71 | LOSS: 5.210843937675236e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 8/71 | LOSS: 5.136100955698768e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 9/71 | LOSS: 5.165021093489486e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 10/71 | LOSS: 5.188118228199363e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 11/71 | LOSS: 5.1790874901295565e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 12/71 | LOSS: 5.187498572922777e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 13/71 | LOSS: 5.139605296530395e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 14/71 | LOSS: 5.035304836079983e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 15/71 | LOSS: 5.001979403118639e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 16/71 | LOSS: 5.037342241527821e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 17/71 | LOSS: 5.059430463916215e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 18/71 | LOSS: 5.008215447033811e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 19/71 | LOSS: 5.039116069838201e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 20/71 | LOSS: 5.116626468970069e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 21/71 | LOSS: 5.0496634106490425e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 22/71 | LOSS: 5.078198287462684e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 23/71 | LOSS: 5.14966509020572e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 24/71 | LOSS: 5.140935336385155e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 25/71 | LOSS: 5.132666134299003e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 26/71 | LOSS: 5.12031408140535e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 27/71 | LOSS: 5.153450680544276e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 28/71 | LOSS: 5.126879919959625e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 29/71 | LOSS: 5.131375003960178e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 30/71 | LOSS: 5.159546617505839e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 31/71 | LOSS: 5.173241149236674e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 32/71 | LOSS: 5.179952977689964e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 33/71 | LOSS: 5.19729092850278e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 34/71 | LOSS: 5.248338194568143e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 35/71 | LOSS: 5.220201892876099e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 36/71 | LOSS: 5.24499778662544e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 37/71 | LOSS: 5.241332904628698e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 38/71 | LOSS: 5.2418585465392634e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 39/71 | LOSS: 5.312272071478219e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 40/71 | LOSS: 5.303986865062431e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 41/71 | LOSS: 5.354998544741345e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 42/71 | LOSS: 5.349917416403282e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 43/71 | LOSS: 5.353359090969951e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 44/71 | LOSS: 5.354849781724624e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 45/71 | LOSS: 5.368400499482169e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 46/71 | LOSS: 5.415270094245182e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 47/71 | LOSS: 5.390296915190144e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 48/71 | LOSS: 5.408329603337084e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 49/71 | LOSS: 5.413394392235205e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 50/71 | LOSS: 5.41694528995625e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 51/71 | LOSS: 5.404422364126031e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 52/71 | LOSS: 5.390185421829129e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 53/71 | LOSS: 5.436435424565355e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 54/71 | LOSS: 5.410574778414395e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 55/71 | LOSS: 5.415901106126901e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 56/71 | LOSS: 5.398316022221575e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 57/71 | LOSS: 5.36759034730494e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 58/71 | LOSS: 5.3551591964467165e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 59/71 | LOSS: 5.33877666839544e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 60/71 | LOSS: 5.3541176519084715e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 61/71 | LOSS: 5.335705656116554e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 62/71 | LOSS: 5.335620504151198e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 63/71 | LOSS: 5.337781232128691e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 64/71 | LOSS: 5.333671435017515e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 65/71 | LOSS: 5.3363633772581425e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 66/71 | LOSS: 5.308785216245147e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 67/71 | LOSS: 5.29246668680999e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 68/71 | LOSS: 5.281965243646983e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 69/71 | LOSS: 5.288041263286557e-06\n",
      "TRAIN: EPOCH 313/1000 | BATCH 70/71 | LOSS: 5.2983041122145e-06\n",
      "VAL: EPOCH 313/1000 | BATCH 0/8 | LOSS: 6.086644589231582e-06\n",
      "VAL: EPOCH 313/1000 | BATCH 1/8 | LOSS: 5.755246320404694e-06\n",
      "VAL: EPOCH 313/1000 | BATCH 2/8 | LOSS: 5.503359564803152e-06\n",
      "VAL: EPOCH 313/1000 | BATCH 3/8 | LOSS: 5.666591846420488e-06\n",
      "VAL: EPOCH 313/1000 | BATCH 4/8 | LOSS: 5.529919417313067e-06\n",
      "VAL: EPOCH 313/1000 | BATCH 5/8 | LOSS: 5.293115236781887e-06\n",
      "VAL: EPOCH 313/1000 | BATCH 6/8 | LOSS: 5.228771767308769e-06\n",
      "VAL: EPOCH 313/1000 | BATCH 7/8 | LOSS: 5.051464142979967e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 0/71 | LOSS: 3.975651452492457e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 1/71 | LOSS: 5.487631369760493e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 2/71 | LOSS: 5.5519822126370855e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 3/71 | LOSS: 6.004497322464886e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 4/71 | LOSS: 6.23629812253057e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 5/71 | LOSS: 6.021607456811277e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 6/71 | LOSS: 5.760413711998678e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 7/71 | LOSS: 5.757834458108846e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 8/71 | LOSS: 5.576282344716472e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 9/71 | LOSS: 5.5191344472405035e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 10/71 | LOSS: 5.5861451487131e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 11/71 | LOSS: 5.512823842461027e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 12/71 | LOSS: 5.464777656855474e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 13/71 | LOSS: 5.401010899471917e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 14/71 | LOSS: 5.343900981339781e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 15/71 | LOSS: 5.295934045079775e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 16/71 | LOSS: 5.1726063328019606e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 17/71 | LOSS: 5.139027848599653e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 18/71 | LOSS: 5.151273817797406e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 19/71 | LOSS: 5.087262150027527e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 20/71 | LOSS: 5.106697233800549e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 21/71 | LOSS: 5.013014949575088e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 22/71 | LOSS: 5.03517871354985e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 23/71 | LOSS: 5.080547301380041e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 24/71 | LOSS: 5.161440149095142e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 25/71 | LOSS: 5.2102141300845406e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 26/71 | LOSS: 5.2229485037160985e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 27/71 | LOSS: 5.261859428173921e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 28/71 | LOSS: 5.192765354814614e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 29/71 | LOSS: 5.1581465337828074e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 30/71 | LOSS: 5.171618333347837e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 31/71 | LOSS: 5.185430985932271e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 32/71 | LOSS: 5.170135251098669e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 33/71 | LOSS: 5.140625817415746e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 34/71 | LOSS: 5.136503380396919e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 35/71 | LOSS: 5.116205803309033e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 36/71 | LOSS: 5.1253136732427535e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 37/71 | LOSS: 5.138671757489838e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 38/71 | LOSS: 5.1350985079177635e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 39/71 | LOSS: 5.142455694340242e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 40/71 | LOSS: 5.134128830432906e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 41/71 | LOSS: 5.1317135297982275e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 42/71 | LOSS: 5.108389163873399e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 43/71 | LOSS: 5.065597013692075e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 44/71 | LOSS: 5.079115867879914e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 45/71 | LOSS: 5.055637263032385e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 46/71 | LOSS: 5.052438493603791e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 47/71 | LOSS: 5.029904258435636e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 48/71 | LOSS: 5.00846809533256e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 49/71 | LOSS: 4.984547949788975e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 50/71 | LOSS: 4.9705480277599935e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 51/71 | LOSS: 4.9650569648152805e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 52/71 | LOSS: 4.960699509368014e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 53/71 | LOSS: 4.96839394751684e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 54/71 | LOSS: 4.979095493498872e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 55/71 | LOSS: 4.982530567433839e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 56/71 | LOSS: 4.96645356849369e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 57/71 | LOSS: 4.9665981868170565e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 58/71 | LOSS: 4.9764699123427864e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 59/71 | LOSS: 4.9778715037973594e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 60/71 | LOSS: 4.977518685455968e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 61/71 | LOSS: 4.991746847384004e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 62/71 | LOSS: 4.999122666049516e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 63/71 | LOSS: 5.013323992386631e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 64/71 | LOSS: 5.011931051950132e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 65/71 | LOSS: 4.997621359011233e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 66/71 | LOSS: 4.991857283008059e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 67/71 | LOSS: 4.994240545302626e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 68/71 | LOSS: 4.996609883770968e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 69/71 | LOSS: 4.9886381540480735e-06\n",
      "TRAIN: EPOCH 314/1000 | BATCH 70/71 | LOSS: 5.019835235787109e-06\n",
      "VAL: EPOCH 314/1000 | BATCH 0/8 | LOSS: 6.29952410236001e-06\n",
      "VAL: EPOCH 314/1000 | BATCH 1/8 | LOSS: 6.038918172635022e-06\n",
      "VAL: EPOCH 314/1000 | BATCH 2/8 | LOSS: 6.0723412692217e-06\n",
      "VAL: EPOCH 314/1000 | BATCH 3/8 | LOSS: 6.42655948013271e-06\n",
      "VAL: EPOCH 314/1000 | BATCH 4/8 | LOSS: 6.198995470185764e-06\n",
      "VAL: EPOCH 314/1000 | BATCH 5/8 | LOSS: 6.034648549757549e-06\n",
      "VAL: EPOCH 314/1000 | BATCH 6/8 | LOSS: 5.897156110482424e-06\n",
      "VAL: EPOCH 314/1000 | BATCH 7/8 | LOSS: 5.706081765310955e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 0/71 | LOSS: 6.111014954512939e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 1/71 | LOSS: 6.7236078393762e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 2/71 | LOSS: 6.334871613944415e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 3/71 | LOSS: 5.785145845038642e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 4/71 | LOSS: 5.804803913633805e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 5/71 | LOSS: 6.183313416840974e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 6/71 | LOSS: 6.066320695806228e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 7/71 | LOSS: 5.912523420192883e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 8/71 | LOSS: 5.864779773724473e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 9/71 | LOSS: 5.885225618840195e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 10/71 | LOSS: 5.885080471753926e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 11/71 | LOSS: 5.689516342499701e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 12/71 | LOSS: 5.727592311954e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 13/71 | LOSS: 5.691374391452493e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 14/71 | LOSS: 5.603644270498383e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 15/71 | LOSS: 5.5285380256009375e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 16/71 | LOSS: 5.472939659846495e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 17/71 | LOSS: 5.489082695930847e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 18/71 | LOSS: 5.4210613837456455e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 19/71 | LOSS: 5.398340226747677e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 20/71 | LOSS: 5.369843815320305e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 21/71 | LOSS: 5.386466620455559e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 22/71 | LOSS: 5.3726322893572105e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 23/71 | LOSS: 5.422163724233542e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 24/71 | LOSS: 5.404658513725735e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 25/71 | LOSS: 5.390965900285384e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 26/71 | LOSS: 5.354167418166374e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 27/71 | LOSS: 5.3498428574779866e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 28/71 | LOSS: 5.354951616905525e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 29/71 | LOSS: 5.346726766219945e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 30/71 | LOSS: 5.3485401886357594e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 31/71 | LOSS: 5.379667754823458e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 32/71 | LOSS: 5.32933434056538e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 33/71 | LOSS: 5.3245877926896995e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 34/71 | LOSS: 5.306554326775118e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 35/71 | LOSS: 5.274017856512121e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 36/71 | LOSS: 5.2539278570023e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 37/71 | LOSS: 5.2658614142848435e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 38/71 | LOSS: 5.263514396621105e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 39/71 | LOSS: 5.256688632471196e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 40/71 | LOSS: 5.250710584244566e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 41/71 | LOSS: 5.217370146835622e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 42/71 | LOSS: 5.186091849007129e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 43/71 | LOSS: 5.1976393148255404e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 44/71 | LOSS: 5.188746723029504e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 45/71 | LOSS: 5.17763660552547e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 46/71 | LOSS: 5.178033577667133e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 47/71 | LOSS: 5.196029727964439e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 48/71 | LOSS: 5.1853897888094604e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 49/71 | LOSS: 5.180620210012421e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 50/71 | LOSS: 5.20167848698489e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 51/71 | LOSS: 5.1922587457598775e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 52/71 | LOSS: 5.190661006122784e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 53/71 | LOSS: 5.209902139069055e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 54/71 | LOSS: 5.183060321459462e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 55/71 | LOSS: 5.17373313511728e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 56/71 | LOSS: 5.181193820023684e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 57/71 | LOSS: 5.19449513533267e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 58/71 | LOSS: 5.180537917081985e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 59/71 | LOSS: 5.175755237966465e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 60/71 | LOSS: 5.171080314927376e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 61/71 | LOSS: 5.153451492069214e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 62/71 | LOSS: 5.1498353795544795e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 63/71 | LOSS: 5.182718709306755e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 64/71 | LOSS: 5.180484535790478e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 65/71 | LOSS: 5.164240492101244e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 66/71 | LOSS: 5.155599383351049e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 67/71 | LOSS: 5.137134936001744e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 68/71 | LOSS: 5.130443274749163e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 69/71 | LOSS: 5.133087435232093e-06\n",
      "TRAIN: EPOCH 315/1000 | BATCH 70/71 | LOSS: 5.1211006432143695e-06\n",
      "VAL: EPOCH 315/1000 | BATCH 0/8 | LOSS: 7.398637535516173e-06\n",
      "VAL: EPOCH 315/1000 | BATCH 1/8 | LOSS: 7.218568498501554e-06\n",
      "VAL: EPOCH 315/1000 | BATCH 2/8 | LOSS: 7.0669539127266034e-06\n",
      "VAL: EPOCH 315/1000 | BATCH 3/8 | LOSS: 7.138827982089424e-06\n",
      "VAL: EPOCH 315/1000 | BATCH 4/8 | LOSS: 7.044438370940043e-06\n",
      "VAL: EPOCH 315/1000 | BATCH 5/8 | LOSS: 6.704676631367572e-06\n",
      "VAL: EPOCH 315/1000 | BATCH 6/8 | LOSS: 6.66209007249563e-06\n",
      "VAL: EPOCH 315/1000 | BATCH 7/8 | LOSS: 6.495045397514332e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 0/71 | LOSS: 7.46857722333516e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 1/71 | LOSS: 6.0935551573493285e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 2/71 | LOSS: 5.494884135259781e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 3/71 | LOSS: 5.693567345588235e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 4/71 | LOSS: 5.793637683382258e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 5/71 | LOSS: 5.702312364519457e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 6/71 | LOSS: 5.722327971722864e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 7/71 | LOSS: 5.8478929076954955e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 8/71 | LOSS: 5.940412645739141e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 9/71 | LOSS: 5.778471677331254e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 10/71 | LOSS: 5.7833073945155116e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 11/71 | LOSS: 5.860276814928511e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 12/71 | LOSS: 5.763764945116306e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 13/71 | LOSS: 5.69212459465364e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 14/71 | LOSS: 5.723741833207896e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 15/71 | LOSS: 5.701257492773948e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 16/71 | LOSS: 5.674506824374582e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 17/71 | LOSS: 5.674444107878824e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 18/71 | LOSS: 5.6936024329364664e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 19/71 | LOSS: 5.715278825846326e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 20/71 | LOSS: 5.686445417398188e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 21/71 | LOSS: 5.679081554758754e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 22/71 | LOSS: 5.815194546575304e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 23/71 | LOSS: 5.82763408374376e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 24/71 | LOSS: 5.906080641580047e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 25/71 | LOSS: 5.818518686212953e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 26/71 | LOSS: 5.873931522054826e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 27/71 | LOSS: 5.832473862353384e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 28/71 | LOSS: 5.990044577441134e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 29/71 | LOSS: 5.953611973078902e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 30/71 | LOSS: 6.104115050954034e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 31/71 | LOSS: 6.129369062080059e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 32/71 | LOSS: 6.083469853282705e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 33/71 | LOSS: 6.215336691853094e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 34/71 | LOSS: 6.30990296355906e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 35/71 | LOSS: 6.384529209905547e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 36/71 | LOSS: 6.3490272291469535e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 37/71 | LOSS: 6.483341711540752e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 38/71 | LOSS: 6.493980780037535e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 39/71 | LOSS: 6.463932896849655e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 40/71 | LOSS: 6.555807137219079e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 41/71 | LOSS: 6.516177298484156e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 42/71 | LOSS: 6.630153150179594e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 43/71 | LOSS: 6.566134867501784e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 44/71 | LOSS: 6.586306689213314e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 45/71 | LOSS: 6.60210191753045e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 46/71 | LOSS: 6.600529462609321e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 47/71 | LOSS: 6.60309747975892e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 48/71 | LOSS: 6.604597079526034e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 49/71 | LOSS: 6.594810911337845e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 50/71 | LOSS: 6.5436787753795075e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 51/71 | LOSS: 6.5636883863799785e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 52/71 | LOSS: 6.522689648779803e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 53/71 | LOSS: 6.503411138207109e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 54/71 | LOSS: 6.4746389977120664e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 55/71 | LOSS: 6.467722114069017e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 56/71 | LOSS: 6.423094746104764e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 57/71 | LOSS: 6.43387442449176e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 58/71 | LOSS: 6.4049887688439815e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 59/71 | LOSS: 6.410772607523541e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 60/71 | LOSS: 6.3811876614517e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 61/71 | LOSS: 6.3430635280735345e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 62/71 | LOSS: 6.322277313879495e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 63/71 | LOSS: 6.311133326164509e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 64/71 | LOSS: 6.2830490605847444e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 65/71 | LOSS: 6.26195562538421e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 66/71 | LOSS: 6.252911175375738e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 67/71 | LOSS: 6.218034641011293e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 68/71 | LOSS: 6.188567429331749e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 69/71 | LOSS: 6.181093470201761e-06\n",
      "TRAIN: EPOCH 316/1000 | BATCH 70/71 | LOSS: 6.181369717780117e-06\n",
      "VAL: EPOCH 316/1000 | BATCH 0/8 | LOSS: 6.312288860499393e-06\n",
      "VAL: EPOCH 316/1000 | BATCH 1/8 | LOSS: 5.860480769115384e-06\n",
      "VAL: EPOCH 316/1000 | BATCH 2/8 | LOSS: 5.9990851089726975e-06\n",
      "VAL: EPOCH 316/1000 | BATCH 3/8 | LOSS: 5.904866270611819e-06\n",
      "VAL: EPOCH 316/1000 | BATCH 4/8 | LOSS: 5.924260403844528e-06\n",
      "VAL: EPOCH 316/1000 | BATCH 5/8 | LOSS: 5.770529090417161e-06\n",
      "VAL: EPOCH 316/1000 | BATCH 6/8 | LOSS: 5.837118806084618e-06\n",
      "VAL: EPOCH 316/1000 | BATCH 7/8 | LOSS: 5.810896993807546e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 0/71 | LOSS: 6.010000106471125e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 1/71 | LOSS: 6.0655438574030995e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 2/71 | LOSS: 5.309386779117631e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 3/71 | LOSS: 5.253430344964727e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 4/71 | LOSS: 5.359814713301602e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 5/71 | LOSS: 5.053883948373065e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 6/71 | LOSS: 5.117469169947851e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 7/71 | LOSS: 5.228055897532613e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 8/71 | LOSS: 5.090479034050885e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 9/71 | LOSS: 5.266855441732332e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 10/71 | LOSS: 5.169179447958331e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 11/71 | LOSS: 5.063968994060512e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 12/71 | LOSS: 5.014130378045733e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 13/71 | LOSS: 5.0471309610397185e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 14/71 | LOSS: 4.985116205110292e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 15/71 | LOSS: 4.9422801282617e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 16/71 | LOSS: 4.971035383170922e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 17/71 | LOSS: 4.963327455698163e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 18/71 | LOSS: 4.984151952445054e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 19/71 | LOSS: 4.92980323087977e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 20/71 | LOSS: 4.904813609062417e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 21/71 | LOSS: 4.8637521103955805e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 22/71 | LOSS: 4.8203663264639145e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 23/71 | LOSS: 4.784217784011465e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 24/71 | LOSS: 4.7957694550859744e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 25/71 | LOSS: 4.763006262902099e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 26/71 | LOSS: 4.758548331867334e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 27/71 | LOSS: 4.710530747940668e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 28/71 | LOSS: 4.662022476999294e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 29/71 | LOSS: 4.641974032892904e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 30/71 | LOSS: 4.618417330176829e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 31/71 | LOSS: 4.581012753135383e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 32/71 | LOSS: 4.592684414698549e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 33/71 | LOSS: 4.576951081456654e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 34/71 | LOSS: 4.56945641807189e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 35/71 | LOSS: 4.595604227435817e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 36/71 | LOSS: 4.6122552985990125e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 37/71 | LOSS: 4.597104880064619e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 38/71 | LOSS: 4.627878793027589e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 39/71 | LOSS: 4.639161016939397e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 40/71 | LOSS: 4.642728820882195e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 41/71 | LOSS: 4.6375253889761685e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 42/71 | LOSS: 4.659400532503685e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 43/71 | LOSS: 4.666169362818354e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 44/71 | LOSS: 4.6654890613556036e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 45/71 | LOSS: 4.675636580981044e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 46/71 | LOSS: 4.690549937375645e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 47/71 | LOSS: 4.674777249154734e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 48/71 | LOSS: 4.690186521972408e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 49/71 | LOSS: 4.687444211413094e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 50/71 | LOSS: 4.678963183751501e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 51/71 | LOSS: 4.669543548411289e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 52/71 | LOSS: 4.659333896752115e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 53/71 | LOSS: 4.66156056215065e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 54/71 | LOSS: 4.659999207102704e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 55/71 | LOSS: 4.645665825364631e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 56/71 | LOSS: 4.6543937099940785e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 57/71 | LOSS: 4.6409412405033625e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 58/71 | LOSS: 4.635348451471546e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 59/71 | LOSS: 4.63818035238243e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 60/71 | LOSS: 4.642484271721141e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 61/71 | LOSS: 4.635050055596014e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 62/71 | LOSS: 4.643568882019036e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 63/71 | LOSS: 4.6625147795964494e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 64/71 | LOSS: 4.663112972244003e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 65/71 | LOSS: 4.655601628655859e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 66/71 | LOSS: 4.6590044403002586e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 67/71 | LOSS: 4.6697646044937276e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 68/71 | LOSS: 4.676705874890499e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 69/71 | LOSS: 4.689867484687836e-06\n",
      "TRAIN: EPOCH 317/1000 | BATCH 70/71 | LOSS: 4.716667330234178e-06\n",
      "VAL: EPOCH 317/1000 | BATCH 0/8 | LOSS: 6.209123512235237e-06\n",
      "VAL: EPOCH 317/1000 | BATCH 1/8 | LOSS: 5.603123327091453e-06\n",
      "VAL: EPOCH 317/1000 | BATCH 2/8 | LOSS: 5.520969655966231e-06\n",
      "VAL: EPOCH 317/1000 | BATCH 3/8 | LOSS: 5.537825586543477e-06\n",
      "VAL: EPOCH 317/1000 | BATCH 4/8 | LOSS: 5.389378293330082e-06\n",
      "VAL: EPOCH 317/1000 | BATCH 5/8 | LOSS: 5.056638277286159e-06\n",
      "VAL: EPOCH 317/1000 | BATCH 6/8 | LOSS: 4.907830836080913e-06\n",
      "VAL: EPOCH 317/1000 | BATCH 7/8 | LOSS: 4.770384038010889e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 0/71 | LOSS: 3.3204023566213436e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 1/71 | LOSS: 5.4827701205795165e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 2/71 | LOSS: 5.32841810733468e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 3/71 | LOSS: 5.90547301726474e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 4/71 | LOSS: 5.9271442296449095e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 5/71 | LOSS: 5.86444154275038e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 6/71 | LOSS: 5.890939389376269e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 7/71 | LOSS: 5.790524596704927e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 8/71 | LOSS: 5.764178341552098e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 9/71 | LOSS: 5.731692817789736e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 10/71 | LOSS: 5.857218308268454e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 11/71 | LOSS: 5.776442359698801e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 12/71 | LOSS: 5.806741018414658e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 13/71 | LOSS: 5.850065885429753e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 14/71 | LOSS: 5.789382400204583e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 15/71 | LOSS: 5.932390592988668e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 16/71 | LOSS: 5.8430469460504625e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 17/71 | LOSS: 5.852156669384891e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 18/71 | LOSS: 5.747395957051164e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 19/71 | LOSS: 5.884766915187356e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 20/71 | LOSS: 5.92863532455383e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 21/71 | LOSS: 6.072113013612e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 22/71 | LOSS: 5.9848938215746665e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 23/71 | LOSS: 6.011835675205172e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 24/71 | LOSS: 5.942772932030494e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 25/71 | LOSS: 5.871014624202731e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 26/71 | LOSS: 5.857713202001631e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 27/71 | LOSS: 5.862953539040713e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 28/71 | LOSS: 5.843119793255404e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 29/71 | LOSS: 5.792750986681009e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 30/71 | LOSS: 5.731036312241603e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 31/71 | LOSS: 5.698459204950268e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 32/71 | LOSS: 5.663947376888245e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 33/71 | LOSS: 5.62771206472569e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 34/71 | LOSS: 5.6147147136341246e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 35/71 | LOSS: 5.555492685339737e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 36/71 | LOSS: 5.50467201891651e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 37/71 | LOSS: 5.445970869962031e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 38/71 | LOSS: 5.432880669057993e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 39/71 | LOSS: 5.419297070829998e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 40/71 | LOSS: 5.420858681135564e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 41/71 | LOSS: 5.421816224173069e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 42/71 | LOSS: 5.395325520429503e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 43/71 | LOSS: 5.38616060132633e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 44/71 | LOSS: 5.346605308255271e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 45/71 | LOSS: 5.340665364636389e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 46/71 | LOSS: 5.310253609809963e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 47/71 | LOSS: 5.303509979626142e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 48/71 | LOSS: 5.292810138431675e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 49/71 | LOSS: 5.297251832416805e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 50/71 | LOSS: 5.261646532851634e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 51/71 | LOSS: 5.259537273944331e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 52/71 | LOSS: 5.249609145875921e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 53/71 | LOSS: 5.24361797084162e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 54/71 | LOSS: 5.240609991894665e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 55/71 | LOSS: 5.234998894820819e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 56/71 | LOSS: 5.223508567628175e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 57/71 | LOSS: 5.204459335831855e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 58/71 | LOSS: 5.189160516484832e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 59/71 | LOSS: 5.191069722817095e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 60/71 | LOSS: 5.191789020992459e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 61/71 | LOSS: 5.184636822172427e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 62/71 | LOSS: 5.191798205516303e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 63/71 | LOSS: 5.194903852867583e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 64/71 | LOSS: 5.195800318128581e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 65/71 | LOSS: 5.179111381590756e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 66/71 | LOSS: 5.163036589624765e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 67/71 | LOSS: 5.174122517313235e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 68/71 | LOSS: 5.157019866574969e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 69/71 | LOSS: 5.187858840015127e-06\n",
      "TRAIN: EPOCH 318/1000 | BATCH 70/71 | LOSS: 5.15642054010577e-06\n",
      "VAL: EPOCH 318/1000 | BATCH 0/8 | LOSS: 8.154026545525994e-06\n",
      "VAL: EPOCH 318/1000 | BATCH 1/8 | LOSS: 7.5284424383426085e-06\n",
      "VAL: EPOCH 318/1000 | BATCH 2/8 | LOSS: 7.262387346903172e-06\n",
      "VAL: EPOCH 318/1000 | BATCH 3/8 | LOSS: 7.298959644685965e-06\n",
      "VAL: EPOCH 318/1000 | BATCH 4/8 | LOSS: 7.121451835701009e-06\n",
      "VAL: EPOCH 318/1000 | BATCH 5/8 | LOSS: 6.698739601536848e-06\n",
      "VAL: EPOCH 318/1000 | BATCH 6/8 | LOSS: 6.630958783456922e-06\n",
      "VAL: EPOCH 318/1000 | BATCH 7/8 | LOSS: 6.385400183717138e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 0/71 | LOSS: 6.1348200688371435e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 1/71 | LOSS: 5.316794613463571e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 2/71 | LOSS: 5.393213844702889e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 3/71 | LOSS: 5.2462429493971285e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 4/71 | LOSS: 4.991580499336123e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 5/71 | LOSS: 4.90557082836555e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 6/71 | LOSS: 4.749829713546205e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 7/71 | LOSS: 4.711370650056779e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 8/71 | LOSS: 4.712366464648059e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 9/71 | LOSS: 4.73342161058099e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 10/71 | LOSS: 4.743497067300433e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 11/71 | LOSS: 4.771762026697009e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 12/71 | LOSS: 4.786299261748074e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 13/71 | LOSS: 4.740746979743042e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 14/71 | LOSS: 4.739556182660939e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 15/71 | LOSS: 4.75118821441356e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 16/71 | LOSS: 4.733846360656625e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 17/71 | LOSS: 4.8237519776092895e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 18/71 | LOSS: 4.783634458841621e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 19/71 | LOSS: 4.696332712228468e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 20/71 | LOSS: 4.686949913385823e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 21/71 | LOSS: 4.72655101062132e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 22/71 | LOSS: 4.744640393809e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 23/71 | LOSS: 4.748741105231602e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 24/71 | LOSS: 4.697501772170654e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 25/71 | LOSS: 4.736932239141494e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 26/71 | LOSS: 4.784658846835589e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 27/71 | LOSS: 4.835974907889197e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 28/71 | LOSS: 4.870777800157345e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 29/71 | LOSS: 4.8809584465440516e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 30/71 | LOSS: 5.156910828513754e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 31/71 | LOSS: 5.123007085217068e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 32/71 | LOSS: 5.2931428730186205e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 33/71 | LOSS: 5.2882996763347524e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 34/71 | LOSS: 5.367886491772619e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 35/71 | LOSS: 5.3969455267280055e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 36/71 | LOSS: 5.449576267867972e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 37/71 | LOSS: 5.519308555640716e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 38/71 | LOSS: 5.517203881493352e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 39/71 | LOSS: 5.598329630629451e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 40/71 | LOSS: 5.5949883461857455e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 41/71 | LOSS: 5.664720804686242e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 42/71 | LOSS: 5.651450486089574e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 43/71 | LOSS: 5.702561369699569e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 44/71 | LOSS: 5.703740286359486e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 45/71 | LOSS: 5.702077606142468e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 46/71 | LOSS: 5.731495209747276e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 47/71 | LOSS: 5.730044250640276e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 48/71 | LOSS: 5.715364184244227e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 49/71 | LOSS: 5.708393282475299e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 50/71 | LOSS: 5.69064266920916e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 51/71 | LOSS: 5.652412242467947e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 52/71 | LOSS: 5.639587526566861e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 53/71 | LOSS: 5.629090028209989e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 54/71 | LOSS: 5.6099931705168935e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 55/71 | LOSS: 5.608644735437858e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 56/71 | LOSS: 5.601699194014879e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 57/71 | LOSS: 5.623274911033212e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 58/71 | LOSS: 5.6010273811360525e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 59/71 | LOSS: 5.615414742502859e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 60/71 | LOSS: 5.608942810944367e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 61/71 | LOSS: 5.593491378462087e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 62/71 | LOSS: 5.59501868987614e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 63/71 | LOSS: 5.5886822671880054e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 64/71 | LOSS: 5.595657459987426e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 65/71 | LOSS: 5.5679177483563596e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 66/71 | LOSS: 5.573421361335552e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 67/71 | LOSS: 5.5731180636672325e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 68/71 | LOSS: 5.551800534479865e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 69/71 | LOSS: 5.539884643569946e-06\n",
      "TRAIN: EPOCH 319/1000 | BATCH 70/71 | LOSS: 5.518389798453427e-06\n",
      "VAL: EPOCH 319/1000 | BATCH 0/8 | LOSS: 5.4734059631300624e-06\n",
      "VAL: EPOCH 319/1000 | BATCH 1/8 | LOSS: 5.031635964769521e-06\n",
      "VAL: EPOCH 319/1000 | BATCH 2/8 | LOSS: 4.925152855624522e-06\n",
      "VAL: EPOCH 319/1000 | BATCH 3/8 | LOSS: 5.061137358097767e-06\n",
      "VAL: EPOCH 319/1000 | BATCH 4/8 | LOSS: 4.898895076621556e-06\n",
      "VAL: EPOCH 319/1000 | BATCH 5/8 | LOSS: 4.6408346558261355e-06\n",
      "VAL: EPOCH 319/1000 | BATCH 6/8 | LOSS: 4.541222811634985e-06\n",
      "VAL: EPOCH 319/1000 | BATCH 7/8 | LOSS: 4.4029473826867616e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 0/71 | LOSS: 4.267631993570831e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 1/71 | LOSS: 4.116732043257798e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 2/71 | LOSS: 3.9718152796316035e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 3/71 | LOSS: 4.187846627701219e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 4/71 | LOSS: 4.36693849223957e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 5/71 | LOSS: 4.371236210924205e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 6/71 | LOSS: 4.3222131418068396e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 7/71 | LOSS: 4.278232523802217e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 8/71 | LOSS: 4.3235162239499105e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 9/71 | LOSS: 4.273683703104325e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 10/71 | LOSS: 4.253172040659013e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 11/71 | LOSS: 4.2065059346896305e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 12/71 | LOSS: 4.194668428261559e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 13/71 | LOSS: 4.2265704678357e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 14/71 | LOSS: 4.267437043381506e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 15/71 | LOSS: 4.272415694117626e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 16/71 | LOSS: 4.300449304269825e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 17/71 | LOSS: 4.305954171791705e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 18/71 | LOSS: 4.40186885211197e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 19/71 | LOSS: 4.407845597143023e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 20/71 | LOSS: 4.430774478827343e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 21/71 | LOSS: 4.4878206091942214e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 22/71 | LOSS: 4.517575758733943e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 23/71 | LOSS: 4.514072533841802e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 24/71 | LOSS: 4.52077943009499e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 25/71 | LOSS: 4.536397061481413e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 26/71 | LOSS: 4.523275501250518e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 27/71 | LOSS: 4.522004579549892e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 28/71 | LOSS: 4.5056125020045294e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 29/71 | LOSS: 4.512636595184934e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 30/71 | LOSS: 4.507751335099343e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 31/71 | LOSS: 4.508562405192151e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 32/71 | LOSS: 4.506892523612336e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 33/71 | LOSS: 4.515153872328965e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 34/71 | LOSS: 4.517890069369709e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 35/71 | LOSS: 4.517641204731464e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 36/71 | LOSS: 4.534009773320773e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 37/71 | LOSS: 4.520501611774594e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 38/71 | LOSS: 4.532378724410918e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 39/71 | LOSS: 4.5247136256421076e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 40/71 | LOSS: 4.561102691806765e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 41/71 | LOSS: 4.563393970004524e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 42/71 | LOSS: 4.595799377649348e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 43/71 | LOSS: 4.6004277785207455e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 44/71 | LOSS: 4.61185065129636e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 45/71 | LOSS: 4.6085575802309275e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 46/71 | LOSS: 4.633170797109066e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 47/71 | LOSS: 4.622355935642493e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 48/71 | LOSS: 4.627994754553918e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 49/71 | LOSS: 4.628632691492385e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 50/71 | LOSS: 4.605911659810397e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 51/71 | LOSS: 4.602940787907965e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 52/71 | LOSS: 4.604461773336845e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 53/71 | LOSS: 4.6060011561200584e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 54/71 | LOSS: 4.594804741745148e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 55/71 | LOSS: 4.6120433653738606e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 56/71 | LOSS: 4.6019416347183305e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 57/71 | LOSS: 4.606352757130888e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 58/71 | LOSS: 4.613991010909697e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 59/71 | LOSS: 4.608545248174778e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 60/71 | LOSS: 4.618228905335437e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 61/71 | LOSS: 4.629621457752364e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 62/71 | LOSS: 4.6242559713550326e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 63/71 | LOSS: 4.645149378035285e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 64/71 | LOSS: 4.6477440369366936e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 65/71 | LOSS: 4.670074576317733e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 66/71 | LOSS: 4.658779204616092e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 67/71 | LOSS: 4.654498602175248e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 68/71 | LOSS: 4.65135947021169e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 69/71 | LOSS: 4.652930078918871e-06\n",
      "TRAIN: EPOCH 320/1000 | BATCH 70/71 | LOSS: 4.63943805198782e-06\n",
      "VAL: EPOCH 320/1000 | BATCH 0/8 | LOSS: 6.837936780357268e-06\n",
      "VAL: EPOCH 320/1000 | BATCH 1/8 | LOSS: 6.228756319615059e-06\n",
      "VAL: EPOCH 320/1000 | BATCH 2/8 | LOSS: 5.9613976191030815e-06\n",
      "VAL: EPOCH 320/1000 | BATCH 3/8 | LOSS: 5.97050757278339e-06\n",
      "VAL: EPOCH 320/1000 | BATCH 4/8 | LOSS: 5.838790639245417e-06\n",
      "VAL: EPOCH 320/1000 | BATCH 5/8 | LOSS: 5.446359333897514e-06\n",
      "VAL: EPOCH 320/1000 | BATCH 6/8 | LOSS: 5.387629698816454e-06\n",
      "VAL: EPOCH 320/1000 | BATCH 7/8 | LOSS: 5.246853845619626e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 0/71 | LOSS: 4.776214609591989e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 1/71 | LOSS: 4.188649540992628e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 2/71 | LOSS: 4.426328132467461e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 3/71 | LOSS: 4.535591585863585e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 4/71 | LOSS: 4.582161045618704e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 5/71 | LOSS: 4.916851177464802e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 6/71 | LOSS: 4.9375659533065374e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 7/71 | LOSS: 5.008097701875158e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 8/71 | LOSS: 4.942347080335215e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 9/71 | LOSS: 4.848856792705192e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 10/71 | LOSS: 4.841244907021808e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 11/71 | LOSS: 5.004040663910321e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 12/71 | LOSS: 4.94997981639664e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 13/71 | LOSS: 4.984103903942534e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 14/71 | LOSS: 4.938244531634458e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 15/71 | LOSS: 5.165999837686286e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 16/71 | LOSS: 5.1669785400864544e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 17/71 | LOSS: 5.394843798159046e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 18/71 | LOSS: 5.412347718447563e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 19/71 | LOSS: 5.326352413703716e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 20/71 | LOSS: 5.338832417172463e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 21/71 | LOSS: 5.3904570387947555e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 22/71 | LOSS: 5.380922541165922e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 23/71 | LOSS: 5.36970802045289e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 24/71 | LOSS: 5.382826830100385e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 25/71 | LOSS: 5.396498715107065e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 26/71 | LOSS: 5.387858445155197e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 27/71 | LOSS: 5.414273808972732e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 28/71 | LOSS: 5.356469822298346e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 29/71 | LOSS: 5.332247163399492e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 30/71 | LOSS: 5.300469278916998e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 31/71 | LOSS: 5.293642992398873e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 32/71 | LOSS: 5.300150054278095e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 33/71 | LOSS: 5.278426596412507e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 34/71 | LOSS: 5.263528425660167e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 35/71 | LOSS: 5.228899245442638e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 36/71 | LOSS: 5.250203211953458e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 37/71 | LOSS: 5.2514257049551816e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 38/71 | LOSS: 5.250386511061851e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 39/71 | LOSS: 5.218925008421138e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 40/71 | LOSS: 5.214306500464573e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 41/71 | LOSS: 5.195859054245895e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 42/71 | LOSS: 5.2061330670978855e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 43/71 | LOSS: 5.221547630960479e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 44/71 | LOSS: 5.213656494460237e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 45/71 | LOSS: 5.194649084525894e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 46/71 | LOSS: 5.171025683940822e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 47/71 | LOSS: 5.1733604446250565e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 48/71 | LOSS: 5.178001095614769e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 49/71 | LOSS: 5.161860012776743e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 50/71 | LOSS: 5.167372636623993e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 51/71 | LOSS: 5.1429771238312805e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 52/71 | LOSS: 5.127009302977624e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 53/71 | LOSS: 5.13144756395276e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 54/71 | LOSS: 5.143847072924307e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 55/71 | LOSS: 5.140491781828262e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 56/71 | LOSS: 5.122715524601262e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 57/71 | LOSS: 5.130608311164592e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 58/71 | LOSS: 5.119420069666191e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 59/71 | LOSS: 5.128659552156023e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 60/71 | LOSS: 5.106169130128558e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 61/71 | LOSS: 5.115866319131108e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 62/71 | LOSS: 5.127389212724492e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 63/71 | LOSS: 5.12144187680974e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 64/71 | LOSS: 5.104568107036847e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 65/71 | LOSS: 5.102398521364993e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 66/71 | LOSS: 5.098887993935661e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 67/71 | LOSS: 5.097476556623887e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 68/71 | LOSS: 5.079925869353431e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 69/71 | LOSS: 5.061258035051911e-06\n",
      "TRAIN: EPOCH 321/1000 | BATCH 70/71 | LOSS: 5.063614688763882e-06\n",
      "VAL: EPOCH 321/1000 | BATCH 0/8 | LOSS: 5.460825377667788e-06\n",
      "VAL: EPOCH 321/1000 | BATCH 1/8 | LOSS: 5.149928483660915e-06\n",
      "VAL: EPOCH 321/1000 | BATCH 2/8 | LOSS: 5.088160984693483e-06\n",
      "VAL: EPOCH 321/1000 | BATCH 3/8 | LOSS: 5.198553139962314e-06\n",
      "VAL: EPOCH 321/1000 | BATCH 4/8 | LOSS: 5.089584465167718e-06\n",
      "VAL: EPOCH 321/1000 | BATCH 5/8 | LOSS: 4.826034266140293e-06\n",
      "VAL: EPOCH 321/1000 | BATCH 6/8 | LOSS: 4.779625442097313e-06\n",
      "VAL: EPOCH 321/1000 | BATCH 7/8 | LOSS: 4.651030366176201e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 0/71 | LOSS: 3.55616816705151e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 1/71 | LOSS: 4.74876367206889e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 2/71 | LOSS: 4.742188972765386e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 3/71 | LOSS: 4.886418707883422e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 4/71 | LOSS: 4.856879741055309e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 5/71 | LOSS: 4.930929397535995e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 6/71 | LOSS: 4.906368076262879e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 7/71 | LOSS: 4.833081987953847e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 8/71 | LOSS: 4.843495364386601e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 9/71 | LOSS: 4.866304038841918e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 10/71 | LOSS: 4.8035520566455405e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 11/71 | LOSS: 4.870170850305537e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 12/71 | LOSS: 4.904191515519275e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 13/71 | LOSS: 4.867051643486775e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 14/71 | LOSS: 4.900636561918267e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 15/71 | LOSS: 4.885805779508701e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 16/71 | LOSS: 4.934992282013648e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 17/71 | LOSS: 4.887898803139655e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 18/71 | LOSS: 4.896966438058084e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 19/71 | LOSS: 4.869214774316788e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 20/71 | LOSS: 4.921821105848981e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 21/71 | LOSS: 4.904744245819033e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 22/71 | LOSS: 4.944533400149616e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 23/71 | LOSS: 4.873403526062248e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 24/71 | LOSS: 4.905396408503293e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 25/71 | LOSS: 4.895205244373266e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 26/71 | LOSS: 4.911491815720284e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 27/71 | LOSS: 4.954245915606796e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 28/71 | LOSS: 5.002173244567279e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 29/71 | LOSS: 4.971430818538162e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 30/71 | LOSS: 4.973594976094788e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 31/71 | LOSS: 5.011914844033072e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 32/71 | LOSS: 4.99374025573409e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 33/71 | LOSS: 4.966721643112953e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 34/71 | LOSS: 4.9890613744147625e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 35/71 | LOSS: 4.972268552844778e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 36/71 | LOSS: 5.00547519771538e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 37/71 | LOSS: 4.970405287071787e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 38/71 | LOSS: 5.004956794716865e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 39/71 | LOSS: 4.983165433714021e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 40/71 | LOSS: 4.993029384289602e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 41/71 | LOSS: 4.981155987999435e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 42/71 | LOSS: 4.993740583094497e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 43/71 | LOSS: 4.976407870510064e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 44/71 | LOSS: 4.9803850743046494e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 45/71 | LOSS: 4.997251805790533e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 46/71 | LOSS: 5.007656048466278e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 47/71 | LOSS: 5.0319295136584214e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 48/71 | LOSS: 5.033948897961432e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 49/71 | LOSS: 5.0509779066487685e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 50/71 | LOSS: 5.058206500275535e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 51/71 | LOSS: 5.035130960515324e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 52/71 | LOSS: 5.034079463858123e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 53/71 | LOSS: 5.038091162364084e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 54/71 | LOSS: 5.091098567142828e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 55/71 | LOSS: 5.081591967025036e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 56/71 | LOSS: 5.087359504454582e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 57/71 | LOSS: 5.0849281282792394e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 58/71 | LOSS: 5.071795200465357e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 59/71 | LOSS: 5.072574867881485e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 60/71 | LOSS: 5.0720921938248424e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 61/71 | LOSS: 5.063259014869184e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 62/71 | LOSS: 5.064326831502729e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 63/71 | LOSS: 5.060273906565271e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 64/71 | LOSS: 5.073670161856661e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 65/71 | LOSS: 5.067903340673321e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 66/71 | LOSS: 5.0686329162619666e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 67/71 | LOSS: 5.059308966123995e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 68/71 | LOSS: 5.068748255259867e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 69/71 | LOSS: 5.057309051283353e-06\n",
      "TRAIN: EPOCH 322/1000 | BATCH 70/71 | LOSS: 5.053368042840894e-06\n",
      "VAL: EPOCH 322/1000 | BATCH 0/8 | LOSS: 5.878220235899789e-06\n",
      "VAL: EPOCH 322/1000 | BATCH 1/8 | LOSS: 5.606883405562257e-06\n",
      "VAL: EPOCH 322/1000 | BATCH 2/8 | LOSS: 5.366806211289561e-06\n",
      "VAL: EPOCH 322/1000 | BATCH 3/8 | LOSS: 5.424021424005332e-06\n",
      "VAL: EPOCH 322/1000 | BATCH 4/8 | LOSS: 5.302100817061728e-06\n",
      "VAL: EPOCH 322/1000 | BATCH 5/8 | LOSS: 5.016522436562809e-06\n",
      "VAL: EPOCH 322/1000 | BATCH 6/8 | LOSS: 4.8950980401839066e-06\n",
      "VAL: EPOCH 322/1000 | BATCH 7/8 | LOSS: 4.724566508684802e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 0/71 | LOSS: 4.460802301764488e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 1/71 | LOSS: 4.516104127105791e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 2/71 | LOSS: 4.7463522605539765e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 3/71 | LOSS: 4.820402523364464e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 4/71 | LOSS: 4.8699624130676964e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 5/71 | LOSS: 4.964382393760995e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 6/71 | LOSS: 4.9788332034950145e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 7/71 | LOSS: 4.909521919671533e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 8/71 | LOSS: 5.0960541860452695e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 9/71 | LOSS: 5.089779187983368e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 10/71 | LOSS: 5.122773886383087e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 11/71 | LOSS: 5.1993334485208225e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 12/71 | LOSS: 5.278063816569136e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 13/71 | LOSS: 5.314228863945962e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 14/71 | LOSS: 5.3017575434447885e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 15/71 | LOSS: 5.329438494072747e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 16/71 | LOSS: 5.282055125837146e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 17/71 | LOSS: 5.258009398150736e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 18/71 | LOSS: 5.244774600118705e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 19/71 | LOSS: 5.2263025281718e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 20/71 | LOSS: 5.2422547014430165e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 21/71 | LOSS: 5.208366432766938e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 22/71 | LOSS: 5.236535581270405e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 23/71 | LOSS: 5.171840863719505e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 24/71 | LOSS: 5.179014879104216e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 25/71 | LOSS: 5.177342210607514e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 26/71 | LOSS: 5.2148339414593974e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 27/71 | LOSS: 5.221795699331519e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 28/71 | LOSS: 5.190641472993242e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 29/71 | LOSS: 5.208950278756675e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 30/71 | LOSS: 5.196744906800543e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 31/71 | LOSS: 5.241301025193934e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 32/71 | LOSS: 5.227379171463491e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 33/71 | LOSS: 5.252030164496425e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 34/71 | LOSS: 5.241629644712833e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 35/71 | LOSS: 5.259337437261517e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 36/71 | LOSS: 5.261037463119342e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 37/71 | LOSS: 5.212677784153416e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 38/71 | LOSS: 5.198325378585604e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 39/71 | LOSS: 5.229993752209338e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 40/71 | LOSS: 5.289610443491611e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 41/71 | LOSS: 5.281257531784095e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 42/71 | LOSS: 5.285828534439599e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 43/71 | LOSS: 5.314930278514525e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 44/71 | LOSS: 5.295326875259181e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 45/71 | LOSS: 5.2922657242442766e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 46/71 | LOSS: 5.313678708803986e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 47/71 | LOSS: 5.32564353742752e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 48/71 | LOSS: 5.301020620413046e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 49/71 | LOSS: 5.303484472278796e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 50/71 | LOSS: 5.2864354440281005e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 51/71 | LOSS: 5.2861884245640585e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 52/71 | LOSS: 5.245408275649819e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 53/71 | LOSS: 5.257939514430527e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 54/71 | LOSS: 5.2334763371287736e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 55/71 | LOSS: 5.225104623767558e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 56/71 | LOSS: 5.22889355013097e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 57/71 | LOSS: 5.222869860737209e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 58/71 | LOSS: 5.201149290661629e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 59/71 | LOSS: 5.208975990929806e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 60/71 | LOSS: 5.2179317156172105e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 61/71 | LOSS: 5.213421779385939e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 62/71 | LOSS: 5.221981093690995e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 63/71 | LOSS: 5.22322173068801e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 64/71 | LOSS: 5.240583018698649e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 65/71 | LOSS: 5.243562461807662e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 66/71 | LOSS: 5.244715247730466e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 67/71 | LOSS: 5.26316118593433e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 68/71 | LOSS: 5.238750856628083e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 69/71 | LOSS: 5.248130456233671e-06\n",
      "TRAIN: EPOCH 323/1000 | BATCH 70/71 | LOSS: 5.228355569631955e-06\n",
      "VAL: EPOCH 323/1000 | BATCH 0/8 | LOSS: 7.564941370219458e-06\n",
      "VAL: EPOCH 323/1000 | BATCH 1/8 | LOSS: 7.015425353529281e-06\n",
      "VAL: EPOCH 323/1000 | BATCH 2/8 | LOSS: 6.602653077910266e-06\n",
      "VAL: EPOCH 323/1000 | BATCH 3/8 | LOSS: 6.80243772421818e-06\n",
      "VAL: EPOCH 323/1000 | BATCH 4/8 | LOSS: 6.573473274329444e-06\n",
      "VAL: EPOCH 323/1000 | BATCH 5/8 | LOSS: 6.266301018816496e-06\n",
      "VAL: EPOCH 323/1000 | BATCH 6/8 | LOSS: 6.1063370334782774e-06\n",
      "VAL: EPOCH 323/1000 | BATCH 7/8 | LOSS: 5.831674400269549e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 0/71 | LOSS: 5.514403710549232e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 1/71 | LOSS: 5.0222045047121355e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 2/71 | LOSS: 4.828805382809757e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 3/71 | LOSS: 4.736182972919778e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 4/71 | LOSS: 4.769458428199869e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 5/71 | LOSS: 4.788715993223984e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 6/71 | LOSS: 4.741235833145245e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 7/71 | LOSS: 4.790528237208491e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 8/71 | LOSS: 4.8497242662253684e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 9/71 | LOSS: 4.9059440243581776e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 10/71 | LOSS: 4.899544736955167e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 11/71 | LOSS: 4.857626549892302e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 12/71 | LOSS: 5.195383367278667e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 13/71 | LOSS: 5.175662246464137e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 14/71 | LOSS: 5.1537148162121104e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 15/71 | LOSS: 5.1585886069460685e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 16/71 | LOSS: 5.121088473542306e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 17/71 | LOSS: 5.121672908596035e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 18/71 | LOSS: 5.0885997600409225e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 19/71 | LOSS: 5.117871432958055e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 20/71 | LOSS: 5.132861739790921e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 21/71 | LOSS: 5.13910971345798e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 22/71 | LOSS: 5.157574061899061e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 23/71 | LOSS: 5.199773719747706e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 24/71 | LOSS: 5.192319476918783e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 25/71 | LOSS: 5.190363236369404e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 26/71 | LOSS: 5.139313114139239e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 27/71 | LOSS: 5.1471887039562195e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 28/71 | LOSS: 5.216753355978591e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 29/71 | LOSS: 5.215715259510034e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 30/71 | LOSS: 5.212422417072606e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 31/71 | LOSS: 5.245344077309255e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 32/71 | LOSS: 5.295262801816782e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 33/71 | LOSS: 5.289095089638602e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 34/71 | LOSS: 5.3336015103663416e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 35/71 | LOSS: 5.336648522794955e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 36/71 | LOSS: 5.414385244679383e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 37/71 | LOSS: 5.385552502670371e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 38/71 | LOSS: 5.382827059572894e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 39/71 | LOSS: 5.410494526358889e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 40/71 | LOSS: 5.416870247400522e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 41/71 | LOSS: 5.446718838746685e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 42/71 | LOSS: 5.46130130249705e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 43/71 | LOSS: 5.461176096394576e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 44/71 | LOSS: 5.450436335902648e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 45/71 | LOSS: 5.4913278182025156e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 46/71 | LOSS: 5.492375047400314e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 47/71 | LOSS: 5.492759152995556e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 48/71 | LOSS: 5.480881364579842e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 49/71 | LOSS: 5.458753962557239e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 50/71 | LOSS: 5.504461573675108e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 51/71 | LOSS: 5.492005212860162e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 52/71 | LOSS: 5.520480658131693e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 53/71 | LOSS: 5.5315953962538815e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 54/71 | LOSS: 5.538599771170993e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 55/71 | LOSS: 5.5250105585206e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 56/71 | LOSS: 5.527350539156478e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 57/71 | LOSS: 5.511470256915972e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 58/71 | LOSS: 5.500003075890857e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 59/71 | LOSS: 5.478204241171625e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 60/71 | LOSS: 5.480662161599106e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 61/71 | LOSS: 5.475362773986783e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 62/71 | LOSS: 5.471777588657763e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 63/71 | LOSS: 5.474848325803805e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 64/71 | LOSS: 5.482595997818862e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 65/71 | LOSS: 5.472770698306152e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 66/71 | LOSS: 5.481914269096963e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 67/71 | LOSS: 5.488605649241115e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 68/71 | LOSS: 5.479471597488779e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 69/71 | LOSS: 5.495260571868649e-06\n",
      "TRAIN: EPOCH 324/1000 | BATCH 70/71 | LOSS: 5.511655124689536e-06\n",
      "VAL: EPOCH 324/1000 | BATCH 0/8 | LOSS: 9.323551239504013e-06\n",
      "VAL: EPOCH 324/1000 | BATCH 1/8 | LOSS: 8.653119493828854e-06\n",
      "VAL: EPOCH 324/1000 | BATCH 2/8 | LOSS: 8.334054655279033e-06\n",
      "VAL: EPOCH 324/1000 | BATCH 3/8 | LOSS: 8.59142028275528e-06\n",
      "VAL: EPOCH 324/1000 | BATCH 4/8 | LOSS: 8.256510773207992e-06\n",
      "VAL: EPOCH 324/1000 | BATCH 5/8 | LOSS: 7.848096174711827e-06\n",
      "VAL: EPOCH 324/1000 | BATCH 6/8 | LOSS: 7.761403984269627e-06\n",
      "VAL: EPOCH 324/1000 | BATCH 7/8 | LOSS: 7.485570620247017e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 0/71 | LOSS: 6.0340298659866676e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 1/71 | LOSS: 5.453779749586829e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 2/71 | LOSS: 6.015038403954047e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 3/71 | LOSS: 5.572639111051103e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 4/71 | LOSS: 5.9966405387967825e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 5/71 | LOSS: 5.779643212614853e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 6/71 | LOSS: 5.879879868838803e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 7/71 | LOSS: 5.777063336154242e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 8/71 | LOSS: 5.736670724824459e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 9/71 | LOSS: 5.735813329010853e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 10/71 | LOSS: 5.693221654920225e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 11/71 | LOSS: 5.858250726002249e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 12/71 | LOSS: 5.737195176782105e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 13/71 | LOSS: 5.685916676156921e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 14/71 | LOSS: 5.582901394518558e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 15/71 | LOSS: 5.697383585356874e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 16/71 | LOSS: 5.647549428163877e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 17/71 | LOSS: 5.601013097273406e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 18/71 | LOSS: 5.540472102438798e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 19/71 | LOSS: 5.496662720361201e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 20/71 | LOSS: 5.479123783083023e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 21/71 | LOSS: 5.4529985763193425e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 22/71 | LOSS: 5.426399994852861e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 23/71 | LOSS: 5.402935338831109e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 24/71 | LOSS: 5.401441067078849e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 25/71 | LOSS: 5.3609139390923355e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 26/71 | LOSS: 5.3773741018193065e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 27/71 | LOSS: 5.307640549290227e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 28/71 | LOSS: 5.2702849047661934e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 29/71 | LOSS: 5.228412949994284e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 30/71 | LOSS: 5.247543501149442e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 31/71 | LOSS: 5.2438993947134804e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 32/71 | LOSS: 5.234651615120873e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 33/71 | LOSS: 5.22333503153384e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 34/71 | LOSS: 5.233482430152695e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 35/71 | LOSS: 5.359569854590518e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 36/71 | LOSS: 5.390198210178012e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 37/71 | LOSS: 5.412309447867876e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 38/71 | LOSS: 5.4366611528586454e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 39/71 | LOSS: 5.479087815274397e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 40/71 | LOSS: 5.450605585349145e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 41/71 | LOSS: 5.4656840932563814e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 42/71 | LOSS: 5.4905985750739875e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 43/71 | LOSS: 5.495703693337749e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 44/71 | LOSS: 5.498015060665137e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 45/71 | LOSS: 5.501590037412193e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 46/71 | LOSS: 5.50613752606528e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 47/71 | LOSS: 5.4931842517665546e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 48/71 | LOSS: 5.4917612367123124e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 49/71 | LOSS: 5.482128535732045e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 50/71 | LOSS: 5.452665159371499e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 51/71 | LOSS: 5.4665426887368085e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 52/71 | LOSS: 5.450184958907863e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 53/71 | LOSS: 5.447164649349698e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 54/71 | LOSS: 5.447826325425095e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 55/71 | LOSS: 5.4814746671841675e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 56/71 | LOSS: 5.530031374515251e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 57/71 | LOSS: 5.539864105230663e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 58/71 | LOSS: 5.571313892487215e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 59/71 | LOSS: 5.610432754110661e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 60/71 | LOSS: 5.621524846379752e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 61/71 | LOSS: 5.597937843049294e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 62/71 | LOSS: 5.629592889153938e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 63/71 | LOSS: 5.634257881581561e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 64/71 | LOSS: 5.652520513439623e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 65/71 | LOSS: 5.653219142748805e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 66/71 | LOSS: 5.676012862560169e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 67/71 | LOSS: 5.661153395789477e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 68/71 | LOSS: 5.658490883196583e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 69/71 | LOSS: 5.669909796779393e-06\n",
      "TRAIN: EPOCH 325/1000 | BATCH 70/71 | LOSS: 5.700691801880758e-06\n",
      "VAL: EPOCH 325/1000 | BATCH 0/8 | LOSS: 5.3538069550995715e-06\n",
      "VAL: EPOCH 325/1000 | BATCH 1/8 | LOSS: 5.165715720067965e-06\n",
      "VAL: EPOCH 325/1000 | BATCH 2/8 | LOSS: 5.584164985824221e-06\n",
      "VAL: EPOCH 325/1000 | BATCH 3/8 | LOSS: 5.633536261484551e-06\n",
      "VAL: EPOCH 325/1000 | BATCH 4/8 | LOSS: 5.647685065923725e-06\n",
      "VAL: EPOCH 325/1000 | BATCH 5/8 | LOSS: 5.5884318802175885e-06\n",
      "VAL: EPOCH 325/1000 | BATCH 6/8 | LOSS: 5.58172268938506e-06\n",
      "VAL: EPOCH 325/1000 | BATCH 7/8 | LOSS: 5.6329409403588215e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 0/71 | LOSS: 5.704741852241568e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 1/71 | LOSS: 5.287898829919868e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 2/71 | LOSS: 5.6550911722297315e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 3/71 | LOSS: 5.614788051389041e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 4/71 | LOSS: 5.703480110241799e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 5/71 | LOSS: 5.784381149472513e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 6/71 | LOSS: 5.702904413088358e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 7/71 | LOSS: 6.019515694788424e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 8/71 | LOSS: 5.90300997929363e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 9/71 | LOSS: 5.907286094952724e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 10/71 | LOSS: 5.7800689319265075e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 11/71 | LOSS: 5.806425330471636e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 12/71 | LOSS: 5.713550728195025e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 13/71 | LOSS: 5.6599951026977835e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 14/71 | LOSS: 5.5861959784427505e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 15/71 | LOSS: 5.506323390136458e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 16/71 | LOSS: 5.576641904232983e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 17/71 | LOSS: 5.5838081859999674e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 18/71 | LOSS: 5.594754288483129e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 19/71 | LOSS: 5.607860271084064e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 20/71 | LOSS: 5.661939580715539e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 21/71 | LOSS: 5.606969456163245e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 22/71 | LOSS: 5.677286006412823e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 23/71 | LOSS: 5.632483691897505e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 24/71 | LOSS: 5.652155559801031e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 25/71 | LOSS: 5.695922132312821e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 26/71 | LOSS: 5.701687162703213e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 27/71 | LOSS: 5.657389806401625e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 28/71 | LOSS: 5.561464565195412e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 29/71 | LOSS: 5.575976994502223e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 30/71 | LOSS: 5.573215385964197e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 31/71 | LOSS: 5.605234925099012e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 32/71 | LOSS: 5.575578742619572e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 33/71 | LOSS: 5.597444435486122e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 34/71 | LOSS: 5.603583726302272e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 35/71 | LOSS: 5.622395222973056e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 36/71 | LOSS: 5.6540243656707245e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 37/71 | LOSS: 5.67879180835007e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 38/71 | LOSS: 5.691014749596107e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 39/71 | LOSS: 5.656021374988995e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 40/71 | LOSS: 5.653037117265673e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 41/71 | LOSS: 5.625713426871655e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 42/71 | LOSS: 5.612734422927958e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 43/71 | LOSS: 5.5934085542668814e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 44/71 | LOSS: 5.564286539083696e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 45/71 | LOSS: 5.560734996873615e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 46/71 | LOSS: 5.517654607443701e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 47/71 | LOSS: 5.475158090462173e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 48/71 | LOSS: 5.4550566151842105e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 49/71 | LOSS: 5.447423404802976e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 50/71 | LOSS: 5.417757232434644e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 51/71 | LOSS: 5.431029504821849e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 52/71 | LOSS: 5.445339853257843e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 53/71 | LOSS: 5.424027638885794e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 54/71 | LOSS: 5.400831646924912e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 55/71 | LOSS: 5.402871762173943e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 56/71 | LOSS: 5.382004150976029e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 57/71 | LOSS: 5.366307226691349e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 58/71 | LOSS: 5.360160920167415e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 59/71 | LOSS: 5.334698581312599e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 60/71 | LOSS: 5.322427714269057e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 61/71 | LOSS: 5.318302596019601e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 62/71 | LOSS: 5.297329914630234e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 63/71 | LOSS: 5.299816546511238e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 64/71 | LOSS: 5.285271621547649e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 65/71 | LOSS: 5.2686134598512915e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 66/71 | LOSS: 5.258464228457193e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 67/71 | LOSS: 5.251985274914178e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 68/71 | LOSS: 5.269945180428466e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 69/71 | LOSS: 5.255536971812295e-06\n",
      "TRAIN: EPOCH 326/1000 | BATCH 70/71 | LOSS: 5.2499690323010084e-06\n",
      "VAL: EPOCH 326/1000 | BATCH 0/8 | LOSS: 7.310558885365026e-06\n",
      "VAL: EPOCH 326/1000 | BATCH 1/8 | LOSS: 6.742718824170879e-06\n",
      "VAL: EPOCH 326/1000 | BATCH 2/8 | LOSS: 6.930005080600192e-06\n",
      "VAL: EPOCH 326/1000 | BATCH 3/8 | LOSS: 6.834293913016154e-06\n",
      "VAL: EPOCH 326/1000 | BATCH 4/8 | LOSS: 6.8467493292700965e-06\n",
      "VAL: EPOCH 326/1000 | BATCH 5/8 | LOSS: 6.598865259851057e-06\n",
      "VAL: EPOCH 326/1000 | BATCH 6/8 | LOSS: 6.72495421635436e-06\n",
      "VAL: EPOCH 326/1000 | BATCH 7/8 | LOSS: 6.652799811490695e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 0/71 | LOSS: 6.301426310528768e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 1/71 | LOSS: 5.900092446609051e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 2/71 | LOSS: 5.356599406998915e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 3/71 | LOSS: 5.258032842903049e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 4/71 | LOSS: 5.363713808037574e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 5/71 | LOSS: 5.1865204871622455e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 6/71 | LOSS: 5.29817154009444e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 7/71 | LOSS: 5.228348697983165e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 8/71 | LOSS: 5.002276465246622e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 9/71 | LOSS: 4.900105682281719e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 10/71 | LOSS: 4.864391712544602e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 11/71 | LOSS: 4.8923567987912975e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 12/71 | LOSS: 4.879097892066616e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 13/71 | LOSS: 4.846034845026484e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 14/71 | LOSS: 4.834538913200959e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 15/71 | LOSS: 4.8122649474180434e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 16/71 | LOSS: 4.75813178180782e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 17/71 | LOSS: 4.7396903508140694e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 18/71 | LOSS: 4.759312954966414e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 19/71 | LOSS: 4.724655161680857e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 20/71 | LOSS: 4.730191633262577e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 21/71 | LOSS: 4.740456161429889e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 22/71 | LOSS: 4.793179700952707e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 23/71 | LOSS: 4.78447849635207e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 24/71 | LOSS: 4.7659519350418125e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 25/71 | LOSS: 4.758600416957611e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 26/71 | LOSS: 4.776864940409993e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 27/71 | LOSS: 4.782668944309469e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 28/71 | LOSS: 4.832512355153148e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 29/71 | LOSS: 4.8137772637346645e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 30/71 | LOSS: 4.8091737807549035e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 31/71 | LOSS: 4.833753841637645e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 32/71 | LOSS: 4.832079033197563e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 33/71 | LOSS: 4.807238510425925e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 34/71 | LOSS: 4.851897087324427e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 35/71 | LOSS: 4.8722089016135014e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 36/71 | LOSS: 4.925302526906897e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 37/71 | LOSS: 4.932137884631964e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 38/71 | LOSS: 4.972805509868732e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 39/71 | LOSS: 4.962469455449537e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 40/71 | LOSS: 4.949544207680979e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 41/71 | LOSS: 4.96953125440043e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 42/71 | LOSS: 4.98762295687494e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 43/71 | LOSS: 4.984902142785524e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 44/71 | LOSS: 4.966901193862819e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 45/71 | LOSS: 4.9934413442789936e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 46/71 | LOSS: 5.0037056533913895e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 47/71 | LOSS: 5.013779585283373e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 48/71 | LOSS: 5.016569117770461e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 49/71 | LOSS: 5.031793621128599e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 50/71 | LOSS: 5.021090917615962e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 51/71 | LOSS: 4.994115574146025e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 52/71 | LOSS: 4.993672047141513e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 53/71 | LOSS: 4.995789982718567e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 54/71 | LOSS: 4.996142318056875e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 55/71 | LOSS: 5.007615681863951e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 56/71 | LOSS: 5.002170861476579e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 57/71 | LOSS: 4.987682265483931e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 58/71 | LOSS: 4.9844259561815935e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 59/71 | LOSS: 4.9988001364909e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 60/71 | LOSS: 5.005115028444513e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 61/71 | LOSS: 4.990415229786454e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 62/71 | LOSS: 4.96882111768909e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 63/71 | LOSS: 4.96255109538879e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 64/71 | LOSS: 4.959786334578754e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 65/71 | LOSS: 4.95772580892883e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 66/71 | LOSS: 4.962574859013357e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 67/71 | LOSS: 4.95554964425751e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 68/71 | LOSS: 4.9601662992601e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 69/71 | LOSS: 4.973454178980319e-06\n",
      "TRAIN: EPOCH 327/1000 | BATCH 70/71 | LOSS: 4.976674197575743e-06\n",
      "VAL: EPOCH 327/1000 | BATCH 0/8 | LOSS: 6.557924734806875e-06\n",
      "VAL: EPOCH 327/1000 | BATCH 1/8 | LOSS: 6.323573643385316e-06\n",
      "VAL: EPOCH 327/1000 | BATCH 2/8 | LOSS: 6.695402589684818e-06\n",
      "VAL: EPOCH 327/1000 | BATCH 3/8 | LOSS: 6.632806162087945e-06\n",
      "VAL: EPOCH 327/1000 | BATCH 4/8 | LOSS: 6.668145942967385e-06\n",
      "VAL: EPOCH 327/1000 | BATCH 5/8 | LOSS: 6.442993859915684e-06\n",
      "VAL: EPOCH 327/1000 | BATCH 6/8 | LOSS: 6.283670180502148e-06\n",
      "VAL: EPOCH 327/1000 | BATCH 7/8 | LOSS: 6.214669951987162e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 0/71 | LOSS: 5.306466846377589e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 1/71 | LOSS: 5.022830237066955e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 2/71 | LOSS: 4.9881838701063925e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 3/71 | LOSS: 5.154316227162781e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 4/71 | LOSS: 5.161856824997812e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 5/71 | LOSS: 5.119370446967271e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 6/71 | LOSS: 5.128972458935875e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 7/71 | LOSS: 5.26847543369513e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 8/71 | LOSS: 5.243660123394673e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 9/71 | LOSS: 5.183605981073924e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 10/71 | LOSS: 5.2546046886577225e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 11/71 | LOSS: 5.1339131156661706e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 12/71 | LOSS: 5.036599899400384e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 13/71 | LOSS: 5.016465463216134e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 14/71 | LOSS: 5.00585001645959e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 15/71 | LOSS: 5.057668687413752e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 16/71 | LOSS: 5.007477498378755e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 17/71 | LOSS: 5.031372211306007e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 18/71 | LOSS: 5.003159195008553e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 19/71 | LOSS: 4.9747143066269924e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 20/71 | LOSS: 5.005390990152678e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 21/71 | LOSS: 5.0530203555932864e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 22/71 | LOSS: 5.169167549035552e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 23/71 | LOSS: 5.149730668563279e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 24/71 | LOSS: 5.1482003982528115e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 25/71 | LOSS: 5.1278147321472015e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 26/71 | LOSS: 5.117806789780001e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 27/71 | LOSS: 5.105288534780682e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 28/71 | LOSS: 5.1136543302634205e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 29/71 | LOSS: 5.126359595427251e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 30/71 | LOSS: 5.1043287884010254e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 31/71 | LOSS: 5.106020367406927e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 32/71 | LOSS: 5.114826225590977e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 33/71 | LOSS: 5.1784083071277775e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 34/71 | LOSS: 5.144465480303292e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 35/71 | LOSS: 5.149608947855692e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 36/71 | LOSS: 5.168777755209925e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 37/71 | LOSS: 5.232468658122295e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 38/71 | LOSS: 5.229912660116754e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 39/71 | LOSS: 5.225415031873127e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 40/71 | LOSS: 5.286121838097886e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 41/71 | LOSS: 5.258535522790182e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 42/71 | LOSS: 5.2678995331895515e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 43/71 | LOSS: 5.245103321530306e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 44/71 | LOSS: 5.245709508017171e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 45/71 | LOSS: 5.228095668154191e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 46/71 | LOSS: 5.211152130597954e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 47/71 | LOSS: 5.195893038489885e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 48/71 | LOSS: 5.169089852605961e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 49/71 | LOSS: 5.1409562183835075e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 50/71 | LOSS: 5.127396167047075e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 51/71 | LOSS: 5.120009002614377e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 52/71 | LOSS: 5.12823151370438e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 53/71 | LOSS: 5.10275927151977e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 54/71 | LOSS: 5.113740706457809e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 55/71 | LOSS: 5.0919494023154196e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 56/71 | LOSS: 5.090655581067008e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 57/71 | LOSS: 5.097510881631902e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 58/71 | LOSS: 5.068309481091199e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 59/71 | LOSS: 5.124618765724639e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 60/71 | LOSS: 5.123212091706084e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 61/71 | LOSS: 5.163702967952451e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 62/71 | LOSS: 5.159377880974918e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 63/71 | LOSS: 5.202177565166721e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 64/71 | LOSS: 5.194626174460819e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 65/71 | LOSS: 5.195719498977159e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 66/71 | LOSS: 5.219779753662887e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 67/71 | LOSS: 5.2416529737296085e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 68/71 | LOSS: 5.311249546191876e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 69/71 | LOSS: 5.3150150409107614e-06\n",
      "TRAIN: EPOCH 328/1000 | BATCH 70/71 | LOSS: 5.397916228003582e-06\n",
      "VAL: EPOCH 328/1000 | BATCH 0/8 | LOSS: 9.29732050281018e-06\n",
      "VAL: EPOCH 328/1000 | BATCH 1/8 | LOSS: 9.879639947030228e-06\n",
      "VAL: EPOCH 328/1000 | BATCH 2/8 | LOSS: 1.0000382341483297e-05\n",
      "VAL: EPOCH 328/1000 | BATCH 3/8 | LOSS: 1.0089974693983095e-05\n",
      "VAL: EPOCH 328/1000 | BATCH 4/8 | LOSS: 1.0046661736851093e-05\n",
      "VAL: EPOCH 328/1000 | BATCH 5/8 | LOSS: 9.922046501742443e-06\n",
      "VAL: EPOCH 328/1000 | BATCH 6/8 | LOSS: 9.8026847419013e-06\n",
      "VAL: EPOCH 328/1000 | BATCH 7/8 | LOSS: 9.945296937985404e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 0/71 | LOSS: 1.1924890713999048e-05\n",
      "TRAIN: EPOCH 329/1000 | BATCH 1/71 | LOSS: 9.962559033738216e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 2/71 | LOSS: 9.677849751218067e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 3/71 | LOSS: 8.893601943782414e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 4/71 | LOSS: 8.841449198371264e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 5/71 | LOSS: 8.291334552268381e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 6/71 | LOSS: 8.064358358491777e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 7/71 | LOSS: 8.100927686882642e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 8/71 | LOSS: 8.035594722362779e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 9/71 | LOSS: 7.77093373471871e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 10/71 | LOSS: 7.5840202953258995e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 11/71 | LOSS: 7.415822362114947e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 12/71 | LOSS: 7.2491762134282344e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 13/71 | LOSS: 7.010781148762492e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 14/71 | LOSS: 6.9727186504072355e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 15/71 | LOSS: 6.8400165389448375e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 16/71 | LOSS: 6.7890204103177355e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 17/71 | LOSS: 6.673942860086552e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 18/71 | LOSS: 6.506412630283433e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 19/71 | LOSS: 6.3946200725695235e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 20/71 | LOSS: 6.311091337487423e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 21/71 | LOSS: 6.250544236701733e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 22/71 | LOSS: 6.182502839573831e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 23/71 | LOSS: 6.095033408352417e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 24/71 | LOSS: 5.985144471196691e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 25/71 | LOSS: 5.950267170471936e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 26/71 | LOSS: 5.907601815932948e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 27/71 | LOSS: 5.833287429725585e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 28/71 | LOSS: 5.8132472045615255e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 29/71 | LOSS: 5.783409248275954e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 30/71 | LOSS: 5.744799944941112e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 31/71 | LOSS: 5.757281030582817e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 32/71 | LOSS: 5.71767802257179e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 33/71 | LOSS: 5.693138478711583e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 34/71 | LOSS: 5.652050497571639e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 35/71 | LOSS: 5.61583808828598e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 36/71 | LOSS: 5.5893471432461186e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 37/71 | LOSS: 5.541082146010012e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 38/71 | LOSS: 5.5098198455921074e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 39/71 | LOSS: 5.472745215229224e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 40/71 | LOSS: 5.442193258470691e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 41/71 | LOSS: 5.4532571582621175e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 42/71 | LOSS: 5.475336947288164e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 43/71 | LOSS: 5.444024158019932e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 44/71 | LOSS: 5.432425779064134e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 45/71 | LOSS: 5.450798356753734e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 46/71 | LOSS: 5.4512833462758085e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 47/71 | LOSS: 5.456372529503521e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 48/71 | LOSS: 5.42823192740705e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 49/71 | LOSS: 5.427197429526131e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 50/71 | LOSS: 5.411877326896145e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 51/71 | LOSS: 5.386010728286167e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 52/71 | LOSS: 5.397306233375413e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 53/71 | LOSS: 5.369029816359182e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 54/71 | LOSS: 5.372107981119453e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 55/71 | LOSS: 5.3690403660766606e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 56/71 | LOSS: 5.349022701825322e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 57/71 | LOSS: 5.33862022825031e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 58/71 | LOSS: 5.323356949162898e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 59/71 | LOSS: 5.3223154282022735e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 60/71 | LOSS: 5.299874836234013e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 61/71 | LOSS: 5.286713670307842e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 62/71 | LOSS: 5.314399459166452e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 63/71 | LOSS: 5.297727916797612e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 64/71 | LOSS: 5.305114686612237e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 65/71 | LOSS: 5.303902395153824e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 66/71 | LOSS: 5.296647615788101e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 67/71 | LOSS: 5.289366807868218e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 68/71 | LOSS: 5.3024961243204745e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 69/71 | LOSS: 5.280000193254507e-06\n",
      "TRAIN: EPOCH 329/1000 | BATCH 70/71 | LOSS: 5.254726560656629e-06\n",
      "VAL: EPOCH 329/1000 | BATCH 0/8 | LOSS: 6.419166311388835e-06\n",
      "VAL: EPOCH 329/1000 | BATCH 1/8 | LOSS: 5.76627098780591e-06\n",
      "VAL: EPOCH 329/1000 | BATCH 2/8 | LOSS: 5.7223536108115996e-06\n",
      "VAL: EPOCH 329/1000 | BATCH 3/8 | LOSS: 5.63238666018151e-06\n",
      "VAL: EPOCH 329/1000 | BATCH 4/8 | LOSS: 5.561794660025044e-06\n",
      "VAL: EPOCH 329/1000 | BATCH 5/8 | LOSS: 5.404907445457259e-06\n",
      "VAL: EPOCH 329/1000 | BATCH 6/8 | LOSS: 5.340064620083597e-06\n",
      "VAL: EPOCH 329/1000 | BATCH 7/8 | LOSS: 5.334562445113988e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 0/71 | LOSS: 5.2720224630320445e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 1/71 | LOSS: 4.696472387877293e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 2/71 | LOSS: 5.067567447743689e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 3/71 | LOSS: 5.126795258547645e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 4/71 | LOSS: 5.381108530855272e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 5/71 | LOSS: 5.268962695481605e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 6/71 | LOSS: 5.482712497593768e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 7/71 | LOSS: 5.60652648573523e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 8/71 | LOSS: 5.5068871410589454e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 9/71 | LOSS: 5.47608510714781e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 10/71 | LOSS: 5.413906861080746e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 11/71 | LOSS: 5.584048456815556e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 12/71 | LOSS: 5.5010065037864615e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 13/71 | LOSS: 5.617918694562312e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 14/71 | LOSS: 5.6462999054929245e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 15/71 | LOSS: 5.6370800791682996e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 16/71 | LOSS: 5.837559777265757e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 17/71 | LOSS: 5.871717197199662e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 18/71 | LOSS: 6.021310582582373e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 19/71 | LOSS: 5.954660264251288e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 20/71 | LOSS: 5.958448604187219e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 21/71 | LOSS: 6.154283668861766e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 22/71 | LOSS: 6.166923907110923e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 23/71 | LOSS: 6.22350465088554e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 24/71 | LOSS: 6.235651326278457e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 25/71 | LOSS: 6.191870834999673e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 26/71 | LOSS: 6.126476448655551e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 27/71 | LOSS: 6.113661161829701e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 28/71 | LOSS: 6.135763873442494e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 29/71 | LOSS: 6.06823482485197e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 30/71 | LOSS: 6.068755239187251e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 31/71 | LOSS: 6.061802636736502e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 32/71 | LOSS: 6.00032373051473e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 33/71 | LOSS: 5.951833185570328e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 34/71 | LOSS: 5.914238032086619e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 35/71 | LOSS: 5.895091943683737e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 36/71 | LOSS: 5.877940320469615e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 37/71 | LOSS: 5.869561954042231e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 38/71 | LOSS: 5.824616845273294e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 39/71 | LOSS: 5.811051869386574e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 40/71 | LOSS: 5.799190215084974e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 41/71 | LOSS: 5.775132551986774e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 42/71 | LOSS: 5.762006388300337e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 43/71 | LOSS: 5.728844164629911e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 44/71 | LOSS: 5.696190025143248e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 45/71 | LOSS: 5.6600169856487215e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 46/71 | LOSS: 5.642051475777098e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 47/71 | LOSS: 5.601297440686419e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 48/71 | LOSS: 5.57052785830931e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 49/71 | LOSS: 5.525255037355237e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 50/71 | LOSS: 5.4901805229104976e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 51/71 | LOSS: 5.497255389085554e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 52/71 | LOSS: 5.494173039105075e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 53/71 | LOSS: 5.48596967241287e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 54/71 | LOSS: 5.465842654798921e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 55/71 | LOSS: 5.459096972312441e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 56/71 | LOSS: 5.450719064563664e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 57/71 | LOSS: 5.449711211283635e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 58/71 | LOSS: 5.483022921568698e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 59/71 | LOSS: 5.474483123180107e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 60/71 | LOSS: 5.472726414035472e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 61/71 | LOSS: 5.499297949720658e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 62/71 | LOSS: 5.477409732044353e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 63/71 | LOSS: 5.4963529123597255e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 64/71 | LOSS: 5.49122904396007e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 65/71 | LOSS: 5.498095898667199e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 66/71 | LOSS: 5.488253905309309e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 67/71 | LOSS: 5.471453639128252e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 68/71 | LOSS: 5.4602767414652975e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 69/71 | LOSS: 5.444341292916631e-06\n",
      "TRAIN: EPOCH 330/1000 | BATCH 70/71 | LOSS: 5.406049775072615e-06\n",
      "VAL: EPOCH 330/1000 | BATCH 0/8 | LOSS: 7.517120138800237e-06\n",
      "VAL: EPOCH 330/1000 | BATCH 1/8 | LOSS: 6.579311502719065e-06\n",
      "VAL: EPOCH 330/1000 | BATCH 2/8 | LOSS: 6.440525491295072e-06\n",
      "VAL: EPOCH 330/1000 | BATCH 3/8 | LOSS: 6.221156013452855e-06\n",
      "VAL: EPOCH 330/1000 | BATCH 4/8 | LOSS: 6.10219713053084e-06\n",
      "VAL: EPOCH 330/1000 | BATCH 5/8 | LOSS: 5.784595866013357e-06\n",
      "VAL: EPOCH 330/1000 | BATCH 6/8 | LOSS: 5.687703703318091e-06\n",
      "VAL: EPOCH 330/1000 | BATCH 7/8 | LOSS: 5.5647923318247194e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 0/71 | LOSS: 4.623619133781176e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 1/71 | LOSS: 5.4229790293902624e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 2/71 | LOSS: 5.435907648158415e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 3/71 | LOSS: 5.959000645816559e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 4/71 | LOSS: 5.499676080944482e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 5/71 | LOSS: 5.3405606195156e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 6/71 | LOSS: 5.3518449151722185e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 7/71 | LOSS: 5.417171337285254e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 8/71 | LOSS: 5.402576587382808e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 9/71 | LOSS: 5.744011014030548e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 10/71 | LOSS: 5.616271705961977e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 11/71 | LOSS: 5.569561570458366e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 12/71 | LOSS: 5.44659280002144e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 13/71 | LOSS: 5.4802771631200035e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 14/71 | LOSS: 5.485910332936328e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 15/71 | LOSS: 5.427445358918703e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 16/71 | LOSS: 5.4659806878857235e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 17/71 | LOSS: 5.332683334321094e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 18/71 | LOSS: 5.388267565745321e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 19/71 | LOSS: 5.409437562775565e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 20/71 | LOSS: 5.359325227584036e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 21/71 | LOSS: 5.346422746945543e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 22/71 | LOSS: 5.288930681316465e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 23/71 | LOSS: 5.296265499055153e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 24/71 | LOSS: 5.340095249266597e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 25/71 | LOSS: 5.429299440388254e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 26/71 | LOSS: 5.415162495073336e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 27/71 | LOSS: 5.3804347349536585e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 28/71 | LOSS: 5.369885174927888e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 29/71 | LOSS: 5.34401065124257e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 30/71 | LOSS: 5.36356643468964e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 31/71 | LOSS: 5.397057520895032e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 32/71 | LOSS: 5.3718996572239455e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 33/71 | LOSS: 5.350270612325403e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 34/71 | LOSS: 5.4088118304207455e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 35/71 | LOSS: 5.451091523253935e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 36/71 | LOSS: 5.457930124189383e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 37/71 | LOSS: 5.45273904830729e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 38/71 | LOSS: 5.432657930307472e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 39/71 | LOSS: 5.3985255249244805e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 40/71 | LOSS: 5.417945952259044e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 41/71 | LOSS: 5.461001516475032e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 42/71 | LOSS: 5.444290214062528e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 43/71 | LOSS: 5.419720764621161e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 44/71 | LOSS: 5.388088417627538e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 45/71 | LOSS: 5.346606821663243e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 46/71 | LOSS: 5.310509884130876e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 47/71 | LOSS: 5.328123757900964e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 48/71 | LOSS: 5.310575317002521e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 49/71 | LOSS: 5.277686746012478e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 50/71 | LOSS: 5.259170915023253e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 51/71 | LOSS: 5.237736108763504e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 52/71 | LOSS: 5.225079764884113e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 53/71 | LOSS: 5.233062998505139e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 54/71 | LOSS: 5.235612500407362e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 55/71 | LOSS: 5.2433733677454645e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 56/71 | LOSS: 5.239510823740969e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 57/71 | LOSS: 5.259797117575764e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 58/71 | LOSS: 5.241508218251372e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 59/71 | LOSS: 5.2841138123464285e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 60/71 | LOSS: 5.274662826792099e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 61/71 | LOSS: 5.329676472713125e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 62/71 | LOSS: 5.329746738338838e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 63/71 | LOSS: 5.328711733199043e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 64/71 | LOSS: 5.343171782302446e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 65/71 | LOSS: 5.358164333880598e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 66/71 | LOSS: 5.395613552129579e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 67/71 | LOSS: 5.414522203248934e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 68/71 | LOSS: 5.43064948033134e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 69/71 | LOSS: 5.413441499513283e-06\n",
      "TRAIN: EPOCH 331/1000 | BATCH 70/71 | LOSS: 5.41864785870056e-06\n",
      "VAL: EPOCH 331/1000 | BATCH 0/8 | LOSS: 7.288321285159327e-06\n",
      "VAL: EPOCH 331/1000 | BATCH 1/8 | LOSS: 7.054439265630208e-06\n",
      "VAL: EPOCH 331/1000 | BATCH 2/8 | LOSS: 6.751402603792182e-06\n",
      "VAL: EPOCH 331/1000 | BATCH 3/8 | LOSS: 6.932542760296201e-06\n",
      "VAL: EPOCH 331/1000 | BATCH 4/8 | LOSS: 6.69156625008327e-06\n",
      "VAL: EPOCH 331/1000 | BATCH 5/8 | LOSS: 6.372924493310468e-06\n",
      "VAL: EPOCH 331/1000 | BATCH 6/8 | LOSS: 6.26869528527355e-06\n",
      "VAL: EPOCH 331/1000 | BATCH 7/8 | LOSS: 5.990059378291335e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 0/71 | LOSS: 5.561829311773181e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 1/71 | LOSS: 5.8652017287386116e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 2/71 | LOSS: 6.044543624739163e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 3/71 | LOSS: 5.60580599540117e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 4/71 | LOSS: 5.6471905736543706e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 5/71 | LOSS: 5.6008082841193145e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 6/71 | LOSS: 5.696254512648531e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 7/71 | LOSS: 5.432970169749751e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 8/71 | LOSS: 5.603714473383863e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 9/71 | LOSS: 5.621368927677395e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 10/71 | LOSS: 5.522165669059508e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 11/71 | LOSS: 5.816220361036055e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 12/71 | LOSS: 5.8492166947242086e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 13/71 | LOSS: 5.900163419677743e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 14/71 | LOSS: 5.894111382076517e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 15/71 | LOSS: 5.927444476583332e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 16/71 | LOSS: 5.895451174771605e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 17/71 | LOSS: 5.836256630371079e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 18/71 | LOSS: 5.780831064552522e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 19/71 | LOSS: 5.699930034097633e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 20/71 | LOSS: 5.642684646729668e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 21/71 | LOSS: 5.610435850526714e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 22/71 | LOSS: 5.565451493052735e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 23/71 | LOSS: 5.557529637674937e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 24/71 | LOSS: 5.4939692199695856e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 25/71 | LOSS: 5.461915144713623e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 26/71 | LOSS: 5.46273440704681e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 27/71 | LOSS: 5.4299714357901915e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 28/71 | LOSS: 5.414568147835812e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 29/71 | LOSS: 5.389547050071997e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 30/71 | LOSS: 5.360764809607175e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 31/71 | LOSS: 5.326388389903514e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 32/71 | LOSS: 5.390877642198713e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 33/71 | LOSS: 5.389758438040564e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 34/71 | LOSS: 5.396057076723082e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 35/71 | LOSS: 5.3792262835688434e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 36/71 | LOSS: 5.394404512559491e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 37/71 | LOSS: 5.374195145697358e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 38/71 | LOSS: 5.34764566486373e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 39/71 | LOSS: 5.343242094113521e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 40/71 | LOSS: 5.32404783371021e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 41/71 | LOSS: 5.319226342440483e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 42/71 | LOSS: 5.302130396790575e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 43/71 | LOSS: 5.288154563293739e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 44/71 | LOSS: 5.266352986331589e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 45/71 | LOSS: 5.2676927640751945e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 46/71 | LOSS: 5.246581206441252e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 47/71 | LOSS: 5.264332685328554e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 48/71 | LOSS: 5.250313047234121e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 49/71 | LOSS: 5.233254851191304e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 50/71 | LOSS: 5.23343224617123e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 51/71 | LOSS: 5.219015721335241e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 52/71 | LOSS: 5.208028553925204e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 53/71 | LOSS: 5.2005340320546466e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 54/71 | LOSS: 5.177987796659264e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 55/71 | LOSS: 5.167339549773585e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 56/71 | LOSS: 5.133482398122667e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 57/71 | LOSS: 5.102913252718783e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 58/71 | LOSS: 5.075751456172577e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 59/71 | LOSS: 5.081120010193748e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 60/71 | LOSS: 5.063433670076221e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 61/71 | LOSS: 5.044045675247382e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 62/71 | LOSS: 5.046017353350411e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 63/71 | LOSS: 5.042257321008492e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 64/71 | LOSS: 5.028677277061014e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 65/71 | LOSS: 5.019487168807025e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 66/71 | LOSS: 5.026296609834552e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 67/71 | LOSS: 5.0070331433423285e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 68/71 | LOSS: 5.013312022418941e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 69/71 | LOSS: 4.987848355345445e-06\n",
      "TRAIN: EPOCH 332/1000 | BATCH 70/71 | LOSS: 4.99203138709893e-06\n",
      "VAL: EPOCH 332/1000 | BATCH 0/8 | LOSS: 5.666855486197164e-06\n",
      "VAL: EPOCH 332/1000 | BATCH 1/8 | LOSS: 5.533461489903857e-06\n",
      "VAL: EPOCH 332/1000 | BATCH 2/8 | LOSS: 5.896716326484845e-06\n",
      "VAL: EPOCH 332/1000 | BATCH 3/8 | LOSS: 5.926127528255165e-06\n",
      "VAL: EPOCH 332/1000 | BATCH 4/8 | LOSS: 5.933063221164048e-06\n",
      "VAL: EPOCH 332/1000 | BATCH 5/8 | LOSS: 5.885129515566708e-06\n",
      "VAL: EPOCH 332/1000 | BATCH 6/8 | LOSS: 5.803203618727691e-06\n",
      "VAL: EPOCH 332/1000 | BATCH 7/8 | LOSS: 5.86971606253428e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 0/71 | LOSS: 6.174156624183524e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 1/71 | LOSS: 5.303967100189766e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 2/71 | LOSS: 5.4777462234293734e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 3/71 | LOSS: 5.202373358770274e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 4/71 | LOSS: 5.120318292028969e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 5/71 | LOSS: 5.028299710829742e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 6/71 | LOSS: 4.9594337464701055e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 7/71 | LOSS: 4.847785930905957e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 8/71 | LOSS: 4.872577493289201e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 9/71 | LOSS: 4.915454792353558e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 10/71 | LOSS: 5.039404558457053e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 11/71 | LOSS: 5.087242584522755e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 12/71 | LOSS: 5.1398627385004566e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 13/71 | LOSS: 5.218045966362947e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 14/71 | LOSS: 5.357419710586934e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 15/71 | LOSS: 5.280762479742407e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 16/71 | LOSS: 5.1715729595718375e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 17/71 | LOSS: 5.257005606533009e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 18/71 | LOSS: 5.261547297969197e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 19/71 | LOSS: 5.267273138542805e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 20/71 | LOSS: 5.271448019682014e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 21/71 | LOSS: 5.287394628794573e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 22/71 | LOSS: 5.246258109289793e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 23/71 | LOSS: 5.237567819449396e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 24/71 | LOSS: 5.2616649281844725e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 25/71 | LOSS: 5.326862623615642e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 26/71 | LOSS: 5.361851242048067e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 27/71 | LOSS: 5.371566114068368e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 28/71 | LOSS: 5.372573994087549e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 29/71 | LOSS: 5.366308785899794e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 30/71 | LOSS: 5.348908908056347e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 31/71 | LOSS: 5.389049043458272e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 32/71 | LOSS: 5.387799174885996e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 33/71 | LOSS: 5.316872134511975e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 34/71 | LOSS: 5.33937116935184e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 35/71 | LOSS: 5.350494790137519e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 36/71 | LOSS: 5.392665398913606e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 37/71 | LOSS: 5.3750309593612515e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 38/71 | LOSS: 5.4040188866239104e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 39/71 | LOSS: 5.393651736085303e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 40/71 | LOSS: 5.396475189183725e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 41/71 | LOSS: 5.366851556096836e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 42/71 | LOSS: 5.386662133935525e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 43/71 | LOSS: 5.427255762862677e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 44/71 | LOSS: 5.399130855544677e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 45/71 | LOSS: 5.4616404628651925e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 46/71 | LOSS: 5.488660530743878e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 47/71 | LOSS: 5.54185039428982e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 48/71 | LOSS: 5.52137902448113e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 49/71 | LOSS: 5.528274969037739e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 50/71 | LOSS: 5.549826683444039e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 51/71 | LOSS: 5.559472155115285e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 52/71 | LOSS: 5.5493143815322085e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 53/71 | LOSS: 5.5549386935901436e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 54/71 | LOSS: 5.546103553073904e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 55/71 | LOSS: 5.526330953996096e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 56/71 | LOSS: 5.517634953752062e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 57/71 | LOSS: 5.5144765720515034e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 58/71 | LOSS: 5.485531438985086e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 59/71 | LOSS: 5.461573914544715e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 60/71 | LOSS: 5.454675188122535e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 61/71 | LOSS: 5.451048874390601e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 62/71 | LOSS: 5.437841168930477e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 63/71 | LOSS: 5.434686777050501e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 64/71 | LOSS: 5.423966782613066e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 65/71 | LOSS: 5.412535336180361e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 66/71 | LOSS: 5.384120277764185e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 67/71 | LOSS: 5.369900945344639e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 68/71 | LOSS: 5.35763710091039e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 69/71 | LOSS: 5.340586825955792e-06\n",
      "TRAIN: EPOCH 333/1000 | BATCH 70/71 | LOSS: 5.337324624556907e-06\n",
      "VAL: EPOCH 333/1000 | BATCH 0/8 | LOSS: 5.938870344834868e-06\n",
      "VAL: EPOCH 333/1000 | BATCH 1/8 | LOSS: 5.724102265958209e-06\n",
      "VAL: EPOCH 333/1000 | BATCH 2/8 | LOSS: 5.513501491805073e-06\n",
      "VAL: EPOCH 333/1000 | BATCH 3/8 | LOSS: 5.642141331918538e-06\n",
      "VAL: EPOCH 333/1000 | BATCH 4/8 | LOSS: 5.5381417041644456e-06\n",
      "VAL: EPOCH 333/1000 | BATCH 5/8 | LOSS: 5.350116149808552e-06\n",
      "VAL: EPOCH 333/1000 | BATCH 6/8 | LOSS: 5.288266038405709e-06\n",
      "VAL: EPOCH 333/1000 | BATCH 7/8 | LOSS: 5.136985748777079e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 0/71 | LOSS: 6.163655598356854e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 1/71 | LOSS: 5.170883014216088e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 2/71 | LOSS: 4.900709579184574e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 3/71 | LOSS: 4.974471607965825e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 4/71 | LOSS: 5.033228990214411e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 5/71 | LOSS: 4.844154924891579e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 6/71 | LOSS: 4.755089126514836e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 7/71 | LOSS: 4.693719063197932e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 8/71 | LOSS: 4.749441611945966e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 9/71 | LOSS: 4.754992141897674e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 10/71 | LOSS: 4.7577086248069955e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 11/71 | LOSS: 4.974383803831491e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 12/71 | LOSS: 4.991891938306463e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 13/71 | LOSS: 5.13443235052234e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 14/71 | LOSS: 5.142745643145948e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 15/71 | LOSS: 5.110144314812715e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 16/71 | LOSS: 5.065422903819982e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 17/71 | LOSS: 5.068052183155993e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 18/71 | LOSS: 5.117458840686595e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 19/71 | LOSS: 5.091422167424753e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 20/71 | LOSS: 5.097306387012525e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 21/71 | LOSS: 5.073070993107236e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 22/71 | LOSS: 5.04361746014025e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 23/71 | LOSS: 5.113485144647711e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 24/71 | LOSS: 5.120586065459065e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 25/71 | LOSS: 5.184542820083711e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 26/71 | LOSS: 5.186478625142223e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 27/71 | LOSS: 5.183058858295096e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 28/71 | LOSS: 5.193454838043745e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 29/71 | LOSS: 5.19801349886014e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 30/71 | LOSS: 5.241337006619658e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 31/71 | LOSS: 5.226553653869814e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 32/71 | LOSS: 5.268632032146508e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 33/71 | LOSS: 5.2381531478364625e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 34/71 | LOSS: 5.282147230900591e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 35/71 | LOSS: 5.286434518186272e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 36/71 | LOSS: 5.328344925785771e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 37/71 | LOSS: 5.315145807272577e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 38/71 | LOSS: 5.282606311177518e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 39/71 | LOSS: 5.273039516850986e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 40/71 | LOSS: 5.2615515836273185e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 41/71 | LOSS: 5.29305643361849e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 42/71 | LOSS: 5.298608632370434e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 43/71 | LOSS: 5.2875733031628025e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 44/71 | LOSS: 5.28497513288231e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 45/71 | LOSS: 5.2800004418120645e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 46/71 | LOSS: 5.265564402392595e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 47/71 | LOSS: 5.235700740513494e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 48/71 | LOSS: 5.2361166609091234e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 49/71 | LOSS: 5.216062927502207e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 50/71 | LOSS: 5.217787385809774e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 51/71 | LOSS: 5.2033851541357135e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 52/71 | LOSS: 5.191579389848246e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 53/71 | LOSS: 5.184149291313521e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 54/71 | LOSS: 5.176439968966985e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 55/71 | LOSS: 5.189835072932121e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 56/71 | LOSS: 5.194758159632329e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 57/71 | LOSS: 5.1930234396171856e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 58/71 | LOSS: 5.211409389029526e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 59/71 | LOSS: 5.197594631075238e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 60/71 | LOSS: 5.183577010685723e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 61/71 | LOSS: 5.180047376216539e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 62/71 | LOSS: 5.182797726847061e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 63/71 | LOSS: 5.189793597537573e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 64/71 | LOSS: 5.1720159367855206e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 65/71 | LOSS: 5.161206197292565e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 66/71 | LOSS: 5.192414079571261e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 67/71 | LOSS: 5.161908036905729e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 68/71 | LOSS: 5.153258776456072e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 69/71 | LOSS: 5.137670622648979e-06\n",
      "TRAIN: EPOCH 334/1000 | BATCH 70/71 | LOSS: 5.129857408718007e-06\n",
      "VAL: EPOCH 334/1000 | BATCH 0/8 | LOSS: 5.347593287297059e-06\n",
      "VAL: EPOCH 334/1000 | BATCH 1/8 | LOSS: 5.146540843270486e-06\n",
      "VAL: EPOCH 334/1000 | BATCH 2/8 | LOSS: 5.345627262916726e-06\n",
      "VAL: EPOCH 334/1000 | BATCH 3/8 | LOSS: 5.339227072909125e-06\n",
      "VAL: EPOCH 334/1000 | BATCH 4/8 | LOSS: 5.30490297023789e-06\n",
      "VAL: EPOCH 334/1000 | BATCH 5/8 | LOSS: 5.1060566571929184e-06\n",
      "VAL: EPOCH 334/1000 | BATCH 6/8 | LOSS: 5.043822836471788e-06\n",
      "VAL: EPOCH 334/1000 | BATCH 7/8 | LOSS: 4.989066326288594e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 0/71 | LOSS: 4.552481641439954e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 1/71 | LOSS: 4.922197831547237e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 2/71 | LOSS: 5.0013615388403805e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 3/71 | LOSS: 4.694478889177844e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 4/71 | LOSS: 5.009116375731537e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 5/71 | LOSS: 4.943658268530271e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 6/71 | LOSS: 4.957179628815668e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 7/71 | LOSS: 5.011509756513988e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 8/71 | LOSS: 5.083548078093574e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 9/71 | LOSS: 5.037491155235329e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 10/71 | LOSS: 4.9152069798517255e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 11/71 | LOSS: 4.902000341644452e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 12/71 | LOSS: 4.834923611969526e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 13/71 | LOSS: 4.937028117118254e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 14/71 | LOSS: 4.890107932927397e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 15/71 | LOSS: 4.894931691978854e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 16/71 | LOSS: 4.861452586791368e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 17/71 | LOSS: 4.7839205055626935e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 18/71 | LOSS: 4.8562793429147475e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 19/71 | LOSS: 4.880385995420511e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 20/71 | LOSS: 4.883324479687005e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 21/71 | LOSS: 4.901379146427974e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 22/71 | LOSS: 4.903216845180039e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 23/71 | LOSS: 4.86772552221737e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 24/71 | LOSS: 4.837792967009591e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 25/71 | LOSS: 4.8215943024843e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 26/71 | LOSS: 4.79968796045998e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 27/71 | LOSS: 4.796471606433832e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 28/71 | LOSS: 4.7917550828374106e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 29/71 | LOSS: 4.785489530452954e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 30/71 | LOSS: 4.815642495809551e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 31/71 | LOSS: 4.777793719767942e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 32/71 | LOSS: 4.790901074915327e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 33/71 | LOSS: 4.762795456639758e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 34/71 | LOSS: 4.811287401805332e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 35/71 | LOSS: 4.830845808909443e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 36/71 | LOSS: 4.804782809121723e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 37/71 | LOSS: 4.783603871098756e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 38/71 | LOSS: 4.764217115175247e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 39/71 | LOSS: 4.75096455829771e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 40/71 | LOSS: 4.7367312706738605e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 41/71 | LOSS: 4.75817448984474e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 42/71 | LOSS: 4.738771347251185e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 43/71 | LOSS: 4.7297895662268274e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 44/71 | LOSS: 4.725794294952519e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 45/71 | LOSS: 4.715536652324574e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 46/71 | LOSS: 4.747307694413664e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 47/71 | LOSS: 4.750907436346097e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 48/71 | LOSS: 4.730938306758097e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 49/71 | LOSS: 4.7385138395839025e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 50/71 | LOSS: 4.734411830070837e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 51/71 | LOSS: 4.719310582528963e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 52/71 | LOSS: 4.72019655288297e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 53/71 | LOSS: 4.719842136351131e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 54/71 | LOSS: 4.708250505493859e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 55/71 | LOSS: 4.722193754754958e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 56/71 | LOSS: 4.719689134920675e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 57/71 | LOSS: 4.71320228810108e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 58/71 | LOSS: 4.701679680523339e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 59/71 | LOSS: 4.703091398520579e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 60/71 | LOSS: 4.678520114242422e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 61/71 | LOSS: 4.661826624801653e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 62/71 | LOSS: 4.662275437618115e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 63/71 | LOSS: 4.654260720826642e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 64/71 | LOSS: 4.670378733023356e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 65/71 | LOSS: 4.6671033845941015e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 66/71 | LOSS: 4.660305421566591e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 67/71 | LOSS: 4.649325734028346e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 68/71 | LOSS: 4.662120562094008e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 69/71 | LOSS: 4.652132182465201e-06\n",
      "TRAIN: EPOCH 335/1000 | BATCH 70/71 | LOSS: 4.647636410161692e-06\n",
      "VAL: EPOCH 335/1000 | BATCH 0/8 | LOSS: 6.568929165950976e-06\n",
      "VAL: EPOCH 335/1000 | BATCH 1/8 | LOSS: 6.0097838741057785e-06\n",
      "VAL: EPOCH 335/1000 | BATCH 2/8 | LOSS: 5.724610218749149e-06\n",
      "VAL: EPOCH 335/1000 | BATCH 3/8 | LOSS: 5.862953571522667e-06\n",
      "VAL: EPOCH 335/1000 | BATCH 4/8 | LOSS: 5.686422537110048e-06\n",
      "VAL: EPOCH 335/1000 | BATCH 5/8 | LOSS: 5.380273705668515e-06\n",
      "VAL: EPOCH 335/1000 | BATCH 6/8 | LOSS: 5.273611480203856e-06\n",
      "VAL: EPOCH 335/1000 | BATCH 7/8 | LOSS: 5.121479944136809e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 0/71 | LOSS: 4.965404968970688e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 1/71 | LOSS: 5.5806358432164416e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 2/71 | LOSS: 5.391524458294346e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 3/71 | LOSS: 5.6950239013531245e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 4/71 | LOSS: 5.252571190794697e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 5/71 | LOSS: 5.781863592346781e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 6/71 | LOSS: 5.614560840123366e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 7/71 | LOSS: 6.216297663286241e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 8/71 | LOSS: 5.983980018855719e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 9/71 | LOSS: 6.1157747495599326e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 10/71 | LOSS: 6.321398896861038e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 11/71 | LOSS: 6.4245400987298735e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 12/71 | LOSS: 6.660496777700386e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 13/71 | LOSS: 6.478955810962361e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 14/71 | LOSS: 6.889424033336884e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 15/71 | LOSS: 6.699622730366173e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 16/71 | LOSS: 6.900941892073367e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 17/71 | LOSS: 6.735078007598834e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 18/71 | LOSS: 6.761096564763361e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 19/71 | LOSS: 6.644079257966951e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 20/71 | LOSS: 6.586706365938187e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 21/71 | LOSS: 6.566995435522668e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 22/71 | LOSS: 6.515577494966514e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 23/71 | LOSS: 6.486774945339373e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 24/71 | LOSS: 6.4945289159368254e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 25/71 | LOSS: 6.498218698862519e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 26/71 | LOSS: 6.521022088337405e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 27/71 | LOSS: 6.4694592083469615e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 28/71 | LOSS: 6.509856873514042e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 29/71 | LOSS: 6.472467354493953e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 30/71 | LOSS: 6.4847587905645804e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 31/71 | LOSS: 6.42155644925424e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 32/71 | LOSS: 6.473069871307499e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 33/71 | LOSS: 6.4219634222193715e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 34/71 | LOSS: 6.4217484577966385e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 35/71 | LOSS: 6.352191778407561e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 36/71 | LOSS: 6.371190146318121e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 37/71 | LOSS: 6.348735663969389e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 38/71 | LOSS: 6.321419615955958e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 39/71 | LOSS: 6.2824993278809416e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 40/71 | LOSS: 6.269803312844909e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 41/71 | LOSS: 6.244025793681171e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 42/71 | LOSS: 6.219611234058014e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 43/71 | LOSS: 6.179950704873508e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 44/71 | LOSS: 6.137602920514635e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 45/71 | LOSS: 6.1251084053251166e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 46/71 | LOSS: 6.119081766147692e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 47/71 | LOSS: 6.11298766746889e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 48/71 | LOSS: 6.0821061921086195e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 49/71 | LOSS: 6.08852493314771e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 50/71 | LOSS: 6.035508615205603e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 51/71 | LOSS: 6.017541444145116e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 52/71 | LOSS: 5.991286600891606e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 53/71 | LOSS: 5.954712824623803e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 54/71 | LOSS: 5.934326517572944e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 55/71 | LOSS: 5.87699265354656e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 56/71 | LOSS: 5.86016241804668e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 57/71 | LOSS: 5.856756576482941e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 58/71 | LOSS: 5.842987682904172e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 59/71 | LOSS: 5.818397350291586e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 60/71 | LOSS: 5.789849063924029e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 61/71 | LOSS: 5.760373127369319e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 62/71 | LOSS: 5.74521059134563e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 63/71 | LOSS: 5.723493380571654e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 64/71 | LOSS: 5.6833682824967234e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 65/71 | LOSS: 5.64319435108535e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 66/71 | LOSS: 5.633023508394149e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 67/71 | LOSS: 5.647683434851976e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 68/71 | LOSS: 5.6428626963231405e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 69/71 | LOSS: 5.636574532478075e-06\n",
      "TRAIN: EPOCH 336/1000 | BATCH 70/71 | LOSS: 5.608452322352169e-06\n",
      "VAL: EPOCH 336/1000 | BATCH 0/8 | LOSS: 6.309454875008669e-06\n",
      "VAL: EPOCH 336/1000 | BATCH 1/8 | LOSS: 5.937685273238458e-06\n",
      "VAL: EPOCH 336/1000 | BATCH 2/8 | LOSS: 5.711081939807627e-06\n",
      "VAL: EPOCH 336/1000 | BATCH 3/8 | LOSS: 5.629897486869595e-06\n",
      "VAL: EPOCH 336/1000 | BATCH 4/8 | LOSS: 5.573864837060683e-06\n",
      "VAL: EPOCH 336/1000 | BATCH 5/8 | LOSS: 5.253015084842143e-06\n",
      "VAL: EPOCH 336/1000 | BATCH 6/8 | LOSS: 5.200368832447566e-06\n",
      "VAL: EPOCH 336/1000 | BATCH 7/8 | LOSS: 5.135256913035846e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 0/71 | LOSS: 3.311625732749235e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 1/71 | LOSS: 4.602969738698448e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 2/71 | LOSS: 4.632770014723064e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 3/71 | LOSS: 4.335151231771306e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 4/71 | LOSS: 4.47107045147277e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 5/71 | LOSS: 4.335410987247694e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 6/71 | LOSS: 4.47731466530448e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 7/71 | LOSS: 4.671406003353695e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 8/71 | LOSS: 4.666035718198853e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 9/71 | LOSS: 4.7790684902793146e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 10/71 | LOSS: 4.758817980969483e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 11/71 | LOSS: 4.804944277717975e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 12/71 | LOSS: 4.862118255634694e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 13/71 | LOSS: 4.847301024061121e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 14/71 | LOSS: 4.896628388451063e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 15/71 | LOSS: 4.893329887067921e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 16/71 | LOSS: 4.929975295239915e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 17/71 | LOSS: 4.953652566857474e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 18/71 | LOSS: 5.0185254071708955e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 19/71 | LOSS: 5.0162429829470055e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 20/71 | LOSS: 5.022751305919623e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 21/71 | LOSS: 4.993944778355151e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 22/71 | LOSS: 4.952138456444474e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 23/71 | LOSS: 4.967600820767378e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 24/71 | LOSS: 4.936562572765979e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 25/71 | LOSS: 4.995146941137836e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 26/71 | LOSS: 5.029619008685889e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 27/71 | LOSS: 5.038940043635064e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 28/71 | LOSS: 5.067337207069143e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 29/71 | LOSS: 5.085112062867362e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 30/71 | LOSS: 5.068325905203012e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 31/71 | LOSS: 5.0351111511304225e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 32/71 | LOSS: 5.150781810174742e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 33/71 | LOSS: 5.113418818409093e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 34/71 | LOSS: 5.120361057769124e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 35/71 | LOSS: 5.135970398997516e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 36/71 | LOSS: 5.113953187213377e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 37/71 | LOSS: 5.1216928676168256e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 38/71 | LOSS: 5.152226053840609e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 39/71 | LOSS: 5.186590982475536e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 40/71 | LOSS: 5.20491279552871e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 41/71 | LOSS: 5.212592392516464e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 42/71 | LOSS: 5.269616548143541e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 43/71 | LOSS: 5.252041117645686e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 44/71 | LOSS: 5.276079360151521e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 45/71 | LOSS: 5.295879621213396e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 46/71 | LOSS: 5.293541209268485e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 47/71 | LOSS: 5.276886644386043e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 48/71 | LOSS: 5.269359012438182e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 49/71 | LOSS: 5.2636501050074e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 50/71 | LOSS: 5.249432366754798e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 51/71 | LOSS: 5.256492775410711e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 52/71 | LOSS: 5.2337674384583605e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 53/71 | LOSS: 5.23038541241813e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 54/71 | LOSS: 5.219229656590456e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 55/71 | LOSS: 5.218342952925273e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 56/71 | LOSS: 5.216188970224354e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 57/71 | LOSS: 5.203605649606696e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 58/71 | LOSS: 5.213984095308213e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 59/71 | LOSS: 5.228335601259459e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 60/71 | LOSS: 5.20747849790138e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 61/71 | LOSS: 5.190954847157334e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 62/71 | LOSS: 5.1985442886299486e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 63/71 | LOSS: 5.20315862573284e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 64/71 | LOSS: 5.18210819021848e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 65/71 | LOSS: 5.18996042005297e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 66/71 | LOSS: 5.18971035204747e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 67/71 | LOSS: 5.191949014715647e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 68/71 | LOSS: 5.16052710222341e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 69/71 | LOSS: 5.168721947614228e-06\n",
      "TRAIN: EPOCH 337/1000 | BATCH 70/71 | LOSS: 5.172927945801656e-06\n",
      "VAL: EPOCH 337/1000 | BATCH 0/8 | LOSS: 5.681879883923102e-06\n",
      "VAL: EPOCH 337/1000 | BATCH 1/8 | LOSS: 5.238973699306371e-06\n",
      "VAL: EPOCH 337/1000 | BATCH 2/8 | LOSS: 5.188978017637662e-06\n",
      "VAL: EPOCH 337/1000 | BATCH 3/8 | LOSS: 5.235024900684948e-06\n",
      "VAL: EPOCH 337/1000 | BATCH 4/8 | LOSS: 5.142281224834733e-06\n",
      "VAL: EPOCH 337/1000 | BATCH 5/8 | LOSS: 4.952491963194916e-06\n",
      "VAL: EPOCH 337/1000 | BATCH 6/8 | LOSS: 4.899384163893826e-06\n",
      "VAL: EPOCH 337/1000 | BATCH 7/8 | LOSS: 4.785719511346542e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 0/71 | LOSS: 6.163527359603904e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 1/71 | LOSS: 5.6634698921698146e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 2/71 | LOSS: 5.362635268587231e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 3/71 | LOSS: 4.960952878718672e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 4/71 | LOSS: 4.887275554210646e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 5/71 | LOSS: 4.904645872253847e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 6/71 | LOSS: 5.089833134011964e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 7/71 | LOSS: 5.2533696361933835e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 8/71 | LOSS: 5.3791013164704455e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 9/71 | LOSS: 5.406485524872551e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 10/71 | LOSS: 5.242583938525058e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 11/71 | LOSS: 5.263025741442107e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 12/71 | LOSS: 5.267541242714147e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 13/71 | LOSS: 5.20935327585903e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 14/71 | LOSS: 5.244750142689251e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 15/71 | LOSS: 5.206289387160723e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 16/71 | LOSS: 5.161598063055221e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 17/71 | LOSS: 5.188384850245913e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 18/71 | LOSS: 5.215711601187267e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 19/71 | LOSS: 5.1333736564629365e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 20/71 | LOSS: 5.12060599551963e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 21/71 | LOSS: 5.114233111023416e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 22/71 | LOSS: 5.055201101008013e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 23/71 | LOSS: 5.090984889951263e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 24/71 | LOSS: 5.068041937192902e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 25/71 | LOSS: 5.1628532953775275e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 26/71 | LOSS: 5.144494244396574e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 27/71 | LOSS: 5.175073982042834e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 28/71 | LOSS: 5.196815687542829e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 29/71 | LOSS: 5.276286295459917e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 30/71 | LOSS: 5.281109063235494e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 31/71 | LOSS: 5.39480349459609e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 32/71 | LOSS: 5.4642617380357645e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 33/71 | LOSS: 5.4368582261625256e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 34/71 | LOSS: 5.470266257491728e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 35/71 | LOSS: 5.440310613064665e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 36/71 | LOSS: 5.52387118168062e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 37/71 | LOSS: 5.523419507593835e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 38/71 | LOSS: 5.634319934007437e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 39/71 | LOSS: 5.6247026805067435e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 40/71 | LOSS: 5.6956914926470235e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 41/71 | LOSS: 5.704742750908952e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 42/71 | LOSS: 5.752774731538307e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 43/71 | LOSS: 5.7549458047519275e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 44/71 | LOSS: 5.737935761216149e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 45/71 | LOSS: 5.788717455447034e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 46/71 | LOSS: 5.782273237468387e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 47/71 | LOSS: 5.792393172517525e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 48/71 | LOSS: 5.795290448708217e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 49/71 | LOSS: 5.769442223026999e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 50/71 | LOSS: 5.796515199786061e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 51/71 | LOSS: 5.771097833600764e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 52/71 | LOSS: 5.7895408730782605e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 53/71 | LOSS: 5.774204779569388e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 54/71 | LOSS: 5.766330708535283e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 55/71 | LOSS: 5.75745482527574e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 56/71 | LOSS: 5.7426543223795175e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 57/71 | LOSS: 5.717646092370128e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 58/71 | LOSS: 5.703654813383394e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 59/71 | LOSS: 5.687326893166755e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 60/71 | LOSS: 5.660952858127784e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 61/71 | LOSS: 5.633065895029088e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 62/71 | LOSS: 5.598085231718465e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 63/71 | LOSS: 5.580437576924169e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 64/71 | LOSS: 5.559950271232922e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 65/71 | LOSS: 5.562492267954715e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 66/71 | LOSS: 5.552141201134411e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 67/71 | LOSS: 5.536770997343196e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 68/71 | LOSS: 5.530203099914471e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 69/71 | LOSS: 5.514062926132672e-06\n",
      "TRAIN: EPOCH 338/1000 | BATCH 70/71 | LOSS: 5.5077157957013285e-06\n",
      "VAL: EPOCH 338/1000 | BATCH 0/8 | LOSS: 5.421613877842901e-06\n",
      "VAL: EPOCH 338/1000 | BATCH 1/8 | LOSS: 5.057006319475477e-06\n",
      "VAL: EPOCH 338/1000 | BATCH 2/8 | LOSS: 5.419222209942139e-06\n",
      "VAL: EPOCH 338/1000 | BATCH 3/8 | LOSS: 5.438395533019502e-06\n",
      "VAL: EPOCH 338/1000 | BATCH 4/8 | LOSS: 5.389303532865597e-06\n",
      "VAL: EPOCH 338/1000 | BATCH 5/8 | LOSS: 5.348561899154447e-06\n",
      "VAL: EPOCH 338/1000 | BATCH 6/8 | LOSS: 5.270817187660473e-06\n",
      "VAL: EPOCH 338/1000 | BATCH 7/8 | LOSS: 5.310108008416137e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 0/71 | LOSS: 6.147326075733872e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 1/71 | LOSS: 5.697728965969873e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 2/71 | LOSS: 5.163500190974446e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 3/71 | LOSS: 5.124887479723839e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 4/71 | LOSS: 4.895312940789154e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 5/71 | LOSS: 4.64608721510255e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 6/71 | LOSS: 4.5627486672726396e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 7/71 | LOSS: 4.5346906176746415e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 8/71 | LOSS: 4.508337598154968e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 9/71 | LOSS: 4.553876306090388e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 10/71 | LOSS: 4.625503276077904e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 11/71 | LOSS: 4.53872445405068e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 12/71 | LOSS: 4.627392120080633e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 13/71 | LOSS: 4.614283674137758e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 14/71 | LOSS: 4.582485613961277e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 15/71 | LOSS: 4.632079040334247e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 16/71 | LOSS: 4.698638015136513e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 17/71 | LOSS: 4.728928527381666e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 18/71 | LOSS: 4.637565959791046e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 19/71 | LOSS: 4.604138678132586e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 20/71 | LOSS: 4.631663235462232e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 21/71 | LOSS: 4.631541959166828e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 22/71 | LOSS: 4.7190340944286184e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 23/71 | LOSS: 4.704050439841012e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 24/71 | LOSS: 4.868797759627341e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 25/71 | LOSS: 4.872493762447378e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 26/71 | LOSS: 4.909333113203131e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 27/71 | LOSS: 4.886211424296302e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 28/71 | LOSS: 4.86266005031337e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 29/71 | LOSS: 4.943615029636324e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 30/71 | LOSS: 4.9004727389406215e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 31/71 | LOSS: 4.886749820798286e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 32/71 | LOSS: 4.8576114777736645e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 33/71 | LOSS: 4.8415818930473456e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 34/71 | LOSS: 4.868758774786589e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 35/71 | LOSS: 4.8861334612916434e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 36/71 | LOSS: 4.926983082613505e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 37/71 | LOSS: 4.921426149226854e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 38/71 | LOSS: 4.9674714691951895e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 39/71 | LOSS: 4.9553986173123125e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 40/71 | LOSS: 4.975545513244891e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 41/71 | LOSS: 4.9714114668729185e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 42/71 | LOSS: 4.962668881066642e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 43/71 | LOSS: 4.997468388203495e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 44/71 | LOSS: 4.994789701918813e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 45/71 | LOSS: 4.986130405719134e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 46/71 | LOSS: 4.981351565297797e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 47/71 | LOSS: 4.9866790163832775e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 48/71 | LOSS: 4.996449781768262e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 49/71 | LOSS: 5.0225505492562665e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 50/71 | LOSS: 5.005016156050545e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 51/71 | LOSS: 5.002237886541339e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 52/71 | LOSS: 4.983732548491801e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 53/71 | LOSS: 4.957447244022559e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 54/71 | LOSS: 4.954676065112422e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 55/71 | LOSS: 4.990457179181769e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 56/71 | LOSS: 4.997719331680728e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 57/71 | LOSS: 4.975097971478605e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 58/71 | LOSS: 4.990527154944376e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 59/71 | LOSS: 4.977901433752171e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 60/71 | LOSS: 4.961528993291841e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 61/71 | LOSS: 4.964939982469772e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 62/71 | LOSS: 4.955034376233391e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 63/71 | LOSS: 4.957814702777341e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 64/71 | LOSS: 4.940025592776902e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 65/71 | LOSS: 4.931620696190679e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 66/71 | LOSS: 4.925520466821947e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 67/71 | LOSS: 4.919397079394163e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 68/71 | LOSS: 4.913593273527837e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 69/71 | LOSS: 4.905149376099871e-06\n",
      "TRAIN: EPOCH 339/1000 | BATCH 70/71 | LOSS: 4.881553134599555e-06\n",
      "VAL: EPOCH 339/1000 | BATCH 0/8 | LOSS: 5.250506546872202e-06\n",
      "VAL: EPOCH 339/1000 | BATCH 1/8 | LOSS: 4.806803644896718e-06\n",
      "VAL: EPOCH 339/1000 | BATCH 2/8 | LOSS: 4.766517425499235e-06\n",
      "VAL: EPOCH 339/1000 | BATCH 3/8 | LOSS: 4.81296945054055e-06\n",
      "VAL: EPOCH 339/1000 | BATCH 4/8 | LOSS: 4.72705560241593e-06\n",
      "VAL: EPOCH 339/1000 | BATCH 5/8 | LOSS: 4.5791841785103315e-06\n",
      "VAL: EPOCH 339/1000 | BATCH 6/8 | LOSS: 4.537850340316904e-06\n",
      "VAL: EPOCH 339/1000 | BATCH 7/8 | LOSS: 4.441119358489232e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 0/71 | LOSS: 2.925087756011635e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 1/71 | LOSS: 3.509245516397641e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 2/71 | LOSS: 3.8027863714281316e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 3/71 | LOSS: 3.7940239394629316e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 4/71 | LOSS: 4.028605826533749e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 5/71 | LOSS: 4.043329492257423e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 6/71 | LOSS: 4.258927966865096e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 7/71 | LOSS: 4.254143476600802e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 8/71 | LOSS: 4.433066376602963e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 9/71 | LOSS: 4.34850851434021e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 10/71 | LOSS: 4.506551228835385e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 11/71 | LOSS: 4.618458111356934e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 12/71 | LOSS: 4.7477305123231e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 13/71 | LOSS: 4.726210802021212e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 14/71 | LOSS: 4.84174202028953e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 15/71 | LOSS: 4.859338147866765e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 16/71 | LOSS: 4.8171587101986756e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 17/71 | LOSS: 4.798247661912885e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 18/71 | LOSS: 4.9474572997017895e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 19/71 | LOSS: 5.066168694156659e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 20/71 | LOSS: 5.046219192601191e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 21/71 | LOSS: 5.14956893353387e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 22/71 | LOSS: 5.218686065627085e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 23/71 | LOSS: 5.204970240887026e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 24/71 | LOSS: 5.3178330290393205e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 25/71 | LOSS: 5.356616620352165e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 26/71 | LOSS: 5.325004344235316e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 27/71 | LOSS: 5.385907367586437e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 28/71 | LOSS: 5.452252237554258e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 29/71 | LOSS: 5.427449415644029e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 30/71 | LOSS: 5.4029731164783086e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 31/71 | LOSS: 5.396835909721176e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 32/71 | LOSS: 5.382238703422252e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 33/71 | LOSS: 5.348695922561199e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 34/71 | LOSS: 5.314615483647295e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 35/71 | LOSS: 5.338941598943671e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 36/71 | LOSS: 5.316693850053943e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 37/71 | LOSS: 5.300929286166042e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 38/71 | LOSS: 5.275011243354992e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 39/71 | LOSS: 5.266259375957816e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 40/71 | LOSS: 5.287007336558351e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 41/71 | LOSS: 5.2936843476778626e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 42/71 | LOSS: 5.272689603834594e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 43/71 | LOSS: 5.2756788332689295e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 44/71 | LOSS: 5.308445820951925e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 45/71 | LOSS: 5.302826786607076e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 46/71 | LOSS: 5.335016751021303e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 47/71 | LOSS: 5.346069556821931e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 48/71 | LOSS: 5.418220706239796e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 49/71 | LOSS: 5.435454036160081e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 50/71 | LOSS: 5.4643218364242995e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 51/71 | LOSS: 5.467037108421489e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 52/71 | LOSS: 5.512613554701774e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 53/71 | LOSS: 5.501690225173661e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 54/71 | LOSS: 5.513036529175059e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 55/71 | LOSS: 5.493482908442664e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 56/71 | LOSS: 5.514188515326966e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 57/71 | LOSS: 5.502623703877957e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 58/71 | LOSS: 5.514899990206902e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 59/71 | LOSS: 5.522791764178692e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 60/71 | LOSS: 5.521398831854744e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 61/71 | LOSS: 5.503279479149393e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 62/71 | LOSS: 5.46196277658358e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 63/71 | LOSS: 5.4377037663755345e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 64/71 | LOSS: 5.448136404923459e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 65/71 | LOSS: 5.424178146478715e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 66/71 | LOSS: 5.417310593480552e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 67/71 | LOSS: 5.440827491758468e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 68/71 | LOSS: 5.454437044044017e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 69/71 | LOSS: 5.456799470136632e-06\n",
      "TRAIN: EPOCH 340/1000 | BATCH 70/71 | LOSS: 5.434764975881324e-06\n",
      "VAL: EPOCH 340/1000 | BATCH 0/8 | LOSS: 5.480043455463601e-06\n",
      "VAL: EPOCH 340/1000 | BATCH 1/8 | LOSS: 5.327669668986346e-06\n",
      "VAL: EPOCH 340/1000 | BATCH 2/8 | LOSS: 5.108478035253938e-06\n",
      "VAL: EPOCH 340/1000 | BATCH 3/8 | LOSS: 5.226888902143401e-06\n",
      "VAL: EPOCH 340/1000 | BATCH 4/8 | LOSS: 5.133555623615393e-06\n",
      "VAL: EPOCH 340/1000 | BATCH 5/8 | LOSS: 4.861967454417027e-06\n",
      "VAL: EPOCH 340/1000 | BATCH 6/8 | LOSS: 4.7735230249859995e-06\n",
      "VAL: EPOCH 340/1000 | BATCH 7/8 | LOSS: 4.617689796759805e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 0/71 | LOSS: 4.4193902795086615e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 1/71 | LOSS: 4.119606728636427e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 2/71 | LOSS: 4.436796643858543e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 3/71 | LOSS: 4.599741600941343e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 4/71 | LOSS: 4.732205070467899e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 5/71 | LOSS: 4.8612087084620725e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 6/71 | LOSS: 4.859638595787276e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 7/71 | LOSS: 4.858875570334931e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 8/71 | LOSS: 4.746303299422531e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 9/71 | LOSS: 4.888185094387154e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 10/71 | LOSS: 5.0503427205099305e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 11/71 | LOSS: 5.002029183742707e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 12/71 | LOSS: 4.944268102218092e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 13/71 | LOSS: 4.934254807916919e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 14/71 | LOSS: 5.040257231788322e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 15/71 | LOSS: 4.9804766888428276e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 16/71 | LOSS: 5.029850747044239e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 17/71 | LOSS: 4.9713957600437825e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 18/71 | LOSS: 5.0183858356352794e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 19/71 | LOSS: 5.0056525196851e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 20/71 | LOSS: 5.11164199408432e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 21/71 | LOSS: 5.078576867328428e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 22/71 | LOSS: 5.068129093674741e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 23/71 | LOSS: 5.047586303893088e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 24/71 | LOSS: 5.060262665210757e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 25/71 | LOSS: 5.083983051708156e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 26/71 | LOSS: 5.057930980843527e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 27/71 | LOSS: 5.06815643218163e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 28/71 | LOSS: 5.036586007290994e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 29/71 | LOSS: 5.016112860782111e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 30/71 | LOSS: 4.984165438908084e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 31/71 | LOSS: 4.9662198335909125e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 32/71 | LOSS: 4.965982939073442e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 33/71 | LOSS: 4.941145896416e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 34/71 | LOSS: 4.907952877277109e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 35/71 | LOSS: 4.912169970339164e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 36/71 | LOSS: 4.894660550407197e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 37/71 | LOSS: 4.915345889936648e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 38/71 | LOSS: 4.915505623779236e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 39/71 | LOSS: 4.902747559754062e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 40/71 | LOSS: 4.906191828390215e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 41/71 | LOSS: 4.920862855734802e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 42/71 | LOSS: 5.01274460179428e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 43/71 | LOSS: 5.0106888651994685e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 44/71 | LOSS: 5.080695266062523e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 45/71 | LOSS: 5.0965914634498004e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 46/71 | LOSS: 5.170860963807308e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 47/71 | LOSS: 5.173976461492202e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 48/71 | LOSS: 5.244012726859869e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 49/71 | LOSS: 5.26341166732891e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 50/71 | LOSS: 5.258810866579035e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 51/71 | LOSS: 5.290139789884472e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 52/71 | LOSS: 5.29872474698833e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 53/71 | LOSS: 5.336758687446019e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 54/71 | LOSS: 5.329096877010835e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 55/71 | LOSS: 5.357690091451721e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 56/71 | LOSS: 5.3959563541738725e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 57/71 | LOSS: 5.389869376562399e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 58/71 | LOSS: 5.427721173303218e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 59/71 | LOSS: 5.445300894280081e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 60/71 | LOSS: 5.4661474051932065e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 61/71 | LOSS: 5.452177762685272e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 62/71 | LOSS: 5.482106312156825e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 63/71 | LOSS: 5.50797967946437e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 64/71 | LOSS: 5.528591645997949e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 65/71 | LOSS: 5.54192796919255e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 66/71 | LOSS: 5.54958528686148e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 67/71 | LOSS: 5.5772236999687535e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 68/71 | LOSS: 5.568417295960345e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 69/71 | LOSS: 5.569523052275015e-06\n",
      "TRAIN: EPOCH 341/1000 | BATCH 70/71 | LOSS: 5.547155280868871e-06\n",
      "VAL: EPOCH 341/1000 | BATCH 0/8 | LOSS: 6.945335826458177e-06\n",
      "VAL: EPOCH 341/1000 | BATCH 1/8 | LOSS: 6.643979986620252e-06\n",
      "VAL: EPOCH 341/1000 | BATCH 2/8 | LOSS: 6.632929550202486e-06\n",
      "VAL: EPOCH 341/1000 | BATCH 3/8 | LOSS: 6.855762990198855e-06\n",
      "VAL: EPOCH 341/1000 | BATCH 4/8 | LOSS: 6.6164920099254235e-06\n",
      "VAL: EPOCH 341/1000 | BATCH 5/8 | LOSS: 6.358491646096809e-06\n",
      "VAL: EPOCH 341/1000 | BATCH 6/8 | LOSS: 6.262626811803784e-06\n",
      "VAL: EPOCH 341/1000 | BATCH 7/8 | LOSS: 6.1085798961357796e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 0/71 | LOSS: 7.35338517188211e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 1/71 | LOSS: 5.695432491847896e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 2/71 | LOSS: 5.474886165757198e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 3/71 | LOSS: 5.3870708143222146e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 4/71 | LOSS: 5.55479582544649e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 5/71 | LOSS: 5.289794065295912e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 6/71 | LOSS: 5.347735493290072e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 7/71 | LOSS: 5.329004522991454e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 8/71 | LOSS: 5.593193388146271e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 9/71 | LOSS: 5.516325427379343e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 10/71 | LOSS: 5.7086597487134645e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 11/71 | LOSS: 5.802405932323988e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 12/71 | LOSS: 5.687463096994459e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 13/71 | LOSS: 5.653407275271352e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 14/71 | LOSS: 5.594359481619904e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 15/71 | LOSS: 5.579211432404918e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 16/71 | LOSS: 5.44910811667625e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 17/71 | LOSS: 5.3809736881602375e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 18/71 | LOSS: 5.398120145783698e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 19/71 | LOSS: 5.36112665940891e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 20/71 | LOSS: 5.3052068497414615e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 21/71 | LOSS: 5.266716780996913e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 22/71 | LOSS: 5.262056694608992e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 23/71 | LOSS: 5.224021833782899e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 24/71 | LOSS: 5.185445425013313e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 25/71 | LOSS: 5.182287255733032e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 26/71 | LOSS: 5.132816517693249e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 27/71 | LOSS: 5.118898457112664e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 28/71 | LOSS: 5.042960647815248e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 29/71 | LOSS: 5.179168488211871e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 30/71 | LOSS: 5.150791902187555e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 31/71 | LOSS: 5.2160682528779034e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 32/71 | LOSS: 5.192987642162321e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 33/71 | LOSS: 5.318284573096368e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 34/71 | LOSS: 5.32975102162579e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 35/71 | LOSS: 5.370069585650425e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 36/71 | LOSS: 5.439015627121694e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 37/71 | LOSS: 5.436199929040174e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 38/71 | LOSS: 5.481811297450361e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 39/71 | LOSS: 5.456740660747528e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 40/71 | LOSS: 5.508715048563317e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 41/71 | LOSS: 5.488168031184787e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 42/71 | LOSS: 5.510718707693402e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 43/71 | LOSS: 5.505960649026582e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 44/71 | LOSS: 5.527780563675656e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 45/71 | LOSS: 5.499123601467822e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 46/71 | LOSS: 5.4774715060394805e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 47/71 | LOSS: 5.463822641142239e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 48/71 | LOSS: 5.457425459098886e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 49/71 | LOSS: 5.4697993573427085e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 50/71 | LOSS: 5.457433635634587e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 51/71 | LOSS: 5.418609029780223e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 52/71 | LOSS: 5.409629835103604e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 53/71 | LOSS: 5.424526703050274e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 54/71 | LOSS: 5.425125858561263e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 55/71 | LOSS: 5.401965040618961e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 56/71 | LOSS: 5.411182236888348e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 57/71 | LOSS: 5.407294456117911e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 58/71 | LOSS: 5.39808102615298e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 59/71 | LOSS: 5.392278982678059e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 60/71 | LOSS: 5.411806826293182e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 61/71 | LOSS: 5.399036838078304e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 62/71 | LOSS: 5.387762080823046e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 63/71 | LOSS: 5.38763774926565e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 64/71 | LOSS: 5.388410241161742e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 65/71 | LOSS: 5.3612230369088305e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 66/71 | LOSS: 5.347481874196092e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 67/71 | LOSS: 5.360976047086297e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 68/71 | LOSS: 5.3442185083480105e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 69/71 | LOSS: 5.324716156402636e-06\n",
      "TRAIN: EPOCH 342/1000 | BATCH 70/71 | LOSS: 5.3264853307501e-06\n",
      "VAL: EPOCH 342/1000 | BATCH 0/8 | LOSS: 5.93059712628019e-06\n",
      "VAL: EPOCH 342/1000 | BATCH 1/8 | LOSS: 5.590440650848905e-06\n",
      "VAL: EPOCH 342/1000 | BATCH 2/8 | LOSS: 5.469448903265099e-06\n",
      "VAL: EPOCH 342/1000 | BATCH 3/8 | LOSS: 5.4665406423737295e-06\n",
      "VAL: EPOCH 342/1000 | BATCH 4/8 | LOSS: 5.367693393054651e-06\n",
      "VAL: EPOCH 342/1000 | BATCH 5/8 | LOSS: 5.107371293888718e-06\n",
      "VAL: EPOCH 342/1000 | BATCH 6/8 | LOSS: 5.060964960128851e-06\n",
      "VAL: EPOCH 342/1000 | BATCH 7/8 | LOSS: 4.9489643458855426e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 0/71 | LOSS: 5.083472387923393e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 1/71 | LOSS: 4.937185849485104e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 2/71 | LOSS: 5.084654427870798e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 3/71 | LOSS: 5.1451036142680096e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 4/71 | LOSS: 5.3344978368841115e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 5/71 | LOSS: 5.155656557083906e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 6/71 | LOSS: 5.225987414243198e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 7/71 | LOSS: 5.150000447429193e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 8/71 | LOSS: 5.0822790803471835e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 9/71 | LOSS: 5.126222686158144e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 10/71 | LOSS: 5.151919057508084e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 11/71 | LOSS: 5.139110423139452e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 12/71 | LOSS: 5.101070620674784e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 13/71 | LOSS: 5.137031101704841e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 14/71 | LOSS: 5.103279727336485e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 15/71 | LOSS: 5.130144870690856e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 16/71 | LOSS: 5.0891487583559116e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 17/71 | LOSS: 5.017757884060201e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 18/71 | LOSS: 4.9806241661013035e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 19/71 | LOSS: 4.9576353148950146e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 20/71 | LOSS: 4.894530342252914e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 21/71 | LOSS: 4.903080542300648e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 22/71 | LOSS: 4.884827989292757e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 23/71 | LOSS: 4.8410787864365075e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 24/71 | LOSS: 4.788372007169528e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 25/71 | LOSS: 4.781864845426753e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 26/71 | LOSS: 4.751682387938706e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 27/71 | LOSS: 4.7581306879302635e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 28/71 | LOSS: 4.801067587260181e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 29/71 | LOSS: 4.805669550478342e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 30/71 | LOSS: 4.802492551994698e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 31/71 | LOSS: 4.80077136444379e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 32/71 | LOSS: 4.7883265572388405e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 33/71 | LOSS: 4.769419212928315e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 34/71 | LOSS: 4.7732944299890996e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 35/71 | LOSS: 4.766658308754914e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 36/71 | LOSS: 4.761967424250592e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 37/71 | LOSS: 4.767297169612568e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 38/71 | LOSS: 4.790905455966145e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 39/71 | LOSS: 4.767053462728655e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 40/71 | LOSS: 4.809596514572083e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 41/71 | LOSS: 4.791024853797613e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 42/71 | LOSS: 4.798430498255563e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 43/71 | LOSS: 4.780819887690251e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 44/71 | LOSS: 4.783703803291751e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 45/71 | LOSS: 4.78704682000566e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 46/71 | LOSS: 4.805356213105273e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 47/71 | LOSS: 4.790045835534329e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 48/71 | LOSS: 4.84182763622292e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 49/71 | LOSS: 4.828212595384684e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 50/71 | LOSS: 4.855983590067469e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 51/71 | LOSS: 4.846654825576899e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 52/71 | LOSS: 4.834264104228734e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 53/71 | LOSS: 4.844065583879624e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 54/71 | LOSS: 4.852081664549504e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 55/71 | LOSS: 4.846111575521458e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 56/71 | LOSS: 4.843968007774317e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 57/71 | LOSS: 4.8388768187453085e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 58/71 | LOSS: 4.820323098656599e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 59/71 | LOSS: 4.82588546522796e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 60/71 | LOSS: 4.824341110671397e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 61/71 | LOSS: 4.812296178750975e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 62/71 | LOSS: 4.799627009878537e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 63/71 | LOSS: 4.806061788542593e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 64/71 | LOSS: 4.8068901448389695e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 65/71 | LOSS: 4.794860188033481e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 66/71 | LOSS: 4.794697489879622e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 67/71 | LOSS: 4.79470712461454e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 68/71 | LOSS: 4.79683495027332e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 69/71 | LOSS: 4.789727277706593e-06\n",
      "TRAIN: EPOCH 343/1000 | BATCH 70/71 | LOSS: 4.812713599112868e-06\n",
      "VAL: EPOCH 343/1000 | BATCH 0/8 | LOSS: 5.28244936504052e-06\n",
      "VAL: EPOCH 343/1000 | BATCH 1/8 | LOSS: 4.9872462568600895e-06\n",
      "VAL: EPOCH 343/1000 | BATCH 2/8 | LOSS: 4.99099390556997e-06\n",
      "VAL: EPOCH 343/1000 | BATCH 3/8 | LOSS: 4.923517167298996e-06\n",
      "VAL: EPOCH 343/1000 | BATCH 4/8 | LOSS: 4.886282204097369e-06\n",
      "VAL: EPOCH 343/1000 | BATCH 5/8 | LOSS: 4.629757806166405e-06\n",
      "VAL: EPOCH 343/1000 | BATCH 6/8 | LOSS: 4.497011884398359e-06\n",
      "VAL: EPOCH 343/1000 | BATCH 7/8 | LOSS: 4.409032953844871e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 0/71 | LOSS: 4.7561215978930704e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 1/71 | LOSS: 4.424134203873109e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 2/71 | LOSS: 4.510606080051123e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 3/71 | LOSS: 4.515349814937508e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 4/71 | LOSS: 4.505814922595164e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 5/71 | LOSS: 4.448101435627905e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 6/71 | LOSS: 4.4217832899968405e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 7/71 | LOSS: 4.501943521972862e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 8/71 | LOSS: 4.49472412987638e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 9/71 | LOSS: 4.384975090943044e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 10/71 | LOSS: 4.361484074748163e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 11/71 | LOSS: 4.470566030552921e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 12/71 | LOSS: 4.485252317793381e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 13/71 | LOSS: 4.654955124091689e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 14/71 | LOSS: 4.7444822484976615e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 15/71 | LOSS: 4.75658765708431e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 16/71 | LOSS: 4.814363873104894e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 17/71 | LOSS: 4.944417014485225e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 18/71 | LOSS: 4.9831712060136185e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 19/71 | LOSS: 5.017301464249613e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 20/71 | LOSS: 5.0290042227494445e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 21/71 | LOSS: 5.060093433877972e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 22/71 | LOSS: 5.13088626457434e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 23/71 | LOSS: 5.156273118700483e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 24/71 | LOSS: 5.17102780577261e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 25/71 | LOSS: 5.1674340179138544e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 26/71 | LOSS: 5.1640287252936375e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 27/71 | LOSS: 5.127709544337579e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 28/71 | LOSS: 5.096956383881504e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 29/71 | LOSS: 5.0652340026620855e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 30/71 | LOSS: 5.044845707861016e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 31/71 | LOSS: 5.008590548527536e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 32/71 | LOSS: 5.007921751684714e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 33/71 | LOSS: 4.965143288620097e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 34/71 | LOSS: 4.9575743235306745e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 35/71 | LOSS: 4.9245139861240104e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 36/71 | LOSS: 4.945603745294943e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 37/71 | LOSS: 4.941682726367665e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 38/71 | LOSS: 4.944146742976348e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 39/71 | LOSS: 4.936069728955772e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 40/71 | LOSS: 4.92138605761461e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 41/71 | LOSS: 4.898416699732133e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 42/71 | LOSS: 4.894236405752328e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 43/71 | LOSS: 4.910503600347676e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 44/71 | LOSS: 4.916873220079449e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 45/71 | LOSS: 4.91829894639048e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 46/71 | LOSS: 4.890426375726399e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 47/71 | LOSS: 4.890261801430522e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 48/71 | LOSS: 4.888794504096244e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 49/71 | LOSS: 4.902211908301979e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 50/71 | LOSS: 4.882127037561469e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 51/71 | LOSS: 4.8742697038925635e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 52/71 | LOSS: 4.87963659409461e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 53/71 | LOSS: 4.862110510354493e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 54/71 | LOSS: 4.8710961519563256e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 55/71 | LOSS: 4.870221805016886e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 56/71 | LOSS: 4.893324917008469e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 57/71 | LOSS: 4.887723258465924e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 58/71 | LOSS: 4.88486179458081e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 59/71 | LOSS: 4.865392031661031e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 60/71 | LOSS: 4.868991377630574e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 61/71 | LOSS: 4.871666353591859e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 62/71 | LOSS: 4.85484940240678e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 63/71 | LOSS: 4.850124756927698e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 64/71 | LOSS: 4.855416790843055e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 65/71 | LOSS: 4.8472971911905915e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 66/71 | LOSS: 4.833512378748161e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 67/71 | LOSS: 4.83318487955758e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 68/71 | LOSS: 4.845124166145663e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 69/71 | LOSS: 4.840070606145933e-06\n",
      "TRAIN: EPOCH 344/1000 | BATCH 70/71 | LOSS: 4.833774245923105e-06\n",
      "VAL: EPOCH 344/1000 | BATCH 0/8 | LOSS: 5.677991339325672e-06\n",
      "VAL: EPOCH 344/1000 | BATCH 1/8 | LOSS: 5.330074600351509e-06\n",
      "VAL: EPOCH 344/1000 | BATCH 2/8 | LOSS: 5.3473916826381656e-06\n",
      "VAL: EPOCH 344/1000 | BATCH 3/8 | LOSS: 5.3195412874629255e-06\n",
      "VAL: EPOCH 344/1000 | BATCH 4/8 | LOSS: 5.270731526252348e-06\n",
      "VAL: EPOCH 344/1000 | BATCH 5/8 | LOSS: 5.075517644096787e-06\n",
      "VAL: EPOCH 344/1000 | BATCH 6/8 | LOSS: 5.060688896004909e-06\n",
      "VAL: EPOCH 344/1000 | BATCH 7/8 | LOSS: 5.025233519972971e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 0/71 | LOSS: 4.644093223760137e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 1/71 | LOSS: 4.553307007881813e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 2/71 | LOSS: 4.47089390339291e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 3/71 | LOSS: 4.664862785830337e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 4/71 | LOSS: 4.591043125401484e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 5/71 | LOSS: 5.086806822873768e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 6/71 | LOSS: 5.221259600927754e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 7/71 | LOSS: 5.503254271843616e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 8/71 | LOSS: 5.385546955949394e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 9/71 | LOSS: 5.270637166177039e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 10/71 | LOSS: 5.1614391519582235e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 11/71 | LOSS: 5.213222948441398e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 12/71 | LOSS: 5.180355226632226e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 13/71 | LOSS: 5.256520028394464e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 14/71 | LOSS: 5.243577137055884e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 15/71 | LOSS: 5.350600844167275e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 16/71 | LOSS: 5.3321327601117766e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 17/71 | LOSS: 5.264401983772081e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 18/71 | LOSS: 5.21874979217261e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 19/71 | LOSS: 5.223493985795358e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 20/71 | LOSS: 5.190222834374124e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 21/71 | LOSS: 5.198601560220016e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 22/71 | LOSS: 5.1890118139335355e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 23/71 | LOSS: 5.182754269602204e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 24/71 | LOSS: 5.160209984751418e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 25/71 | LOSS: 5.095336037951571e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 26/71 | LOSS: 5.0696529612010906e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 27/71 | LOSS: 5.049488736728693e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 28/71 | LOSS: 5.011291430742733e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 29/71 | LOSS: 5.014308961411492e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 30/71 | LOSS: 5.0003334895483285e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 31/71 | LOSS: 4.991525862863e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 32/71 | LOSS: 4.959547595717595e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 33/71 | LOSS: 4.952453824604591e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 34/71 | LOSS: 4.958006469938222e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 35/71 | LOSS: 4.954957874916646e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 36/71 | LOSS: 4.9381507127494644e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 37/71 | LOSS: 4.89836720350324e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 38/71 | LOSS: 4.885455580687192e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 39/71 | LOSS: 4.890428749604326e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 40/71 | LOSS: 4.868205684122186e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 41/71 | LOSS: 4.852968210062023e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 42/71 | LOSS: 4.8324239731639036e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 43/71 | LOSS: 4.824929750281047e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 44/71 | LOSS: 4.816380775688837e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 45/71 | LOSS: 4.790156738280253e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 46/71 | LOSS: 4.801424583615215e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 47/71 | LOSS: 4.795220642487645e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 48/71 | LOSS: 4.7663510421054e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 49/71 | LOSS: 4.796811304004222e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 50/71 | LOSS: 4.8022786367986485e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 51/71 | LOSS: 4.801029503526549e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 52/71 | LOSS: 4.812875907723452e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 53/71 | LOSS: 4.837297743270702e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 54/71 | LOSS: 4.863172319925401e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 55/71 | LOSS: 4.8742665878113e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 56/71 | LOSS: 4.882116559600578e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 57/71 | LOSS: 4.884119221060246e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 58/71 | LOSS: 4.906073268950607e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 59/71 | LOSS: 4.916425689316384e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 60/71 | LOSS: 4.934707319870406e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 61/71 | LOSS: 4.937903297779426e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 62/71 | LOSS: 4.930175753052315e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 63/71 | LOSS: 4.93806208723413e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 64/71 | LOSS: 4.942541709869357e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 65/71 | LOSS: 4.942858309374115e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 66/71 | LOSS: 4.935558145821708e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 67/71 | LOSS: 4.940791337262721e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 68/71 | LOSS: 4.923189800283053e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 69/71 | LOSS: 4.912744209052886e-06\n",
      "TRAIN: EPOCH 345/1000 | BATCH 70/71 | LOSS: 4.9324347708559955e-06\n",
      "VAL: EPOCH 345/1000 | BATCH 0/8 | LOSS: 5.571038855123334e-06\n",
      "VAL: EPOCH 345/1000 | BATCH 1/8 | LOSS: 5.0799574182747165e-06\n",
      "VAL: EPOCH 345/1000 | BATCH 2/8 | LOSS: 5.082360833815376e-06\n",
      "VAL: EPOCH 345/1000 | BATCH 3/8 | LOSS: 5.0664142463574535e-06\n",
      "VAL: EPOCH 345/1000 | BATCH 4/8 | LOSS: 4.9940216740651525e-06\n",
      "VAL: EPOCH 345/1000 | BATCH 5/8 | LOSS: 4.871978868929243e-06\n",
      "VAL: EPOCH 345/1000 | BATCH 6/8 | LOSS: 4.835953144980262e-06\n",
      "VAL: EPOCH 345/1000 | BATCH 7/8 | LOSS: 4.7737873387632135e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 0/71 | LOSS: 4.942287887388375e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 1/71 | LOSS: 4.814196472580079e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 2/71 | LOSS: 4.836730568058556e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 3/71 | LOSS: 4.762229195875989e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 4/71 | LOSS: 4.572751913656248e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 5/71 | LOSS: 4.500340992308338e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 6/71 | LOSS: 4.585852691109592e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 7/71 | LOSS: 4.591859067204496e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 8/71 | LOSS: 4.539177603469903e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 9/71 | LOSS: 4.5361407501332e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 10/71 | LOSS: 4.550865345332898e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 11/71 | LOSS: 4.5000153932051035e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 12/71 | LOSS: 4.4199191856583075e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 13/71 | LOSS: 4.451658079623095e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 14/71 | LOSS: 4.465233602483446e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 15/71 | LOSS: 4.535831010343827e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 16/71 | LOSS: 4.556440912625369e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 17/71 | LOSS: 4.593535398574507e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 18/71 | LOSS: 4.522625108381219e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 19/71 | LOSS: 4.532685284175386e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 20/71 | LOSS: 4.550351908013302e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 21/71 | LOSS: 4.572688567350269e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 22/71 | LOSS: 4.517636694878962e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 23/71 | LOSS: 4.542235207812458e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 24/71 | LOSS: 4.547109465420363e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 25/71 | LOSS: 4.505559366827163e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 26/71 | LOSS: 4.518459440482224e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 27/71 | LOSS: 4.522540190724353e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 28/71 | LOSS: 4.478145119092319e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 29/71 | LOSS: 4.458925794400178e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 30/71 | LOSS: 4.472547420992933e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 31/71 | LOSS: 4.469344567326061e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 32/71 | LOSS: 4.475612906224507e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 33/71 | LOSS: 4.46098264252642e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 34/71 | LOSS: 4.480198595047114e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 35/71 | LOSS: 4.459597214968704e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 36/71 | LOSS: 4.486176362046831e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 37/71 | LOSS: 4.461305928472077e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 38/71 | LOSS: 4.50316976569481e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 39/71 | LOSS: 4.518012298149188e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 40/71 | LOSS: 4.551328507526395e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 41/71 | LOSS: 4.5404602923597505e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 42/71 | LOSS: 4.554390221359384e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 43/71 | LOSS: 4.540495963696727e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 44/71 | LOSS: 4.552641495239287e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 45/71 | LOSS: 4.5539388051822796e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 46/71 | LOSS: 4.55786510990057e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 47/71 | LOSS: 4.556925380446349e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 48/71 | LOSS: 4.556265654470308e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 49/71 | LOSS: 4.58123251064535e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 50/71 | LOSS: 4.587514507982312e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 51/71 | LOSS: 4.604268615191988e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 52/71 | LOSS: 4.626915241251996e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 53/71 | LOSS: 4.636528261572595e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 54/71 | LOSS: 4.6445714650127975e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 55/71 | LOSS: 4.629907920161713e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 56/71 | LOSS: 4.624023033198012e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 57/71 | LOSS: 4.603591586099606e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 58/71 | LOSS: 4.631356694640629e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 59/71 | LOSS: 4.646629921959781e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 60/71 | LOSS: 4.635966340666513e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 61/71 | LOSS: 4.631936803889403e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 62/71 | LOSS: 4.651067528689127e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 63/71 | LOSS: 4.656813533898685e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 64/71 | LOSS: 4.673544687578285e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 65/71 | LOSS: 4.6978327886928595e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 66/71 | LOSS: 4.705050745298052e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 67/71 | LOSS: 4.693957966056212e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 68/71 | LOSS: 4.702404095845902e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 69/71 | LOSS: 4.699145363572045e-06\n",
      "TRAIN: EPOCH 346/1000 | BATCH 70/71 | LOSS: 4.6992370372067915e-06\n",
      "VAL: EPOCH 346/1000 | BATCH 0/8 | LOSS: 6.049335752322804e-06\n",
      "VAL: EPOCH 346/1000 | BATCH 1/8 | LOSS: 6.207356136656017e-06\n",
      "VAL: EPOCH 346/1000 | BATCH 2/8 | LOSS: 6.428983397199772e-06\n",
      "VAL: EPOCH 346/1000 | BATCH 3/8 | LOSS: 6.479465127995354e-06\n",
      "VAL: EPOCH 346/1000 | BATCH 4/8 | LOSS: 6.4310068410122765e-06\n",
      "VAL: EPOCH 346/1000 | BATCH 5/8 | LOSS: 6.310272510745563e-06\n",
      "VAL: EPOCH 346/1000 | BATCH 6/8 | LOSS: 6.169398277831663e-06\n",
      "VAL: EPOCH 346/1000 | BATCH 7/8 | LOSS: 6.168430218167487e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 0/71 | LOSS: 6.489697625511326e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 1/71 | LOSS: 6.641553454755922e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 2/71 | LOSS: 5.7622316186704365e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 3/71 | LOSS: 5.890472493774723e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 4/71 | LOSS: 6.160279008327052e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 5/71 | LOSS: 5.940506525803357e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 6/71 | LOSS: 6.0417069757282405e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 7/71 | LOSS: 5.999963832437061e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 8/71 | LOSS: 5.893967681913637e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 9/71 | LOSS: 5.763366834798944e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 10/71 | LOSS: 5.6856102177830925e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 11/71 | LOSS: 5.783258188785112e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 12/71 | LOSS: 5.793770959336633e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 13/71 | LOSS: 5.761829990141061e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 14/71 | LOSS: 5.7816313528746836e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 15/71 | LOSS: 5.687369423412747e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 16/71 | LOSS: 5.596517707197242e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 17/71 | LOSS: 5.553074818938815e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 18/71 | LOSS: 5.531586961488326e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 19/71 | LOSS: 5.5054528502296305e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 20/71 | LOSS: 5.51111079836292e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 21/71 | LOSS: 5.506833965476042e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 22/71 | LOSS: 5.507657948802934e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 23/71 | LOSS: 5.496525356344743e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 24/71 | LOSS: 5.502805015566991e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 25/71 | LOSS: 5.4759833486829075e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 26/71 | LOSS: 5.4089520273447745e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 27/71 | LOSS: 5.386349195240265e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 28/71 | LOSS: 5.40466483251858e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 29/71 | LOSS: 5.379172277268178e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 30/71 | LOSS: 5.377777054696532e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 31/71 | LOSS: 5.375970246745965e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 32/71 | LOSS: 5.428378451390679e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 33/71 | LOSS: 5.414609310772742e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 34/71 | LOSS: 5.393751741524153e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 35/71 | LOSS: 5.4057478059298265e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 36/71 | LOSS: 5.38193306646143e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 37/71 | LOSS: 5.379017083166553e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 38/71 | LOSS: 5.4065331264260185e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 39/71 | LOSS: 5.412177404195972e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 40/71 | LOSS: 5.404738192560106e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 41/71 | LOSS: 5.5332775446004234e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 42/71 | LOSS: 5.564030923482659e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 43/71 | LOSS: 5.647285681995775e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 44/71 | LOSS: 5.712369885360305e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 45/71 | LOSS: 5.765922137272908e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 46/71 | LOSS: 5.739138061394345e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 47/71 | LOSS: 5.791049990951554e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 48/71 | LOSS: 5.807008907437676e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 49/71 | LOSS: 5.8366518260299925e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 50/71 | LOSS: 5.845840842441391e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 51/71 | LOSS: 5.839965629442863e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 52/71 | LOSS: 5.8349117713284085e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 53/71 | LOSS: 5.7798691252039e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 54/71 | LOSS: 5.764527135066931e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 55/71 | LOSS: 5.809809404614238e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 56/71 | LOSS: 5.79943023519279e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 57/71 | LOSS: 5.819289569821006e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 58/71 | LOSS: 5.832129545524367e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 59/71 | LOSS: 5.908591125110736e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 60/71 | LOSS: 5.905749183094471e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 61/71 | LOSS: 5.930524583075772e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 62/71 | LOSS: 5.909469134874396e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 63/71 | LOSS: 5.8852063808956245e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 64/71 | LOSS: 5.864275978568073e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 65/71 | LOSS: 5.8610814938109605e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 66/71 | LOSS: 5.852773806034395e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 67/71 | LOSS: 5.852425331073284e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 68/71 | LOSS: 5.842840111021676e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 69/71 | LOSS: 5.821017232067658e-06\n",
      "TRAIN: EPOCH 347/1000 | BATCH 70/71 | LOSS: 5.792685605754255e-06\n",
      "VAL: EPOCH 347/1000 | BATCH 0/8 | LOSS: 5.461853106680792e-06\n",
      "VAL: EPOCH 347/1000 | BATCH 1/8 | LOSS: 5.40341761734453e-06\n",
      "VAL: EPOCH 347/1000 | BATCH 2/8 | LOSS: 5.708887025927349e-06\n",
      "VAL: EPOCH 347/1000 | BATCH 3/8 | LOSS: 5.738236836805299e-06\n",
      "VAL: EPOCH 347/1000 | BATCH 4/8 | LOSS: 5.759819305239944e-06\n",
      "VAL: EPOCH 347/1000 | BATCH 5/8 | LOSS: 5.6392737709150724e-06\n",
      "VAL: EPOCH 347/1000 | BATCH 6/8 | LOSS: 5.528211464219826e-06\n",
      "VAL: EPOCH 347/1000 | BATCH 7/8 | LOSS: 5.497128313436406e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 0/71 | LOSS: 6.132383532531094e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 1/71 | LOSS: 5.472000111694797e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 2/71 | LOSS: 5.90184890825185e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 3/71 | LOSS: 5.666768743139983e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 4/71 | LOSS: 5.506688921741443e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 5/71 | LOSS: 5.396203581161292e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 6/71 | LOSS: 5.268003665800539e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 7/71 | LOSS: 4.986663043382578e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 8/71 | LOSS: 5.013635624588157e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 9/71 | LOSS: 4.876279081145185e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 10/71 | LOSS: 4.762435806200798e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 11/71 | LOSS: 4.853503052496914e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 12/71 | LOSS: 4.840808602379044e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 13/71 | LOSS: 4.7971964899521225e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 14/71 | LOSS: 4.7985662301168e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 15/71 | LOSS: 4.771613163256916e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 16/71 | LOSS: 4.765478853882728e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 17/71 | LOSS: 4.764313593518131e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 18/71 | LOSS: 4.765898857409698e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 19/71 | LOSS: 4.738727193398518e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 20/71 | LOSS: 4.705653823510234e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 21/71 | LOSS: 4.7104373482976705e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 22/71 | LOSS: 4.730939627220125e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 23/71 | LOSS: 4.72014517072239e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 24/71 | LOSS: 4.746440463350155e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 25/71 | LOSS: 4.740713463364115e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 26/71 | LOSS: 4.81451381886102e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 27/71 | LOSS: 4.832889365908548e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 28/71 | LOSS: 4.869455943693977e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 29/71 | LOSS: 4.8895246285004155e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 30/71 | LOSS: 4.909891260702411e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 31/71 | LOSS: 5.0383097374151475e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 32/71 | LOSS: 5.001394969660781e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 33/71 | LOSS: 5.210223675663549e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 34/71 | LOSS: 5.249323489156918e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 35/71 | LOSS: 5.2772084776127786e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 36/71 | LOSS: 5.36888598395996e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 37/71 | LOSS: 5.435392602904406e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 38/71 | LOSS: 5.468291699519166e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 39/71 | LOSS: 5.445218039312749e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 40/71 | LOSS: 5.556265932682436e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 41/71 | LOSS: 5.557798972620817e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 42/71 | LOSS: 5.639452421408266e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 43/71 | LOSS: 5.61923382685398e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 44/71 | LOSS: 5.720001475613875e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 45/71 | LOSS: 5.685063154922555e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 46/71 | LOSS: 5.762439654295412e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 47/71 | LOSS: 5.7947176136015815e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 48/71 | LOSS: 5.884040120338109e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 49/71 | LOSS: 5.9058909209852575e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 50/71 | LOSS: 5.927417630885554e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 51/71 | LOSS: 5.959636155239423e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 52/71 | LOSS: 5.964962898395002e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 53/71 | LOSS: 6.009843260741306e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 54/71 | LOSS: 6.010874511213677e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 55/71 | LOSS: 6.014528220735624e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 56/71 | LOSS: 5.976736207358363e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 57/71 | LOSS: 5.971461726811208e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 58/71 | LOSS: 5.956290868013284e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 59/71 | LOSS: 5.9451436906480614e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 60/71 | LOSS: 5.932814496727736e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 61/71 | LOSS: 5.92713755512734e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 62/71 | LOSS: 5.92807259997085e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 63/71 | LOSS: 5.8987881885741444e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 64/71 | LOSS: 5.874865116941062e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 65/71 | LOSS: 5.894763124601785e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 66/71 | LOSS: 5.901637294626388e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 67/71 | LOSS: 5.903460927423593e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 68/71 | LOSS: 5.904475311462707e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 69/71 | LOSS: 5.898641055474789e-06\n",
      "TRAIN: EPOCH 348/1000 | BATCH 70/71 | LOSS: 5.870789267082984e-06\n",
      "VAL: EPOCH 348/1000 | BATCH 0/8 | LOSS: 5.1795595936710015e-06\n",
      "VAL: EPOCH 348/1000 | BATCH 1/8 | LOSS: 4.82000655210868e-06\n",
      "VAL: EPOCH 348/1000 | BATCH 2/8 | LOSS: 4.7975428666783655e-06\n",
      "VAL: EPOCH 348/1000 | BATCH 3/8 | LOSS: 4.854463554693211e-06\n",
      "VAL: EPOCH 348/1000 | BATCH 4/8 | LOSS: 4.766293477587169e-06\n",
      "VAL: EPOCH 348/1000 | BATCH 5/8 | LOSS: 4.583924161731072e-06\n",
      "VAL: EPOCH 348/1000 | BATCH 6/8 | LOSS: 4.443897783598264e-06\n",
      "VAL: EPOCH 348/1000 | BATCH 7/8 | LOSS: 4.303311698095058e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 0/71 | LOSS: 5.083252290205564e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 1/71 | LOSS: 4.047267566420487e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 2/71 | LOSS: 4.028638765400198e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 3/71 | LOSS: 4.238963924763084e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 4/71 | LOSS: 4.25237021772773e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 5/71 | LOSS: 4.309923572994497e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 6/71 | LOSS: 4.413638310195113e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 7/71 | LOSS: 4.381479698167823e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 8/71 | LOSS: 4.403385345439246e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 9/71 | LOSS: 4.412207772475085e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 10/71 | LOSS: 4.4756642306641545e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 11/71 | LOSS: 4.585309663222385e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 12/71 | LOSS: 4.6641674508954075e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 13/71 | LOSS: 4.5995700150212675e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 14/71 | LOSS: 4.680351139541017e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 15/71 | LOSS: 4.695400917853476e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 16/71 | LOSS: 4.77059366483659e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 17/71 | LOSS: 4.77032035127599e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 18/71 | LOSS: 4.80434951704841e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 19/71 | LOSS: 4.760170577355894e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 20/71 | LOSS: 4.799437560688516e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 21/71 | LOSS: 4.758180569875879e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 22/71 | LOSS: 4.859530730280549e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 23/71 | LOSS: 4.880785221909416e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 24/71 | LOSS: 4.979993555025431e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 25/71 | LOSS: 4.993238987411202e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 26/71 | LOSS: 5.050011943105211e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 27/71 | LOSS: 5.054035682405811e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 28/71 | LOSS: 5.042083667549063e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 29/71 | LOSS: 5.119477918924531e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 30/71 | LOSS: 5.142326640892744e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 31/71 | LOSS: 5.193235182332501e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 32/71 | LOSS: 5.148914762244341e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 33/71 | LOSS: 5.187326156802181e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 34/71 | LOSS: 5.2157384483767344e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 35/71 | LOSS: 5.278822552807267e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 36/71 | LOSS: 5.311472176725982e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 37/71 | LOSS: 5.3696742988425904e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 38/71 | LOSS: 5.349343644831186e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 39/71 | LOSS: 5.338118330655561e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 40/71 | LOSS: 5.362756516061803e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 41/71 | LOSS: 5.36318941071624e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 42/71 | LOSS: 5.34858521817558e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 43/71 | LOSS: 5.360119561456403e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 44/71 | LOSS: 5.33598818542992e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 45/71 | LOSS: 5.319480608349018e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 46/71 | LOSS: 5.3090789538629825e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 47/71 | LOSS: 5.287453622789447e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 48/71 | LOSS: 5.248288567711324e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 49/71 | LOSS: 5.212640680838376e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 50/71 | LOSS: 5.206245398713851e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 51/71 | LOSS: 5.192891351795809e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 52/71 | LOSS: 5.174048604907134e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 53/71 | LOSS: 5.166093850911474e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 54/71 | LOSS: 5.154381555753802e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 55/71 | LOSS: 5.146173797194413e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 56/71 | LOSS: 5.1537838277127185e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 57/71 | LOSS: 5.146404324979562e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 58/71 | LOSS: 5.167062103652624e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 59/71 | LOSS: 5.149808293936075e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 60/71 | LOSS: 5.140761339910076e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 61/71 | LOSS: 5.114362517316852e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 62/71 | LOSS: 5.1212615751807344e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 63/71 | LOSS: 5.092606564005564e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 64/71 | LOSS: 5.075206711178628e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 65/71 | LOSS: 5.0684613363431925e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 66/71 | LOSS: 5.071103042786517e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 67/71 | LOSS: 5.058246094503214e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 68/71 | LOSS: 5.065236702806863e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 69/71 | LOSS: 5.0604921073370085e-06\n",
      "TRAIN: EPOCH 349/1000 | BATCH 70/71 | LOSS: 5.127198408086436e-06\n",
      "VAL: EPOCH 349/1000 | BATCH 0/8 | LOSS: 7.311791250685928e-06\n",
      "VAL: EPOCH 349/1000 | BATCH 1/8 | LOSS: 7.062577196847997e-06\n",
      "VAL: EPOCH 349/1000 | BATCH 2/8 | LOSS: 7.418361292366171e-06\n",
      "VAL: EPOCH 349/1000 | BATCH 3/8 | LOSS: 7.11558118382527e-06\n",
      "VAL: EPOCH 349/1000 | BATCH 4/8 | LOSS: 7.173894300649408e-06\n",
      "VAL: EPOCH 349/1000 | BATCH 5/8 | LOSS: 7.099939769735404e-06\n",
      "VAL: EPOCH 349/1000 | BATCH 6/8 | LOSS: 7.0578451283966255e-06\n",
      "VAL: EPOCH 349/1000 | BATCH 7/8 | LOSS: 7.21863608532658e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 0/71 | LOSS: 5.020657226850744e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 1/71 | LOSS: 6.670260972896358e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 2/71 | LOSS: 6.6825314206653275e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 3/71 | LOSS: 6.6211226794621325e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 4/71 | LOSS: 6.24982540102792e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 5/71 | LOSS: 6.662145930628564e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 6/71 | LOSS: 6.67187474131684e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 7/71 | LOSS: 6.439550816139672e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 8/71 | LOSS: 6.347111593640875e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 9/71 | LOSS: 6.510240382340271e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 10/71 | LOSS: 6.382639185176231e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 11/71 | LOSS: 6.257810317341257e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 12/71 | LOSS: 6.39149042259445e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 13/71 | LOSS: 6.3676232977221455e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 14/71 | LOSS: 6.385516977995091e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 15/71 | LOSS: 6.2496021655533696e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 16/71 | LOSS: 6.239359048349263e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 17/71 | LOSS: 6.246130043210643e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 18/71 | LOSS: 6.224941276549974e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 19/71 | LOSS: 6.177804812068643e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 20/71 | LOSS: 6.084313987403653e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 21/71 | LOSS: 6.185397255235743e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 22/71 | LOSS: 6.121173595127639e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 23/71 | LOSS: 6.26385517913756e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 24/71 | LOSS: 6.254442705539987e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 25/71 | LOSS: 6.345544219444631e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 26/71 | LOSS: 6.336508384400211e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 27/71 | LOSS: 6.377550448632974e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 28/71 | LOSS: 6.42393879075073e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 29/71 | LOSS: 6.386956268518892e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 30/71 | LOSS: 6.494040066642018e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 31/71 | LOSS: 6.4348451047635535e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 32/71 | LOSS: 6.488710920221462e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 33/71 | LOSS: 6.424085821730414e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 34/71 | LOSS: 6.387644628245783e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 35/71 | LOSS: 6.3646677467153895e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 36/71 | LOSS: 6.313900866115472e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 37/71 | LOSS: 6.337625001450713e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 38/71 | LOSS: 6.3486696163659544e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 39/71 | LOSS: 6.353852700158313e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 40/71 | LOSS: 6.32702734414701e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 41/71 | LOSS: 6.2964260169316e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 42/71 | LOSS: 6.318984793155281e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 43/71 | LOSS: 6.273932389045843e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 44/71 | LOSS: 6.2959992343773285e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 45/71 | LOSS: 6.262604865301198e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 46/71 | LOSS: 6.270222243945179e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 47/71 | LOSS: 6.252198052910292e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 48/71 | LOSS: 6.241348821314037e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 49/71 | LOSS: 6.233615686142002e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 50/71 | LOSS: 6.226742011766e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 51/71 | LOSS: 6.233673511465336e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 52/71 | LOSS: 6.203086363626117e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 53/71 | LOSS: 6.1870628171702804e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 54/71 | LOSS: 6.199226308209208e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 55/71 | LOSS: 6.14485217300041e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 56/71 | LOSS: 6.144412010830261e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 57/71 | LOSS: 6.114466482985931e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 58/71 | LOSS: 6.100741267677592e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 59/71 | LOSS: 6.058769880231315e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 60/71 | LOSS: 6.033111358341497e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 61/71 | LOSS: 6.001160368067494e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 62/71 | LOSS: 5.984644368644305e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 63/71 | LOSS: 5.9509625245368625e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 64/71 | LOSS: 5.93081489429236e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 65/71 | LOSS: 5.913063598034734e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 66/71 | LOSS: 5.8910126386057415e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 67/71 | LOSS: 5.856546769845248e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 68/71 | LOSS: 5.844554709546029e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 69/71 | LOSS: 5.820635357979751e-06\n",
      "TRAIN: EPOCH 350/1000 | BATCH 70/71 | LOSS: 5.835870268410744e-06\n",
      "VAL: EPOCH 350/1000 | BATCH 0/8 | LOSS: 4.765199264511466e-06\n",
      "VAL: EPOCH 350/1000 | BATCH 1/8 | LOSS: 4.597472525347257e-06\n",
      "VAL: EPOCH 350/1000 | BATCH 2/8 | LOSS: 4.63021327353393e-06\n",
      "VAL: EPOCH 350/1000 | BATCH 3/8 | LOSS: 4.692721290666668e-06\n",
      "VAL: EPOCH 350/1000 | BATCH 4/8 | LOSS: 4.628963415598264e-06\n",
      "VAL: EPOCH 350/1000 | BATCH 5/8 | LOSS: 4.5074806015084805e-06\n",
      "VAL: EPOCH 350/1000 | BATCH 6/8 | LOSS: 4.467414588102006e-06\n",
      "VAL: EPOCH 350/1000 | BATCH 7/8 | LOSS: 4.417484547047934e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 0/71 | LOSS: 4.749633262690622e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 1/71 | LOSS: 4.480336883716518e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 2/71 | LOSS: 4.6314774711693945e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 3/71 | LOSS: 4.697536951425718e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 4/71 | LOSS: 4.524411815509666e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 5/71 | LOSS: 4.371278616114675e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 6/71 | LOSS: 4.5932335654340155e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 7/71 | LOSS: 4.6663299144711345e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 8/71 | LOSS: 4.6130287753637985e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 9/71 | LOSS: 4.692622496804688e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 10/71 | LOSS: 4.67573520588551e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 11/71 | LOSS: 4.649235999446925e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 12/71 | LOSS: 4.607298146416165e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 13/71 | LOSS: 4.674198245473755e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 14/71 | LOSS: 4.654357295900506e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 15/71 | LOSS: 4.706602794612991e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 16/71 | LOSS: 4.67145232238074e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 17/71 | LOSS: 4.7012371800923975e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 18/71 | LOSS: 4.639135568140773e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 19/71 | LOSS: 4.661334401134809e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 20/71 | LOSS: 4.633960694870136e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 21/71 | LOSS: 4.617687119951535e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 22/71 | LOSS: 4.62145645423848e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 23/71 | LOSS: 4.617074012003286e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 24/71 | LOSS: 4.673342209571274e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 25/71 | LOSS: 4.766205288083606e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 26/71 | LOSS: 4.72722467748099e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 27/71 | LOSS: 4.735175398959005e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 28/71 | LOSS: 4.7603212994451085e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 29/71 | LOSS: 4.771890341241185e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 30/71 | LOSS: 4.782291222410652e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 31/71 | LOSS: 4.838313543586992e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 32/71 | LOSS: 4.8752096214836156e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 33/71 | LOSS: 4.872925822329468e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 34/71 | LOSS: 4.894816044855231e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 35/71 | LOSS: 4.895217001729988e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 36/71 | LOSS: 4.926346030736789e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 37/71 | LOSS: 4.93073722090661e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 38/71 | LOSS: 4.944486777392926e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 39/71 | LOSS: 4.904626769075548e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 40/71 | LOSS: 4.909511306589305e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 41/71 | LOSS: 4.893640325896004e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 42/71 | LOSS: 4.883940047960116e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 43/71 | LOSS: 4.861212935545402e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 44/71 | LOSS: 4.849654902096113e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 45/71 | LOSS: 4.8327919617092805e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 46/71 | LOSS: 4.832358056174076e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 47/71 | LOSS: 4.841170436975517e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 48/71 | LOSS: 4.8437401272501435e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 49/71 | LOSS: 4.836872221858357e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 50/71 | LOSS: 4.851191658649352e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 51/71 | LOSS: 4.859691994869746e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 52/71 | LOSS: 4.873030792503346e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 53/71 | LOSS: 4.86121234644088e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 54/71 | LOSS: 4.867098091794601e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 55/71 | LOSS: 4.8561249985011405e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 56/71 | LOSS: 4.829995301055565e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 57/71 | LOSS: 4.831503980344208e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 58/71 | LOSS: 4.801314983107998e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 59/71 | LOSS: 4.787802231476234e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 60/71 | LOSS: 4.778053518420478e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 61/71 | LOSS: 4.784000023597224e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 62/71 | LOSS: 4.7846406304747765e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 63/71 | LOSS: 4.781386266472509e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 64/71 | LOSS: 4.768825643763054e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 65/71 | LOSS: 4.770816062635717e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 66/71 | LOSS: 4.76728851637419e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 67/71 | LOSS: 4.748599224058125e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 68/71 | LOSS: 4.742649374800722e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 69/71 | LOSS: 4.768399831586326e-06\n",
      "TRAIN: EPOCH 351/1000 | BATCH 70/71 | LOSS: 4.7672110059453115e-06\n",
      "VAL: EPOCH 351/1000 | BATCH 0/8 | LOSS: 5.021004653826822e-06\n",
      "VAL: EPOCH 351/1000 | BATCH 1/8 | LOSS: 4.8543679440626875e-06\n",
      "VAL: EPOCH 351/1000 | BATCH 2/8 | LOSS: 4.955583790433593e-06\n",
      "VAL: EPOCH 351/1000 | BATCH 3/8 | LOSS: 5.135200922268268e-06\n",
      "VAL: EPOCH 351/1000 | BATCH 4/8 | LOSS: 5.076477464172058e-06\n",
      "VAL: EPOCH 351/1000 | BATCH 5/8 | LOSS: 5.0219643981108675e-06\n",
      "VAL: EPOCH 351/1000 | BATCH 6/8 | LOSS: 4.944108370961788e-06\n",
      "VAL: EPOCH 351/1000 | BATCH 7/8 | LOSS: 4.874191461112787e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 0/71 | LOSS: 4.448119398148265e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 1/71 | LOSS: 5.027037104810006e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 2/71 | LOSS: 4.835788028382619e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 3/71 | LOSS: 4.7607587703168974e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 4/71 | LOSS: 4.814788917428814e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 5/71 | LOSS: 4.841868909958673e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 6/71 | LOSS: 4.7439328097555385e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 7/71 | LOSS: 4.755912925702432e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 8/71 | LOSS: 4.770607776865492e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 9/71 | LOSS: 4.734662297778414e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 10/71 | LOSS: 4.759013873726045e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 11/71 | LOSS: 4.7934643892707145e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 12/71 | LOSS: 4.920752417092444e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 13/71 | LOSS: 5.2074192028937564e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 14/71 | LOSS: 5.250961627704479e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 15/71 | LOSS: 5.292635506748411e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 16/71 | LOSS: 5.3756169322963724e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 17/71 | LOSS: 5.3796636620973e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 18/71 | LOSS: 5.4349265347378895e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 19/71 | LOSS: 5.502091948983434e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 20/71 | LOSS: 5.552967498564006e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 21/71 | LOSS: 5.435602423114522e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 22/71 | LOSS: 5.52858437399466e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 23/71 | LOSS: 5.556937310302601e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 24/71 | LOSS: 5.597219333139947e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 25/71 | LOSS: 5.561327183246389e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 26/71 | LOSS: 5.6158682195086e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 27/71 | LOSS: 5.521361612993912e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 28/71 | LOSS: 5.5125254550211105e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 29/71 | LOSS: 5.4907597359488134e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 30/71 | LOSS: 5.486972624214925e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 31/71 | LOSS: 5.48095201935439e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 32/71 | LOSS: 5.473366296758865e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 33/71 | LOSS: 5.459514267493111e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 34/71 | LOSS: 5.44711651855323e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 35/71 | LOSS: 5.426223860164707e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 36/71 | LOSS: 5.420306534516213e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 37/71 | LOSS: 5.429691513152073e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 38/71 | LOSS: 5.405873258869411e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 39/71 | LOSS: 5.416237587496653e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 40/71 | LOSS: 5.444547269223953e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 41/71 | LOSS: 5.469791225160796e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 42/71 | LOSS: 5.475288992593685e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 43/71 | LOSS: 5.444225874643135e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 44/71 | LOSS: 5.429356653823763e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 45/71 | LOSS: 5.4036528947266484e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 46/71 | LOSS: 5.424228409034921e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 47/71 | LOSS: 5.413299803080918e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 48/71 | LOSS: 5.4051553136986865e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 49/71 | LOSS: 5.398801008595911e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 50/71 | LOSS: 5.392340234917071e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 51/71 | LOSS: 5.360664138501866e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 52/71 | LOSS: 5.382638773709296e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 53/71 | LOSS: 5.3741410713175556e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 54/71 | LOSS: 5.344946446877492e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 55/71 | LOSS: 5.319473509806423e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 56/71 | LOSS: 5.316052948457359e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 57/71 | LOSS: 5.305970791502252e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 58/71 | LOSS: 5.293224838601033e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 59/71 | LOSS: 5.278578256214435e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 60/71 | LOSS: 5.270360834849092e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 61/71 | LOSS: 5.259446216006191e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 62/71 | LOSS: 5.246102582243602e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 63/71 | LOSS: 5.237100335619971e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 64/71 | LOSS: 5.201455794052051e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 65/71 | LOSS: 5.205231290843089e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 66/71 | LOSS: 5.206361638963937e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 67/71 | LOSS: 5.235367388971045e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 68/71 | LOSS: 5.2367909021363035e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 69/71 | LOSS: 5.254324147634699e-06\n",
      "TRAIN: EPOCH 352/1000 | BATCH 70/71 | LOSS: 5.251013484487719e-06\n",
      "VAL: EPOCH 352/1000 | BATCH 0/8 | LOSS: 4.936229743179865e-06\n",
      "VAL: EPOCH 352/1000 | BATCH 1/8 | LOSS: 5.174516672923346e-06\n",
      "VAL: EPOCH 352/1000 | BATCH 2/8 | LOSS: 5.235062417341396e-06\n",
      "VAL: EPOCH 352/1000 | BATCH 3/8 | LOSS: 5.477468221215531e-06\n",
      "VAL: EPOCH 352/1000 | BATCH 4/8 | LOSS: 5.411282290879171e-06\n",
      "VAL: EPOCH 352/1000 | BATCH 5/8 | LOSS: 5.216647650740924e-06\n",
      "VAL: EPOCH 352/1000 | BATCH 6/8 | LOSS: 5.083235074770138e-06\n",
      "VAL: EPOCH 352/1000 | BATCH 7/8 | LOSS: 4.91737802121861e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 0/71 | LOSS: 6.133564056653995e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 1/71 | LOSS: 5.559232249652268e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 2/71 | LOSS: 5.830180574169693e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 3/71 | LOSS: 5.368241659198247e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 4/71 | LOSS: 5.6346365454373885e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 5/71 | LOSS: 5.646319474787258e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 6/71 | LOSS: 5.424293898873397e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 7/71 | LOSS: 5.449158777537377e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 8/71 | LOSS: 5.2366047687731525e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 9/71 | LOSS: 5.345587032934418e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 10/71 | LOSS: 5.284335409007988e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 11/71 | LOSS: 5.291806777070936e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 12/71 | LOSS: 5.259945003042678e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 13/71 | LOSS: 5.250910882231048e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 14/71 | LOSS: 5.286885546714378e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 15/71 | LOSS: 5.295618109357747e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 16/71 | LOSS: 5.382755047963549e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 17/71 | LOSS: 5.27670936713144e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 18/71 | LOSS: 5.2805189864700784e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 19/71 | LOSS: 5.319330352904217e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 20/71 | LOSS: 5.3660916949281405e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 21/71 | LOSS: 5.401343431995801e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 22/71 | LOSS: 5.445533568727905e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 23/71 | LOSS: 5.519951287169533e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 24/71 | LOSS: 5.581379118666519e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 25/71 | LOSS: 5.6463533592949925e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 26/71 | LOSS: 5.597535214797568e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 27/71 | LOSS: 5.752926020769726e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 28/71 | LOSS: 5.706269928788092e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 29/71 | LOSS: 5.761881554159724e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 30/71 | LOSS: 5.720379322742848e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 31/71 | LOSS: 5.816651878376433e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 32/71 | LOSS: 5.997333904484879e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 33/71 | LOSS: 5.973114264848204e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 34/71 | LOSS: 6.07892548682035e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 35/71 | LOSS: 6.175746117757323e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 36/71 | LOSS: 6.177628632498012e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 37/71 | LOSS: 6.189899283383152e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 38/71 | LOSS: 6.215821929053606e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 39/71 | LOSS: 6.176609485919471e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 40/71 | LOSS: 6.160905608029258e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 41/71 | LOSS: 6.1882904317047045e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 42/71 | LOSS: 6.222857358130399e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 43/71 | LOSS: 6.2030922084555975e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 44/71 | LOSS: 6.150751667317107e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 45/71 | LOSS: 6.20672416635384e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 46/71 | LOSS: 6.180288901885704e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 47/71 | LOSS: 6.200914536217776e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 48/71 | LOSS: 6.155684554776561e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 49/71 | LOSS: 6.143335567685426e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 50/71 | LOSS: 6.110387697417002e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 51/71 | LOSS: 6.1106485943058105e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 52/71 | LOSS: 6.156998689169447e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 53/71 | LOSS: 6.157648015316765e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 54/71 | LOSS: 6.219606538334946e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 55/71 | LOSS: 6.187230504955161e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 56/71 | LOSS: 6.209156716769876e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 57/71 | LOSS: 6.183819299747421e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 58/71 | LOSS: 6.190009132250591e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 59/71 | LOSS: 6.202797279305135e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 60/71 | LOSS: 6.169386674851832e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 61/71 | LOSS: 6.148918622960581e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 62/71 | LOSS: 6.112873506936012e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 63/71 | LOSS: 6.103773607435414e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 64/71 | LOSS: 6.081657326061619e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 65/71 | LOSS: 6.072571013095457e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 66/71 | LOSS: 6.053818380216888e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 67/71 | LOSS: 6.0304613698998065e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 68/71 | LOSS: 6.029262380437011e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 69/71 | LOSS: 6.0061301935223e-06\n",
      "TRAIN: EPOCH 353/1000 | BATCH 70/71 | LOSS: 6.006931375273959e-06\n",
      "VAL: EPOCH 353/1000 | BATCH 0/8 | LOSS: 5.9139947552466765e-06\n",
      "VAL: EPOCH 353/1000 | BATCH 1/8 | LOSS: 5.537407105293823e-06\n",
      "VAL: EPOCH 353/1000 | BATCH 2/8 | LOSS: 5.473558303492609e-06\n",
      "VAL: EPOCH 353/1000 | BATCH 3/8 | LOSS: 5.4629640544590075e-06\n",
      "VAL: EPOCH 353/1000 | BATCH 4/8 | LOSS: 5.408201832324266e-06\n",
      "VAL: EPOCH 353/1000 | BATCH 5/8 | LOSS: 5.212091006493817e-06\n",
      "VAL: EPOCH 353/1000 | BATCH 6/8 | LOSS: 5.178823682529453e-06\n",
      "VAL: EPOCH 353/1000 | BATCH 7/8 | LOSS: 5.08398551346545e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 0/71 | LOSS: 3.993688551418018e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 1/71 | LOSS: 5.269306029731524e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 2/71 | LOSS: 4.840554083784809e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 3/71 | LOSS: 4.691437879955629e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 4/71 | LOSS: 5.005077764508315e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 5/71 | LOSS: 4.869542863161769e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 6/71 | LOSS: 4.656395503843669e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 7/71 | LOSS: 4.622792232567008e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 8/71 | LOSS: 4.755419619565752e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 9/71 | LOSS: 4.695183497460675e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 10/71 | LOSS: 4.922642429764065e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 11/71 | LOSS: 4.944503530168731e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 12/71 | LOSS: 4.984518070089577e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 13/71 | LOSS: 4.909427973741133e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 14/71 | LOSS: 4.933103809889872e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 15/71 | LOSS: 4.899890342358049e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 16/71 | LOSS: 4.942247254375081e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 17/71 | LOSS: 4.963890736083461e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 18/71 | LOSS: 4.945347547235494e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 19/71 | LOSS: 4.9486043963042904e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 20/71 | LOSS: 4.96925421040422e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 21/71 | LOSS: 4.953337359190548e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 22/71 | LOSS: 5.0353963991295565e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 23/71 | LOSS: 5.019249158522143e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 24/71 | LOSS: 5.091218135930831e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 25/71 | LOSS: 5.0991063394926295e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 26/71 | LOSS: 5.1197915261716974e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 27/71 | LOSS: 5.161376699496552e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 28/71 | LOSS: 5.147653061205096e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 29/71 | LOSS: 5.250006430893942e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 30/71 | LOSS: 5.206334750959462e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 31/71 | LOSS: 5.271610078239064e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 32/71 | LOSS: 5.247124122296086e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 33/71 | LOSS: 5.258670478938524e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 34/71 | LOSS: 5.3356331331347714e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 35/71 | LOSS: 5.3063407929382974e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 36/71 | LOSS: 5.365314717818291e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 37/71 | LOSS: 5.35737083402254e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 38/71 | LOSS: 5.354756001160683e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 39/71 | LOSS: 5.356210090212699e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 40/71 | LOSS: 5.398238155931922e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 41/71 | LOSS: 5.523984013745489e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 42/71 | LOSS: 5.556276032303833e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 43/71 | LOSS: 5.582193983338419e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 44/71 | LOSS: 5.689325967574001e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 45/71 | LOSS: 5.643151909047763e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 46/71 | LOSS: 5.725525731596714e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 47/71 | LOSS: 5.784105354678104e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 48/71 | LOSS: 5.757134779426507e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 49/71 | LOSS: 6.019521733833244e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 50/71 | LOSS: 6.043054585746836e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 51/71 | LOSS: 6.190033848095201e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 52/71 | LOSS: 6.152185943512518e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 53/71 | LOSS: 6.256913113449829e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 54/71 | LOSS: 6.306292545394337e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 55/71 | LOSS: 6.319991241200894e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 56/71 | LOSS: 6.438599947406992e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 57/71 | LOSS: 6.402952255390621e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 58/71 | LOSS: 6.507439300518037e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 59/71 | LOSS: 6.472213052196215e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 60/71 | LOSS: 6.511744649302631e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 61/71 | LOSS: 6.576344474289668e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 62/71 | LOSS: 6.5595317758538475e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 63/71 | LOSS: 6.631277017277171e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 64/71 | LOSS: 6.627280394101175e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 65/71 | LOSS: 6.738567587187087e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 66/71 | LOSS: 6.699226254731097e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 67/71 | LOSS: 6.81358642395935e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 68/71 | LOSS: 6.798388892713009e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 69/71 | LOSS: 6.816431713169939e-06\n",
      "TRAIN: EPOCH 354/1000 | BATCH 70/71 | LOSS: 6.848281488383294e-06\n",
      "VAL: EPOCH 354/1000 | BATCH 0/8 | LOSS: 8.20063996798126e-06\n",
      "VAL: EPOCH 354/1000 | BATCH 1/8 | LOSS: 7.341127002291614e-06\n",
      "VAL: EPOCH 354/1000 | BATCH 2/8 | LOSS: 7.229591877451942e-06\n",
      "VAL: EPOCH 354/1000 | BATCH 3/8 | LOSS: 7.170801382017089e-06\n",
      "VAL: EPOCH 354/1000 | BATCH 4/8 | LOSS: 7.003776772762649e-06\n",
      "VAL: EPOCH 354/1000 | BATCH 5/8 | LOSS: 6.739738106868269e-06\n",
      "VAL: EPOCH 354/1000 | BATCH 6/8 | LOSS: 6.67613871233438e-06\n",
      "VAL: EPOCH 354/1000 | BATCH 7/8 | LOSS: 6.469315735557757e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 0/71 | LOSS: 7.1810413828643505e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 1/71 | LOSS: 6.7670614498638315e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 2/71 | LOSS: 7.037707746349042e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 3/71 | LOSS: 6.830674124103098e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 4/71 | LOSS: 6.6906218307849485e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 5/71 | LOSS: 6.509860440928605e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 6/71 | LOSS: 6.2029262153373566e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 7/71 | LOSS: 5.973238444312301e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 8/71 | LOSS: 5.916555892326869e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 9/71 | LOSS: 5.7083048886852336e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 10/71 | LOSS: 5.633946123204871e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 11/71 | LOSS: 5.551829190153512e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 12/71 | LOSS: 5.506525282656255e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 13/71 | LOSS: 5.462575862631118e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 14/71 | LOSS: 5.410886145303569e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 15/71 | LOSS: 5.542183970419501e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 16/71 | LOSS: 5.5711925864777775e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 17/71 | LOSS: 5.46755935223094e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 18/71 | LOSS: 5.468141664191774e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 19/71 | LOSS: 5.460976376525651e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 20/71 | LOSS: 5.406559791611341e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 21/71 | LOSS: 5.414507582331267e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 22/71 | LOSS: 5.473219318999959e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 23/71 | LOSS: 5.415550219822762e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 24/71 | LOSS: 5.397756049205782e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 25/71 | LOSS: 5.36610453029705e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 26/71 | LOSS: 5.343147390872387e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 27/71 | LOSS: 5.294542282691899e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 28/71 | LOSS: 5.2646123392684065e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 29/71 | LOSS: 5.241296184976818e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 30/71 | LOSS: 5.204156672510679e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 31/71 | LOSS: 5.164057114370735e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 32/71 | LOSS: 5.1261285438103545e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 33/71 | LOSS: 5.134206142375821e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 34/71 | LOSS: 5.1008792330061885e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 35/71 | LOSS: 5.067519775063071e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 36/71 | LOSS: 5.0436879187268586e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 37/71 | LOSS: 5.023465339610611e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 38/71 | LOSS: 5.02774715781785e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 39/71 | LOSS: 4.991356121308854e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 40/71 | LOSS: 4.996358805927718e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 41/71 | LOSS: 4.977113473528118e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 42/71 | LOSS: 4.994295443603458e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 43/71 | LOSS: 4.945623567726381e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 44/71 | LOSS: 4.9362373273551286e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 45/71 | LOSS: 4.928602913384131e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 46/71 | LOSS: 4.918078236593655e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 47/71 | LOSS: 4.887544174418205e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 48/71 | LOSS: 4.857091690443055e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 49/71 | LOSS: 4.849536267101939e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 50/71 | LOSS: 4.8495122098079174e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 51/71 | LOSS: 4.820547828260785e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 52/71 | LOSS: 4.857549772666216e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 53/71 | LOSS: 4.8654994560382435e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 54/71 | LOSS: 4.852542964396311e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 55/71 | LOSS: 4.872660273999177e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 56/71 | LOSS: 4.87240976996729e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 57/71 | LOSS: 4.860831021185982e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 58/71 | LOSS: 4.8646803846214285e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 59/71 | LOSS: 4.858163291980115e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 60/71 | LOSS: 4.834349153095333e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 61/71 | LOSS: 4.822996134539755e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 62/71 | LOSS: 4.833292829668305e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 63/71 | LOSS: 4.823966733624729e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 64/71 | LOSS: 4.832804650224996e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 65/71 | LOSS: 4.814776750870144e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 66/71 | LOSS: 4.82273771379355e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 67/71 | LOSS: 4.819536008974811e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 68/71 | LOSS: 4.834446523697504e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 69/71 | LOSS: 4.831614608389957e-06\n",
      "TRAIN: EPOCH 355/1000 | BATCH 70/71 | LOSS: 4.815709401004248e-06\n",
      "VAL: EPOCH 355/1000 | BATCH 0/8 | LOSS: 7.2642478698980995e-06\n",
      "VAL: EPOCH 355/1000 | BATCH 1/8 | LOSS: 6.80517860018881e-06\n",
      "VAL: EPOCH 355/1000 | BATCH 2/8 | LOSS: 6.6624406827031635e-06\n",
      "VAL: EPOCH 355/1000 | BATCH 3/8 | LOSS: 6.729221695422893e-06\n",
      "VAL: EPOCH 355/1000 | BATCH 4/8 | LOSS: 6.536199543916155e-06\n",
      "VAL: EPOCH 355/1000 | BATCH 5/8 | LOSS: 6.241505464762061e-06\n",
      "VAL: EPOCH 355/1000 | BATCH 6/8 | LOSS: 6.208946875371371e-06\n",
      "VAL: EPOCH 355/1000 | BATCH 7/8 | LOSS: 6.025285699706728e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 0/71 | LOSS: 5.169225460122107e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 1/71 | LOSS: 5.042983275416191e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 2/71 | LOSS: 4.878362536449761e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 3/71 | LOSS: 5.040543555878685e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 4/71 | LOSS: 4.856638042838313e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 5/71 | LOSS: 4.948409393061108e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 6/71 | LOSS: 4.831379685908489e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 7/71 | LOSS: 5.099682368836511e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 8/71 | LOSS: 5.074128087774928e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 9/71 | LOSS: 4.925856774207204e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 10/71 | LOSS: 5.268201758471233e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 11/71 | LOSS: 5.293398923337615e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 12/71 | LOSS: 5.503128284964567e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 13/71 | LOSS: 5.536466460398515e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 14/71 | LOSS: 5.606543315176774e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 15/71 | LOSS: 5.633668109794598e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 16/71 | LOSS: 5.591701103506185e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 17/71 | LOSS: 5.627084621866945e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 18/71 | LOSS: 5.529712205699118e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 19/71 | LOSS: 5.534655088013096e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 20/71 | LOSS: 5.51846939006715e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 21/71 | LOSS: 5.455186593198133e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 22/71 | LOSS: 5.440758820401717e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 23/71 | LOSS: 5.389782870679483e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 24/71 | LOSS: 5.3820568064111285e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 25/71 | LOSS: 5.399177692967127e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 26/71 | LOSS: 5.390137765196549e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 27/71 | LOSS: 5.407868424559378e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 28/71 | LOSS: 5.383604620247156e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 29/71 | LOSS: 5.385485155784408e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 30/71 | LOSS: 5.342318790128282e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 31/71 | LOSS: 5.313381606697476e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 32/71 | LOSS: 5.3138335191981625e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 33/71 | LOSS: 5.29659932373547e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 34/71 | LOSS: 5.30677302127255e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 35/71 | LOSS: 5.280878288734432e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 36/71 | LOSS: 5.294763394812361e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 37/71 | LOSS: 5.256024451227859e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 38/71 | LOSS: 5.2232750745017065e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 39/71 | LOSS: 5.231279772033304e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 40/71 | LOSS: 5.233497165868247e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 41/71 | LOSS: 5.290509534461307e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 42/71 | LOSS: 5.263243946141894e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 43/71 | LOSS: 5.285180412267271e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 44/71 | LOSS: 5.290592343953904e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 45/71 | LOSS: 5.290081973093354e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 46/71 | LOSS: 5.296296576682622e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 47/71 | LOSS: 5.369159727782365e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 48/71 | LOSS: 5.371116430422039e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 49/71 | LOSS: 5.357990385164158e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 50/71 | LOSS: 5.3711378403058206e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 51/71 | LOSS: 5.3732992630382514e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 52/71 | LOSS: 5.416865723470076e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 53/71 | LOSS: 5.397797293779958e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 54/71 | LOSS: 5.391598659281788e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 55/71 | LOSS: 5.374082905161361e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 56/71 | LOSS: 5.36284775527045e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 57/71 | LOSS: 5.365305994552006e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 58/71 | LOSS: 5.35222844226124e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 59/71 | LOSS: 5.340543255745918e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 60/71 | LOSS: 5.3282705803361695e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 61/71 | LOSS: 5.331156466303172e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 62/71 | LOSS: 5.339095351368924e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 63/71 | LOSS: 5.350743627730026e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 64/71 | LOSS: 5.356187557481462e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 65/71 | LOSS: 5.344852559463823e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 66/71 | LOSS: 5.338058762485595e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 67/71 | LOSS: 5.328266538259306e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 68/71 | LOSS: 5.324113818209878e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 69/71 | LOSS: 5.295926598591905e-06\n",
      "TRAIN: EPOCH 356/1000 | BATCH 70/71 | LOSS: 5.330700118142437e-06\n",
      "VAL: EPOCH 356/1000 | BATCH 0/8 | LOSS: 5.9789772421936505e-06\n",
      "VAL: EPOCH 356/1000 | BATCH 1/8 | LOSS: 5.463936304295203e-06\n",
      "VAL: EPOCH 356/1000 | BATCH 2/8 | LOSS: 5.455363861983642e-06\n",
      "VAL: EPOCH 356/1000 | BATCH 3/8 | LOSS: 5.368520987758529e-06\n",
      "VAL: EPOCH 356/1000 | BATCH 4/8 | LOSS: 5.3190038670436476e-06\n",
      "VAL: EPOCH 356/1000 | BATCH 5/8 | LOSS: 5.092139569266389e-06\n",
      "VAL: EPOCH 356/1000 | BATCH 6/8 | LOSS: 5.03534907433537e-06\n",
      "VAL: EPOCH 356/1000 | BATCH 7/8 | LOSS: 4.970914744717447e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 0/71 | LOSS: 4.105484094907297e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 1/71 | LOSS: 5.883323638045113e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 2/71 | LOSS: 5.580607800463137e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 3/71 | LOSS: 5.149367780177272e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 4/71 | LOSS: 5.010429777030367e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 5/71 | LOSS: 5.210753518743634e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 6/71 | LOSS: 5.132746796984325e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 7/71 | LOSS: 5.265252468689141e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 8/71 | LOSS: 5.214459684389618e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 9/71 | LOSS: 5.625196308756131e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 10/71 | LOSS: 5.483010185840116e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 11/71 | LOSS: 5.496753450036825e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 12/71 | LOSS: 5.410945889777325e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 13/71 | LOSS: 5.21624912965178e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 14/71 | LOSS: 5.149789164230848e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 15/71 | LOSS: 5.15370300036011e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 16/71 | LOSS: 5.141471988548908e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 17/71 | LOSS: 5.214877395095148e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 18/71 | LOSS: 5.22323857933914e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 19/71 | LOSS: 5.269767848403717e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 20/71 | LOSS: 5.1875618585957755e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 21/71 | LOSS: 5.134994404959963e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 22/71 | LOSS: 5.183552127124605e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 23/71 | LOSS: 5.173142208529195e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 24/71 | LOSS: 5.19444919518719e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 25/71 | LOSS: 5.1874001813373e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 26/71 | LOSS: 5.213814642114879e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 27/71 | LOSS: 5.225106284407437e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 28/71 | LOSS: 5.253836638201329e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 29/71 | LOSS: 5.277318412784855e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 30/71 | LOSS: 5.2387370913258505e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 31/71 | LOSS: 5.271892398184264e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 32/71 | LOSS: 5.255318387064493e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 33/71 | LOSS: 5.237050913819985e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 34/71 | LOSS: 5.2572531201024374e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 35/71 | LOSS: 5.212705755436926e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 36/71 | LOSS: 5.210961477574971e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 37/71 | LOSS: 5.230330609600442e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 38/71 | LOSS: 5.2671094950850465e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 39/71 | LOSS: 5.289137453701187e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 40/71 | LOSS: 5.294118897459489e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 41/71 | LOSS: 5.2671449024966844e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 42/71 | LOSS: 5.277564262774662e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 43/71 | LOSS: 5.258720002323323e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 44/71 | LOSS: 5.27421323871143e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 45/71 | LOSS: 5.262624016643753e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 46/71 | LOSS: 5.270425985377478e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 47/71 | LOSS: 5.251832623306048e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 48/71 | LOSS: 5.267246989410059e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 49/71 | LOSS: 5.295629739521246e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 50/71 | LOSS: 5.302786839106999e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 51/71 | LOSS: 5.328305235252628e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 52/71 | LOSS: 5.298792075336678e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 53/71 | LOSS: 5.29132174691498e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 54/71 | LOSS: 5.283082902513508e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 55/71 | LOSS: 5.254089184550789e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 56/71 | LOSS: 5.267389265658526e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 57/71 | LOSS: 5.236097289023997e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 58/71 | LOSS: 5.243809575126684e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 59/71 | LOSS: 5.222094349240554e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 60/71 | LOSS: 5.211248632998026e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 61/71 | LOSS: 5.2096545826521655e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 62/71 | LOSS: 5.215776105788314e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 63/71 | LOSS: 5.20631400036109e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 64/71 | LOSS: 5.206373890149944e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 65/71 | LOSS: 5.1907530963614015e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 66/71 | LOSS: 5.2058339521741505e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 67/71 | LOSS: 5.183443196234721e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 68/71 | LOSS: 5.186496612926324e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 69/71 | LOSS: 5.194000459596282e-06\n",
      "TRAIN: EPOCH 357/1000 | BATCH 70/71 | LOSS: 5.174217106504726e-06\n",
      "VAL: EPOCH 357/1000 | BATCH 0/8 | LOSS: 5.5636737670283765e-06\n",
      "VAL: EPOCH 357/1000 | BATCH 1/8 | LOSS: 5.320132913766429e-06\n",
      "VAL: EPOCH 357/1000 | BATCH 2/8 | LOSS: 5.517688502247135e-06\n",
      "VAL: EPOCH 357/1000 | BATCH 3/8 | LOSS: 5.475407078847638e-06\n",
      "VAL: EPOCH 357/1000 | BATCH 4/8 | LOSS: 5.450462231237907e-06\n",
      "VAL: EPOCH 357/1000 | BATCH 5/8 | LOSS: 5.400088184615015e-06\n",
      "VAL: EPOCH 357/1000 | BATCH 6/8 | LOSS: 5.414216632613846e-06\n",
      "VAL: EPOCH 357/1000 | BATCH 7/8 | LOSS: 5.4813873475723085e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 0/71 | LOSS: 5.21407264386653e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 1/71 | LOSS: 5.20734488418384e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 2/71 | LOSS: 5.1507997037939885e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 3/71 | LOSS: 4.789540582805785e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 4/71 | LOSS: 4.871961073149577e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 5/71 | LOSS: 4.735417898397524e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 6/71 | LOSS: 4.809221179259891e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 7/71 | LOSS: 4.750969452516074e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 8/71 | LOSS: 4.844824944585626e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 9/71 | LOSS: 4.862268201577535e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 10/71 | LOSS: 4.923279055384972e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 11/71 | LOSS: 5.131440142728631e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 12/71 | LOSS: 5.158002033147988e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 13/71 | LOSS: 5.348329701908889e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 14/71 | LOSS: 5.26644133363637e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 15/71 | LOSS: 5.317238091606669e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 16/71 | LOSS: 5.287915762571228e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 17/71 | LOSS: 5.231096553062444e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 18/71 | LOSS: 5.208918980171898e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 19/71 | LOSS: 5.105382581405138e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 20/71 | LOSS: 5.0564918486391735e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 21/71 | LOSS: 5.056718908814549e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 22/71 | LOSS: 5.013504041135488e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 23/71 | LOSS: 4.966142502856504e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 24/71 | LOSS: 4.9551483152754376e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 25/71 | LOSS: 4.964764536033237e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 26/71 | LOSS: 4.941376093686105e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 27/71 | LOSS: 4.936678798068377e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 28/71 | LOSS: 4.908079525758412e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 29/71 | LOSS: 4.929980157915755e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 30/71 | LOSS: 4.923893951150427e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 31/71 | LOSS: 4.969665646115118e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 32/71 | LOSS: 4.923750786860403e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 33/71 | LOSS: 4.924167275449316e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 34/71 | LOSS: 4.894658377452288e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 35/71 | LOSS: 4.893551464293624e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 36/71 | LOSS: 4.888144119192745e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 37/71 | LOSS: 4.9006147922849965e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 38/71 | LOSS: 4.878427215515284e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 39/71 | LOSS: 4.896127927622729e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 40/71 | LOSS: 4.88996269401779e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 41/71 | LOSS: 4.875879128684617e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 42/71 | LOSS: 4.895641627944167e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 43/71 | LOSS: 4.8972818624613206e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 44/71 | LOSS: 4.878715299128089e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 45/71 | LOSS: 4.888210083742666e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 46/71 | LOSS: 4.873565500174857e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 47/71 | LOSS: 4.895083672333082e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 48/71 | LOSS: 4.880326816085156e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 49/71 | LOSS: 4.860849394390243e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 50/71 | LOSS: 4.8336648575844035e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 51/71 | LOSS: 4.8178543640740545e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 52/71 | LOSS: 4.7996542443283e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 53/71 | LOSS: 4.7773094812092256e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 54/71 | LOSS: 4.785719292241e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 55/71 | LOSS: 4.789413342872909e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 56/71 | LOSS: 4.781866752174417e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 57/71 | LOSS: 4.7917067463302e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 58/71 | LOSS: 4.783775855654405e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 59/71 | LOSS: 4.764579819038772e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 60/71 | LOSS: 4.762609482075605e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 61/71 | LOSS: 4.7685581641358554e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 62/71 | LOSS: 4.774887364484519e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 63/71 | LOSS: 4.7811651704421365e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 64/71 | LOSS: 4.77860980236773e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 65/71 | LOSS: 4.8124254969176645e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 66/71 | LOSS: 4.801387043997563e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 67/71 | LOSS: 4.832808997434768e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 68/71 | LOSS: 4.828813327707315e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 69/71 | LOSS: 4.8279574879675884e-06\n",
      "TRAIN: EPOCH 358/1000 | BATCH 70/71 | LOSS: 4.844241120864213e-06\n",
      "VAL: EPOCH 358/1000 | BATCH 0/8 | LOSS: 4.2331948861829005e-06\n",
      "VAL: EPOCH 358/1000 | BATCH 1/8 | LOSS: 4.252577582519734e-06\n",
      "VAL: EPOCH 358/1000 | BATCH 2/8 | LOSS: 4.45321074948879e-06\n",
      "VAL: EPOCH 358/1000 | BATCH 3/8 | LOSS: 4.5548740672529675e-06\n",
      "VAL: EPOCH 358/1000 | BATCH 4/8 | LOSS: 4.5053084249957465e-06\n",
      "VAL: EPOCH 358/1000 | BATCH 5/8 | LOSS: 4.378197521267187e-06\n",
      "VAL: EPOCH 358/1000 | BATCH 6/8 | LOSS: 4.303368768887594e-06\n",
      "VAL: EPOCH 358/1000 | BATCH 7/8 | LOSS: 4.231028668755243e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 0/71 | LOSS: 3.863602614728734e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 1/71 | LOSS: 3.832761194644263e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 2/71 | LOSS: 4.0039609909096425e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 3/71 | LOSS: 4.154179123361246e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 4/71 | LOSS: 4.238166184222791e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 5/71 | LOSS: 4.299438539116333e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 6/71 | LOSS: 4.277927343667086e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 7/71 | LOSS: 4.2891459770544316e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 8/71 | LOSS: 4.449768817317414e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 9/71 | LOSS: 4.376509264147899e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 10/71 | LOSS: 4.441393126729633e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 11/71 | LOSS: 4.4912898715665506e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 12/71 | LOSS: 4.559422537707947e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 13/71 | LOSS: 4.516429417630466e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 14/71 | LOSS: 4.5130010676075475e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 15/71 | LOSS: 4.525093814322645e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 16/71 | LOSS: 4.508398125621812e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 17/71 | LOSS: 4.527305198583538e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 18/71 | LOSS: 4.509862205132346e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 19/71 | LOSS: 4.480141649310099e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 20/71 | LOSS: 4.484509547962391e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 21/71 | LOSS: 4.480418913937518e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 22/71 | LOSS: 4.4892887533775214e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 23/71 | LOSS: 4.441642062147366e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 24/71 | LOSS: 4.4657325179287e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 25/71 | LOSS: 4.473776374862399e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 26/71 | LOSS: 4.479580664135736e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 27/71 | LOSS: 4.455643956394592e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 28/71 | LOSS: 4.430420278430543e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 29/71 | LOSS: 4.391694005789759e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 30/71 | LOSS: 4.407890210425267e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 31/71 | LOSS: 4.4480370533506175e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 32/71 | LOSS: 4.468884257717449e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 33/71 | LOSS: 4.511683280671766e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 34/71 | LOSS: 4.487987208839124e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 35/71 | LOSS: 4.486183197766675e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 36/71 | LOSS: 4.480123023411677e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 37/71 | LOSS: 4.476180702352538e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 38/71 | LOSS: 4.525919481378738e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 39/71 | LOSS: 4.516313202884703e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 40/71 | LOSS: 4.526667170488864e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 41/71 | LOSS: 4.576582152064172e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 42/71 | LOSS: 4.552179213300575e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 43/71 | LOSS: 4.577351588026928e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 44/71 | LOSS: 4.579192217432946e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 45/71 | LOSS: 4.618167523680629e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 46/71 | LOSS: 4.636132484363848e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 47/71 | LOSS: 4.66784354102856e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 48/71 | LOSS: 4.6549897127320316e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 49/71 | LOSS: 4.637922720576171e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 50/71 | LOSS: 4.652867413396427e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 51/71 | LOSS: 4.636514876647897e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 52/71 | LOSS: 4.624101148812838e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 53/71 | LOSS: 4.611987109186615e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 54/71 | LOSS: 4.641609650041739e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 55/71 | LOSS: 4.655756794948372e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 56/71 | LOSS: 4.6529735858744145e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 57/71 | LOSS: 4.6370441458716886e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 58/71 | LOSS: 4.622724137041556e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 59/71 | LOSS: 4.638949788689691e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 60/71 | LOSS: 4.6498949277371195e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 61/71 | LOSS: 4.657328806769651e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 62/71 | LOSS: 4.655175669338658e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 63/71 | LOSS: 4.65681783623495e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 64/71 | LOSS: 4.670105506822718e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 65/71 | LOSS: 4.665593816312277e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 66/71 | LOSS: 4.6792883406838045e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 67/71 | LOSS: 4.686896692660144e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 68/71 | LOSS: 4.6814989811695264e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 69/71 | LOSS: 4.69024692262922e-06\n",
      "TRAIN: EPOCH 359/1000 | BATCH 70/71 | LOSS: 4.677256455720389e-06\n",
      "VAL: EPOCH 359/1000 | BATCH 0/8 | LOSS: 5.695999789168127e-06\n",
      "VAL: EPOCH 359/1000 | BATCH 1/8 | LOSS: 5.476968908624258e-06\n",
      "VAL: EPOCH 359/1000 | BATCH 2/8 | LOSS: 5.622034526216642e-06\n",
      "VAL: EPOCH 359/1000 | BATCH 3/8 | LOSS: 5.541972200262535e-06\n",
      "VAL: EPOCH 359/1000 | BATCH 4/8 | LOSS: 5.567290827457327e-06\n",
      "VAL: EPOCH 359/1000 | BATCH 5/8 | LOSS: 5.424404965500192e-06\n",
      "VAL: EPOCH 359/1000 | BATCH 6/8 | LOSS: 5.416077458773673e-06\n",
      "VAL: EPOCH 359/1000 | BATCH 7/8 | LOSS: 5.421260482307844e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 0/71 | LOSS: 5.014374437450897e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 1/71 | LOSS: 4.989039098290959e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 2/71 | LOSS: 4.963873986222704e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 3/71 | LOSS: 4.851408220929443e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 4/71 | LOSS: 5.101452370581683e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 5/71 | LOSS: 5.069624421594199e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 6/71 | LOSS: 5.212359545631833e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 7/71 | LOSS: 5.116618524425576e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 8/71 | LOSS: 5.294661605148576e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 9/71 | LOSS: 5.298542600939982e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 10/71 | LOSS: 5.271055752845396e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 11/71 | LOSS: 5.3979219577134545e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 12/71 | LOSS: 5.4720242307946765e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 13/71 | LOSS: 5.513950619777981e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 14/71 | LOSS: 5.573527626741755e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 15/71 | LOSS: 5.518476655197446e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 16/71 | LOSS: 5.445000811853199e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 17/71 | LOSS: 5.394169167832135e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 18/71 | LOSS: 5.340279537784609e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 19/71 | LOSS: 5.31706471065263e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 20/71 | LOSS: 5.295076183150134e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 21/71 | LOSS: 5.215939195708912e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 22/71 | LOSS: 5.272357335026912e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 23/71 | LOSS: 5.2712796142865654e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 24/71 | LOSS: 5.301470409904141e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 25/71 | LOSS: 5.308228852440012e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 26/71 | LOSS: 5.289697364113871e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 27/71 | LOSS: 5.264363624389391e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 28/71 | LOSS: 5.275593876235532e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 29/71 | LOSS: 5.295935291845429e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 30/71 | LOSS: 5.296664532520921e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 31/71 | LOSS: 5.307231674578361e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 32/71 | LOSS: 5.4254380136791784e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 33/71 | LOSS: 5.397847109420314e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 34/71 | LOSS: 5.5672444562203185e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 35/71 | LOSS: 5.520128638636379e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 36/71 | LOSS: 5.581482300103006e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 37/71 | LOSS: 5.551041783148417e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 38/71 | LOSS: 5.576187364707808e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 39/71 | LOSS: 5.558366876812215e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 40/71 | LOSS: 5.519718220833655e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 41/71 | LOSS: 5.529739458628076e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 42/71 | LOSS: 5.507235707841045e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 43/71 | LOSS: 5.495991114333844e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 44/71 | LOSS: 5.490229957811405e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 45/71 | LOSS: 5.5006629261934785e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 46/71 | LOSS: 5.480142135638744e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 47/71 | LOSS: 5.455328041155856e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 48/71 | LOSS: 5.446499082089369e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 49/71 | LOSS: 5.414802344603231e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 50/71 | LOSS: 5.387522645940087e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 51/71 | LOSS: 5.382464005029201e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 52/71 | LOSS: 5.374923191238678e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 53/71 | LOSS: 5.371251459278197e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 54/71 | LOSS: 5.370019648994044e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 55/71 | LOSS: 5.349215403579624e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 56/71 | LOSS: 5.342194734175951e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 57/71 | LOSS: 5.33579427140247e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 58/71 | LOSS: 5.31651867431257e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 59/71 | LOSS: 5.294439893077651e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 60/71 | LOSS: 5.260276115312409e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 61/71 | LOSS: 5.265038482555669e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 62/71 | LOSS: 5.261509967466635e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 63/71 | LOSS: 5.259715955219235e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 64/71 | LOSS: 5.2563384857561545e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 65/71 | LOSS: 5.24182347004226e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 66/71 | LOSS: 5.22192106052274e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 67/71 | LOSS: 5.203316304152181e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 68/71 | LOSS: 5.196847730300194e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 69/71 | LOSS: 5.176803343991716e-06\n",
      "TRAIN: EPOCH 360/1000 | BATCH 70/71 | LOSS: 5.153036809902069e-06\n",
      "VAL: EPOCH 360/1000 | BATCH 0/8 | LOSS: 6.332432803901611e-06\n",
      "VAL: EPOCH 360/1000 | BATCH 1/8 | LOSS: 5.784237828265759e-06\n",
      "VAL: EPOCH 360/1000 | BATCH 2/8 | LOSS: 5.733513717132155e-06\n",
      "VAL: EPOCH 360/1000 | BATCH 3/8 | LOSS: 5.6336302804993466e-06\n",
      "VAL: EPOCH 360/1000 | BATCH 4/8 | LOSS: 5.5372808674292175e-06\n",
      "VAL: EPOCH 360/1000 | BATCH 5/8 | LOSS: 5.324198506665804e-06\n",
      "VAL: EPOCH 360/1000 | BATCH 6/8 | LOSS: 5.291826320379707e-06\n",
      "VAL: EPOCH 360/1000 | BATCH 7/8 | LOSS: 5.176006141027756e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 0/71 | LOSS: 4.8045371840999e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 1/71 | LOSS: 4.527767032413976e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 2/71 | LOSS: 4.595415248331847e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 3/71 | LOSS: 4.430312628755928e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 4/71 | LOSS: 4.477279071579688e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 5/71 | LOSS: 4.378387226703732e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 6/71 | LOSS: 4.4714371532401335e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 7/71 | LOSS: 4.467771361760242e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 8/71 | LOSS: 4.536239076615958e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 9/71 | LOSS: 4.642268550014705e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 10/71 | LOSS: 4.703070813279324e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 11/71 | LOSS: 4.709734525931708e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 12/71 | LOSS: 4.790163354930253e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 13/71 | LOSS: 4.839409874486071e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 14/71 | LOSS: 4.761151376442285e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 15/71 | LOSS: 4.829869652667185e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 16/71 | LOSS: 4.793688838296529e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 17/71 | LOSS: 4.773663502217258e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 18/71 | LOSS: 4.772749721987087e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 19/71 | LOSS: 4.833957086702867e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 20/71 | LOSS: 4.787872924117394e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 21/71 | LOSS: 4.884881966724854e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 22/71 | LOSS: 4.932455893772978e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 23/71 | LOSS: 4.993476428201878e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 24/71 | LOSS: 5.0478873890824615e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 25/71 | LOSS: 5.045200606008831e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 26/71 | LOSS: 5.102172306376613e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 27/71 | LOSS: 5.071357528712335e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 28/71 | LOSS: 5.107764191679648e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 29/71 | LOSS: 5.089951855552499e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 30/71 | LOSS: 5.093409086956883e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 31/71 | LOSS: 5.112846451993391e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 32/71 | LOSS: 5.113003653329515e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 33/71 | LOSS: 5.160378885407494e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 34/71 | LOSS: 5.141160311593142e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 35/71 | LOSS: 5.124052917279995e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 36/71 | LOSS: 5.114004850199578e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 37/71 | LOSS: 5.140834470071959e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 38/71 | LOSS: 5.103028587864873e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 39/71 | LOSS: 5.097509472307138e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 40/71 | LOSS: 5.094596650540153e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 41/71 | LOSS: 5.110610854466185e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 42/71 | LOSS: 5.191784482100724e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 43/71 | LOSS: 5.202819393575324e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 44/71 | LOSS: 5.2309740466524894e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 45/71 | LOSS: 5.280798849644666e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 46/71 | LOSS: 5.299106325107092e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 47/71 | LOSS: 5.309705613854021e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 48/71 | LOSS: 5.303283243699056e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 49/71 | LOSS: 5.326747932485887e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 50/71 | LOSS: 5.315206163133696e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 51/71 | LOSS: 5.312424710050086e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 52/71 | LOSS: 5.3325174087389454e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 53/71 | LOSS: 5.321833441485069e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 54/71 | LOSS: 5.308167593201771e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 55/71 | LOSS: 5.32176495328583e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 56/71 | LOSS: 5.31963326609816e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 57/71 | LOSS: 5.2964187489124015e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 58/71 | LOSS: 5.281157443231816e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 59/71 | LOSS: 5.265524646347331e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 60/71 | LOSS: 5.253135858780655e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 61/71 | LOSS: 5.234886510797585e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 62/71 | LOSS: 5.223303892379922e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 63/71 | LOSS: 5.214113585338964e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 64/71 | LOSS: 5.200086063548672e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 65/71 | LOSS: 5.194170667395652e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 66/71 | LOSS: 5.198519252800581e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 67/71 | LOSS: 5.200877079486792e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 68/71 | LOSS: 5.200451464654601e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 69/71 | LOSS: 5.177096097343435e-06\n",
      "TRAIN: EPOCH 361/1000 | BATCH 70/71 | LOSS: 5.197898503929921e-06\n",
      "VAL: EPOCH 361/1000 | BATCH 0/8 | LOSS: 4.81614142699982e-06\n",
      "VAL: EPOCH 361/1000 | BATCH 1/8 | LOSS: 4.753695748149767e-06\n",
      "VAL: EPOCH 361/1000 | BATCH 2/8 | LOSS: 4.967985508604518e-06\n",
      "VAL: EPOCH 361/1000 | BATCH 3/8 | LOSS: 5.008885750612535e-06\n",
      "VAL: EPOCH 361/1000 | BATCH 4/8 | LOSS: 4.9806586503109426e-06\n",
      "VAL: EPOCH 361/1000 | BATCH 5/8 | LOSS: 4.883985714817148e-06\n",
      "VAL: EPOCH 361/1000 | BATCH 6/8 | LOSS: 4.852863379970326e-06\n",
      "VAL: EPOCH 361/1000 | BATCH 7/8 | LOSS: 4.848092373777035e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 0/71 | LOSS: 4.788418664247729e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 1/71 | LOSS: 4.578290372592164e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 2/71 | LOSS: 4.422545619794012e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 3/71 | LOSS: 4.472974978853017e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 4/71 | LOSS: 4.551309484668309e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 5/71 | LOSS: 4.43032869649566e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 6/71 | LOSS: 4.484023065742804e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 7/71 | LOSS: 4.365844745279901e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 8/71 | LOSS: 4.469790433682243e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 9/71 | LOSS: 4.581492203215021e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 10/71 | LOSS: 4.671675634943478e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 11/71 | LOSS: 4.721693206496032e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 12/71 | LOSS: 4.682644570353799e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 13/71 | LOSS: 4.660051542616982e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 14/71 | LOSS: 4.698211220481122e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 15/71 | LOSS: 4.738292716410797e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 16/71 | LOSS: 4.724058405425988e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 17/71 | LOSS: 4.739833280432827e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 18/71 | LOSS: 4.744451895308693e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 19/71 | LOSS: 4.7248289092749475e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 20/71 | LOSS: 4.7684017902481306e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 21/71 | LOSS: 4.828336469389879e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 22/71 | LOSS: 4.78208117665022e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 23/71 | LOSS: 4.8183039685530576e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 24/71 | LOSS: 4.8217234598268986e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 25/71 | LOSS: 4.792526283381449e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 26/71 | LOSS: 4.777408245596841e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 27/71 | LOSS: 4.760734498177044e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 28/71 | LOSS: 4.757672388285723e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 29/71 | LOSS: 4.7529484694071774e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 30/71 | LOSS: 4.7615867588906584e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 31/71 | LOSS: 4.734246992654789e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 32/71 | LOSS: 4.705370918489996e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 33/71 | LOSS: 4.697732566348671e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 34/71 | LOSS: 4.708290225607925e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 35/71 | LOSS: 4.704490095264191e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 36/71 | LOSS: 4.6831644954796205e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 37/71 | LOSS: 4.670749849298629e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 38/71 | LOSS: 4.703515987085456e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 39/71 | LOSS: 4.673794450127389e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 40/71 | LOSS: 4.681472873704567e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 41/71 | LOSS: 4.661179646280375e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 42/71 | LOSS: 4.652089950637716e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 43/71 | LOSS: 4.686480839278705e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 44/71 | LOSS: 4.6827987464186865e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 45/71 | LOSS: 4.679906559747748e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 46/71 | LOSS: 4.705514550338155e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 47/71 | LOSS: 4.7029295776231566e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 48/71 | LOSS: 4.6952007518744466e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 49/71 | LOSS: 4.693005162152986e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 50/71 | LOSS: 4.709845254682495e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 51/71 | LOSS: 4.692263268260342e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 52/71 | LOSS: 4.7009282040545375e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 53/71 | LOSS: 4.689659678863083e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 54/71 | LOSS: 4.686328838943154e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 55/71 | LOSS: 4.716128350829811e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 56/71 | LOSS: 4.7156201832733226e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 57/71 | LOSS: 4.748415476726599e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 58/71 | LOSS: 4.736189886621028e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 59/71 | LOSS: 4.746463639548893e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 60/71 | LOSS: 4.747228912805678e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 61/71 | LOSS: 4.748382615792751e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 62/71 | LOSS: 4.7583832694062804e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 63/71 | LOSS: 4.783764065763307e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 64/71 | LOSS: 4.775224998901159e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 65/71 | LOSS: 4.783969302479818e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 66/71 | LOSS: 4.785921609214005e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 67/71 | LOSS: 4.8044286833882165e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 68/71 | LOSS: 4.81694127816097e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 69/71 | LOSS: 4.815842089556099e-06\n",
      "TRAIN: EPOCH 362/1000 | BATCH 70/71 | LOSS: 4.826090981995049e-06\n",
      "VAL: EPOCH 362/1000 | BATCH 0/8 | LOSS: 5.62278728466481e-06\n",
      "VAL: EPOCH 362/1000 | BATCH 1/8 | LOSS: 5.261573733150726e-06\n",
      "VAL: EPOCH 362/1000 | BATCH 2/8 | LOSS: 5.253001897168967e-06\n",
      "VAL: EPOCH 362/1000 | BATCH 3/8 | LOSS: 5.255984547147818e-06\n",
      "VAL: EPOCH 362/1000 | BATCH 4/8 | LOSS: 5.112170856591547e-06\n",
      "VAL: EPOCH 362/1000 | BATCH 5/8 | LOSS: 4.881213802339819e-06\n",
      "VAL: EPOCH 362/1000 | BATCH 6/8 | LOSS: 4.809473986305031e-06\n",
      "VAL: EPOCH 362/1000 | BATCH 7/8 | LOSS: 4.6854781317051675e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 0/71 | LOSS: 4.495695975492708e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 1/71 | LOSS: 5.255138603388332e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 2/71 | LOSS: 5.3991058545458754e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 3/71 | LOSS: 5.124318818161555e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 4/71 | LOSS: 5.230417173152091e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 5/71 | LOSS: 5.0920189854271785e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 6/71 | LOSS: 5.114449387682336e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 7/71 | LOSS: 5.070729400813434e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 8/71 | LOSS: 5.1359697863517795e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 9/71 | LOSS: 5.0549942443467446e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 10/71 | LOSS: 4.925122394442124e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 11/71 | LOSS: 4.977182641141553e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 12/71 | LOSS: 4.907878502098566e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 13/71 | LOSS: 5.058842443109565e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 14/71 | LOSS: 4.95984775928567e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 15/71 | LOSS: 4.958132748811295e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 16/71 | LOSS: 4.98125996273302e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 17/71 | LOSS: 4.970544788799695e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 18/71 | LOSS: 4.97228301068581e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 19/71 | LOSS: 4.929267754505418e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 20/71 | LOSS: 4.934838920061815e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 21/71 | LOSS: 4.946453300776326e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 22/71 | LOSS: 4.963550954135312e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 23/71 | LOSS: 4.959249404616155e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 24/71 | LOSS: 4.927812669848208e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 25/71 | LOSS: 4.939002014207989e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 26/71 | LOSS: 4.957641025342767e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 27/71 | LOSS: 4.922358219313797e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 28/71 | LOSS: 4.905947795625002e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 29/71 | LOSS: 4.8946417261201225e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 30/71 | LOSS: 4.898016553148233e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 31/71 | LOSS: 4.875550011718133e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 32/71 | LOSS: 4.854001975796894e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 33/71 | LOSS: 4.819691619505808e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 34/71 | LOSS: 4.78015660649232e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 35/71 | LOSS: 4.768058021100943e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 36/71 | LOSS: 4.772647854052421e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 37/71 | LOSS: 4.769137376570143e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 38/71 | LOSS: 4.750386312643502e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 39/71 | LOSS: 4.755924385335674e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 40/71 | LOSS: 4.765545659987455e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 41/71 | LOSS: 4.74670006648618e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 42/71 | LOSS: 4.781853201779467e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 43/71 | LOSS: 4.75615216931725e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 44/71 | LOSS: 4.762204258036541e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 45/71 | LOSS: 4.78490028399392e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 46/71 | LOSS: 4.799334051990336e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 47/71 | LOSS: 4.787079196451789e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 48/71 | LOSS: 4.7985764579106194e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 49/71 | LOSS: 4.813812738575507e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 50/71 | LOSS: 4.787485157102506e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 51/71 | LOSS: 4.7742734680538815e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 52/71 | LOSS: 4.793040819283803e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 53/71 | LOSS: 4.792551778707464e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 54/71 | LOSS: 4.787110389191763e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 55/71 | LOSS: 4.789121500639989e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 56/71 | LOSS: 4.780405345771119e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 57/71 | LOSS: 4.764699081627849e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 58/71 | LOSS: 4.755094092399952e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 59/71 | LOSS: 4.7635803942588e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 60/71 | LOSS: 4.7559091712412096e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 61/71 | LOSS: 4.765694022294596e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 62/71 | LOSS: 4.745731140636573e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 63/71 | LOSS: 4.733583018889931e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 64/71 | LOSS: 4.724046105436103e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 65/71 | LOSS: 4.740392086839138e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 66/71 | LOSS: 4.729034531213875e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 67/71 | LOSS: 4.7280595468049906e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 68/71 | LOSS: 4.7277571339045474e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 69/71 | LOSS: 4.70319444697712e-06\n",
      "TRAIN: EPOCH 363/1000 | BATCH 70/71 | LOSS: 4.71341158899263e-06\n",
      "VAL: EPOCH 363/1000 | BATCH 0/8 | LOSS: 4.60850333183771e-06\n",
      "VAL: EPOCH 363/1000 | BATCH 1/8 | LOSS: 4.559752824206953e-06\n",
      "VAL: EPOCH 363/1000 | BATCH 2/8 | LOSS: 4.62304691003131e-06\n",
      "VAL: EPOCH 363/1000 | BATCH 3/8 | LOSS: 4.7774227596164565e-06\n",
      "VAL: EPOCH 363/1000 | BATCH 4/8 | LOSS: 4.675338186643785e-06\n",
      "VAL: EPOCH 363/1000 | BATCH 5/8 | LOSS: 4.58276410123896e-06\n",
      "VAL: EPOCH 363/1000 | BATCH 6/8 | LOSS: 4.453645260582562e-06\n",
      "VAL: EPOCH 363/1000 | BATCH 7/8 | LOSS: 4.347798238768519e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 0/71 | LOSS: 4.009005806437926e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 1/71 | LOSS: 4.291201321393601e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 2/71 | LOSS: 4.726748102257261e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 3/71 | LOSS: 4.690807372753625e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 4/71 | LOSS: 4.601744331012015e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 5/71 | LOSS: 4.722029188997112e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 6/71 | LOSS: 4.7112427117619e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 7/71 | LOSS: 4.753281586999947e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 8/71 | LOSS: 4.8319224737901496e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 9/71 | LOSS: 4.767527161675389e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 10/71 | LOSS: 4.744419832356718e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 11/71 | LOSS: 4.813002419723489e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 12/71 | LOSS: 4.836408676895259e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 13/71 | LOSS: 4.786798951629732e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 14/71 | LOSS: 4.817101792771913e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 15/71 | LOSS: 4.868060301532751e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 16/71 | LOSS: 4.8596032898143785e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 17/71 | LOSS: 4.883315114057041e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 18/71 | LOSS: 4.836891381082272e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 19/71 | LOSS: 4.790837101609213e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 20/71 | LOSS: 4.752638146566737e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 21/71 | LOSS: 4.74190499682498e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 22/71 | LOSS: 4.720648180607342e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 23/71 | LOSS: 4.692397984247994e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 24/71 | LOSS: 4.741495522466721e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 25/71 | LOSS: 4.74475894826845e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 26/71 | LOSS: 4.742444818677743e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 27/71 | LOSS: 4.802769012712815e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 28/71 | LOSS: 4.757756916409687e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 29/71 | LOSS: 4.80416570098896e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 30/71 | LOSS: 4.818617335624895e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 31/71 | LOSS: 4.864174812269084e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 32/71 | LOSS: 4.837500827389621e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 33/71 | LOSS: 4.84349242041764e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 34/71 | LOSS: 4.88082932861289e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 35/71 | LOSS: 4.86727756449707e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 36/71 | LOSS: 4.942090699104655e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 37/71 | LOSS: 4.928491379312043e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 38/71 | LOSS: 4.999211365103316e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 39/71 | LOSS: 4.970551685801183e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 40/71 | LOSS: 5.008062300209819e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 41/71 | LOSS: 5.026522384945392e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 42/71 | LOSS: 5.012881205783383e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 43/71 | LOSS: 5.0364002597482545e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 44/71 | LOSS: 5.026905950621262e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 45/71 | LOSS: 5.094636524359964e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 46/71 | LOSS: 5.073585841873477e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 47/71 | LOSS: 5.084915860228041e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 48/71 | LOSS: 5.109952326165512e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 49/71 | LOSS: 5.115019503136864e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 50/71 | LOSS: 5.140924740236883e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 51/71 | LOSS: 5.11653451422507e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 52/71 | LOSS: 5.11015229487796e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 53/71 | LOSS: 5.082577895673576e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 54/71 | LOSS: 5.067019360053977e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 55/71 | LOSS: 5.140280348671565e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 56/71 | LOSS: 5.112435214789257e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 57/71 | LOSS: 5.2126884399732605e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 58/71 | LOSS: 5.198004377450423e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 59/71 | LOSS: 5.251652434405211e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 60/71 | LOSS: 5.269415627570525e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 61/71 | LOSS: 5.291590659614867e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 62/71 | LOSS: 5.287742934587951e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 63/71 | LOSS: 5.304728603050535e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 64/71 | LOSS: 5.339334389785878e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 65/71 | LOSS: 5.331063854982583e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 66/71 | LOSS: 5.387396983556457e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 67/71 | LOSS: 5.366936046116547e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 68/71 | LOSS: 5.411776447030918e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 69/71 | LOSS: 5.415614019251994e-06\n",
      "TRAIN: EPOCH 364/1000 | BATCH 70/71 | LOSS: 5.467249510230162e-06\n",
      "VAL: EPOCH 364/1000 | BATCH 0/8 | LOSS: 5.9527983466978185e-06\n",
      "VAL: EPOCH 364/1000 | BATCH 1/8 | LOSS: 5.6018925533862785e-06\n",
      "VAL: EPOCH 364/1000 | BATCH 2/8 | LOSS: 5.479616750866019e-06\n",
      "VAL: EPOCH 364/1000 | BATCH 3/8 | LOSS: 5.42965699423803e-06\n",
      "VAL: EPOCH 364/1000 | BATCH 4/8 | LOSS: 5.324990070221247e-06\n",
      "VAL: EPOCH 364/1000 | BATCH 5/8 | LOSS: 5.115677671104398e-06\n",
      "VAL: EPOCH 364/1000 | BATCH 6/8 | LOSS: 5.030380569743491e-06\n",
      "VAL: EPOCH 364/1000 | BATCH 7/8 | LOSS: 4.944653483107686e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 0/71 | LOSS: 4.848903699894436e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 1/71 | LOSS: 5.1783499657176435e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 2/71 | LOSS: 5.3750979229031754e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 3/71 | LOSS: 5.032768285673228e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 4/71 | LOSS: 5.424157097877469e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 5/71 | LOSS: 5.051312389999414e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 6/71 | LOSS: 5.345893669073121e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 7/71 | LOSS: 5.3699447732924455e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 8/71 | LOSS: 5.519517723466076e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 9/71 | LOSS: 5.51908271972934e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 10/71 | LOSS: 5.351210781803027e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 11/71 | LOSS: 5.339778056168143e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 12/71 | LOSS: 5.257239133872925e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 13/71 | LOSS: 5.302645100917809e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 14/71 | LOSS: 5.20229694605708e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 15/71 | LOSS: 5.146314308035471e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 16/71 | LOSS: 5.205270728016758e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 17/71 | LOSS: 5.097716477090823e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 18/71 | LOSS: 5.043940111710598e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 19/71 | LOSS: 5.046248577400547e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 20/71 | LOSS: 5.057785648161717e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 21/71 | LOSS: 5.013698755233649e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 22/71 | LOSS: 4.939993311342297e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 23/71 | LOSS: 4.944005524976092e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 24/71 | LOSS: 4.950950915372232e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 25/71 | LOSS: 4.990109521084098e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 26/71 | LOSS: 4.994872496253164e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 27/71 | LOSS: 4.981173869314911e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 28/71 | LOSS: 4.99787041729272e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 29/71 | LOSS: 4.980112695799713e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 30/71 | LOSS: 4.969832917272617e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 31/71 | LOSS: 5.003416532645133e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 32/71 | LOSS: 5.012471099303873e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 33/71 | LOSS: 4.976600527751382e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 34/71 | LOSS: 4.999075710137342e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 35/71 | LOSS: 4.971757119973417e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 36/71 | LOSS: 4.979665735280267e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 37/71 | LOSS: 4.991047286526545e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 38/71 | LOSS: 4.995125226951831e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 39/71 | LOSS: 4.978299796221109e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 40/71 | LOSS: 5.034726766053809e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 41/71 | LOSS: 5.043124918388674e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 42/71 | LOSS: 5.042140760768996e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 43/71 | LOSS: 5.028159979371015e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 44/71 | LOSS: 5.057495157719435e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 45/71 | LOSS: 5.034560968931106e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 46/71 | LOSS: 5.008622996141872e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 47/71 | LOSS: 5.033972541923504e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 48/71 | LOSS: 5.026989054720022e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 49/71 | LOSS: 5.028561472499859e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 50/71 | LOSS: 5.030912448359338e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 51/71 | LOSS: 5.00708106826026e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 52/71 | LOSS: 5.011298955376435e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 53/71 | LOSS: 5.00906500949188e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 54/71 | LOSS: 5.040115039181811e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 55/71 | LOSS: 5.060555354573028e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 56/71 | LOSS: 5.053390659189685e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 57/71 | LOSS: 5.04844465781232e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 58/71 | LOSS: 5.0343929553676745e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 59/71 | LOSS: 5.032195720862849e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 60/71 | LOSS: 5.017226665019779e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 61/71 | LOSS: 4.999361914498525e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 62/71 | LOSS: 5.001507202356902e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 63/71 | LOSS: 4.997304081655329e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 64/71 | LOSS: 4.993807005331082e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 65/71 | LOSS: 5.004502134287384e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 66/71 | LOSS: 5.000851730861243e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 67/71 | LOSS: 4.9912148236163535e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 68/71 | LOSS: 4.976144526259252e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 69/71 | LOSS: 4.961503165239784e-06\n",
      "TRAIN: EPOCH 365/1000 | BATCH 70/71 | LOSS: 4.953061819269607e-06\n",
      "VAL: EPOCH 365/1000 | BATCH 0/8 | LOSS: 5.197143309487728e-06\n",
      "VAL: EPOCH 365/1000 | BATCH 1/8 | LOSS: 4.84098268316302e-06\n",
      "VAL: EPOCH 365/1000 | BATCH 2/8 | LOSS: 4.844864027594061e-06\n",
      "VAL: EPOCH 365/1000 | BATCH 3/8 | LOSS: 4.90197749059007e-06\n",
      "VAL: EPOCH 365/1000 | BATCH 4/8 | LOSS: 4.790223647432867e-06\n",
      "VAL: EPOCH 365/1000 | BATCH 5/8 | LOSS: 4.680937536249985e-06\n",
      "VAL: EPOCH 365/1000 | BATCH 6/8 | LOSS: 4.5296950312539205e-06\n",
      "VAL: EPOCH 365/1000 | BATCH 7/8 | LOSS: 4.419184620019223e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 0/71 | LOSS: 4.596809048962314e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 1/71 | LOSS: 4.435193886820343e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 2/71 | LOSS: 4.343852197052911e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 3/71 | LOSS: 4.400538045956637e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 4/71 | LOSS: 4.3482992623466995e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 5/71 | LOSS: 4.375125248164598e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 6/71 | LOSS: 4.525287944748665e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 7/71 | LOSS: 4.390093550910024e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 8/71 | LOSS: 4.372451889543703e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 9/71 | LOSS: 4.34579171724181e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 10/71 | LOSS: 4.339389657946992e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 11/71 | LOSS: 4.373160758556575e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 12/71 | LOSS: 4.389722369943718e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 13/71 | LOSS: 4.419659636109177e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 14/71 | LOSS: 4.39138295102263e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 15/71 | LOSS: 4.387709523712147e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 16/71 | LOSS: 4.373172982120784e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 17/71 | LOSS: 4.377852709088377e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 18/71 | LOSS: 4.423947051404148e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 19/71 | LOSS: 4.411804422943533e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 20/71 | LOSS: 4.462347565744754e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 21/71 | LOSS: 4.437708133247559e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 22/71 | LOSS: 4.504336382761602e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 23/71 | LOSS: 4.501030777722311e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 24/71 | LOSS: 4.499988435782143e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 25/71 | LOSS: 4.502373634260528e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 26/71 | LOSS: 4.535732125953439e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 27/71 | LOSS: 4.555659147951831e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 28/71 | LOSS: 4.6052078169881935e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 29/71 | LOSS: 4.587639167160281e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 30/71 | LOSS: 4.6091711429917566e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 31/71 | LOSS: 4.646473492186942e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 32/71 | LOSS: 4.665535904926153e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 33/71 | LOSS: 4.6935894033595105e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 34/71 | LOSS: 4.7009820588885176e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 35/71 | LOSS: 4.731601765999787e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 36/71 | LOSS: 4.768279925859109e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 37/71 | LOSS: 4.779916043605083e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 38/71 | LOSS: 4.799538198942453e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 39/71 | LOSS: 4.827978392540899e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 40/71 | LOSS: 4.844412021667245e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 41/71 | LOSS: 4.859345440742063e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 42/71 | LOSS: 4.842072902922451e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 43/71 | LOSS: 4.856179972731215e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 44/71 | LOSS: 4.8357836981772885e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 45/71 | LOSS: 4.832369150473982e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 46/71 | LOSS: 4.8519519633044455e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 47/71 | LOSS: 4.846692874593828e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 48/71 | LOSS: 4.86242369593778e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 49/71 | LOSS: 4.8657556135367486e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 50/71 | LOSS: 4.876673796495786e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 51/71 | LOSS: 4.883088645503729e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 52/71 | LOSS: 4.879111892872936e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 53/71 | LOSS: 4.880490918266231e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 54/71 | LOSS: 4.862988585459375e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 55/71 | LOSS: 4.860297455674559e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 56/71 | LOSS: 4.888255888178256e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 57/71 | LOSS: 4.87651237202452e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 58/71 | LOSS: 4.85438125891097e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 59/71 | LOSS: 4.847571809326231e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 60/71 | LOSS: 4.868815606597145e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 61/71 | LOSS: 4.8651031170999384e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 62/71 | LOSS: 4.84799287704167e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 63/71 | LOSS: 4.833398818959722e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 64/71 | LOSS: 4.829818772122963e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 65/71 | LOSS: 4.810933432963793e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 66/71 | LOSS: 4.81369470978293e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 67/71 | LOSS: 4.814499902749958e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 68/71 | LOSS: 4.840516247487107e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 69/71 | LOSS: 4.831530486626434e-06\n",
      "TRAIN: EPOCH 366/1000 | BATCH 70/71 | LOSS: 4.864632578125604e-06\n",
      "VAL: EPOCH 366/1000 | BATCH 0/8 | LOSS: 5.7652414398035035e-06\n",
      "VAL: EPOCH 366/1000 | BATCH 1/8 | LOSS: 5.472443262988236e-06\n",
      "VAL: EPOCH 366/1000 | BATCH 2/8 | LOSS: 5.731582859880291e-06\n",
      "VAL: EPOCH 366/1000 | BATCH 3/8 | LOSS: 5.601186330750352e-06\n",
      "VAL: EPOCH 366/1000 | BATCH 4/8 | LOSS: 5.576499461312778e-06\n",
      "VAL: EPOCH 366/1000 | BATCH 5/8 | LOSS: 5.414956225043473e-06\n",
      "VAL: EPOCH 366/1000 | BATCH 6/8 | LOSS: 5.337281046584914e-06\n",
      "VAL: EPOCH 366/1000 | BATCH 7/8 | LOSS: 5.329257646735641e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 0/71 | LOSS: 4.4907810661243275e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 1/71 | LOSS: 4.434002903508372e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 2/71 | LOSS: 4.449387688509887e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 3/71 | LOSS: 4.51988194072328e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 4/71 | LOSS: 4.539144538284745e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 5/71 | LOSS: 4.835817965916552e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 6/71 | LOSS: 5.050853425408215e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 7/71 | LOSS: 4.8542543709118036e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 8/71 | LOSS: 4.774941468591957e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 9/71 | LOSS: 4.8063708618428794e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 10/71 | LOSS: 4.951547659650466e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 11/71 | LOSS: 4.9427783703019186e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 12/71 | LOSS: 5.015470798333319e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 13/71 | LOSS: 4.952632187788757e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 14/71 | LOSS: 4.906435970042367e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 15/71 | LOSS: 4.890755974429339e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 16/71 | LOSS: 4.919179339006351e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 17/71 | LOSS: 4.909969852128092e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 18/71 | LOSS: 4.856333290311006e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 19/71 | LOSS: 4.886452211394499e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 20/71 | LOSS: 4.898856678621433e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 21/71 | LOSS: 4.837457667731955e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 22/71 | LOSS: 4.797645191422856e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 23/71 | LOSS: 4.892660304752401e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 24/71 | LOSS: 4.878749732597498e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 25/71 | LOSS: 4.902675055536048e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 26/71 | LOSS: 4.913698123548076e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 27/71 | LOSS: 4.953354359063919e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 28/71 | LOSS: 4.983771414744892e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 29/71 | LOSS: 5.011878753672742e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 30/71 | LOSS: 4.982759873523572e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 31/71 | LOSS: 4.963597660889718e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 32/71 | LOSS: 4.991994942291498e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 33/71 | LOSS: 4.9736266405756506e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 34/71 | LOSS: 5.017282637709286e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 35/71 | LOSS: 5.057284700645444e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 36/71 | LOSS: 5.0617551047203915e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 37/71 | LOSS: 5.12888780241682e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 38/71 | LOSS: 5.220412195063918e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 39/71 | LOSS: 5.276500655782002e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 40/71 | LOSS: 5.322067142803528e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 41/71 | LOSS: 5.393360944643584e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 42/71 | LOSS: 5.3688076953877435e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 43/71 | LOSS: 5.434599829227822e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 44/71 | LOSS: 5.42196919721189e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 45/71 | LOSS: 5.444272099576546e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 46/71 | LOSS: 5.4168015285153586e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 47/71 | LOSS: 5.412069488860046e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 48/71 | LOSS: 5.4001658675272724e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 49/71 | LOSS: 5.410303892858792e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 50/71 | LOSS: 5.415986593374435e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 51/71 | LOSS: 5.380307120853663e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 52/71 | LOSS: 5.371635239189689e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 53/71 | LOSS: 5.359480349364242e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 54/71 | LOSS: 5.384805692963048e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 55/71 | LOSS: 5.375112217669604e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 56/71 | LOSS: 5.366408650412555e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 57/71 | LOSS: 5.3542108256304594e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 58/71 | LOSS: 5.329893032837189e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 59/71 | LOSS: 5.3171113828890766e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 60/71 | LOSS: 5.313240673457898e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 61/71 | LOSS: 5.298110064329869e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 62/71 | LOSS: 5.290693299309443e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 63/71 | LOSS: 5.268502640376482e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 64/71 | LOSS: 5.264949091864625e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 65/71 | LOSS: 5.245970669420785e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 66/71 | LOSS: 5.2567670265027605e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 67/71 | LOSS: 5.243300279289542e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 68/71 | LOSS: 5.236987956026417e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 69/71 | LOSS: 5.242973013862086e-06\n",
      "TRAIN: EPOCH 367/1000 | BATCH 70/71 | LOSS: 5.227311684278211e-06\n",
      "VAL: EPOCH 367/1000 | BATCH 0/8 | LOSS: 6.264069270400796e-06\n",
      "VAL: EPOCH 367/1000 | BATCH 1/8 | LOSS: 5.878201591258403e-06\n",
      "VAL: EPOCH 367/1000 | BATCH 2/8 | LOSS: 5.7261956195967896e-06\n",
      "VAL: EPOCH 367/1000 | BATCH 3/8 | LOSS: 5.7419542827119585e-06\n",
      "VAL: EPOCH 367/1000 | BATCH 4/8 | LOSS: 5.595458333118586e-06\n",
      "VAL: EPOCH 367/1000 | BATCH 5/8 | LOSS: 5.310185391257012e-06\n",
      "VAL: EPOCH 367/1000 | BATCH 6/8 | LOSS: 5.250868135980065e-06\n",
      "VAL: EPOCH 367/1000 | BATCH 7/8 | LOSS: 5.087566535166843e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 0/71 | LOSS: 3.555393050191924e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 1/71 | LOSS: 4.671082024287898e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 2/71 | LOSS: 4.754868011029127e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 3/71 | LOSS: 4.769769020640524e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 4/71 | LOSS: 4.838403765461408e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 5/71 | LOSS: 4.883883169289523e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 6/71 | LOSS: 4.7811801258441326e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 7/71 | LOSS: 4.844764475819829e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 8/71 | LOSS: 4.833259026781889e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 9/71 | LOSS: 4.806713104699156e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 10/71 | LOSS: 4.860952727044192e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 11/71 | LOSS: 4.852109251866447e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 12/71 | LOSS: 4.8954344282929715e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 13/71 | LOSS: 4.926133247603762e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 14/71 | LOSS: 5.003028369780319e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 15/71 | LOSS: 4.92370084259619e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 16/71 | LOSS: 4.906582289014991e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 17/71 | LOSS: 4.88306981575685e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 18/71 | LOSS: 4.8686520729365595e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 19/71 | LOSS: 4.8527736339565305e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 20/71 | LOSS: 4.8263357040836815e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 21/71 | LOSS: 4.896970236503958e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 22/71 | LOSS: 4.924777138749841e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 23/71 | LOSS: 4.9181258627110464e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 24/71 | LOSS: 4.981479751222651e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 25/71 | LOSS: 4.988521692002206e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 26/71 | LOSS: 4.9835557294702285e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 27/71 | LOSS: 4.971823248719634e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 28/71 | LOSS: 4.964523598040426e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 29/71 | LOSS: 5.035435113616889e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 30/71 | LOSS: 4.985915446072355e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 31/71 | LOSS: 4.98943862936585e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 32/71 | LOSS: 5.029338057504377e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 33/71 | LOSS: 5.005915193792402e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 34/71 | LOSS: 5.032505857473422e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 35/71 | LOSS: 5.0554802568411915e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 36/71 | LOSS: 5.041194717213747e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 37/71 | LOSS: 5.081205610000133e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 38/71 | LOSS: 5.060683499836253e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 39/71 | LOSS: 5.107155760697424e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 40/71 | LOSS: 5.091293094044043e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 41/71 | LOSS: 5.117333216730913e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 42/71 | LOSS: 5.1140069540134535e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 43/71 | LOSS: 5.100086577220447e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 44/71 | LOSS: 5.100131152681165e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 45/71 | LOSS: 5.078240959570078e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 46/71 | LOSS: 5.069917961009196e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 47/71 | LOSS: 5.065345537976403e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 48/71 | LOSS: 5.022694949730067e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 49/71 | LOSS: 5.030326965425047e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 50/71 | LOSS: 5.016809484682288e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 51/71 | LOSS: 5.012855004119606e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 52/71 | LOSS: 5.01110818457267e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 53/71 | LOSS: 5.013524312542157e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 54/71 | LOSS: 5.0053366728455085e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 55/71 | LOSS: 5.000638566343696e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 56/71 | LOSS: 4.991154064395743e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 57/71 | LOSS: 4.982018700847503e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 58/71 | LOSS: 4.98325362521859e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 59/71 | LOSS: 4.984430347576563e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 60/71 | LOSS: 5.007423892568083e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 61/71 | LOSS: 5.019969076754698e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 62/71 | LOSS: 5.016099568444862e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 63/71 | LOSS: 4.999046176124011e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 64/71 | LOSS: 5.018993201352155e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 65/71 | LOSS: 5.036082518817046e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 66/71 | LOSS: 5.033307483679539e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 67/71 | LOSS: 5.0623997099939835e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 68/71 | LOSS: 5.0559529301898545e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 69/71 | LOSS: 5.053325958215283e-06\n",
      "TRAIN: EPOCH 368/1000 | BATCH 70/71 | LOSS: 5.048913779729177e-06\n",
      "VAL: EPOCH 368/1000 | BATCH 0/8 | LOSS: 7.685008313274011e-06\n",
      "VAL: EPOCH 368/1000 | BATCH 1/8 | LOSS: 7.3232818067481276e-06\n",
      "VAL: EPOCH 368/1000 | BATCH 2/8 | LOSS: 7.082347262136561e-06\n",
      "VAL: EPOCH 368/1000 | BATCH 3/8 | LOSS: 7.161290113799623e-06\n",
      "VAL: EPOCH 368/1000 | BATCH 4/8 | LOSS: 6.97708146617515e-06\n",
      "VAL: EPOCH 368/1000 | BATCH 5/8 | LOSS: 6.773357805892981e-06\n",
      "VAL: EPOCH 368/1000 | BATCH 6/8 | LOSS: 6.691278875743072e-06\n",
      "VAL: EPOCH 368/1000 | BATCH 7/8 | LOSS: 6.43640834141479e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 0/71 | LOSS: 4.369891939859372e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 1/71 | LOSS: 5.6807473356457194e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 2/71 | LOSS: 5.164639484670867e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 3/71 | LOSS: 5.52745609638805e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 4/71 | LOSS: 5.332428736437578e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 5/71 | LOSS: 5.269415244886962e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 6/71 | LOSS: 5.3120200910988e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 7/71 | LOSS: 5.302741215018614e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 8/71 | LOSS: 5.31558139805889e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 9/71 | LOSS: 5.160927867109422e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 10/71 | LOSS: 5.443971117942552e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 11/71 | LOSS: 5.372258783609141e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 12/71 | LOSS: 5.3356030688379315e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 13/71 | LOSS: 5.315205726219574e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 14/71 | LOSS: 5.229739084218939e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 15/71 | LOSS: 5.166470373296761e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 16/71 | LOSS: 5.132919688870007e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 17/71 | LOSS: 5.06909661150227e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 18/71 | LOSS: 5.024448455515085e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 19/71 | LOSS: 5.02299315030541e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 20/71 | LOSS: 5.066208034674803e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 21/71 | LOSS: 5.042343342541823e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 22/71 | LOSS: 5.049294110864613e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 23/71 | LOSS: 5.077118260032876e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 24/71 | LOSS: 5.082342377136229e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 25/71 | LOSS: 5.0806601953249464e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 26/71 | LOSS: 5.091804538366247e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 27/71 | LOSS: 5.113439702394576e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 28/71 | LOSS: 5.123210171561745e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 29/71 | LOSS: 5.14242099901215e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 30/71 | LOSS: 5.160286359422307e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 31/71 | LOSS: 5.150019603661349e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 32/71 | LOSS: 5.252268100307309e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 33/71 | LOSS: 5.240889911018551e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 34/71 | LOSS: 5.3368376679178e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 35/71 | LOSS: 5.315991605433131e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 36/71 | LOSS: 5.315946067197286e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 37/71 | LOSS: 5.363768698436233e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 38/71 | LOSS: 5.341252640041953e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 39/71 | LOSS: 5.3722894904240094e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 40/71 | LOSS: 5.364981816677667e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 41/71 | LOSS: 5.374799900978576e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 42/71 | LOSS: 5.31728569301269e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 43/71 | LOSS: 5.2845352342956176e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 44/71 | LOSS: 5.256183173211563e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 45/71 | LOSS: 5.265906076562279e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 46/71 | LOSS: 5.2560518666878315e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 47/71 | LOSS: 5.239718911790685e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 48/71 | LOSS: 5.233965747701943e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 49/71 | LOSS: 5.226356574894453e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 50/71 | LOSS: 5.2046691061881504e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 51/71 | LOSS: 5.198546296889197e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 52/71 | LOSS: 5.182741937082852e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 53/71 | LOSS: 5.177778259874483e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 54/71 | LOSS: 5.151678661522138e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 55/71 | LOSS: 5.161359419097218e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 56/71 | LOSS: 5.13625498873201e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 57/71 | LOSS: 5.10483639886686e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 58/71 | LOSS: 5.090430265180796e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 59/71 | LOSS: 5.077560418461265e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 60/71 | LOSS: 5.0713633717365515e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 61/71 | LOSS: 5.068813808770022e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 62/71 | LOSS: 5.057298180656203e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 63/71 | LOSS: 5.070172822030372e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 64/71 | LOSS: 5.045456985569147e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 65/71 | LOSS: 5.035320148663507e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 66/71 | LOSS: 5.030052110289116e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 67/71 | LOSS: 5.014993930404564e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 68/71 | LOSS: 4.999122461742735e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 69/71 | LOSS: 4.997621304677783e-06\n",
      "TRAIN: EPOCH 369/1000 | BATCH 70/71 | LOSS: 4.9842549383932965e-06\n",
      "VAL: EPOCH 369/1000 | BATCH 0/8 | LOSS: 6.768933417333756e-06\n",
      "VAL: EPOCH 369/1000 | BATCH 1/8 | LOSS: 6.567608352270327e-06\n",
      "VAL: EPOCH 369/1000 | BATCH 2/8 | LOSS: 6.666606320019734e-06\n",
      "VAL: EPOCH 369/1000 | BATCH 3/8 | LOSS: 6.5903437871384085e-06\n",
      "VAL: EPOCH 369/1000 | BATCH 4/8 | LOSS: 6.60678451822605e-06\n",
      "VAL: EPOCH 369/1000 | BATCH 5/8 | LOSS: 6.416133146558423e-06\n",
      "VAL: EPOCH 369/1000 | BATCH 6/8 | LOSS: 6.499437274344798e-06\n",
      "VAL: EPOCH 369/1000 | BATCH 7/8 | LOSS: 6.451665115037031e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 0/71 | LOSS: 5.534073352464475e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 1/71 | LOSS: 5.46659748579259e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 2/71 | LOSS: 4.834592649179588e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 3/71 | LOSS: 5.012317160435487e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 4/71 | LOSS: 5.128129396325676e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 5/71 | LOSS: 5.171272884278248e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 6/71 | LOSS: 5.132896149007138e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 7/71 | LOSS: 5.396202197971434e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 8/71 | LOSS: 5.421446631872742e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 9/71 | LOSS: 5.444854195957305e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 10/71 | LOSS: 5.586711722571636e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 11/71 | LOSS: 5.561500302064815e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 12/71 | LOSS: 5.466857706218993e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 13/71 | LOSS: 5.440518309894417e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 14/71 | LOSS: 5.46776691408013e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 15/71 | LOSS: 5.479843963485109e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 16/71 | LOSS: 5.454768437202267e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 17/71 | LOSS: 5.4886111987192026e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 18/71 | LOSS: 5.444739815031521e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 19/71 | LOSS: 5.40848604941857e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 20/71 | LOSS: 5.405500728340398e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 21/71 | LOSS: 5.446489880837775e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 22/71 | LOSS: 5.435470998433986e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 23/71 | LOSS: 5.457452781835552e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 24/71 | LOSS: 5.439231099444441e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 25/71 | LOSS: 5.5040014698394e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 26/71 | LOSS: 5.564210941547235e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 27/71 | LOSS: 5.609353690618134e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 28/71 | LOSS: 5.56488692319394e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 29/71 | LOSS: 5.661412721262119e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 30/71 | LOSS: 5.646138693156254e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 31/71 | LOSS: 5.683517443344499e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 32/71 | LOSS: 5.69594586715649e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 33/71 | LOSS: 5.701977750718738e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 34/71 | LOSS: 5.723232282304837e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 35/71 | LOSS: 5.699065582120966e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 36/71 | LOSS: 5.847322144463023e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 37/71 | LOSS: 5.864602718774174e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 38/71 | LOSS: 6.018898334448936e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 39/71 | LOSS: 6.0017526834599265e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 40/71 | LOSS: 6.142830044132651e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 41/71 | LOSS: 6.126931826396945e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 42/71 | LOSS: 6.163276614029729e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 43/71 | LOSS: 6.131423529867871e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 44/71 | LOSS: 6.135329921461461e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 45/71 | LOSS: 6.117186706472674e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 46/71 | LOSS: 6.084908421898751e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 47/71 | LOSS: 6.093599334159687e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 48/71 | LOSS: 6.100189211236061e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 49/71 | LOSS: 6.084212500354624e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 50/71 | LOSS: 6.080229905266749e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 51/71 | LOSS: 6.076551305620971e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 52/71 | LOSS: 6.051737101480062e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 53/71 | LOSS: 6.0146600625793565e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 54/71 | LOSS: 5.9632362122515175e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 55/71 | LOSS: 5.971088674933915e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 56/71 | LOSS: 5.946855716294458e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 57/71 | LOSS: 5.9349867005321675e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 58/71 | LOSS: 5.8946426734264146e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 59/71 | LOSS: 5.874898442925769e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 60/71 | LOSS: 5.8422774525782e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 61/71 | LOSS: 5.812508722937632e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 62/71 | LOSS: 5.807243247718794e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 63/71 | LOSS: 5.803867459519552e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 64/71 | LOSS: 5.79216947209296e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 65/71 | LOSS: 5.787847791879992e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 66/71 | LOSS: 5.779888709442805e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 67/71 | LOSS: 5.78040228179709e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 68/71 | LOSS: 5.780066869087075e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 69/71 | LOSS: 5.753430955987174e-06\n",
      "TRAIN: EPOCH 370/1000 | BATCH 70/71 | LOSS: 5.7326746634355235e-06\n",
      "VAL: EPOCH 370/1000 | BATCH 0/8 | LOSS: 5.244646672508679e-06\n",
      "VAL: EPOCH 370/1000 | BATCH 1/8 | LOSS: 4.839778284804197e-06\n",
      "VAL: EPOCH 370/1000 | BATCH 2/8 | LOSS: 4.8935741385018145e-06\n",
      "VAL: EPOCH 370/1000 | BATCH 3/8 | LOSS: 5.099760073790094e-06\n",
      "VAL: EPOCH 370/1000 | BATCH 4/8 | LOSS: 4.946380249748472e-06\n",
      "VAL: EPOCH 370/1000 | BATCH 5/8 | LOSS: 5.008039200523247e-06\n",
      "VAL: EPOCH 370/1000 | BATCH 6/8 | LOSS: 4.915421543825817e-06\n",
      "VAL: EPOCH 370/1000 | BATCH 7/8 | LOSS: 4.850424545566057e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 0/71 | LOSS: 4.6434797695837915e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 1/71 | LOSS: 4.4148462166049285e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 2/71 | LOSS: 4.615258300570228e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 3/71 | LOSS: 4.523214442997414e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 4/71 | LOSS: 4.564911159832264e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 5/71 | LOSS: 4.739490956732577e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 6/71 | LOSS: 4.523791761000341e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 7/71 | LOSS: 4.691144170010375e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 8/71 | LOSS: 4.7854538631024e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 9/71 | LOSS: 4.851013682127814e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 10/71 | LOSS: 4.808219108037354e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 11/71 | LOSS: 4.737124640996626e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 12/71 | LOSS: 4.786096549371276e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 13/71 | LOSS: 4.784355204329975e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 14/71 | LOSS: 4.820156512626758e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 15/71 | LOSS: 4.858255010731227e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 16/71 | LOSS: 4.873955410456641e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 17/71 | LOSS: 4.879833997822263e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 18/71 | LOSS: 4.9856241371583706e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 19/71 | LOSS: 5.058117403677898e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 20/71 | LOSS: 5.087821504968729e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 21/71 | LOSS: 5.228821175313152e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 22/71 | LOSS: 5.274402492036841e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 23/71 | LOSS: 5.3583097496812115e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 24/71 | LOSS: 5.365083197830245e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 25/71 | LOSS: 5.433910316172002e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 26/71 | LOSS: 5.402646820584778e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 27/71 | LOSS: 5.409360580545451e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 28/71 | LOSS: 5.399373627557812e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 29/71 | LOSS: 5.429256088973489e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 30/71 | LOSS: 5.41715576952312e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 31/71 | LOSS: 5.406182552292194e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 32/71 | LOSS: 5.4438344870339446e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 33/71 | LOSS: 5.411580091544345e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 34/71 | LOSS: 5.385613465997656e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 35/71 | LOSS: 5.401679724551893e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 36/71 | LOSS: 5.3883628990317105e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 37/71 | LOSS: 5.3846349840712625e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 38/71 | LOSS: 5.349282743666733e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 39/71 | LOSS: 5.326605582922639e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 40/71 | LOSS: 5.303766345871448e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 41/71 | LOSS: 5.284518606242186e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 42/71 | LOSS: 5.283316050038191e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 43/71 | LOSS: 5.283651386310918e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 44/71 | LOSS: 5.2650297725954764e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 45/71 | LOSS: 5.249940356762913e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 46/71 | LOSS: 5.219209876312116e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 47/71 | LOSS: 5.203787253549308e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 48/71 | LOSS: 5.178874846246706e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 49/71 | LOSS: 5.1475004511303265e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 50/71 | LOSS: 5.16514572547178e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 51/71 | LOSS: 5.163408961659745e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 52/71 | LOSS: 5.161306584135295e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 53/71 | LOSS: 5.144063893398938e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 54/71 | LOSS: 5.126781202720436e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 55/71 | LOSS: 5.113839035532562e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 56/71 | LOSS: 5.121639285215152e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 57/71 | LOSS: 5.114442721041369e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 58/71 | LOSS: 5.107513947747312e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 59/71 | LOSS: 5.119413322063337e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 60/71 | LOSS: 5.127442513125449e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 61/71 | LOSS: 5.108681109284648e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 62/71 | LOSS: 5.104554200818437e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 63/71 | LOSS: 5.085272690052989e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 64/71 | LOSS: 5.069624169734127e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 65/71 | LOSS: 5.056912028301282e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 66/71 | LOSS: 5.070235425564669e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 67/71 | LOSS: 5.061851886560308e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 68/71 | LOSS: 5.068004209507321e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 69/71 | LOSS: 5.071809073342592e-06\n",
      "TRAIN: EPOCH 371/1000 | BATCH 70/71 | LOSS: 5.05554608409998e-06\n",
      "VAL: EPOCH 371/1000 | BATCH 0/8 | LOSS: 5.4171432566363364e-06\n",
      "VAL: EPOCH 371/1000 | BATCH 1/8 | LOSS: 5.384994665291742e-06\n",
      "VAL: EPOCH 371/1000 | BATCH 2/8 | LOSS: 5.662550544608773e-06\n",
      "VAL: EPOCH 371/1000 | BATCH 3/8 | LOSS: 5.629379188576422e-06\n",
      "VAL: EPOCH 371/1000 | BATCH 4/8 | LOSS: 5.649435388477287e-06\n",
      "VAL: EPOCH 371/1000 | BATCH 5/8 | LOSS: 5.639133784522225e-06\n",
      "VAL: EPOCH 371/1000 | BATCH 6/8 | LOSS: 5.5816860497413605e-06\n",
      "VAL: EPOCH 371/1000 | BATCH 7/8 | LOSS: 5.663004401412763e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 0/71 | LOSS: 4.299768988857977e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 1/71 | LOSS: 4.623032282324857e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 2/71 | LOSS: 4.714599072030978e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 3/71 | LOSS: 4.598943291966862e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 4/71 | LOSS: 4.6674501390953084e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 5/71 | LOSS: 4.476011857453462e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 6/71 | LOSS: 4.450909403073768e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 7/71 | LOSS: 4.7572989387845155e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 8/71 | LOSS: 4.6649112037913355e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 9/71 | LOSS: 4.63950354969711e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 10/71 | LOSS: 4.643694575696083e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 11/71 | LOSS: 4.72295111345981e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 12/71 | LOSS: 4.684879058913793e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 13/71 | LOSS: 4.607493581586043e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 14/71 | LOSS: 4.625472820407595e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 15/71 | LOSS: 4.5455032733343614e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 16/71 | LOSS: 4.583636262074887e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 17/71 | LOSS: 4.558480630597235e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 18/71 | LOSS: 4.656783487103224e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 19/71 | LOSS: 4.692217555657408e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 20/71 | LOSS: 4.6920662866579644e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 21/71 | LOSS: 4.689707287235995e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 22/71 | LOSS: 4.676446471184255e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 23/71 | LOSS: 4.7507523674994445e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 24/71 | LOSS: 4.727755158455693e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 25/71 | LOSS: 4.793327985470779e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 26/71 | LOSS: 4.784663402730345e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 27/71 | LOSS: 4.757974376648885e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 28/71 | LOSS: 4.7839446481169755e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 29/71 | LOSS: 4.850766504205239e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 30/71 | LOSS: 4.867709813263519e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 31/71 | LOSS: 4.892898097352827e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 32/71 | LOSS: 4.86398056581851e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 33/71 | LOSS: 4.8890507822066226e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 34/71 | LOSS: 4.86377113832402e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 35/71 | LOSS: 4.850003323200427e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 36/71 | LOSS: 4.816992492812239e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 37/71 | LOSS: 4.801670708107658e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 38/71 | LOSS: 4.7849259014280205e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 39/71 | LOSS: 4.765523874539212e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 40/71 | LOSS: 4.745973777661253e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 41/71 | LOSS: 4.7285540230881275e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 42/71 | LOSS: 4.739318863061652e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 43/71 | LOSS: 4.7323380737328655e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 44/71 | LOSS: 4.748918733336419e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 45/71 | LOSS: 4.737440784320706e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 46/71 | LOSS: 4.72943811667179e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 47/71 | LOSS: 4.702381261267874e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 48/71 | LOSS: 4.697056654698158e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 49/71 | LOSS: 4.688682111009257e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 50/71 | LOSS: 4.681253117536578e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 51/71 | LOSS: 4.711387121530536e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 52/71 | LOSS: 4.697408558230272e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 53/71 | LOSS: 4.690682780400729e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 54/71 | LOSS: 4.683539108637805e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 55/71 | LOSS: 4.719552602442231e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 56/71 | LOSS: 4.7313726035849465e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 57/71 | LOSS: 4.721984074923716e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 58/71 | LOSS: 4.736372629541635e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 59/71 | LOSS: 4.741327893498238e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 60/71 | LOSS: 4.732878536384812e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 61/71 | LOSS: 4.7341231067783215e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 62/71 | LOSS: 4.745591497108791e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 63/71 | LOSS: 4.751768827304659e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 64/71 | LOSS: 4.736415509726682e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 65/71 | LOSS: 4.724386306434374e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 66/71 | LOSS: 4.7153478339060345e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 67/71 | LOSS: 4.719237431479141e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 68/71 | LOSS: 4.717721772660456e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 69/71 | LOSS: 4.72414698963673e-06\n",
      "TRAIN: EPOCH 372/1000 | BATCH 70/71 | LOSS: 4.7123056562450584e-06\n",
      "VAL: EPOCH 372/1000 | BATCH 0/8 | LOSS: 5.310328106133966e-06\n",
      "VAL: EPOCH 372/1000 | BATCH 1/8 | LOSS: 4.967202812622418e-06\n",
      "VAL: EPOCH 372/1000 | BATCH 2/8 | LOSS: 4.9550894800631795e-06\n",
      "VAL: EPOCH 372/1000 | BATCH 3/8 | LOSS: 4.978472134098411e-06\n",
      "VAL: EPOCH 372/1000 | BATCH 4/8 | LOSS: 4.8445226639159955e-06\n",
      "VAL: EPOCH 372/1000 | BATCH 5/8 | LOSS: 4.668675728680682e-06\n",
      "VAL: EPOCH 372/1000 | BATCH 6/8 | LOSS: 4.553603308262869e-06\n",
      "VAL: EPOCH 372/1000 | BATCH 7/8 | LOSS: 4.438027445985426e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 0/71 | LOSS: 3.2174311854760163e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 1/71 | LOSS: 3.847279231194989e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 2/71 | LOSS: 4.135324616072467e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 3/71 | LOSS: 4.176078959972074e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 4/71 | LOSS: 4.37427725046291e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 5/71 | LOSS: 4.389376575394029e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 6/71 | LOSS: 4.246082523552884e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 7/71 | LOSS: 4.234092529031841e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 8/71 | LOSS: 4.166402706889332e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 9/71 | LOSS: 4.166435792285483e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 10/71 | LOSS: 4.171613992101894e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 11/71 | LOSS: 4.152322617301252e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 12/71 | LOSS: 4.1709121642970085e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 13/71 | LOSS: 4.166346278517656e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 14/71 | LOSS: 4.214859988375489e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 15/71 | LOSS: 4.18222558096204e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 16/71 | LOSS: 4.123759454694878e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 17/71 | LOSS: 4.128805358050158e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 18/71 | LOSS: 4.176590808033086e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 19/71 | LOSS: 4.190606205156655e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 20/71 | LOSS: 4.191314950557648e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 21/71 | LOSS: 4.213426130015498e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 22/71 | LOSS: 4.276683482559115e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 23/71 | LOSS: 4.242824208707437e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 24/71 | LOSS: 4.2587782536429586e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 25/71 | LOSS: 4.248395489762165e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 26/71 | LOSS: 4.234928896780561e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 27/71 | LOSS: 4.283326437156834e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 28/71 | LOSS: 4.278670068178426e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 29/71 | LOSS: 4.299784806486665e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 30/71 | LOSS: 4.285084837500753e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 31/71 | LOSS: 4.282466612437474e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 32/71 | LOSS: 4.293485861842672e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 33/71 | LOSS: 4.29180389507083e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 34/71 | LOSS: 4.3193076570397746e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 35/71 | LOSS: 4.360399840505933e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 36/71 | LOSS: 4.347937358427255e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 37/71 | LOSS: 4.326559334051701e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 38/71 | LOSS: 4.327276988428471e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 39/71 | LOSS: 4.351778187583477e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 40/71 | LOSS: 4.333195320435973e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 41/71 | LOSS: 4.353173126435736e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 42/71 | LOSS: 4.367967856227932e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 43/71 | LOSS: 4.3731077012539155e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 44/71 | LOSS: 4.3777391157265e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 45/71 | LOSS: 4.365634876906014e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 46/71 | LOSS: 4.349527497207089e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 47/71 | LOSS: 4.3425482800785176e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 48/71 | LOSS: 4.334707594408632e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 49/71 | LOSS: 4.347362687440182e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 50/71 | LOSS: 4.386877471202467e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 51/71 | LOSS: 4.413378926179016e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 52/71 | LOSS: 4.452249126154369e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 53/71 | LOSS: 4.461675808266187e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 54/71 | LOSS: 4.48592220229991e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 55/71 | LOSS: 4.496013433806313e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 56/71 | LOSS: 4.488650730356887e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 57/71 | LOSS: 4.493703645023523e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 58/71 | LOSS: 4.4893217857291235e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 59/71 | LOSS: 4.50645610499123e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 60/71 | LOSS: 4.505102733823091e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 61/71 | LOSS: 4.518680032472936e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 62/71 | LOSS: 4.517362770327184e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 63/71 | LOSS: 4.521606957297308e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 64/71 | LOSS: 4.518452809721706e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 65/71 | LOSS: 4.5231648410856105e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 66/71 | LOSS: 4.510726318870577e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 67/71 | LOSS: 4.497740894080112e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 68/71 | LOSS: 4.514233191055583e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 69/71 | LOSS: 4.5279908200817385e-06\n",
      "TRAIN: EPOCH 373/1000 | BATCH 70/71 | LOSS: 4.540890823194222e-06\n",
      "VAL: EPOCH 373/1000 | BATCH 0/8 | LOSS: 5.056941517977975e-06\n",
      "VAL: EPOCH 373/1000 | BATCH 1/8 | LOSS: 4.857117346546147e-06\n",
      "VAL: EPOCH 373/1000 | BATCH 2/8 | LOSS: 4.852109820300636e-06\n",
      "VAL: EPOCH 373/1000 | BATCH 3/8 | LOSS: 4.825829705623619e-06\n",
      "VAL: EPOCH 373/1000 | BATCH 4/8 | LOSS: 4.77389139632578e-06\n",
      "VAL: EPOCH 373/1000 | BATCH 5/8 | LOSS: 4.547227869503938e-06\n",
      "VAL: EPOCH 373/1000 | BATCH 6/8 | LOSS: 4.417994367551208e-06\n",
      "VAL: EPOCH 373/1000 | BATCH 7/8 | LOSS: 4.342060663020675e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 0/71 | LOSS: 4.173963588982588e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 1/71 | LOSS: 4.659661726691411e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 2/71 | LOSS: 4.660047882983538e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 3/71 | LOSS: 4.716109060609597e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 4/71 | LOSS: 4.701249690697296e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 5/71 | LOSS: 4.448270677433659e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 6/71 | LOSS: 4.452002680669206e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 7/71 | LOSS: 4.497219379118178e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 8/71 | LOSS: 4.446476800189379e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 9/71 | LOSS: 4.443111311047687e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 10/71 | LOSS: 4.310315241375727e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 11/71 | LOSS: 4.211162377032451e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 12/71 | LOSS: 4.233056153260315e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 13/71 | LOSS: 4.213362509679948e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 14/71 | LOSS: 4.144434084688934e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 15/71 | LOSS: 4.0959469060908305e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 16/71 | LOSS: 4.1010246016359066e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 17/71 | LOSS: 4.116574119608332e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 18/71 | LOSS: 4.103556109743965e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 19/71 | LOSS: 4.09123208555684e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 20/71 | LOSS: 4.092090464490909e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 21/71 | LOSS: 4.110762306068367e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 22/71 | LOSS: 4.110964828609924e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 23/71 | LOSS: 4.0949375469760225e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 24/71 | LOSS: 4.110352647330729e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 25/71 | LOSS: 4.113702035423417e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 26/71 | LOSS: 4.094822467474539e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 27/71 | LOSS: 4.155338461129889e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 28/71 | LOSS: 4.188994925525863e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 29/71 | LOSS: 4.182601128377428e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 30/71 | LOSS: 4.202840038429244e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 31/71 | LOSS: 4.177343718936299e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 32/71 | LOSS: 4.263669056440862e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 33/71 | LOSS: 4.289910153227752e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 34/71 | LOSS: 4.334299209014196e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 35/71 | LOSS: 4.3298036176666856e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 36/71 | LOSS: 4.348699195435148e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 37/71 | LOSS: 4.3561512161192554e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 38/71 | LOSS: 4.369956939580077e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 39/71 | LOSS: 4.341417599107444e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 40/71 | LOSS: 4.377736337207862e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 41/71 | LOSS: 4.369316928075152e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 42/71 | LOSS: 4.359963181847233e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 43/71 | LOSS: 4.375856163237173e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 44/71 | LOSS: 4.361223141636907e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 45/71 | LOSS: 4.375123580757645e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 46/71 | LOSS: 4.373978976837726e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 47/71 | LOSS: 4.393273400182807e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 48/71 | LOSS: 4.413198365334526e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 49/71 | LOSS: 4.454811223695288e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 50/71 | LOSS: 4.473587443285128e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 51/71 | LOSS: 4.508492618417502e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 52/71 | LOSS: 4.504753924952381e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 53/71 | LOSS: 4.50502470475503e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 54/71 | LOSS: 4.537310558754887e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 55/71 | LOSS: 4.550965887314565e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 56/71 | LOSS: 4.586000582014349e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 57/71 | LOSS: 4.626244837144723e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 58/71 | LOSS: 4.631117917597294e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 59/71 | LOSS: 4.630496687241248e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 60/71 | LOSS: 4.640745522860403e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 61/71 | LOSS: 4.629680241182101e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 62/71 | LOSS: 4.6423984164744435e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 63/71 | LOSS: 4.6531621791245925e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 64/71 | LOSS: 4.65364666804537e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 65/71 | LOSS: 4.650743579931617e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 66/71 | LOSS: 4.664286429196276e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 67/71 | LOSS: 4.66075749393306e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 68/71 | LOSS: 4.670530363026942e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 69/71 | LOSS: 4.679376499033034e-06\n",
      "TRAIN: EPOCH 374/1000 | BATCH 70/71 | LOSS: 4.6565606732054245e-06\n",
      "VAL: EPOCH 374/1000 | BATCH 0/8 | LOSS: 6.0930647123313975e-06\n",
      "VAL: EPOCH 374/1000 | BATCH 1/8 | LOSS: 5.650256525768782e-06\n",
      "VAL: EPOCH 374/1000 | BATCH 2/8 | LOSS: 5.803682597615989e-06\n",
      "VAL: EPOCH 374/1000 | BATCH 3/8 | LOSS: 5.747443196923996e-06\n",
      "VAL: EPOCH 374/1000 | BATCH 4/8 | LOSS: 5.684586994902929e-06\n",
      "VAL: EPOCH 374/1000 | BATCH 5/8 | LOSS: 5.440198113622803e-06\n",
      "VAL: EPOCH 374/1000 | BATCH 6/8 | LOSS: 5.433256189592482e-06\n",
      "VAL: EPOCH 374/1000 | BATCH 7/8 | LOSS: 5.309716016199673e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 0/71 | LOSS: 4.315338628657628e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 1/71 | LOSS: 4.2662791202019434e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 2/71 | LOSS: 4.341099611337995e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 3/71 | LOSS: 4.400333750709251e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 4/71 | LOSS: 4.682954840973252e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 5/71 | LOSS: 4.724090634529905e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 6/71 | LOSS: 4.739195056962282e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 7/71 | LOSS: 4.809588119769614e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 8/71 | LOSS: 4.784485958629779e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 9/71 | LOSS: 4.83188728139794e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 10/71 | LOSS: 4.814347614063246e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 11/71 | LOSS: 4.8169213944978155e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 12/71 | LOSS: 4.792660932323018e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 13/71 | LOSS: 4.8075540754715414e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 14/71 | LOSS: 4.890176660410361e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 15/71 | LOSS: 4.814132964270357e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 16/71 | LOSS: 4.842917285393068e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 17/71 | LOSS: 4.940725084957699e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 18/71 | LOSS: 4.936668777813075e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 19/71 | LOSS: 4.969252756836795e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 20/71 | LOSS: 4.958960755088594e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 21/71 | LOSS: 4.971590494113281e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 22/71 | LOSS: 4.913514015677957e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 23/71 | LOSS: 4.902887733730192e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 24/71 | LOSS: 4.894181975032552e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 25/71 | LOSS: 4.888102790363729e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 26/71 | LOSS: 4.868472901593863e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 27/71 | LOSS: 4.852331333690277e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 28/71 | LOSS: 4.8580512015930785e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 29/71 | LOSS: 4.843691332704717e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 30/71 | LOSS: 4.894750088257546e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 31/71 | LOSS: 4.891002355122964e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 32/71 | LOSS: 4.887405318434255e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 33/71 | LOSS: 4.908906817036041e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 34/71 | LOSS: 4.9020329957524416e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 35/71 | LOSS: 4.900401279428479e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 36/71 | LOSS: 4.910891706261995e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 37/71 | LOSS: 4.898590011754622e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 38/71 | LOSS: 4.892113973908589e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 39/71 | LOSS: 4.888140944103725e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 40/71 | LOSS: 4.876518049768936e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 41/71 | LOSS: 4.92379056790274e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 42/71 | LOSS: 4.945759455103841e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 43/71 | LOSS: 5.022181565808833e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 44/71 | LOSS: 5.017279277631638e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 45/71 | LOSS: 5.064267720231526e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 46/71 | LOSS: 5.085004504314596e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 47/71 | LOSS: 5.100407354537613e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 48/71 | LOSS: 5.117168057597378e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 49/71 | LOSS: 5.158165899956657e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 50/71 | LOSS: 5.2098321292778205e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 51/71 | LOSS: 5.204717148479596e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 52/71 | LOSS: 5.268222649938939e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 53/71 | LOSS: 5.249973022964022e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 54/71 | LOSS: 5.322053547471031e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 55/71 | LOSS: 5.306192207399363e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 56/71 | LOSS: 5.314239950549611e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 57/71 | LOSS: 5.295256700860748e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 58/71 | LOSS: 5.3172199294117566e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 59/71 | LOSS: 5.289802671389528e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 60/71 | LOSS: 5.342740070743469e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 61/71 | LOSS: 5.3280616575354925e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 62/71 | LOSS: 5.349486476310024e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 63/71 | LOSS: 5.3265044179795495e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 64/71 | LOSS: 5.358199524170442e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 65/71 | LOSS: 5.342836192484714e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 66/71 | LOSS: 5.340097330380518e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 67/71 | LOSS: 5.345873743782945e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 68/71 | LOSS: 5.35217156170077e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 69/71 | LOSS: 5.3244274957770745e-06\n",
      "TRAIN: EPOCH 375/1000 | BATCH 70/71 | LOSS: 5.299319601478383e-06\n",
      "VAL: EPOCH 375/1000 | BATCH 0/8 | LOSS: 6.589913027710281e-06\n",
      "VAL: EPOCH 375/1000 | BATCH 1/8 | LOSS: 7.07518574927235e-06\n",
      "VAL: EPOCH 375/1000 | BATCH 2/8 | LOSS: 6.844144384861768e-06\n",
      "VAL: EPOCH 375/1000 | BATCH 3/8 | LOSS: 7.077656619003392e-06\n",
      "VAL: EPOCH 375/1000 | BATCH 4/8 | LOSS: 6.9484505729633385e-06\n",
      "VAL: EPOCH 375/1000 | BATCH 5/8 | LOSS: 6.6037722111407975e-06\n",
      "VAL: EPOCH 375/1000 | BATCH 6/8 | LOSS: 6.438035597966518e-06\n",
      "VAL: EPOCH 375/1000 | BATCH 7/8 | LOSS: 6.149391026610829e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 0/71 | LOSS: 5.879475793335587e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 1/71 | LOSS: 5.4292954700940754e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 2/71 | LOSS: 5.22614845976932e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 3/71 | LOSS: 5.230401598055323e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 4/71 | LOSS: 5.071339091955451e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 5/71 | LOSS: 5.208249831412104e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 6/71 | LOSS: 5.287374213886713e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 7/71 | LOSS: 5.3724295412393985e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 8/71 | LOSS: 5.339240993230811e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 9/71 | LOSS: 5.301382407196797e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 10/71 | LOSS: 5.386764769355068e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 11/71 | LOSS: 5.366084337765642e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 12/71 | LOSS: 5.503714384338729e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 13/71 | LOSS: 5.546471032045831e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 14/71 | LOSS: 5.744464063657991e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 15/71 | LOSS: 5.638629630766445e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 16/71 | LOSS: 5.929766329619965e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 17/71 | LOSS: 5.84309673791318e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 18/71 | LOSS: 5.869200202524601e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 19/71 | LOSS: 5.822081743644958e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 20/71 | LOSS: 5.861090726068055e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 21/71 | LOSS: 5.842164744701057e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 22/71 | LOSS: 5.810408429565343e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 23/71 | LOSS: 5.798722459834001e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 24/71 | LOSS: 5.7404977087571755e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 25/71 | LOSS: 5.72620541998521e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 26/71 | LOSS: 5.686179385385778e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 27/71 | LOSS: 5.678241336681822e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 28/71 | LOSS: 5.622653573262706e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 29/71 | LOSS: 5.5905484714458e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 30/71 | LOSS: 5.576898245400967e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 31/71 | LOSS: 5.551510540158233e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 32/71 | LOSS: 5.496106082732282e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 33/71 | LOSS: 5.480668893570135e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 34/71 | LOSS: 5.491894132449358e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 35/71 | LOSS: 5.4362121709851235e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 36/71 | LOSS: 5.407132466889625e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 37/71 | LOSS: 5.408405711118815e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 38/71 | LOSS: 5.431135768492076e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 39/71 | LOSS: 5.415355866489336e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 40/71 | LOSS: 5.391616596392762e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 41/71 | LOSS: 5.423071251070291e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 42/71 | LOSS: 5.398592192736836e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 43/71 | LOSS: 5.3848652845028315e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 44/71 | LOSS: 5.3602540750337844e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 45/71 | LOSS: 5.3585813228219985e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 46/71 | LOSS: 5.3223752971423385e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 47/71 | LOSS: 5.317019699191405e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 48/71 | LOSS: 5.336182307380985e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 49/71 | LOSS: 5.306464249770215e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 50/71 | LOSS: 5.306165718923462e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 51/71 | LOSS: 5.314609906620787e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 52/71 | LOSS: 5.3144767104452015e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 53/71 | LOSS: 5.289802558123753e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 54/71 | LOSS: 5.281787207422895e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 55/71 | LOSS: 5.268373363216077e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 56/71 | LOSS: 5.256985352325341e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 57/71 | LOSS: 5.2504971265456145e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 58/71 | LOSS: 5.301070725674775e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 59/71 | LOSS: 5.286238672397303e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 60/71 | LOSS: 5.273920023086486e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 61/71 | LOSS: 5.260866890738168e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 62/71 | LOSS: 5.2549701411494646e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 63/71 | LOSS: 5.229821137930912e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 64/71 | LOSS: 5.2221684654796265e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 65/71 | LOSS: 5.229549532431878e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 66/71 | LOSS: 5.22581888671722e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 67/71 | LOSS: 5.211622568795599e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 68/71 | LOSS: 5.215232266087644e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 69/71 | LOSS: 5.221277456257667e-06\n",
      "TRAIN: EPOCH 376/1000 | BATCH 70/71 | LOSS: 5.218450512625744e-06\n",
      "VAL: EPOCH 376/1000 | BATCH 0/8 | LOSS: 4.698803422797937e-06\n",
      "VAL: EPOCH 376/1000 | BATCH 1/8 | LOSS: 4.8054223498184e-06\n",
      "VAL: EPOCH 376/1000 | BATCH 2/8 | LOSS: 4.9671652959659696e-06\n",
      "VAL: EPOCH 376/1000 | BATCH 3/8 | LOSS: 4.99516204399697e-06\n",
      "VAL: EPOCH 376/1000 | BATCH 4/8 | LOSS: 4.992136928194668e-06\n",
      "VAL: EPOCH 376/1000 | BATCH 5/8 | LOSS: 4.840518689282665e-06\n",
      "VAL: EPOCH 376/1000 | BATCH 6/8 | LOSS: 4.7220084655106935e-06\n",
      "VAL: EPOCH 376/1000 | BATCH 7/8 | LOSS: 4.69402152702969e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 0/71 | LOSS: 4.445219929039013e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 1/71 | LOSS: 4.45742512056313e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 2/71 | LOSS: 4.670132057071896e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 3/71 | LOSS: 4.783292865795374e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 4/71 | LOSS: 4.875096692558145e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 5/71 | LOSS: 4.98147771092287e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 6/71 | LOSS: 4.8365193703960226e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 7/71 | LOSS: 4.808729727301397e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 8/71 | LOSS: 4.758039520109176e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 9/71 | LOSS: 4.704526054410962e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 10/71 | LOSS: 4.609357368033009e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 11/71 | LOSS: 4.587242547889521e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 12/71 | LOSS: 4.623362184037643e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 13/71 | LOSS: 4.667893287140552e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 14/71 | LOSS: 4.691869935413706e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 15/71 | LOSS: 4.706906750584494e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 16/71 | LOSS: 4.647883583054542e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 17/71 | LOSS: 4.712277675227799e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 18/71 | LOSS: 4.74612970959386e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 19/71 | LOSS: 4.765425944697199e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 20/71 | LOSS: 4.723658299727027e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 21/71 | LOSS: 4.820620512708212e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 22/71 | LOSS: 4.792208499012581e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 23/71 | LOSS: 4.79351551992598e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 24/71 | LOSS: 4.7626418472646036e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 25/71 | LOSS: 4.767005354831501e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 26/71 | LOSS: 4.7655540769213715e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 27/71 | LOSS: 4.73727827251683e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 28/71 | LOSS: 4.701603482568191e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 29/71 | LOSS: 4.724479344986321e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 30/71 | LOSS: 4.742423426808184e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 31/71 | LOSS: 4.713138068268563e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 32/71 | LOSS: 4.701184645289851e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 33/71 | LOSS: 4.678908694318927e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 34/71 | LOSS: 4.665339929488255e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 35/71 | LOSS: 4.633529745963945e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 36/71 | LOSS: 4.598915186737007e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 37/71 | LOSS: 4.597822626955779e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 38/71 | LOSS: 4.596910206927509e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 39/71 | LOSS: 4.596918694232955e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 40/71 | LOSS: 4.622950294704912e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 41/71 | LOSS: 4.643122376061608e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 42/71 | LOSS: 4.6421522192970365e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 43/71 | LOSS: 4.62414938469506e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 44/71 | LOSS: 4.6113091179399314e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 45/71 | LOSS: 4.596388205003598e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 46/71 | LOSS: 4.593946651437634e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 47/71 | LOSS: 4.599202663750172e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 48/71 | LOSS: 4.594287813642022e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 49/71 | LOSS: 4.557531951832061e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 50/71 | LOSS: 4.53575586445867e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 51/71 | LOSS: 4.562229395770163e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 52/71 | LOSS: 4.558092442957272e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 53/71 | LOSS: 4.5610117754053875e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 54/71 | LOSS: 4.550967365834036e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 55/71 | LOSS: 4.534137653016452e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 56/71 | LOSS: 4.531903597812184e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 57/71 | LOSS: 4.5385146410714285e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 58/71 | LOSS: 4.532250062827913e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 59/71 | LOSS: 4.520171512467641e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 60/71 | LOSS: 4.516234993702039e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 61/71 | LOSS: 4.50925606107757e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 62/71 | LOSS: 4.520011291080866e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 63/71 | LOSS: 4.523030543879258e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 64/71 | LOSS: 4.523122008602234e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 65/71 | LOSS: 4.527962721912252e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 66/71 | LOSS: 4.535182302218467e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 67/71 | LOSS: 4.526631471435394e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 68/71 | LOSS: 4.530380013987441e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 69/71 | LOSS: 4.542396387868003e-06\n",
      "TRAIN: EPOCH 377/1000 | BATCH 70/71 | LOSS: 4.5686916583805905e-06\n",
      "VAL: EPOCH 377/1000 | BATCH 0/8 | LOSS: 4.934747266815975e-06\n",
      "VAL: EPOCH 377/1000 | BATCH 1/8 | LOSS: 4.652979896491161e-06\n",
      "VAL: EPOCH 377/1000 | BATCH 2/8 | LOSS: 4.710925471348067e-06\n",
      "VAL: EPOCH 377/1000 | BATCH 3/8 | LOSS: 4.758599288834375e-06\n",
      "VAL: EPOCH 377/1000 | BATCH 4/8 | LOSS: 4.665730557462666e-06\n",
      "VAL: EPOCH 377/1000 | BATCH 5/8 | LOSS: 4.539328529062914e-06\n",
      "VAL: EPOCH 377/1000 | BATCH 6/8 | LOSS: 4.447249661357741e-06\n",
      "VAL: EPOCH 377/1000 | BATCH 7/8 | LOSS: 4.35935854170566e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 0/71 | LOSS: 4.02917339670239e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 1/71 | LOSS: 5.696405423805118e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 2/71 | LOSS: 4.981100422204084e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 3/71 | LOSS: 5.042121642873099e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 4/71 | LOSS: 5.070408769825008e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 5/71 | LOSS: 4.887594514002558e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 6/71 | LOSS: 4.7979673191938284e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 7/71 | LOSS: 4.742623559650383e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 8/71 | LOSS: 4.786206015220766e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 9/71 | LOSS: 4.790822231370839e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 10/71 | LOSS: 4.7412349472737825e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 11/71 | LOSS: 4.874614963531106e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 12/71 | LOSS: 4.9164943727821364e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 13/71 | LOSS: 4.907627661460927e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 14/71 | LOSS: 4.96321035825531e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 15/71 | LOSS: 5.0283735504308424e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 16/71 | LOSS: 5.085892526574967e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 17/71 | LOSS: 5.049683876576536e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 18/71 | LOSS: 4.98846649957398e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 19/71 | LOSS: 5.067870779384975e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 20/71 | LOSS: 5.034820744532183e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 21/71 | LOSS: 5.084913730322362e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 22/71 | LOSS: 5.032077909993899e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 23/71 | LOSS: 5.00820156427532e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 24/71 | LOSS: 5.027683891967172e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 25/71 | LOSS: 5.0071587163704234e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 26/71 | LOSS: 5.069297778256306e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 27/71 | LOSS: 5.007741484470378e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 28/71 | LOSS: 5.006154447857688e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 29/71 | LOSS: 5.025598003764268e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 30/71 | LOSS: 5.0190869182364214e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 31/71 | LOSS: 5.02744695296542e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 32/71 | LOSS: 5.017066452196697e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 33/71 | LOSS: 5.056967351639982e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 34/71 | LOSS: 5.053418440833671e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 35/71 | LOSS: 5.061806626195499e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 36/71 | LOSS: 5.022173164016331e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 37/71 | LOSS: 5.0273890972101456e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 38/71 | LOSS: 5.018250010088638e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 39/71 | LOSS: 5.061887685542388e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 40/71 | LOSS: 5.05737933430859e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 41/71 | LOSS: 5.067497476201901e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 42/71 | LOSS: 5.080250984128513e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 43/71 | LOSS: 5.114701898698761e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 44/71 | LOSS: 5.126816565987408e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 45/71 | LOSS: 5.162870271978255e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 46/71 | LOSS: 5.149076354853683e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 47/71 | LOSS: 5.13169206802407e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 48/71 | LOSS: 5.162421070870513e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 49/71 | LOSS: 5.139449235684879e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 50/71 | LOSS: 5.1089887834035945e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 51/71 | LOSS: 5.114176513661033e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 52/71 | LOSS: 5.119646377077618e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 53/71 | LOSS: 5.097896573884269e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 54/71 | LOSS: 5.0958876313829516e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 55/71 | LOSS: 5.116673041324507e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 56/71 | LOSS: 5.102950266071977e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 57/71 | LOSS: 5.087942864081234e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 58/71 | LOSS: 5.087324094131611e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 59/71 | LOSS: 5.080505642733139e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 60/71 | LOSS: 5.081822038965717e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 61/71 | LOSS: 5.085449229667457e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 62/71 | LOSS: 5.064865530267338e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 63/71 | LOSS: 5.070512518301484e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 64/71 | LOSS: 5.070195209974862e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 65/71 | LOSS: 5.073914663129968e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 66/71 | LOSS: 5.074893989778991e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 67/71 | LOSS: 5.074062712270354e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 68/71 | LOSS: 5.073084861104017e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 69/71 | LOSS: 5.081043540095769e-06\n",
      "TRAIN: EPOCH 378/1000 | BATCH 70/71 | LOSS: 5.074227646487793e-06\n",
      "VAL: EPOCH 378/1000 | BATCH 0/8 | LOSS: 7.162092515500262e-06\n",
      "VAL: EPOCH 378/1000 | BATCH 1/8 | LOSS: 7.5726129580289125e-06\n",
      "VAL: EPOCH 378/1000 | BATCH 2/8 | LOSS: 7.853932402213104e-06\n",
      "VAL: EPOCH 378/1000 | BATCH 3/8 | LOSS: 8.108821248242748e-06\n",
      "VAL: EPOCH 378/1000 | BATCH 4/8 | LOSS: 8.06300977274077e-06\n",
      "VAL: EPOCH 378/1000 | BATCH 5/8 | LOSS: 8.19272418084438e-06\n",
      "VAL: EPOCH 378/1000 | BATCH 6/8 | LOSS: 8.133317091310996e-06\n",
      "VAL: EPOCH 378/1000 | BATCH 7/8 | LOSS: 8.122352824102563e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 0/71 | LOSS: 7.737942723906599e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 1/71 | LOSS: 6.023667083354667e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 2/71 | LOSS: 6.863632127836657e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 3/71 | LOSS: 6.2673226466358756e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 4/71 | LOSS: 6.681531613139669e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 5/71 | LOSS: 6.437691733178023e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 6/71 | LOSS: 6.157618437490393e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 7/71 | LOSS: 6.224130345344747e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 8/71 | LOSS: 6.1299954520816554e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 9/71 | LOSS: 6.1590076711581785e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 10/71 | LOSS: 6.058859195367074e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 11/71 | LOSS: 6.026386434617355e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 12/71 | LOSS: 5.90052821700318e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 13/71 | LOSS: 5.9790678018803845e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 14/71 | LOSS: 5.9630094862465436e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 15/71 | LOSS: 5.948464576022161e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 16/71 | LOSS: 5.825615115848932e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 17/71 | LOSS: 5.822541930279436e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 18/71 | LOSS: 5.795469621118268e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 19/71 | LOSS: 5.685953146894462e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 20/71 | LOSS: 5.67302959098015e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 21/71 | LOSS: 5.662810198456017e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 22/71 | LOSS: 5.590999555114768e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 23/71 | LOSS: 5.603267633584134e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 24/71 | LOSS: 5.569438144448213e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 25/71 | LOSS: 5.513758126723517e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 26/71 | LOSS: 5.503013619966589e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 27/71 | LOSS: 5.494423346265519e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 28/71 | LOSS: 5.44560984492599e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 29/71 | LOSS: 5.429221619124291e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 30/71 | LOSS: 5.400507689046208e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 31/71 | LOSS: 5.382251075047861e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 32/71 | LOSS: 5.358152424323083e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 33/71 | LOSS: 5.424063160449698e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 34/71 | LOSS: 5.411101964065373e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 35/71 | LOSS: 5.390351866986344e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 36/71 | LOSS: 5.386598233500892e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 37/71 | LOSS: 5.379193501204626e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 38/71 | LOSS: 5.359210577947893e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 39/71 | LOSS: 5.34937018983328e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 40/71 | LOSS: 5.363180562420805e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 41/71 | LOSS: 5.368196568832943e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 42/71 | LOSS: 5.4196785894482466e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 43/71 | LOSS: 5.38988658924857e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 44/71 | LOSS: 5.400475654621712e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 45/71 | LOSS: 5.401179528828083e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 46/71 | LOSS: 5.407440757482348e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 47/71 | LOSS: 5.375901736215383e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 48/71 | LOSS: 5.373280832920537e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 49/71 | LOSS: 5.3907983146928015e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 50/71 | LOSS: 5.387731383890758e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 51/71 | LOSS: 5.3591619199813486e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 52/71 | LOSS: 5.33944272182504e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 53/71 | LOSS: 5.352615188611607e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 54/71 | LOSS: 5.344788744630271e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 55/71 | LOSS: 5.356355383615405e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 56/71 | LOSS: 5.364986902765068e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 57/71 | LOSS: 5.356752126317588e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 58/71 | LOSS: 5.370421180394537e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 59/71 | LOSS: 5.415862256086257e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 60/71 | LOSS: 5.407796983431059e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 61/71 | LOSS: 5.409665405867629e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 62/71 | LOSS: 5.406960019807604e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 63/71 | LOSS: 5.389250084419928e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 64/71 | LOSS: 5.416909789346391e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 65/71 | LOSS: 5.411409588442908e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 66/71 | LOSS: 5.407962274134729e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 67/71 | LOSS: 5.397966667848831e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 68/71 | LOSS: 5.450401882969174e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 69/71 | LOSS: 5.438250991574023e-06\n",
      "TRAIN: EPOCH 379/1000 | BATCH 70/71 | LOSS: 5.431027698826986e-06\n",
      "VAL: EPOCH 379/1000 | BATCH 0/8 | LOSS: 7.075623216223903e-06\n",
      "VAL: EPOCH 379/1000 | BATCH 1/8 | LOSS: 7.017495590844192e-06\n",
      "VAL: EPOCH 379/1000 | BATCH 2/8 | LOSS: 7.549628511090607e-06\n",
      "VAL: EPOCH 379/1000 | BATCH 3/8 | LOSS: 7.595867600684869e-06\n",
      "VAL: EPOCH 379/1000 | BATCH 4/8 | LOSS: 7.621314580319449e-06\n",
      "VAL: EPOCH 379/1000 | BATCH 5/8 | LOSS: 7.5613159727557404e-06\n",
      "VAL: EPOCH 379/1000 | BATCH 6/8 | LOSS: 7.459616037002499e-06\n",
      "VAL: EPOCH 379/1000 | BATCH 7/8 | LOSS: 7.457845924818685e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 0/71 | LOSS: 8.735645678825676e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 1/71 | LOSS: 7.151970976337907e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 2/71 | LOSS: 7.433697798357268e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 3/71 | LOSS: 6.5833363578349235e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 4/71 | LOSS: 6.845988264103653e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 5/71 | LOSS: 6.414788534433076e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 6/71 | LOSS: 6.356573391842955e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 7/71 | LOSS: 6.190120927840326e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 8/71 | LOSS: 6.151947723588415e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 9/71 | LOSS: 6.049228932170081e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 10/71 | LOSS: 5.957505932614864e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 11/71 | LOSS: 5.836635750711139e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 12/71 | LOSS: 5.774145357953295e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 13/71 | LOSS: 5.730450311602908e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 14/71 | LOSS: 5.606446241775605e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 15/71 | LOSS: 5.607243593885869e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 16/71 | LOSS: 5.652389170470141e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 17/71 | LOSS: 5.560014111930893e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 18/71 | LOSS: 5.497392832790195e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 19/71 | LOSS: 5.429228235698247e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 20/71 | LOSS: 5.388908621604233e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 21/71 | LOSS: 5.3130915747798015e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 22/71 | LOSS: 5.291539438352945e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 23/71 | LOSS: 5.267537697288087e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 24/71 | LOSS: 5.275516859910567e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 25/71 | LOSS: 5.255506747604299e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 26/71 | LOSS: 5.274633866537005e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 27/71 | LOSS: 5.271492268223353e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 28/71 | LOSS: 5.24823896488795e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 29/71 | LOSS: 5.204702188166266e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 30/71 | LOSS: 5.16120665804098e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 31/71 | LOSS: 5.163958597620422e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 32/71 | LOSS: 5.150555483796228e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 33/71 | LOSS: 5.147422899319602e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 34/71 | LOSS: 5.131726714223207e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 35/71 | LOSS: 5.170241940454111e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 36/71 | LOSS: 5.1673587161014835e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 37/71 | LOSS: 5.156328603860809e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 38/71 | LOSS: 5.157893558414518e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 39/71 | LOSS: 5.127473758648193e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 40/71 | LOSS: 5.10961836969328e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 41/71 | LOSS: 5.070942593409286e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 42/71 | LOSS: 5.066967470304727e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 43/71 | LOSS: 5.019253287421386e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 44/71 | LOSS: 5.037535781108697e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 45/71 | LOSS: 5.002870021480308e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 46/71 | LOSS: 4.9971234709138365e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 47/71 | LOSS: 5.0104394517802575e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 48/71 | LOSS: 4.97705411121109e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 49/71 | LOSS: 4.973842178515042e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 50/71 | LOSS: 4.943115750482356e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 51/71 | LOSS: 4.948313866966951e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 52/71 | LOSS: 4.920860981587364e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 53/71 | LOSS: 4.92074846752749e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 54/71 | LOSS: 4.916872430013345e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 55/71 | LOSS: 4.9088007390959576e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 56/71 | LOSS: 4.91044948559424e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 57/71 | LOSS: 4.902027355989236e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 58/71 | LOSS: 4.8961183214703204e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 59/71 | LOSS: 4.9058509754710634e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 60/71 | LOSS: 4.919494869205128e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 61/71 | LOSS: 4.9080730414355075e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 62/71 | LOSS: 4.890108057802464e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 63/71 | LOSS: 4.877734784969334e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 64/71 | LOSS: 4.873011259685602e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 65/71 | LOSS: 4.8736715591461906e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 66/71 | LOSS: 4.889494417891829e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 67/71 | LOSS: 4.895145276109628e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 68/71 | LOSS: 4.888229542315906e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 69/71 | LOSS: 4.889835083434134e-06\n",
      "TRAIN: EPOCH 380/1000 | BATCH 70/71 | LOSS: 4.895928089059504e-06\n",
      "VAL: EPOCH 380/1000 | BATCH 0/8 | LOSS: 6.208010745467618e-06\n",
      "VAL: EPOCH 380/1000 | BATCH 1/8 | LOSS: 6.127275355538586e-06\n",
      "VAL: EPOCH 380/1000 | BATCH 2/8 | LOSS: 6.098183196930525e-06\n",
      "VAL: EPOCH 380/1000 | BATCH 3/8 | LOSS: 6.002772465762973e-06\n",
      "VAL: EPOCH 380/1000 | BATCH 4/8 | LOSS: 6.017559826432262e-06\n",
      "VAL: EPOCH 380/1000 | BATCH 5/8 | LOSS: 5.7374265907128574e-06\n",
      "VAL: EPOCH 380/1000 | BATCH 6/8 | LOSS: 5.718548176706203e-06\n",
      "VAL: EPOCH 380/1000 | BATCH 7/8 | LOSS: 5.607580021660397e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 0/71 | LOSS: 5.630637133435812e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 1/71 | LOSS: 6.33093191027001e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 2/71 | LOSS: 6.900321295688627e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 3/71 | LOSS: 6.591403462152812e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 4/71 | LOSS: 6.759593088645488e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 5/71 | LOSS: 6.862260912991284e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 6/71 | LOSS: 6.619131844282882e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 7/71 | LOSS: 6.581179889053601e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 8/71 | LOSS: 6.540148458750789e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 9/71 | LOSS: 6.469897607530584e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 10/71 | LOSS: 6.386968214593997e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 11/71 | LOSS: 6.651656652441791e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 12/71 | LOSS: 6.708943741474426e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 13/71 | LOSS: 6.705293571940274e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 14/71 | LOSS: 6.595035150288216e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 15/71 | LOSS: 6.52297390502099e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 16/71 | LOSS: 6.447524489939336e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 17/71 | LOSS: 6.3364170980801446e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 18/71 | LOSS: 6.341660896156866e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 19/71 | LOSS: 6.279657418417628e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 20/71 | LOSS: 6.210403149625996e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 21/71 | LOSS: 6.1490643681529695e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 22/71 | LOSS: 6.1025921043081954e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 23/71 | LOSS: 5.997628183725586e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 24/71 | LOSS: 5.892784683965147e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 25/71 | LOSS: 5.890907477106129e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 26/71 | LOSS: 5.902863836969051e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 27/71 | LOSS: 5.854230315500379e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 28/71 | LOSS: 5.824601712761093e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 29/71 | LOSS: 5.811951480912588e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 30/71 | LOSS: 5.813466985625471e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 31/71 | LOSS: 5.758920082143959e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 32/71 | LOSS: 5.754122453667647e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 33/71 | LOSS: 5.7470120629980506e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 34/71 | LOSS: 5.731547896305399e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 35/71 | LOSS: 5.709660513275594e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 36/71 | LOSS: 5.753844328176677e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 37/71 | LOSS: 5.7035956735106946e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 38/71 | LOSS: 5.711970003082966e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 39/71 | LOSS: 5.646284046179062e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 40/71 | LOSS: 5.6624910873169275e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 41/71 | LOSS: 5.6472011432820835e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 42/71 | LOSS: 5.639994173560449e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 43/71 | LOSS: 5.658359348424049e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 44/71 | LOSS: 5.661700066007648e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 45/71 | LOSS: 5.636001303529433e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 46/71 | LOSS: 5.626910846563368e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 47/71 | LOSS: 5.615526816882266e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 48/71 | LOSS: 5.586608597009124e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 49/71 | LOSS: 5.5537101161462484e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 50/71 | LOSS: 5.5386219494667945e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 51/71 | LOSS: 5.5218474611907505e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 52/71 | LOSS: 5.485632308986135e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 53/71 | LOSS: 5.479387844523579e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 54/71 | LOSS: 5.4577315330045005e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 55/71 | LOSS: 5.454907238799933e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 56/71 | LOSS: 5.425766120057679e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 57/71 | LOSS: 5.40767010879163e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 58/71 | LOSS: 5.398572538671048e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 59/71 | LOSS: 5.374325026726486e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 60/71 | LOSS: 5.371754185511772e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 61/71 | LOSS: 5.351931716382575e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 62/71 | LOSS: 5.342035545629823e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 63/71 | LOSS: 5.343317702966033e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 64/71 | LOSS: 5.340440678152775e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 65/71 | LOSS: 5.326491687314543e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 66/71 | LOSS: 5.337196907659283e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 67/71 | LOSS: 5.315024710024077e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 68/71 | LOSS: 5.3050885005665105e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 69/71 | LOSS: 5.314143261005354e-06\n",
      "TRAIN: EPOCH 381/1000 | BATCH 70/71 | LOSS: 5.296582240192383e-06\n",
      "VAL: EPOCH 381/1000 | BATCH 0/8 | LOSS: 6.377865247486625e-06\n",
      "VAL: EPOCH 381/1000 | BATCH 1/8 | LOSS: 5.956657787464792e-06\n",
      "VAL: EPOCH 381/1000 | BATCH 2/8 | LOSS: 5.761505841898422e-06\n",
      "VAL: EPOCH 381/1000 | BATCH 3/8 | LOSS: 5.780250603493187e-06\n",
      "VAL: EPOCH 381/1000 | BATCH 4/8 | LOSS: 5.684801453753607e-06\n",
      "VAL: EPOCH 381/1000 | BATCH 5/8 | LOSS: 5.462624964517697e-06\n",
      "VAL: EPOCH 381/1000 | BATCH 6/8 | LOSS: 5.345881618268322e-06\n",
      "VAL: EPOCH 381/1000 | BATCH 7/8 | LOSS: 5.178921014703519e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 0/71 | LOSS: 3.7772599625895964e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 1/71 | LOSS: 4.318869855524099e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 2/71 | LOSS: 4.368613720847255e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 3/71 | LOSS: 4.545843637515645e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 4/71 | LOSS: 4.5266067445481895e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 5/71 | LOSS: 4.5979448183667655e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 6/71 | LOSS: 4.601875421680493e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 7/71 | LOSS: 4.685030688733605e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 8/71 | LOSS: 4.723940214211729e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 9/71 | LOSS: 4.7945212600097875e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 10/71 | LOSS: 4.782717062632648e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 11/71 | LOSS: 4.783890877509596e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 12/71 | LOSS: 4.809899862196359e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 13/71 | LOSS: 4.839092736931759e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 14/71 | LOSS: 4.825614405490342e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 15/71 | LOSS: 4.992255000502155e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 16/71 | LOSS: 4.980378180869807e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 17/71 | LOSS: 5.0553818482512725e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 18/71 | LOSS: 5.007576598808028e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 19/71 | LOSS: 5.036818299686274e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 20/71 | LOSS: 5.1133254146407126e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 21/71 | LOSS: 5.1083923257134636e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 22/71 | LOSS: 5.216867016908667e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 23/71 | LOSS: 5.23098787160355e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 24/71 | LOSS: 5.300918810462463e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 25/71 | LOSS: 5.249322568418463e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 26/71 | LOSS: 5.30920483807104e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 27/71 | LOSS: 5.311754559248324e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 28/71 | LOSS: 5.308134679329794e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 29/71 | LOSS: 5.306608113642142e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 30/71 | LOSS: 5.3058445392971705e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 31/71 | LOSS: 5.279949355951885e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 32/71 | LOSS: 5.275630945272331e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 33/71 | LOSS: 5.264450351205596e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 34/71 | LOSS: 5.334665619167416e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 35/71 | LOSS: 5.340890734512666e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 36/71 | LOSS: 5.3773882292581536e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 37/71 | LOSS: 5.345160945436969e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 38/71 | LOSS: 5.393967425194345e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 39/71 | LOSS: 5.366423516761643e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 40/71 | LOSS: 5.39843613185484e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 41/71 | LOSS: 5.3631271373974635e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 42/71 | LOSS: 5.38724969395167e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 43/71 | LOSS: 5.375888272076488e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 44/71 | LOSS: 5.351010506476288e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 45/71 | LOSS: 5.337832792816777e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 46/71 | LOSS: 5.3266143745434985e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 47/71 | LOSS: 5.297752537103406e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 48/71 | LOSS: 5.260891027449231e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 49/71 | LOSS: 5.255385285636294e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 50/71 | LOSS: 5.257813685888073e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 51/71 | LOSS: 5.290220603735808e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 52/71 | LOSS: 5.285576061962858e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 53/71 | LOSS: 5.2846291796245e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 54/71 | LOSS: 5.275092677369354e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 55/71 | LOSS: 5.2550741023489015e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 56/71 | LOSS: 5.272724217974699e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 57/71 | LOSS: 5.269095317058161e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 58/71 | LOSS: 5.267537205126847e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 59/71 | LOSS: 5.268953160945481e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 60/71 | LOSS: 5.306291045524661e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 61/71 | LOSS: 5.288947440637839e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 62/71 | LOSS: 5.298128288568622e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 63/71 | LOSS: 5.284445229847279e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 64/71 | LOSS: 5.273617800692311e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 65/71 | LOSS: 5.264904282078285e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 66/71 | LOSS: 5.2851156098442324e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 67/71 | LOSS: 5.26566893366929e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 68/71 | LOSS: 5.270277946745383e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 69/71 | LOSS: 5.275094008538872e-06\n",
      "TRAIN: EPOCH 382/1000 | BATCH 70/71 | LOSS: 5.287911713360661e-06\n",
      "VAL: EPOCH 382/1000 | BATCH 0/8 | LOSS: 5.710435289074667e-06\n",
      "VAL: EPOCH 382/1000 | BATCH 1/8 | LOSS: 5.396628921516822e-06\n",
      "VAL: EPOCH 382/1000 | BATCH 2/8 | LOSS: 5.259893896436552e-06\n",
      "VAL: EPOCH 382/1000 | BATCH 3/8 | LOSS: 5.303441753312654e-06\n",
      "VAL: EPOCH 382/1000 | BATCH 4/8 | LOSS: 5.15905448992271e-06\n",
      "VAL: EPOCH 382/1000 | BATCH 5/8 | LOSS: 4.9072251613324624e-06\n",
      "VAL: EPOCH 382/1000 | BATCH 6/8 | LOSS: 4.736194146711828e-06\n",
      "VAL: EPOCH 382/1000 | BATCH 7/8 | LOSS: 4.6092904995020945e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 0/71 | LOSS: 4.1301418605144136e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 1/71 | LOSS: 3.980256451541209e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 2/71 | LOSS: 4.594899716418392e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 3/71 | LOSS: 4.546401214611251e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 4/71 | LOSS: 4.5717132707068234e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 5/71 | LOSS: 4.688706970531105e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 6/71 | LOSS: 4.668391998815683e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 7/71 | LOSS: 4.597461270350323e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 8/71 | LOSS: 4.737463920416e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 9/71 | LOSS: 4.651833933166927e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 10/71 | LOSS: 4.800576582717688e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 11/71 | LOSS: 4.732252023131878e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 12/71 | LOSS: 4.8047494461710676e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 13/71 | LOSS: 4.886204741134341e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 14/71 | LOSS: 4.959866843516162e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 15/71 | LOSS: 5.096359757317259e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 16/71 | LOSS: 5.180478450818234e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 17/71 | LOSS: 5.28160881508989e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 18/71 | LOSS: 5.246191018446361e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 19/71 | LOSS: 5.248673301139206e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 20/71 | LOSS: 5.269566199352821e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 21/71 | LOSS: 5.1883759045846425e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 22/71 | LOSS: 5.202795047965913e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 23/71 | LOSS: 5.179665511908145e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 24/71 | LOSS: 5.123393129906617e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 25/71 | LOSS: 5.120774211228798e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 26/71 | LOSS: 5.106309648302461e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 27/71 | LOSS: 5.071296478880478e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 28/71 | LOSS: 5.063083680679797e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 29/71 | LOSS: 5.061505241125511e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 30/71 | LOSS: 5.058666462695504e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 31/71 | LOSS: 5.062618313900202e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 32/71 | LOSS: 5.034985337481361e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 33/71 | LOSS: 5.008129806703758e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 34/71 | LOSS: 4.986734490687793e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 35/71 | LOSS: 4.977294755311353e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 36/71 | LOSS: 4.967415898627715e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 37/71 | LOSS: 4.956777765504452e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 38/71 | LOSS: 4.94507271352445e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 39/71 | LOSS: 4.963145192959928e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 40/71 | LOSS: 4.930900913899589e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 41/71 | LOSS: 4.907778453517017e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 42/71 | LOSS: 4.911607077722925e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 43/71 | LOSS: 4.8971453865802195e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 44/71 | LOSS: 4.9063524582177504e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 45/71 | LOSS: 4.898287453097114e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 46/71 | LOSS: 4.923304352795388e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 47/71 | LOSS: 4.9320530971878424e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 48/71 | LOSS: 4.910148442033869e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 49/71 | LOSS: 4.904315210296772e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 50/71 | LOSS: 4.891091904545308e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 51/71 | LOSS: 4.9025828555106554e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 52/71 | LOSS: 4.913381065496239e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 53/71 | LOSS: 4.934621882865839e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 54/71 | LOSS: 4.938511508292074e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 55/71 | LOSS: 4.965391545803348e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 56/71 | LOSS: 4.981330811572486e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 57/71 | LOSS: 4.986826187914944e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 58/71 | LOSS: 5.040428732172586e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 59/71 | LOSS: 5.075945667461686e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 60/71 | LOSS: 5.058372816627704e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 61/71 | LOSS: 5.054603713189511e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 62/71 | LOSS: 5.075754098282824e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 63/71 | LOSS: 5.097032250489519e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 64/71 | LOSS: 5.10041558859834e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 65/71 | LOSS: 5.127840233509425e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 66/71 | LOSS: 5.123425030908391e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 67/71 | LOSS: 5.107487120713974e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 68/71 | LOSS: 5.107124239209266e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 69/71 | LOSS: 5.107722368588189e-06\n",
      "TRAIN: EPOCH 383/1000 | BATCH 70/71 | LOSS: 5.096360312141051e-06\n",
      "VAL: EPOCH 383/1000 | BATCH 0/8 | LOSS: 5.317849172570277e-06\n",
      "VAL: EPOCH 383/1000 | BATCH 1/8 | LOSS: 5.353402457330958e-06\n",
      "VAL: EPOCH 383/1000 | BATCH 2/8 | LOSS: 5.346724265109515e-06\n",
      "VAL: EPOCH 383/1000 | BATCH 3/8 | LOSS: 5.3680321343563264e-06\n",
      "VAL: EPOCH 383/1000 | BATCH 4/8 | LOSS: 5.333217723091366e-06\n",
      "VAL: EPOCH 383/1000 | BATCH 5/8 | LOSS: 5.213443046159227e-06\n",
      "VAL: EPOCH 383/1000 | BATCH 6/8 | LOSS: 5.064017061938232e-06\n",
      "VAL: EPOCH 383/1000 | BATCH 7/8 | LOSS: 5.014179293993948e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 0/71 | LOSS: 4.475231889955467e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 1/71 | LOSS: 4.528458475760999e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 2/71 | LOSS: 5.307836393816008e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 3/71 | LOSS: 4.932074944008491e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 4/71 | LOSS: 5.037766459281556e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 5/71 | LOSS: 5.213104259382817e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 6/71 | LOSS: 5.021082545551638e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 7/71 | LOSS: 4.921988477235573e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 8/71 | LOSS: 5.052556742965761e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 9/71 | LOSS: 4.9888328248925974e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 10/71 | LOSS: 4.994394781864354e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 11/71 | LOSS: 4.874454949307013e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 12/71 | LOSS: 4.894949160398727e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 13/71 | LOSS: 4.836287920235398e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 14/71 | LOSS: 4.826174987708024e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 15/71 | LOSS: 4.779134414434338e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 16/71 | LOSS: 4.750843519265318e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 17/71 | LOSS: 4.736109771228156e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 18/71 | LOSS: 4.734817821372417e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 19/71 | LOSS: 4.742646831346065e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 20/71 | LOSS: 4.760183303785327e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 21/71 | LOSS: 4.764647562958337e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 22/71 | LOSS: 4.757945876206024e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 23/71 | LOSS: 4.780048991885148e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 24/71 | LOSS: 4.7519819327135334e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 25/71 | LOSS: 4.71303167159931e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 26/71 | LOSS: 4.688536667648198e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 27/71 | LOSS: 4.6398768306841835e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 28/71 | LOSS: 4.639250611380076e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 29/71 | LOSS: 4.71632477001549e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 30/71 | LOSS: 4.729339964122177e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 31/71 | LOSS: 4.7290791869158966e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 32/71 | LOSS: 4.765326256154257e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 33/71 | LOSS: 4.78494922952129e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 34/71 | LOSS: 4.810668860955047e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 35/71 | LOSS: 4.797966988058357e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 36/71 | LOSS: 4.790778870596409e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 37/71 | LOSS: 4.79953318613537e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 38/71 | LOSS: 4.815757241279546e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 39/71 | LOSS: 4.763396202633885e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 40/71 | LOSS: 4.773074479311529e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 41/71 | LOSS: 4.755078840562852e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 42/71 | LOSS: 4.782616669704006e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 43/71 | LOSS: 4.7966256963925184e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 44/71 | LOSS: 4.7909671088240835e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 45/71 | LOSS: 4.8450440614112225e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 46/71 | LOSS: 4.841886319364346e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 47/71 | LOSS: 4.854698592756297e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 48/71 | LOSS: 4.880396884299425e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 49/71 | LOSS: 4.877509718426154e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 50/71 | LOSS: 4.908076727017532e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 51/71 | LOSS: 4.9103107008145e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 52/71 | LOSS: 4.920516780743437e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 53/71 | LOSS: 4.950943497937664e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 54/71 | LOSS: 4.951077665861125e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 55/71 | LOSS: 4.980500039307247e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 56/71 | LOSS: 4.970529955659131e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 57/71 | LOSS: 4.977966439363282e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 58/71 | LOSS: 5.006888771939715e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 59/71 | LOSS: 5.0252465446950134e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 60/71 | LOSS: 5.0066453501936165e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 61/71 | LOSS: 4.989976017191845e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 62/71 | LOSS: 5.007118669175451e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 63/71 | LOSS: 4.995626859738422e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 64/71 | LOSS: 4.998873303590629e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 65/71 | LOSS: 5.0100565286728935e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 66/71 | LOSS: 5.042642245445758e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 67/71 | LOSS: 5.042916353993539e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 68/71 | LOSS: 5.044770290808536e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 69/71 | LOSS: 5.121636773505348e-06\n",
      "TRAIN: EPOCH 384/1000 | BATCH 70/71 | LOSS: 5.113384035417177e-06\n",
      "VAL: EPOCH 384/1000 | BATCH 0/8 | LOSS: 1.1593122508202214e-05\n",
      "VAL: EPOCH 384/1000 | BATCH 1/8 | LOSS: 1.1420885130064562e-05\n",
      "VAL: EPOCH 384/1000 | BATCH 2/8 | LOSS: 1.105896444641985e-05\n",
      "VAL: EPOCH 384/1000 | BATCH 3/8 | LOSS: 1.1007057764800265e-05\n",
      "VAL: EPOCH 384/1000 | BATCH 4/8 | LOSS: 1.0915399980149231e-05\n",
      "VAL: EPOCH 384/1000 | BATCH 5/8 | LOSS: 1.0344806241846527e-05\n",
      "VAL: EPOCH 384/1000 | BATCH 6/8 | LOSS: 1.010283572863305e-05\n",
      "VAL: EPOCH 384/1000 | BATCH 7/8 | LOSS: 9.812811583742587e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 0/71 | LOSS: 1.0965397450490855e-05\n",
      "TRAIN: EPOCH 385/1000 | BATCH 1/71 | LOSS: 8.343303761648713e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 2/71 | LOSS: 9.028822508601783e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 3/71 | LOSS: 8.191434062609915e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 4/71 | LOSS: 8.62653942022007e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 5/71 | LOSS: 7.877938818031302e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 6/71 | LOSS: 7.249968738116357e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 7/71 | LOSS: 7.332683310323773e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 8/71 | LOSS: 7.805717106950598e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 9/71 | LOSS: 7.514033268307685e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 10/71 | LOSS: 7.395008486293426e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 11/71 | LOSS: 7.438967107494439e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 12/71 | LOSS: 7.268542503879871e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 13/71 | LOSS: 7.0844476535317625e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 14/71 | LOSS: 6.965636809278901e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 15/71 | LOSS: 6.921154522387951e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 16/71 | LOSS: 6.754093031785862e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 17/71 | LOSS: 6.648564218873314e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 18/71 | LOSS: 6.556705437488793e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 19/71 | LOSS: 6.435016598516086e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 20/71 | LOSS: 6.3928711088443555e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 21/71 | LOSS: 6.291372962798712e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 22/71 | LOSS: 6.2765072035962595e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 23/71 | LOSS: 6.205196389904207e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 24/71 | LOSS: 6.225565157365054e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 25/71 | LOSS: 6.247754474595869e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 26/71 | LOSS: 6.1766101377240075e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 27/71 | LOSS: 6.193866559962251e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 28/71 | LOSS: 6.162025611360902e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 29/71 | LOSS: 6.113701177431115e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 30/71 | LOSS: 6.089799567674955e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 31/71 | LOSS: 6.090541845082953e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 32/71 | LOSS: 6.097771333632849e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 33/71 | LOSS: 6.068918082746677e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 34/71 | LOSS: 6.020571293317646e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 35/71 | LOSS: 6.0482877365252025e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 36/71 | LOSS: 5.9869573330640405e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 37/71 | LOSS: 5.9980587521797706e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 38/71 | LOSS: 5.981538309312712e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 39/71 | LOSS: 5.981869514926075e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 40/71 | LOSS: 5.949718708906425e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 41/71 | LOSS: 5.952090283417852e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 42/71 | LOSS: 5.961755367786472e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 43/71 | LOSS: 5.928807778135789e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 44/71 | LOSS: 5.901119200441624e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 45/71 | LOSS: 5.909962055739015e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 46/71 | LOSS: 5.8912507987035035e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 47/71 | LOSS: 5.850853019258769e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 48/71 | LOSS: 5.823544142913306e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 49/71 | LOSS: 5.7941608793043995e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 50/71 | LOSS: 5.765711702064009e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 51/71 | LOSS: 5.750104719481897e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 52/71 | LOSS: 5.712907171262074e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 53/71 | LOSS: 5.674214321516047e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 54/71 | LOSS: 5.624979034605944e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 55/71 | LOSS: 5.589291561136633e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 56/71 | LOSS: 5.575049176274517e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 57/71 | LOSS: 5.55866957036979e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 58/71 | LOSS: 5.548075424192544e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 59/71 | LOSS: 5.524804282686091e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 60/71 | LOSS: 5.522496006752127e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 61/71 | LOSS: 5.496761295651067e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 62/71 | LOSS: 5.478846329453287e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 63/71 | LOSS: 5.491358784581735e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 64/71 | LOSS: 5.480562958837254e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 65/71 | LOSS: 5.467386961644286e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 66/71 | LOSS: 5.4359181661705255e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 67/71 | LOSS: 5.4068759308232555e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 68/71 | LOSS: 5.388505304954774e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 69/71 | LOSS: 5.37745553888921e-06\n",
      "TRAIN: EPOCH 385/1000 | BATCH 70/71 | LOSS: 5.365844469746611e-06\n",
      "VAL: EPOCH 385/1000 | BATCH 0/8 | LOSS: 4.994024493498728e-06\n",
      "VAL: EPOCH 385/1000 | BATCH 1/8 | LOSS: 4.711881729235756e-06\n",
      "VAL: EPOCH 385/1000 | BATCH 2/8 | LOSS: 4.614065953016204e-06\n",
      "VAL: EPOCH 385/1000 | BATCH 3/8 | LOSS: 4.615909347194247e-06\n",
      "VAL: EPOCH 385/1000 | BATCH 4/8 | LOSS: 4.488209560804534e-06\n",
      "VAL: EPOCH 385/1000 | BATCH 5/8 | LOSS: 4.31083306769627e-06\n",
      "VAL: EPOCH 385/1000 | BATCH 6/8 | LOSS: 4.199874410915072e-06\n",
      "VAL: EPOCH 385/1000 | BATCH 7/8 | LOSS: 4.088964999482414e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 0/71 | LOSS: 4.8971523938234895e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 1/71 | LOSS: 4.776833520736545e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 2/71 | LOSS: 4.68769894723664e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 3/71 | LOSS: 4.617249146576796e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 4/71 | LOSS: 4.436962353793206e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 5/71 | LOSS: 4.490392787677895e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 6/71 | LOSS: 4.361176576951818e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 7/71 | LOSS: 4.313366105179739e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 8/71 | LOSS: 4.372438171331952e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 9/71 | LOSS: 4.446004322744557e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 10/71 | LOSS: 4.536236991208915e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 11/71 | LOSS: 4.57443445611716e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 12/71 | LOSS: 4.621769256538335e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 13/71 | LOSS: 4.691707837472288e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 14/71 | LOSS: 4.656331566366134e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 15/71 | LOSS: 4.739121578722916e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 16/71 | LOSS: 4.693685829860281e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 17/71 | LOSS: 4.675111515300361e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 18/71 | LOSS: 4.712181874454377e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 19/71 | LOSS: 4.705427431872522e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 20/71 | LOSS: 4.727234941778338e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 21/71 | LOSS: 4.708120224511896e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 22/71 | LOSS: 4.650088523861917e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 23/71 | LOSS: 4.6730615110845974e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 24/71 | LOSS: 4.6369453139050166e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 25/71 | LOSS: 4.613714465379487e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 26/71 | LOSS: 4.590585334611845e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 27/71 | LOSS: 4.540118019186983e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 28/71 | LOSS: 4.498267336547824e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 29/71 | LOSS: 4.483588251484131e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 30/71 | LOSS: 4.469018984954962e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 31/71 | LOSS: 4.4744142684294275e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 32/71 | LOSS: 4.4805739586687805e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 33/71 | LOSS: 4.502289718456107e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 34/71 | LOSS: 4.477008685301241e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 35/71 | LOSS: 4.490787123106404e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 36/71 | LOSS: 4.472367651475439e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 37/71 | LOSS: 4.442165283379788e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 38/71 | LOSS: 4.445463382114375e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 39/71 | LOSS: 4.462092203993962e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 40/71 | LOSS: 4.47238246523761e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 41/71 | LOSS: 4.492660807608488e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 42/71 | LOSS: 4.547325758277682e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 43/71 | LOSS: 4.541846413468937e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 44/71 | LOSS: 4.541776807956113e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 45/71 | LOSS: 4.547527020764866e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 46/71 | LOSS: 4.5396551537665565e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 47/71 | LOSS: 4.5645429054275155e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 48/71 | LOSS: 4.55461584500213e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 49/71 | LOSS: 4.588733281707391e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 50/71 | LOSS: 4.614311293670314e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 51/71 | LOSS: 4.595177144367951e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 52/71 | LOSS: 4.631704850564931e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 53/71 | LOSS: 4.626723576048217e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 54/71 | LOSS: 4.658398740337527e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 55/71 | LOSS: 4.6483057108811665e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 56/71 | LOSS: 4.676850071598637e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 57/71 | LOSS: 4.671748950480331e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 58/71 | LOSS: 4.674433604754918e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 59/71 | LOSS: 4.664449132481726e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 60/71 | LOSS: 4.672634844848558e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 61/71 | LOSS: 4.65886316833935e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 62/71 | LOSS: 4.670902680594563e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 63/71 | LOSS: 4.671758190966102e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 64/71 | LOSS: 4.672590218363509e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 65/71 | LOSS: 4.671061285052653e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 66/71 | LOSS: 4.668692808855436e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 67/71 | LOSS: 4.704505386143865e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 68/71 | LOSS: 4.692928601086707e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 69/71 | LOSS: 4.6819386625429615e-06\n",
      "TRAIN: EPOCH 386/1000 | BATCH 70/71 | LOSS: 4.6567842007430625e-06\n",
      "VAL: EPOCH 386/1000 | BATCH 0/8 | LOSS: 5.1540655476856045e-06\n",
      "VAL: EPOCH 386/1000 | BATCH 1/8 | LOSS: 5.143303951626876e-06\n",
      "VAL: EPOCH 386/1000 | BATCH 2/8 | LOSS: 5.1771779302119585e-06\n",
      "VAL: EPOCH 386/1000 | BATCH 3/8 | LOSS: 5.367624680729932e-06\n",
      "VAL: EPOCH 386/1000 | BATCH 4/8 | LOSS: 5.21927950103418e-06\n",
      "VAL: EPOCH 386/1000 | BATCH 5/8 | LOSS: 5.031287855672417e-06\n",
      "VAL: EPOCH 386/1000 | BATCH 6/8 | LOSS: 4.858749759608015e-06\n",
      "VAL: EPOCH 386/1000 | BATCH 7/8 | LOSS: 4.649571081927206e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 0/71 | LOSS: 4.948863534082193e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 1/71 | LOSS: 4.726578936242731e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 2/71 | LOSS: 4.536673259281088e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 3/71 | LOSS: 4.863472099714272e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 4/71 | LOSS: 4.720455490314635e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 5/71 | LOSS: 4.76908462587744e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 6/71 | LOSS: 4.639895970675363e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 7/71 | LOSS: 4.58557019555883e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 8/71 | LOSS: 4.476936257762847e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 9/71 | LOSS: 4.43273377186415e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 10/71 | LOSS: 4.392072221228525e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 11/71 | LOSS: 4.390683310854608e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 12/71 | LOSS: 4.403602378470868e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 13/71 | LOSS: 4.413064532725132e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 14/71 | LOSS: 4.439743482483512e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 15/71 | LOSS: 4.500073558233453e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 16/71 | LOSS: 4.490336550588836e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 17/71 | LOSS: 4.5056497507782e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 18/71 | LOSS: 4.4930978483056316e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 19/71 | LOSS: 4.480161840092478e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 20/71 | LOSS: 4.467796283539169e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 21/71 | LOSS: 4.512454707268758e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 22/71 | LOSS: 4.516355928622627e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 23/71 | LOSS: 4.4900294350706344e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 24/71 | LOSS: 4.504518174144323e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 25/71 | LOSS: 4.507993402022783e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 26/71 | LOSS: 4.535714660286666e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 27/71 | LOSS: 4.5281994403012504e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 28/71 | LOSS: 4.55583804539166e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 29/71 | LOSS: 4.5386093840231e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 30/71 | LOSS: 4.568587245000433e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 31/71 | LOSS: 4.547125108445016e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 32/71 | LOSS: 4.55888425676676e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 33/71 | LOSS: 4.573825148166661e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 34/71 | LOSS: 4.5843693637185165e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 35/71 | LOSS: 4.541928679676251e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 36/71 | LOSS: 4.555766287700641e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 37/71 | LOSS: 4.554128574789884e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 38/71 | LOSS: 4.54780011657418e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 39/71 | LOSS: 4.555016045060256e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 40/71 | LOSS: 4.567740760893041e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 41/71 | LOSS: 4.560814025873177e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 42/71 | LOSS: 4.549140406430355e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 43/71 | LOSS: 4.525802446344394e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 44/71 | LOSS: 4.5434561747646914e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 45/71 | LOSS: 4.551589590253412e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 46/71 | LOSS: 4.555011444492575e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 47/71 | LOSS: 4.559001197890211e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 48/71 | LOSS: 4.549138603412145e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 49/71 | LOSS: 4.5603072658195744e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 50/71 | LOSS: 4.5535418759719306e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 51/71 | LOSS: 4.5373484235474625e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 52/71 | LOSS: 4.5363056217883124e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 53/71 | LOSS: 4.55450003334624e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 54/71 | LOSS: 4.569344901028671e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 55/71 | LOSS: 4.581971808192975e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 56/71 | LOSS: 4.581145806999605e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 57/71 | LOSS: 4.5833710569711225e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 58/71 | LOSS: 4.567708960536747e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 59/71 | LOSS: 4.5470466413159254e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 60/71 | LOSS: 4.548124218776824e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 61/71 | LOSS: 4.5288355896709915e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 62/71 | LOSS: 4.554115176933775e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 63/71 | LOSS: 4.541301912297513e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 64/71 | LOSS: 4.548650164137004e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 65/71 | LOSS: 4.540406135592176e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 66/71 | LOSS: 4.548157265623979e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 67/71 | LOSS: 4.562969673529819e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 68/71 | LOSS: 4.557600738959424e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 69/71 | LOSS: 4.559296907505736e-06\n",
      "TRAIN: EPOCH 387/1000 | BATCH 70/71 | LOSS: 4.5613555876500444e-06\n",
      "VAL: EPOCH 387/1000 | BATCH 0/8 | LOSS: 4.65534958493663e-06\n",
      "VAL: EPOCH 387/1000 | BATCH 1/8 | LOSS: 4.736327809951035e-06\n",
      "VAL: EPOCH 387/1000 | BATCH 2/8 | LOSS: 5.019230229663663e-06\n",
      "VAL: EPOCH 387/1000 | BATCH 3/8 | LOSS: 5.041489430368529e-06\n",
      "VAL: EPOCH 387/1000 | BATCH 4/8 | LOSS: 5.043093733547721e-06\n",
      "VAL: EPOCH 387/1000 | BATCH 5/8 | LOSS: 5.005253494042942e-06\n",
      "VAL: EPOCH 387/1000 | BATCH 6/8 | LOSS: 4.929929478488962e-06\n",
      "VAL: EPOCH 387/1000 | BATCH 7/8 | LOSS: 4.929321221425198e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 0/71 | LOSS: 5.596682967734523e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 1/71 | LOSS: 5.302589670463931e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 2/71 | LOSS: 5.309994776325766e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 3/71 | LOSS: 4.942842792843294e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 4/71 | LOSS: 4.911944324703655e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 5/71 | LOSS: 4.787842802519056e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 6/71 | LOSS: 4.632677311227391e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 7/71 | LOSS: 4.595019447606319e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 8/71 | LOSS: 4.503746241526743e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 9/71 | LOSS: 4.509713357947476e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 10/71 | LOSS: 4.500826497116412e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 11/71 | LOSS: 4.503038534645991e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 12/71 | LOSS: 4.569752788414077e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 13/71 | LOSS: 4.708973116456556e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 14/71 | LOSS: 4.63735639944692e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 15/71 | LOSS: 4.6222344423085815e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 16/71 | LOSS: 4.6774758389159725e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 17/71 | LOSS: 4.634210111160226e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 18/71 | LOSS: 4.567543651558624e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 19/71 | LOSS: 4.655528562125255e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 20/71 | LOSS: 4.681252286380706e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 21/71 | LOSS: 4.687444547099321e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 22/71 | LOSS: 4.749552288004729e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 23/71 | LOSS: 4.8589836580958945e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 24/71 | LOSS: 4.814912581423414e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 25/71 | LOSS: 4.856181572694578e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 26/71 | LOSS: 4.856100910425691e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 27/71 | LOSS: 4.847605144472514e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 28/71 | LOSS: 4.815554732352933e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 29/71 | LOSS: 4.840179773661172e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 30/71 | LOSS: 4.825637923263017e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 31/71 | LOSS: 4.818320526567277e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 32/71 | LOSS: 4.829727451964793e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 33/71 | LOSS: 4.806699768564345e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 34/71 | LOSS: 4.7725071973608074e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 35/71 | LOSS: 4.774507652251083e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 36/71 | LOSS: 4.7735122058617665e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 37/71 | LOSS: 4.778465285868917e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 38/71 | LOSS: 4.745999073444788e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 39/71 | LOSS: 4.710325436008134e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 40/71 | LOSS: 4.7057206734834615e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 41/71 | LOSS: 4.7101321754217635e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 42/71 | LOSS: 4.743364454127934e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 43/71 | LOSS: 4.721725397096386e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 44/71 | LOSS: 4.75151638157614e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 45/71 | LOSS: 4.755072792328934e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 46/71 | LOSS: 4.759596041812403e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 47/71 | LOSS: 4.767209920449507e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 48/71 | LOSS: 4.740364722832111e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 49/71 | LOSS: 4.7500043092441044e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 50/71 | LOSS: 4.745994115807002e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 51/71 | LOSS: 4.738659728130957e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 52/71 | LOSS: 4.740407065291261e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 53/71 | LOSS: 4.7282999107496665e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 54/71 | LOSS: 4.721045793716753e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 55/71 | LOSS: 4.7281231689209695e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 56/71 | LOSS: 4.72359448522503e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 57/71 | LOSS: 4.72711230000202e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 58/71 | LOSS: 4.708652545174365e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 59/71 | LOSS: 4.697426209077093e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 60/71 | LOSS: 4.703553381883377e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 61/71 | LOSS: 4.692737573555684e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 62/71 | LOSS: 4.688597704848184e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 63/71 | LOSS: 4.693984624992709e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 64/71 | LOSS: 4.685037167571584e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 65/71 | LOSS: 4.693876741674268e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 66/71 | LOSS: 4.7102204557156814e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 67/71 | LOSS: 4.699277433380357e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 68/71 | LOSS: 4.692319018853385e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 69/71 | LOSS: 4.700454789079751e-06\n",
      "TRAIN: EPOCH 388/1000 | BATCH 70/71 | LOSS: 4.697022212651422e-06\n",
      "VAL: EPOCH 388/1000 | BATCH 0/8 | LOSS: 4.093654297321336e-06\n",
      "VAL: EPOCH 388/1000 | BATCH 1/8 | LOSS: 4.23448227593326e-06\n",
      "VAL: EPOCH 388/1000 | BATCH 2/8 | LOSS: 4.493279144905197e-06\n",
      "VAL: EPOCH 388/1000 | BATCH 3/8 | LOSS: 4.617653075911221e-06\n",
      "VAL: EPOCH 388/1000 | BATCH 4/8 | LOSS: 4.583923691825476e-06\n",
      "VAL: EPOCH 388/1000 | BATCH 5/8 | LOSS: 4.470942106612104e-06\n",
      "VAL: EPOCH 388/1000 | BATCH 6/8 | LOSS: 4.3826313620749195e-06\n",
      "VAL: EPOCH 388/1000 | BATCH 7/8 | LOSS: 4.329579780915083e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 0/71 | LOSS: 4.657222689274931e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 1/71 | LOSS: 4.1223548805646715e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 2/71 | LOSS: 4.443944893258352e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 3/71 | LOSS: 4.252011819971813e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 4/71 | LOSS: 4.165556265434134e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 5/71 | LOSS: 4.231879036827498e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 6/71 | LOSS: 4.166674281285461e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 7/71 | LOSS: 4.178895750328593e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 8/71 | LOSS: 4.241826369858852e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 9/71 | LOSS: 4.2612854258550215e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 10/71 | LOSS: 4.333804616346077e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 11/71 | LOSS: 4.3606675224812834e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 12/71 | LOSS: 4.363883030796737e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 13/71 | LOSS: 4.405275944918685e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 14/71 | LOSS: 4.519827643889584e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 15/71 | LOSS: 4.616819055058841e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 16/71 | LOSS: 4.5438259402384145e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 17/71 | LOSS: 4.672702478577169e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 18/71 | LOSS: 4.70661333757131e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 19/71 | LOSS: 4.7314439029833014e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 20/71 | LOSS: 4.726273551741975e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 21/71 | LOSS: 4.771573509804677e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 22/71 | LOSS: 4.84174986171559e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 23/71 | LOSS: 4.84206296391676e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 24/71 | LOSS: 4.892469942205935e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 25/71 | LOSS: 4.97339980344035e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 26/71 | LOSS: 4.98643845713997e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 27/71 | LOSS: 5.072100382871472e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 28/71 | LOSS: 5.04604504228688e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 29/71 | LOSS: 5.090995447668926e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 30/71 | LOSS: 5.154004134789331e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 31/71 | LOSS: 5.2387283844268495e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 32/71 | LOSS: 5.21604010828014e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 33/71 | LOSS: 5.3394500395477415e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 34/71 | LOSS: 5.335942926519368e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 35/71 | LOSS: 5.3855683859183046e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 36/71 | LOSS: 5.3910574307165745e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 37/71 | LOSS: 5.352354234769576e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 38/71 | LOSS: 5.360447648154849e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 39/71 | LOSS: 5.395328963686552e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 40/71 | LOSS: 5.472908064692136e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 41/71 | LOSS: 5.442727412974775e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 42/71 | LOSS: 5.5107022416090925e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 43/71 | LOSS: 5.54556957736581e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 44/71 | LOSS: 5.628522159087071e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 45/71 | LOSS: 5.617872637270446e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 46/71 | LOSS: 5.602418072003081e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 47/71 | LOSS: 5.6780488506547044e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 48/71 | LOSS: 5.696817986378791e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 49/71 | LOSS: 5.694580727322318e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 50/71 | LOSS: 5.66862334327358e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 51/71 | LOSS: 5.737737187526083e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 52/71 | LOSS: 5.699463732731979e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 53/71 | LOSS: 5.726587540232652e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 54/71 | LOSS: 5.702901030831113e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 55/71 | LOSS: 5.771636937522219e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 56/71 | LOSS: 5.772341182819218e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 57/71 | LOSS: 5.771121718994279e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 58/71 | LOSS: 5.772892976337817e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 59/71 | LOSS: 5.776760087883304e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 60/71 | LOSS: 5.763152975320185e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 61/71 | LOSS: 5.7600877770739554e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 62/71 | LOSS: 5.728931289432073e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 63/71 | LOSS: 5.71383987235663e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 64/71 | LOSS: 5.693233578522967e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 65/71 | LOSS: 5.695460841211157e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 66/71 | LOSS: 5.686991200029786e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 67/71 | LOSS: 5.6898533537552044e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 68/71 | LOSS: 5.677964341173166e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 69/71 | LOSS: 5.674078527460681e-06\n",
      "TRAIN: EPOCH 389/1000 | BATCH 70/71 | LOSS: 5.656694441659312e-06\n",
      "VAL: EPOCH 389/1000 | BATCH 0/8 | LOSS: 4.243242074153386e-06\n",
      "VAL: EPOCH 389/1000 | BATCH 1/8 | LOSS: 4.326325779402396e-06\n",
      "VAL: EPOCH 389/1000 | BATCH 2/8 | LOSS: 4.5151173253543675e-06\n",
      "VAL: EPOCH 389/1000 | BATCH 3/8 | LOSS: 4.633819685295748e-06\n",
      "VAL: EPOCH 389/1000 | BATCH 4/8 | LOSS: 4.601423552230699e-06\n",
      "VAL: EPOCH 389/1000 | BATCH 5/8 | LOSS: 4.56688409637233e-06\n",
      "VAL: EPOCH 389/1000 | BATCH 6/8 | LOSS: 4.473794173723686e-06\n",
      "VAL: EPOCH 389/1000 | BATCH 7/8 | LOSS: 4.406417588143086e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 0/71 | LOSS: 4.961171725881286e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 1/71 | LOSS: 4.1457906263531186e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 2/71 | LOSS: 4.095474954131835e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 3/71 | LOSS: 3.96505095068278e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 4/71 | LOSS: 3.9083781302906575e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 5/71 | LOSS: 4.007586085208459e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 6/71 | LOSS: 4.046951289637946e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 7/71 | LOSS: 4.261665367266687e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 8/71 | LOSS: 4.362680556369014e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 9/71 | LOSS: 4.350496465121978e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 10/71 | LOSS: 4.4615878984834785e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 11/71 | LOSS: 4.421086828187981e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 12/71 | LOSS: 4.3624589544868704e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 13/71 | LOSS: 4.377520813899796e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 14/71 | LOSS: 4.33940413131495e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 15/71 | LOSS: 4.393044093831122e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 16/71 | LOSS: 4.374970424524851e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 17/71 | LOSS: 4.35146332872844e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 18/71 | LOSS: 4.394038425839665e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 19/71 | LOSS: 4.378037215246877e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 20/71 | LOSS: 4.427636386459372e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 21/71 | LOSS: 4.422601023179595e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 22/71 | LOSS: 4.412838022645978e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 23/71 | LOSS: 4.445878346359677e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 24/71 | LOSS: 4.438544401637045e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 25/71 | LOSS: 4.393244633040293e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 26/71 | LOSS: 4.400764821723526e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 27/71 | LOSS: 4.43082547008089e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 28/71 | LOSS: 4.433227128049174e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 29/71 | LOSS: 4.452909267153397e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 30/71 | LOSS: 4.48994732352965e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 31/71 | LOSS: 4.4468660931329396e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 32/71 | LOSS: 4.455673089261946e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 33/71 | LOSS: 4.484261694415181e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 34/71 | LOSS: 4.480135709984877e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 35/71 | LOSS: 4.498324509919864e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 36/71 | LOSS: 4.545518199724816e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 37/71 | LOSS: 4.52492146283284e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 38/71 | LOSS: 4.538333903325017e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 39/71 | LOSS: 4.5618736749020176e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 40/71 | LOSS: 4.587168848779473e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 41/71 | LOSS: 4.589147422285307e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 42/71 | LOSS: 4.604054379871191e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 43/71 | LOSS: 4.633438245284858e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 44/71 | LOSS: 4.624898065230809e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 45/71 | LOSS: 4.6339223346243985e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 46/71 | LOSS: 4.6314511893526405e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 47/71 | LOSS: 4.6310429411278164e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 48/71 | LOSS: 4.651293767558539e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 49/71 | LOSS: 4.660405174945481e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 50/71 | LOSS: 4.656670536824808e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 51/71 | LOSS: 4.635950919971667e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 52/71 | LOSS: 4.636816159308062e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 53/71 | LOSS: 4.627789326992503e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 54/71 | LOSS: 4.624101389212724e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 55/71 | LOSS: 4.626755696790497e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 56/71 | LOSS: 4.625573199103983e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 57/71 | LOSS: 4.633005581691295e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 58/71 | LOSS: 4.628399476471606e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 59/71 | LOSS: 4.629195041161438e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 60/71 | LOSS: 4.625986414302148e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 61/71 | LOSS: 4.632390579401193e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 62/71 | LOSS: 4.629924386256515e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 63/71 | LOSS: 4.653633432383231e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 64/71 | LOSS: 4.682425417111237e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 65/71 | LOSS: 4.679733199902165e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 66/71 | LOSS: 4.68783081039385e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 67/71 | LOSS: 4.695290171826748e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 68/71 | LOSS: 4.693214373548761e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 69/71 | LOSS: 4.6899738338522314e-06\n",
      "TRAIN: EPOCH 390/1000 | BATCH 70/71 | LOSS: 4.675157966425693e-06\n",
      "VAL: EPOCH 390/1000 | BATCH 0/8 | LOSS: 6.2622693803859875e-06\n",
      "VAL: EPOCH 390/1000 | BATCH 1/8 | LOSS: 6.215800112840952e-06\n",
      "VAL: EPOCH 390/1000 | BATCH 2/8 | LOSS: 6.493787016855397e-06\n",
      "VAL: EPOCH 390/1000 | BATCH 3/8 | LOSS: 6.356612516356108e-06\n",
      "VAL: EPOCH 390/1000 | BATCH 4/8 | LOSS: 6.3700889768369965e-06\n",
      "VAL: EPOCH 390/1000 | BATCH 5/8 | LOSS: 6.319967042145436e-06\n",
      "VAL: EPOCH 390/1000 | BATCH 6/8 | LOSS: 6.24806508702542e-06\n",
      "VAL: EPOCH 390/1000 | BATCH 7/8 | LOSS: 6.295830928593205e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 0/71 | LOSS: 6.066474725230364e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 1/71 | LOSS: 4.839579787585535e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 2/71 | LOSS: 5.423770896110606e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 3/71 | LOSS: 5.278633466332394e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 4/71 | LOSS: 5.252277969702846e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 5/71 | LOSS: 5.097649970290756e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 6/71 | LOSS: 5.324044325659218e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 7/71 | LOSS: 5.366972402498504e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 8/71 | LOSS: 5.170716576685663e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 9/71 | LOSS: 5.209287974139443e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 10/71 | LOSS: 5.141363668702649e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 11/71 | LOSS: 5.069240576934438e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 12/71 | LOSS: 4.957390134362728e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 13/71 | LOSS: 4.9814912180019225e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 14/71 | LOSS: 4.897011134138059e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 15/71 | LOSS: 4.848272467938841e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 16/71 | LOSS: 4.85561487467374e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 17/71 | LOSS: 4.849429046771547e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 18/71 | LOSS: 4.832691202912094e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 19/71 | LOSS: 4.799361170171323e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 20/71 | LOSS: 4.744278904971517e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 21/71 | LOSS: 4.689369875036804e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 22/71 | LOSS: 4.672905304133842e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 23/71 | LOSS: 4.601618873797027e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 24/71 | LOSS: 4.614938588929362e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 25/71 | LOSS: 4.63289405580816e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 26/71 | LOSS: 4.6418736019404605e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 27/71 | LOSS: 4.673416859597117e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 28/71 | LOSS: 4.654652206352631e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 29/71 | LOSS: 4.621688003680902e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 30/71 | LOSS: 4.61644363132373e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 31/71 | LOSS: 4.611985616520542e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 32/71 | LOSS: 4.5992434631140595e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 33/71 | LOSS: 4.5936169900575326e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 34/71 | LOSS: 4.6178446739629314e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 35/71 | LOSS: 4.630845978681464e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 36/71 | LOSS: 4.633347162854153e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 37/71 | LOSS: 4.61430545328767e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 38/71 | LOSS: 4.662803546988214e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 39/71 | LOSS: 4.670259977501701e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 40/71 | LOSS: 4.7158810491787225e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 41/71 | LOSS: 4.710641492454756e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 42/71 | LOSS: 4.80007479064773e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 43/71 | LOSS: 4.791669880432892e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 44/71 | LOSS: 4.868991891271435e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 45/71 | LOSS: 4.8678532691511744e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 46/71 | LOSS: 4.963700092125788e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 47/71 | LOSS: 4.964647691470721e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 48/71 | LOSS: 5.006684205000056e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 49/71 | LOSS: 5.025971595387091e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 50/71 | LOSS: 5.044237125320269e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 51/71 | LOSS: 5.070486264020684e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 52/71 | LOSS: 5.06505348998614e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 53/71 | LOSS: 5.1157931179827926e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 54/71 | LOSS: 5.092219800479308e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 55/71 | LOSS: 5.1729086944111e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 56/71 | LOSS: 5.148704535155016e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 57/71 | LOSS: 5.193787991441335e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 58/71 | LOSS: 5.187181217078725e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 59/71 | LOSS: 5.2181532320598006e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 60/71 | LOSS: 5.289080308266719e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 61/71 | LOSS: 5.333159228646236e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 62/71 | LOSS: 5.369986522979046e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 63/71 | LOSS: 5.377357876312772e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 64/71 | LOSS: 5.434786680780235e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 65/71 | LOSS: 5.42145313498953e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 66/71 | LOSS: 5.443130442187401e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 67/71 | LOSS: 5.434097518526173e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 68/71 | LOSS: 5.466667796982633e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 69/71 | LOSS: 5.452225575806681e-06\n",
      "TRAIN: EPOCH 391/1000 | BATCH 70/71 | LOSS: 5.444187911027103e-06\n",
      "VAL: EPOCH 391/1000 | BATCH 0/8 | LOSS: 5.930467523285188e-06\n",
      "VAL: EPOCH 391/1000 | BATCH 1/8 | LOSS: 5.852250978932716e-06\n",
      "VAL: EPOCH 391/1000 | BATCH 2/8 | LOSS: 5.965469426882919e-06\n",
      "VAL: EPOCH 391/1000 | BATCH 3/8 | LOSS: 5.954100743110757e-06\n",
      "VAL: EPOCH 391/1000 | BATCH 4/8 | LOSS: 5.868803873454453e-06\n",
      "VAL: EPOCH 391/1000 | BATCH 5/8 | LOSS: 5.6380224577878835e-06\n",
      "VAL: EPOCH 391/1000 | BATCH 6/8 | LOSS: 5.4424567679234315e-06\n",
      "VAL: EPOCH 391/1000 | BATCH 7/8 | LOSS: 5.387511521348642e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 0/71 | LOSS: 4.615487341652624e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 1/71 | LOSS: 5.161800345376832e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 2/71 | LOSS: 5.192796682725505e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 3/71 | LOSS: 5.066676976639428e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 4/71 | LOSS: 5.109469293529401e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 5/71 | LOSS: 4.8509000028692144e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 6/71 | LOSS: 4.7917208771000986e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 7/71 | LOSS: 5.1031271084411856e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 8/71 | LOSS: 5.194997937804955e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 9/71 | LOSS: 5.221528476795357e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 10/71 | LOSS: 5.192206592916839e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 11/71 | LOSS: 5.241383007614786e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 12/71 | LOSS: 5.18580094859107e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 13/71 | LOSS: 5.188304498915386e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 14/71 | LOSS: 5.164565770125288e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 15/71 | LOSS: 5.100892494169784e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 16/71 | LOSS: 5.195802888171302e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 17/71 | LOSS: 5.220403522798733e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 18/71 | LOSS: 5.290850307765619e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 19/71 | LOSS: 5.313530903094943e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 20/71 | LOSS: 5.294488508817656e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 21/71 | LOSS: 5.227367337697202e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 22/71 | LOSS: 5.241581032785926e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 23/71 | LOSS: 5.303422625502208e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 24/71 | LOSS: 5.229954231253942e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 25/71 | LOSS: 5.3179364403359405e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 26/71 | LOSS: 5.27552023649386e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 27/71 | LOSS: 5.3220025019332496e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 28/71 | LOSS: 5.319255055367429e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 29/71 | LOSS: 5.403760731799897e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 30/71 | LOSS: 5.3730787004174234e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 31/71 | LOSS: 5.3362400862511095e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 32/71 | LOSS: 5.358407778740829e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 33/71 | LOSS: 5.324314308970369e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 34/71 | LOSS: 5.393653782448382e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 35/71 | LOSS: 5.394022050748188e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 36/71 | LOSS: 5.429487625430763e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 37/71 | LOSS: 5.45858215087049e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 38/71 | LOSS: 5.501689785001546e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 39/71 | LOSS: 5.52749958728782e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 40/71 | LOSS: 5.501568416659167e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 41/71 | LOSS: 5.553442931505699e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 42/71 | LOSS: 5.527208216779738e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 43/71 | LOSS: 5.54477443095245e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 44/71 | LOSS: 5.506490798426158e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 45/71 | LOSS: 5.485014244384176e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 46/71 | LOSS: 5.474613167376795e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 47/71 | LOSS: 5.502281344623346e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 48/71 | LOSS: 5.482225291490642e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 49/71 | LOSS: 5.487271032507124e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 50/71 | LOSS: 5.46989600461886e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 51/71 | LOSS: 5.480163661804209e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 52/71 | LOSS: 5.4519100687238675e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 53/71 | LOSS: 5.453409176812717e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 54/71 | LOSS: 5.422875235374456e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 55/71 | LOSS: 5.408378305966315e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 56/71 | LOSS: 5.400762487178119e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 57/71 | LOSS: 5.396854495804041e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 58/71 | LOSS: 5.392461925096587e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 59/71 | LOSS: 5.376800614461293e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 60/71 | LOSS: 5.35742269228828e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 61/71 | LOSS: 5.328390619568733e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 62/71 | LOSS: 5.311380940088023e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 63/71 | LOSS: 5.2978412412585385e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 64/71 | LOSS: 5.282078137287709e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 65/71 | LOSS: 5.264153769806426e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 66/71 | LOSS: 5.2805288923177135e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 67/71 | LOSS: 5.2565024040685625e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 68/71 | LOSS: 5.243523051119511e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 69/71 | LOSS: 5.2181840926225825e-06\n",
      "TRAIN: EPOCH 392/1000 | BATCH 70/71 | LOSS: 5.20258251052722e-06\n",
      "VAL: EPOCH 392/1000 | BATCH 0/8 | LOSS: 5.742212124459911e-06\n",
      "VAL: EPOCH 392/1000 | BATCH 1/8 | LOSS: 5.478561206473387e-06\n",
      "VAL: EPOCH 392/1000 | BATCH 2/8 | LOSS: 5.410176375638305e-06\n",
      "VAL: EPOCH 392/1000 | BATCH 3/8 | LOSS: 5.451483843899041e-06\n",
      "VAL: EPOCH 392/1000 | BATCH 4/8 | LOSS: 5.295774190017255e-06\n",
      "VAL: EPOCH 392/1000 | BATCH 5/8 | LOSS: 5.0809223921532976e-06\n",
      "VAL: EPOCH 392/1000 | BATCH 6/8 | LOSS: 5.015029208672266e-06\n",
      "VAL: EPOCH 392/1000 | BATCH 7/8 | LOSS: 4.8647232233633986e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 0/71 | LOSS: 4.67443805973744e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 1/71 | LOSS: 4.67891140942811e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 2/71 | LOSS: 4.3816602707617376e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 3/71 | LOSS: 4.213796501062461e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 4/71 | LOSS: 3.973116599809146e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 5/71 | LOSS: 4.321733437488244e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 6/71 | LOSS: 4.299612295913643e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 7/71 | LOSS: 4.314924808568321e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 8/71 | LOSS: 4.2904254971492465e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 9/71 | LOSS: 4.314297257224098e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 10/71 | LOSS: 4.516937430955957e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 11/71 | LOSS: 4.523860942147924e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 12/71 | LOSS: 4.497676400205819e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 13/71 | LOSS: 4.505371537431659e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 14/71 | LOSS: 4.594387337419903e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 15/71 | LOSS: 4.573748725533733e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 16/71 | LOSS: 4.670060099991079e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 17/71 | LOSS: 4.639286013722692e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 18/71 | LOSS: 4.701415659119927e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 19/71 | LOSS: 4.691845128945716e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 20/71 | LOSS: 4.729736983357552e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 21/71 | LOSS: 4.768191378058294e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 22/71 | LOSS: 4.8064758709148485e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 23/71 | LOSS: 4.811887568697178e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 24/71 | LOSS: 4.838049244426657e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 25/71 | LOSS: 4.874179187306883e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 26/71 | LOSS: 4.8510081695793715e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 27/71 | LOSS: 4.8642741441134215e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 28/71 | LOSS: 4.852520681918671e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 29/71 | LOSS: 4.8629369606108716e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 30/71 | LOSS: 4.89772076933502e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 31/71 | LOSS: 4.905230497342927e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 32/71 | LOSS: 4.8659343189104796e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 33/71 | LOSS: 4.903765343175003e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 34/71 | LOSS: 4.9250340842783255e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 35/71 | LOSS: 4.882534091797828e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 36/71 | LOSS: 4.873258966334696e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 37/71 | LOSS: 4.897771622094488e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 38/71 | LOSS: 4.907923218921403e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 39/71 | LOSS: 4.889950724873416e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 40/71 | LOSS: 4.9097253872232835e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 41/71 | LOSS: 4.899264180970758e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 42/71 | LOSS: 4.869055463893536e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 43/71 | LOSS: 4.890588069305192e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 44/71 | LOSS: 4.924305838156013e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 45/71 | LOSS: 4.899680927495786e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 46/71 | LOSS: 4.932534930629388e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 47/71 | LOSS: 4.9368331929144915e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 48/71 | LOSS: 4.930188944334597e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 49/71 | LOSS: 4.940797534800367e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 50/71 | LOSS: 4.9600430251231565e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 51/71 | LOSS: 4.962216350507403e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 52/71 | LOSS: 4.978315778230954e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 53/71 | LOSS: 4.99222094024137e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 54/71 | LOSS: 4.97484687994901e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 55/71 | LOSS: 4.994450120778181e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 56/71 | LOSS: 4.996760907759604e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 57/71 | LOSS: 5.033119107573494e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 58/71 | LOSS: 5.036651269250088e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 59/71 | LOSS: 5.0643090996042394e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 60/71 | LOSS: 5.078031216592043e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 61/71 | LOSS: 5.098187814162943e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 62/71 | LOSS: 5.1182693881393845e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 63/71 | LOSS: 5.122304344240547e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 64/71 | LOSS: 5.106396761570627e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 65/71 | LOSS: 5.095400865684496e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 66/71 | LOSS: 5.114620911789738e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 67/71 | LOSS: 5.107622775861219e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 68/71 | LOSS: 5.103925935185069e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 69/71 | LOSS: 5.077422208289915e-06\n",
      "TRAIN: EPOCH 393/1000 | BATCH 70/71 | LOSS: 5.08409045362217e-06\n",
      "VAL: EPOCH 393/1000 | BATCH 0/8 | LOSS: 5.38680160389049e-06\n",
      "VAL: EPOCH 393/1000 | BATCH 1/8 | LOSS: 5.141419933352154e-06\n",
      "VAL: EPOCH 393/1000 | BATCH 2/8 | LOSS: 5.021465919223071e-06\n",
      "VAL: EPOCH 393/1000 | BATCH 3/8 | LOSS: 4.9851545327328495e-06\n",
      "VAL: EPOCH 393/1000 | BATCH 4/8 | LOSS: 4.877237643086119e-06\n",
      "VAL: EPOCH 393/1000 | BATCH 5/8 | LOSS: 4.6495180564913125e-06\n",
      "VAL: EPOCH 393/1000 | BATCH 6/8 | LOSS: 4.524394171312451e-06\n",
      "VAL: EPOCH 393/1000 | BATCH 7/8 | LOSS: 4.401844961421375e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 0/71 | LOSS: 4.7240746425813995e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 1/71 | LOSS: 4.801948307431303e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 2/71 | LOSS: 4.826338530013648e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 3/71 | LOSS: 4.696794576375396e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 4/71 | LOSS: 4.89394133182941e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 5/71 | LOSS: 5.001098012750542e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 6/71 | LOSS: 5.181508575852993e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 7/71 | LOSS: 5.068006430519745e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 8/71 | LOSS: 4.9398213377571665e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 9/71 | LOSS: 4.833684124605498e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 10/71 | LOSS: 4.891697079239583e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 11/71 | LOSS: 5.004439951032206e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 12/71 | LOSS: 4.901669276827865e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 13/71 | LOSS: 4.993641547506351e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 14/71 | LOSS: 4.992533755891297e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 15/71 | LOSS: 4.961714850537646e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 16/71 | LOSS: 5.005573591594893e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 17/71 | LOSS: 4.979088531500666e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 18/71 | LOSS: 4.962596030485369e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 19/71 | LOSS: 4.899611371911305e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 20/71 | LOSS: 4.832635145898426e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 21/71 | LOSS: 4.8344434886034415e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 22/71 | LOSS: 4.782313255972163e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 23/71 | LOSS: 4.808715014329816e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 24/71 | LOSS: 4.806386923519312e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 25/71 | LOSS: 4.761694395501064e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 26/71 | LOSS: 4.723757001556805e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 27/71 | LOSS: 4.660315872635172e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 28/71 | LOSS: 4.738177040053415e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 29/71 | LOSS: 4.736451326910659e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 30/71 | LOSS: 4.723129215504272e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 31/71 | LOSS: 4.735340304762303e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 32/71 | LOSS: 4.755541484965678e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 33/71 | LOSS: 4.798092618329974e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 34/71 | LOSS: 4.813626901523094e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 35/71 | LOSS: 4.780040860118283e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 36/71 | LOSS: 4.814998726795091e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 37/71 | LOSS: 4.854489403489999e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 38/71 | LOSS: 4.843778551497589e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 39/71 | LOSS: 4.847741121238869e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 40/71 | LOSS: 4.878097287495976e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 41/71 | LOSS: 4.8548137751173015e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 42/71 | LOSS: 4.899400521956755e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 43/71 | LOSS: 4.931446633029489e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 44/71 | LOSS: 4.909949716925945e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 45/71 | LOSS: 4.9527246999233565e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 46/71 | LOSS: 4.9214791275478495e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 47/71 | LOSS: 4.958207592646128e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 48/71 | LOSS: 4.946776619604231e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 49/71 | LOSS: 4.941992560816289e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 50/71 | LOSS: 4.938939328520278e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 51/71 | LOSS: 4.929134158478869e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 52/71 | LOSS: 4.906067891530462e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 53/71 | LOSS: 4.90197439578171e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 54/71 | LOSS: 4.8789588039322885e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 55/71 | LOSS: 4.873682680032029e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 56/71 | LOSS: 4.85691579773342e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 57/71 | LOSS: 4.8534872474129804e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 58/71 | LOSS: 4.858719452293649e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 59/71 | LOSS: 4.875984397282688e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 60/71 | LOSS: 4.863147862989371e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 61/71 | LOSS: 4.885613920592849e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 62/71 | LOSS: 4.8881008932232995e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 63/71 | LOSS: 4.903808751066663e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 64/71 | LOSS: 4.8931882352357765e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 65/71 | LOSS: 4.913212905827977e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 66/71 | LOSS: 4.930722040096871e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 67/71 | LOSS: 4.926093932975645e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 68/71 | LOSS: 4.941632853486858e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 69/71 | LOSS: 4.942600402760685e-06\n",
      "TRAIN: EPOCH 394/1000 | BATCH 70/71 | LOSS: 4.9639152358638604e-06\n",
      "VAL: EPOCH 394/1000 | BATCH 0/8 | LOSS: 6.819514055678155e-06\n",
      "VAL: EPOCH 394/1000 | BATCH 1/8 | LOSS: 6.4674075019865995e-06\n",
      "VAL: EPOCH 394/1000 | BATCH 2/8 | LOSS: 6.729444521624828e-06\n",
      "VAL: EPOCH 394/1000 | BATCH 3/8 | LOSS: 6.50922549993993e-06\n",
      "VAL: EPOCH 394/1000 | BATCH 4/8 | LOSS: 6.532210591103649e-06\n",
      "VAL: EPOCH 394/1000 | BATCH 5/8 | LOSS: 6.4776158221017495e-06\n",
      "VAL: EPOCH 394/1000 | BATCH 6/8 | LOSS: 6.502233320913677e-06\n",
      "VAL: EPOCH 394/1000 | BATCH 7/8 | LOSS: 6.5339111756657076e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 0/71 | LOSS: 5.923062417423353e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 1/71 | LOSS: 5.791735702587175e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 2/71 | LOSS: 5.85396643752271e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 3/71 | LOSS: 5.9817456303790095e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 4/71 | LOSS: 6.050850515748607e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 5/71 | LOSS: 5.941278232057812e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 6/71 | LOSS: 5.936792344333039e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 7/71 | LOSS: 6.03516292585482e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 8/71 | LOSS: 5.865186014691264e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 9/71 | LOSS: 5.740463757319958e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 10/71 | LOSS: 5.6177735297586135e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 11/71 | LOSS: 5.545716210993608e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 12/71 | LOSS: 5.526843394293862e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 13/71 | LOSS: 5.5330951584307646e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 14/71 | LOSS: 5.47453834466675e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 15/71 | LOSS: 5.3917245566026395e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 16/71 | LOSS: 5.30888594178524e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 17/71 | LOSS: 5.253056492114814e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 18/71 | LOSS: 5.225006589171244e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 19/71 | LOSS: 5.196529241402459e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 20/71 | LOSS: 5.180103947904649e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 21/71 | LOSS: 5.183095461027485e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 22/71 | LOSS: 5.1259359837707095e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 23/71 | LOSS: 5.086636330512799e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 24/71 | LOSS: 5.069950493634678e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 25/71 | LOSS: 5.001724887803833e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 26/71 | LOSS: 4.980708632097554e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 27/71 | LOSS: 4.973393678093478e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 28/71 | LOSS: 4.954179938318577e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 29/71 | LOSS: 4.972321721652406e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 30/71 | LOSS: 4.971775187216061e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 31/71 | LOSS: 4.997198928435864e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 32/71 | LOSS: 4.974116439949615e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 33/71 | LOSS: 4.971799479094711e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 34/71 | LOSS: 5.00812123261442e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 35/71 | LOSS: 5.010528095934003e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 36/71 | LOSS: 4.987248131156603e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 37/71 | LOSS: 4.969476166431712e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 38/71 | LOSS: 4.979330673776507e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 39/71 | LOSS: 4.983221310794761e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 40/71 | LOSS: 4.974225814249821e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 41/71 | LOSS: 4.955783597757718e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 42/71 | LOSS: 4.98282527185565e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 43/71 | LOSS: 4.9826265405334364e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 44/71 | LOSS: 4.990719743444869e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 45/71 | LOSS: 4.968045083802755e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 46/71 | LOSS: 5.009744732075326e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 47/71 | LOSS: 5.0126976039640185e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 48/71 | LOSS: 5.0000801783593195e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 49/71 | LOSS: 4.9663922845866186e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 50/71 | LOSS: 4.9658499481701455e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 51/71 | LOSS: 4.945545421938173e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 52/71 | LOSS: 4.9313597465474884e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 53/71 | LOSS: 4.931411817105308e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 54/71 | LOSS: 4.911521207098056e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 55/71 | LOSS: 4.924323024267453e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 56/71 | LOSS: 4.89449010736982e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 57/71 | LOSS: 4.899163157241329e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 58/71 | LOSS: 4.883978559611655e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 59/71 | LOSS: 4.877707560050718e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 60/71 | LOSS: 4.857731631941011e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 61/71 | LOSS: 4.855819178550108e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 62/71 | LOSS: 4.849661442517933e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 63/71 | LOSS: 4.867220770421454e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 64/71 | LOSS: 4.854961770643758e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 65/71 | LOSS: 4.909404413989581e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 66/71 | LOSS: 4.9114451675409535e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 67/71 | LOSS: 4.954882887672925e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 68/71 | LOSS: 4.964803868764041e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 69/71 | LOSS: 5.002325117077686e-06\n",
      "TRAIN: EPOCH 395/1000 | BATCH 70/71 | LOSS: 5.016664554717299e-06\n",
      "VAL: EPOCH 395/1000 | BATCH 0/8 | LOSS: 7.248745077959029e-06\n",
      "VAL: EPOCH 395/1000 | BATCH 1/8 | LOSS: 7.396525916192331e-06\n",
      "VAL: EPOCH 395/1000 | BATCH 2/8 | LOSS: 7.0143977003075024e-06\n",
      "VAL: EPOCH 395/1000 | BATCH 3/8 | LOSS: 7.009535806901113e-06\n",
      "VAL: EPOCH 395/1000 | BATCH 4/8 | LOSS: 6.872340873087524e-06\n",
      "VAL: EPOCH 395/1000 | BATCH 5/8 | LOSS: 6.439545889710037e-06\n",
      "VAL: EPOCH 395/1000 | BATCH 6/8 | LOSS: 6.224354949933643e-06\n",
      "VAL: EPOCH 395/1000 | BATCH 7/8 | LOSS: 5.982551840588712e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 0/71 | LOSS: 7.1463391577708535e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 1/71 | LOSS: 5.477769718709169e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 2/71 | LOSS: 4.725059322178519e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 3/71 | LOSS: 5.0783692131517455e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 4/71 | LOSS: 4.956684097123798e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 5/71 | LOSS: 4.97512981686062e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 6/71 | LOSS: 5.151322316773335e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 7/71 | LOSS: 5.156129816441535e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 8/71 | LOSS: 5.196845045753031e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 9/71 | LOSS: 5.313576230037142e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 10/71 | LOSS: 5.332937813694695e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 11/71 | LOSS: 5.279656420498213e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 12/71 | LOSS: 5.297328208633609e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 13/71 | LOSS: 5.256711477029187e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 14/71 | LOSS: 5.370505308140612e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 15/71 | LOSS: 5.399560052410379e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 16/71 | LOSS: 5.317626159119434e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 17/71 | LOSS: 5.3109624028972275e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 18/71 | LOSS: 5.2637064024900994e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 19/71 | LOSS: 5.278241792439075e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 20/71 | LOSS: 5.289722291005698e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 21/71 | LOSS: 5.293726148507134e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 22/71 | LOSS: 5.295204790973398e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 23/71 | LOSS: 5.351865259702511e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 24/71 | LOSS: 5.3279624444257935e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 25/71 | LOSS: 5.320971275488353e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 26/71 | LOSS: 5.294041245549495e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 27/71 | LOSS: 5.231463148902549e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 28/71 | LOSS: 5.191444752826696e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 29/71 | LOSS: 5.121463838501465e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 30/71 | LOSS: 5.078675258118892e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 31/71 | LOSS: 5.065469835585645e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 32/71 | LOSS: 5.0825387778318e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 33/71 | LOSS: 5.0922693416770715e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 34/71 | LOSS: 5.102091225645771e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 35/71 | LOSS: 5.0948416527616146e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 36/71 | LOSS: 5.0608926825146695e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 37/71 | LOSS: 5.038156658836095e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 38/71 | LOSS: 5.033566641288463e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 39/71 | LOSS: 5.006486549063993e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 40/71 | LOSS: 4.970291894739891e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 41/71 | LOSS: 4.988299386760835e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 42/71 | LOSS: 4.9533708734934615e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 43/71 | LOSS: 4.932608414326916e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 44/71 | LOSS: 4.934375394643414e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 45/71 | LOSS: 4.91677734198279e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 46/71 | LOSS: 4.913647492859572e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 47/71 | LOSS: 4.8993730198769e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 48/71 | LOSS: 4.903352066544264e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 49/71 | LOSS: 4.918910890410188e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 50/71 | LOSS: 4.9079108155673766e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 51/71 | LOSS: 4.893387146974139e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 52/71 | LOSS: 4.880114657972403e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 53/71 | LOSS: 4.882485814892722e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 54/71 | LOSS: 4.879346108116972e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 55/71 | LOSS: 4.884600262552599e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 56/71 | LOSS: 4.8767781647671785e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 57/71 | LOSS: 4.861432012932772e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 58/71 | LOSS: 4.844598159683825e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 59/71 | LOSS: 4.840123763945788e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 60/71 | LOSS: 4.840239816090974e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 61/71 | LOSS: 4.8381946198201004e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 62/71 | LOSS: 4.832770577599553e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 63/71 | LOSS: 4.823986138546843e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 64/71 | LOSS: 4.813638189751565e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 65/71 | LOSS: 4.79675979863025e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 66/71 | LOSS: 4.774745952117747e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 67/71 | LOSS: 4.793739027691652e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 68/71 | LOSS: 4.793868099469641e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 69/71 | LOSS: 4.785459298415974e-06\n",
      "TRAIN: EPOCH 396/1000 | BATCH 70/71 | LOSS: 4.812232002453603e-06\n",
      "VAL: EPOCH 396/1000 | BATCH 0/8 | LOSS: 6.110473350418033e-06\n",
      "VAL: EPOCH 396/1000 | BATCH 1/8 | LOSS: 6.1348061990429414e-06\n",
      "VAL: EPOCH 396/1000 | BATCH 2/8 | LOSS: 6.006717133762625e-06\n",
      "VAL: EPOCH 396/1000 | BATCH 3/8 | LOSS: 6.063025125513377e-06\n",
      "VAL: EPOCH 396/1000 | BATCH 4/8 | LOSS: 5.940711162111256e-06\n",
      "VAL: EPOCH 396/1000 | BATCH 5/8 | LOSS: 5.667372382352672e-06\n",
      "VAL: EPOCH 396/1000 | BATCH 6/8 | LOSS: 5.603723366220947e-06\n",
      "VAL: EPOCH 396/1000 | BATCH 7/8 | LOSS: 5.400566124080797e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 0/71 | LOSS: 5.05447633258882e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 1/71 | LOSS: 4.752275344799273e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 2/71 | LOSS: 4.874918280014147e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 3/71 | LOSS: 5.758391353083425e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 4/71 | LOSS: 5.601802877208684e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 5/71 | LOSS: 6.520061257712466e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 6/71 | LOSS: 6.280425882973109e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 7/71 | LOSS: 6.564075988535478e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 8/71 | LOSS: 6.438037315900955e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 9/71 | LOSS: 6.699857294734102e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 10/71 | LOSS: 6.682144306647718e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 11/71 | LOSS: 6.7703411635496496e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 12/71 | LOSS: 7.319824362639338e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 13/71 | LOSS: 7.090080056773981e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 14/71 | LOSS: 7.6232526832124375e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 15/71 | LOSS: 7.493300728356189e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 16/71 | LOSS: 7.829570275883752e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 17/71 | LOSS: 7.916508795662796e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 18/71 | LOSS: 8.175267626747404e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 19/71 | LOSS: 8.398642899010156e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 20/71 | LOSS: 8.419687197775125e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 21/71 | LOSS: 8.710006258380334e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 22/71 | LOSS: 8.653192846553713e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 23/71 | LOSS: 8.706689470727724e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 24/71 | LOSS: 8.570531408622628e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 25/71 | LOSS: 8.635241364082993e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 26/71 | LOSS: 8.51128558039178e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 27/71 | LOSS: 8.477335914644105e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 28/71 | LOSS: 8.350394370515051e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 29/71 | LOSS: 8.27470009123014e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 30/71 | LOSS: 8.2109045105162e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 31/71 | LOSS: 8.117050626310629e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 32/71 | LOSS: 8.13409512071909e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 33/71 | LOSS: 8.10772612299736e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 34/71 | LOSS: 8.097600091007605e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 35/71 | LOSS: 8.099244546934238e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 36/71 | LOSS: 8.062065766402205e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 37/71 | LOSS: 8.08394976334775e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 38/71 | LOSS: 8.019413144211285e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 39/71 | LOSS: 8.16348883745377e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 40/71 | LOSS: 8.09106023087047e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 41/71 | LOSS: 8.148883374642104e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 42/71 | LOSS: 8.112368356896061e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 43/71 | LOSS: 8.131328213907677e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 44/71 | LOSS: 8.179476784183256e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 45/71 | LOSS: 8.161631977481706e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 46/71 | LOSS: 8.25909061878771e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 47/71 | LOSS: 8.178431471606018e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 48/71 | LOSS: 8.205273416581117e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 49/71 | LOSS: 8.160312499967404e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 50/71 | LOSS: 8.128737706511838e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 51/71 | LOSS: 8.114298872477286e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 52/71 | LOSS: 8.081732047222036e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 53/71 | LOSS: 8.058616450162609e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 54/71 | LOSS: 8.014637528090547e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 55/71 | LOSS: 7.992514416140953e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 56/71 | LOSS: 8.02864445161009e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 57/71 | LOSS: 7.98578712576649e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 58/71 | LOSS: 8.034217752522683e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 59/71 | LOSS: 8.006766711332602e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 60/71 | LOSS: 7.99067838968648e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 61/71 | LOSS: 7.928730808764098e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 62/71 | LOSS: 7.900349453213119e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 63/71 | LOSS: 7.859202284521416e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 64/71 | LOSS: 7.815418911587375e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 65/71 | LOSS: 7.779228099025204e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 66/71 | LOSS: 7.748488851168293e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 67/71 | LOSS: 7.696895081340591e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 68/71 | LOSS: 7.667247641422918e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 69/71 | LOSS: 7.635129973745539e-06\n",
      "TRAIN: EPOCH 397/1000 | BATCH 70/71 | LOSS: 7.662546163950358e-06\n",
      "VAL: EPOCH 397/1000 | BATCH 0/8 | LOSS: 5.0799030759662855e-06\n",
      "VAL: EPOCH 397/1000 | BATCH 1/8 | LOSS: 4.873535090155201e-06\n",
      "VAL: EPOCH 397/1000 | BATCH 2/8 | LOSS: 5.139501354278764e-06\n",
      "VAL: EPOCH 397/1000 | BATCH 3/8 | LOSS: 5.193796482672042e-06\n",
      "VAL: EPOCH 397/1000 | BATCH 4/8 | LOSS: 5.1127316510246604e-06\n",
      "VAL: EPOCH 397/1000 | BATCH 5/8 | LOSS: 5.123238755307587e-06\n",
      "VAL: EPOCH 397/1000 | BATCH 6/8 | LOSS: 5.04644393523839e-06\n",
      "VAL: EPOCH 397/1000 | BATCH 7/8 | LOSS: 5.0073471697942296e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 0/71 | LOSS: 4.767053724208381e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 1/71 | LOSS: 5.151180630491581e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 2/71 | LOSS: 5.84811823500786e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 3/71 | LOSS: 5.677946887772123e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 4/71 | LOSS: 5.712332404073095e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 5/71 | LOSS: 6.052024597617371e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 6/71 | LOSS: 6.023818384294698e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 7/71 | LOSS: 6.022078082423832e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 8/71 | LOSS: 5.869486358278664e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 9/71 | LOSS: 5.857977612322429e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 10/71 | LOSS: 5.823780032766411e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 11/71 | LOSS: 5.718491479456134e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 12/71 | LOSS: 5.623608173594291e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 13/71 | LOSS: 5.661733731748038e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 14/71 | LOSS: 5.632738369361808e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 15/71 | LOSS: 5.562422330740446e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 16/71 | LOSS: 5.489796100899933e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 17/71 | LOSS: 5.513144161442243e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 18/71 | LOSS: 5.606753611094686e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 19/71 | LOSS: 5.531373699341202e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 20/71 | LOSS: 5.584372242343047e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 21/71 | LOSS: 5.5857260163479206e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 22/71 | LOSS: 5.570185531605207e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 23/71 | LOSS: 5.56691579352749e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 24/71 | LOSS: 5.534211250051158e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 25/71 | LOSS: 5.510572096822076e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 26/71 | LOSS: 5.499197329670467e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 27/71 | LOSS: 5.440718479933691e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 28/71 | LOSS: 5.417798547568964e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 29/71 | LOSS: 5.391658866453023e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 30/71 | LOSS: 5.3425382570675274e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 31/71 | LOSS: 5.313386694183464e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 32/71 | LOSS: 5.272708497641636e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 33/71 | LOSS: 5.238647377957343e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 34/71 | LOSS: 5.215072217522123e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 35/71 | LOSS: 5.185176696108505e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 36/71 | LOSS: 5.168022579636198e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 37/71 | LOSS: 5.147991247979103e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 38/71 | LOSS: 5.162107906168482e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 39/71 | LOSS: 5.154682446573133e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 40/71 | LOSS: 5.1782236679594836e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 41/71 | LOSS: 5.1553215165594815e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 42/71 | LOSS: 5.147546226210178e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 43/71 | LOSS: 5.125767178138961e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 44/71 | LOSS: 5.132210753799882e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 45/71 | LOSS: 5.120860706760457e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 46/71 | LOSS: 5.114305932170766e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 47/71 | LOSS: 5.088709599476715e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 48/71 | LOSS: 5.092219861562255e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 49/71 | LOSS: 5.071423047411372e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 50/71 | LOSS: 5.0758942997359265e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 51/71 | LOSS: 5.059791891863614e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 52/71 | LOSS: 5.041198276232089e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 53/71 | LOSS: 5.024899345934736e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 54/71 | LOSS: 5.019340179304973e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 55/71 | LOSS: 5.012124623655316e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 56/71 | LOSS: 4.990061274599468e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 57/71 | LOSS: 4.982284563397868e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 58/71 | LOSS: 4.98628278210625e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 59/71 | LOSS: 4.979019195161527e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 60/71 | LOSS: 4.9558038471496116e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 61/71 | LOSS: 4.955212844951377e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 62/71 | LOSS: 4.942767144177832e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 63/71 | LOSS: 4.9279547020830705e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 64/71 | LOSS: 4.922696878286213e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 65/71 | LOSS: 4.907412772175149e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 66/71 | LOSS: 4.897260749257568e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 67/71 | LOSS: 4.895715228351892e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 68/71 | LOSS: 4.8896221105151465e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 69/71 | LOSS: 4.883216813920756e-06\n",
      "TRAIN: EPOCH 398/1000 | BATCH 70/71 | LOSS: 4.882184528879137e-06\n",
      "VAL: EPOCH 398/1000 | BATCH 0/8 | LOSS: 5.554676590691088e-06\n",
      "VAL: EPOCH 398/1000 | BATCH 1/8 | LOSS: 5.550048626901116e-06\n",
      "VAL: EPOCH 398/1000 | BATCH 2/8 | LOSS: 5.317169931610503e-06\n",
      "VAL: EPOCH 398/1000 | BATCH 3/8 | LOSS: 5.3250695373208146e-06\n",
      "VAL: EPOCH 398/1000 | BATCH 4/8 | LOSS: 5.195119865675224e-06\n",
      "VAL: EPOCH 398/1000 | BATCH 5/8 | LOSS: 4.93467784205374e-06\n",
      "VAL: EPOCH 398/1000 | BATCH 6/8 | LOSS: 4.76274265435807e-06\n",
      "VAL: EPOCH 398/1000 | BATCH 7/8 | LOSS: 4.573605053792562e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 0/71 | LOSS: 5.505182798515307e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 1/71 | LOSS: 5.503300371856312e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 2/71 | LOSS: 5.282246547722025e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 3/71 | LOSS: 5.265594154479913e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 4/71 | LOSS: 4.9487815886095635e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 5/71 | LOSS: 5.42968496120011e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 6/71 | LOSS: 5.274080844433879e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 7/71 | LOSS: 5.481982327637525e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 8/71 | LOSS: 5.4845669789453195e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 9/71 | LOSS: 5.446402838060749e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 10/71 | LOSS: 5.610134353033076e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 11/71 | LOSS: 5.623349845033469e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 12/71 | LOSS: 5.622914299097969e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 13/71 | LOSS: 5.649042415305823e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 14/71 | LOSS: 5.639232555646837e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 15/71 | LOSS: 5.48352841178712e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 16/71 | LOSS: 5.52213446901399e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 17/71 | LOSS: 5.533717108442539e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 18/71 | LOSS: 5.5684568235990716e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 19/71 | LOSS: 5.503764077730011e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 20/71 | LOSS: 5.54510251320261e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 21/71 | LOSS: 5.543950196211385e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 22/71 | LOSS: 5.50668775521563e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 23/71 | LOSS: 5.475935116313242e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 24/71 | LOSS: 5.447414787340677e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 25/71 | LOSS: 5.48756181090609e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 26/71 | LOSS: 5.463923605054367e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 27/71 | LOSS: 5.413557947317063e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 28/71 | LOSS: 5.445093110806895e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 29/71 | LOSS: 5.380078891903395e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 30/71 | LOSS: 5.4003394618649186e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 31/71 | LOSS: 5.383680729664775e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 32/71 | LOSS: 5.408999489494211e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 33/71 | LOSS: 5.441932885600288e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 34/71 | LOSS: 5.404710918810451e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 35/71 | LOSS: 5.449431278571461e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 36/71 | LOSS: 5.418708226627771e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 37/71 | LOSS: 5.447236377987888e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 38/71 | LOSS: 5.425436775579165e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 39/71 | LOSS: 5.424146661425766e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 40/71 | LOSS: 5.411076261491583e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 41/71 | LOSS: 5.391771418587832e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 42/71 | LOSS: 5.391182978285563e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 43/71 | LOSS: 5.3695373687580945e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 44/71 | LOSS: 5.356290451648723e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 45/71 | LOSS: 5.328257360736683e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 46/71 | LOSS: 5.319199549666472e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 47/71 | LOSS: 5.303167891194486e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 48/71 | LOSS: 5.2728906314082355e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 49/71 | LOSS: 5.266807211228297e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 50/71 | LOSS: 5.242924927705305e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 51/71 | LOSS: 5.2449669808013896e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 52/71 | LOSS: 5.229257601727885e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 53/71 | LOSS: 5.232836858542416e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 54/71 | LOSS: 5.244411948462419e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 55/71 | LOSS: 5.244068355685678e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 56/71 | LOSS: 5.228493735462268e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 57/71 | LOSS: 5.225563929092584e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 58/71 | LOSS: 5.222100414465462e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 59/71 | LOSS: 5.207563352390328e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 60/71 | LOSS: 5.206636324444725e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 61/71 | LOSS: 5.235317339904741e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 62/71 | LOSS: 5.217036015805899e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 63/71 | LOSS: 5.221684773459856e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 64/71 | LOSS: 5.200343310627012e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 65/71 | LOSS: 5.189754843244869e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 66/71 | LOSS: 5.164266345238855e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 67/71 | LOSS: 5.17256508454225e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 68/71 | LOSS: 5.1725319610631084e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 69/71 | LOSS: 5.1575581567574e-06\n",
      "TRAIN: EPOCH 399/1000 | BATCH 70/71 | LOSS: 5.183278075568984e-06\n",
      "VAL: EPOCH 399/1000 | BATCH 0/8 | LOSS: 6.424649654945824e-06\n",
      "VAL: EPOCH 399/1000 | BATCH 1/8 | LOSS: 6.127176447989768e-06\n",
      "VAL: EPOCH 399/1000 | BATCH 2/8 | LOSS: 6.194892042306795e-06\n",
      "VAL: EPOCH 399/1000 | BATCH 3/8 | LOSS: 6.231482871044136e-06\n",
      "VAL: EPOCH 399/1000 | BATCH 4/8 | LOSS: 6.109334299253532e-06\n",
      "VAL: EPOCH 399/1000 | BATCH 5/8 | LOSS: 5.875972495535582e-06\n",
      "VAL: EPOCH 399/1000 | BATCH 6/8 | LOSS: 5.798502765433763e-06\n",
      "VAL: EPOCH 399/1000 | BATCH 7/8 | LOSS: 5.6321552506233274e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 0/71 | LOSS: 4.81477172797895e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 1/71 | LOSS: 6.488641474788892e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 2/71 | LOSS: 5.940192598548795e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 3/71 | LOSS: 6.340790037029365e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 4/71 | LOSS: 5.995202718622749e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 5/71 | LOSS: 6.16073180026433e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 6/71 | LOSS: 6.1266956176301135e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 7/71 | LOSS: 6.044356553047692e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 8/71 | LOSS: 6.023762933990737e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 9/71 | LOSS: 5.799313794341288e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 10/71 | LOSS: 5.804155658882916e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 11/71 | LOSS: 5.651714900523075e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 12/71 | LOSS: 5.594690019978981e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 13/71 | LOSS: 5.5477548487812915e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 14/71 | LOSS: 5.5773058799483506e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 15/71 | LOSS: 5.597706490334531e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 16/71 | LOSS: 5.5665673779217556e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 17/71 | LOSS: 5.585571595171738e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 18/71 | LOSS: 5.543274997571164e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 19/71 | LOSS: 5.495664936461253e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 20/71 | LOSS: 5.463123064449367e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 21/71 | LOSS: 5.446033686563499e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 22/71 | LOSS: 5.378328672162278e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 23/71 | LOSS: 5.381174332796945e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 24/71 | LOSS: 5.374997563194484e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 25/71 | LOSS: 5.361999263606356e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 26/71 | LOSS: 5.339451878104161e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 27/71 | LOSS: 5.320691197344526e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 28/71 | LOSS: 5.336125013855053e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 29/71 | LOSS: 5.338226931902075e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 30/71 | LOSS: 5.324548099803994e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 31/71 | LOSS: 5.30146851929203e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 32/71 | LOSS: 5.312003698539532e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 33/71 | LOSS: 5.289657525174655e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 34/71 | LOSS: 5.304207180805471e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 35/71 | LOSS: 5.2652453506299125e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 36/71 | LOSS: 5.2165842073402414e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 37/71 | LOSS: 5.240364597930717e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 38/71 | LOSS: 5.255142468740814e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 39/71 | LOSS: 5.2774533230603994e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 40/71 | LOSS: 5.275429923752133e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 41/71 | LOSS: 5.239387167507846e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 42/71 | LOSS: 5.34738213118121e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 43/71 | LOSS: 5.353236231671459e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 44/71 | LOSS: 5.413206261235044e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 45/71 | LOSS: 5.412267500026286e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 46/71 | LOSS: 5.415395038498707e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 47/71 | LOSS: 5.390675070771067e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 48/71 | LOSS: 5.4022592876765194e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 49/71 | LOSS: 5.391261552176729e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 50/71 | LOSS: 5.407605586322675e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 51/71 | LOSS: 5.41620487236007e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 52/71 | LOSS: 5.4217583802487496e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 53/71 | LOSS: 5.458027995128187e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 54/71 | LOSS: 5.436001633411665e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 55/71 | LOSS: 5.470092856642493e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 56/71 | LOSS: 5.4433487229401e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 57/71 | LOSS: 5.455538645696049e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 58/71 | LOSS: 5.441623204467884e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 59/71 | LOSS: 5.43808983290243e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 60/71 | LOSS: 5.449823745829101e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 61/71 | LOSS: 5.435311502497502e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 62/71 | LOSS: 5.447414557079716e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 63/71 | LOSS: 5.432172539343583e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 64/71 | LOSS: 5.4344089256539095e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 65/71 | LOSS: 5.440457612442542e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 66/71 | LOSS: 5.426797532714745e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 67/71 | LOSS: 5.422489075900718e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 68/71 | LOSS: 5.39175618502233e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 69/71 | LOSS: 5.399283739799492e-06\n",
      "TRAIN: EPOCH 400/1000 | BATCH 70/71 | LOSS: 5.381350582622321e-06\n",
      "VAL: EPOCH 400/1000 | BATCH 0/8 | LOSS: 5.609283562080236e-06\n",
      "VAL: EPOCH 400/1000 | BATCH 1/8 | LOSS: 5.680856702383608e-06\n",
      "VAL: EPOCH 400/1000 | BATCH 2/8 | LOSS: 5.928647775969391e-06\n",
      "VAL: EPOCH 400/1000 | BATCH 3/8 | LOSS: 6.054517939446669e-06\n",
      "VAL: EPOCH 400/1000 | BATCH 4/8 | LOSS: 6.012835820001783e-06\n",
      "VAL: EPOCH 400/1000 | BATCH 5/8 | LOSS: 6.037061439201352e-06\n",
      "VAL: EPOCH 400/1000 | BATCH 6/8 | LOSS: 5.942097037145036e-06\n",
      "VAL: EPOCH 400/1000 | BATCH 7/8 | LOSS: 5.931320004037843e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 0/71 | LOSS: 5.449732725537615e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 1/71 | LOSS: 5.4438403367385035e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 2/71 | LOSS: 5.3291860240278766e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 3/71 | LOSS: 5.721752131648827e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 4/71 | LOSS: 5.371280894905794e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 5/71 | LOSS: 5.43546381474395e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 6/71 | LOSS: 5.509760025493701e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 7/71 | LOSS: 5.360486738936743e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 8/71 | LOSS: 5.270089332043426e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 9/71 | LOSS: 5.098321139485051e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 10/71 | LOSS: 5.037435151032034e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 11/71 | LOSS: 5.016254874590231e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 12/71 | LOSS: 5.0036599777041165e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 13/71 | LOSS: 4.934851874947656e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 14/71 | LOSS: 4.979460345566622e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 15/71 | LOSS: 5.033205994209311e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 16/71 | LOSS: 4.925457112810417e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 17/71 | LOSS: 4.827893096464524e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 18/71 | LOSS: 4.771249474575406e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 19/71 | LOSS: 4.757361568863416e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 20/71 | LOSS: 4.7835959278364454e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 21/71 | LOSS: 4.812190178943803e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 22/71 | LOSS: 4.807474486211583e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 23/71 | LOSS: 4.868175655777425e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 24/71 | LOSS: 4.869180293098907e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 25/71 | LOSS: 4.861490503152219e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 26/71 | LOSS: 4.889306039870091e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 27/71 | LOSS: 4.950011925432461e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 28/71 | LOSS: 4.934384041289691e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 29/71 | LOSS: 4.971988172049654e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 30/71 | LOSS: 4.951806372886377e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 31/71 | LOSS: 4.943472980301067e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 32/71 | LOSS: 4.938811047616058e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 33/71 | LOSS: 4.931054337141498e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 34/71 | LOSS: 4.900634140929989e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 35/71 | LOSS: 4.892529286987863e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 36/71 | LOSS: 4.863134606509718e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 37/71 | LOSS: 4.835199296107212e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 38/71 | LOSS: 4.836022100836346e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 39/71 | LOSS: 4.824241483447622e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 40/71 | LOSS: 4.7838286774072556e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 41/71 | LOSS: 4.808393030197337e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 42/71 | LOSS: 4.773555733557915e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 43/71 | LOSS: 4.750998254042894e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 44/71 | LOSS: 4.731057242679526e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 45/71 | LOSS: 4.733253782116821e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 46/71 | LOSS: 4.765559729190848e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 47/71 | LOSS: 4.752273236855824e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 48/71 | LOSS: 4.757056089778305e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 49/71 | LOSS: 4.761360728480213e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 50/71 | LOSS: 4.747780073611305e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 51/71 | LOSS: 4.746937655944259e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 52/71 | LOSS: 4.777351370717471e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 53/71 | LOSS: 4.778893488340448e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 54/71 | LOSS: 4.759948565565124e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 55/71 | LOSS: 4.767357929884903e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 56/71 | LOSS: 4.769192859735963e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 57/71 | LOSS: 4.76105949160385e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 58/71 | LOSS: 4.766809143355635e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 59/71 | LOSS: 4.756730288590916e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 60/71 | LOSS: 4.752471553371431e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 61/71 | LOSS: 4.751137929991772e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 62/71 | LOSS: 4.7448539438240305e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 63/71 | LOSS: 4.7225361896607865e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 64/71 | LOSS: 4.736135703681682e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 65/71 | LOSS: 4.729156892084795e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 66/71 | LOSS: 4.717697041752529e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 67/71 | LOSS: 4.73966827699127e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 68/71 | LOSS: 4.7350313911590715e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 69/71 | LOSS: 4.735034956736075e-06\n",
      "TRAIN: EPOCH 401/1000 | BATCH 70/71 | LOSS: 4.740114401473375e-06\n",
      "VAL: EPOCH 401/1000 | BATCH 0/8 | LOSS: 4.644380169338547e-06\n",
      "VAL: EPOCH 401/1000 | BATCH 1/8 | LOSS: 4.43826229457045e-06\n",
      "VAL: EPOCH 401/1000 | BATCH 2/8 | LOSS: 4.4700360983066885e-06\n",
      "VAL: EPOCH 401/1000 | BATCH 3/8 | LOSS: 4.516196781878534e-06\n",
      "VAL: EPOCH 401/1000 | BATCH 4/8 | LOSS: 4.433134017745033e-06\n",
      "VAL: EPOCH 401/1000 | BATCH 5/8 | LOSS: 4.282573703070132e-06\n",
      "VAL: EPOCH 401/1000 | BATCH 6/8 | LOSS: 4.183809908551796e-06\n",
      "VAL: EPOCH 401/1000 | BATCH 7/8 | LOSS: 4.059855285731828e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 0/71 | LOSS: 4.492814241530141e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 1/71 | LOSS: 5.093050049254089e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 2/71 | LOSS: 4.960588285030099e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 3/71 | LOSS: 5.3748398158859345e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 4/71 | LOSS: 5.348775175662013e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 5/71 | LOSS: 5.615755071630701e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 6/71 | LOSS: 5.425451750592661e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 7/71 | LOSS: 5.4975274110802275e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 8/71 | LOSS: 5.309707679164906e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 9/71 | LOSS: 5.385667873269995e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 10/71 | LOSS: 5.2456725411916105e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 11/71 | LOSS: 5.173558558150641e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 12/71 | LOSS: 5.1584357746693195e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 13/71 | LOSS: 5.109548672927693e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 14/71 | LOSS: 5.0035190421719255e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 15/71 | LOSS: 4.990283144934438e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 16/71 | LOSS: 4.931855876159662e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 17/71 | LOSS: 4.94448040121319e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 18/71 | LOSS: 4.979474899875258e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 19/71 | LOSS: 4.933836726195295e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 20/71 | LOSS: 4.887579009283356e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 21/71 | LOSS: 4.8314174447031375e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 22/71 | LOSS: 4.821807855794601e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 23/71 | LOSS: 4.811679123880215e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 24/71 | LOSS: 4.782494197570486e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 25/71 | LOSS: 4.7416781923787384e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 26/71 | LOSS: 4.707578328735609e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 27/71 | LOSS: 4.705358670824873e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 28/71 | LOSS: 4.697130956127035e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 29/71 | LOSS: 4.683006303215128e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 30/71 | LOSS: 4.669605415631761e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 31/71 | LOSS: 4.692423573260385e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 32/71 | LOSS: 4.652868896796876e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 33/71 | LOSS: 4.655973444802358e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 34/71 | LOSS: 4.630553621609579e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 35/71 | LOSS: 4.6426068883597e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 36/71 | LOSS: 4.685213021681143e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 37/71 | LOSS: 4.671423354059363e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 38/71 | LOSS: 4.749234723278324e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 39/71 | LOSS: 4.759028189482706e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 40/71 | LOSS: 4.815450249846369e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 41/71 | LOSS: 4.816466706697178e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 42/71 | LOSS: 4.895810661764699e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 43/71 | LOSS: 4.887825169093545e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 44/71 | LOSS: 4.8970151561257406e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 45/71 | LOSS: 4.9174081408162475e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 46/71 | LOSS: 4.912108158563608e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 47/71 | LOSS: 4.927876948575734e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 48/71 | LOSS: 4.922065156687477e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 49/71 | LOSS: 4.916316952403577e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 50/71 | LOSS: 4.905385224571778e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 51/71 | LOSS: 4.882070820363319e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 52/71 | LOSS: 4.883141422019349e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 53/71 | LOSS: 4.871697169787863e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 54/71 | LOSS: 4.87386633639372e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 55/71 | LOSS: 4.869836694914349e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 56/71 | LOSS: 4.851355554005989e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 57/71 | LOSS: 4.835304441163818e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 58/71 | LOSS: 4.8335392436410215e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 59/71 | LOSS: 4.815034628791182e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 60/71 | LOSS: 4.813273510647333e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 61/71 | LOSS: 4.8039629812211065e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 62/71 | LOSS: 4.807183579090171e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 63/71 | LOSS: 4.7772540163748545e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 64/71 | LOSS: 4.777289006177363e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 65/71 | LOSS: 4.765000081726722e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 66/71 | LOSS: 4.771751294885771e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 67/71 | LOSS: 4.767114583447598e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 68/71 | LOSS: 4.761193754282009e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 69/71 | LOSS: 4.750449401009454e-06\n",
      "TRAIN: EPOCH 402/1000 | BATCH 70/71 | LOSS: 4.734019992764856e-06\n",
      "VAL: EPOCH 402/1000 | BATCH 0/8 | LOSS: 6.543251402035821e-06\n",
      "VAL: EPOCH 402/1000 | BATCH 1/8 | LOSS: 6.165807008073898e-06\n",
      "VAL: EPOCH 402/1000 | BATCH 2/8 | LOSS: 6.205435965966899e-06\n",
      "VAL: EPOCH 402/1000 | BATCH 3/8 | LOSS: 6.102795509832504e-06\n",
      "VAL: EPOCH 402/1000 | BATCH 4/8 | LOSS: 6.0744220718333965e-06\n",
      "VAL: EPOCH 402/1000 | BATCH 5/8 | LOSS: 5.84187015798913e-06\n",
      "VAL: EPOCH 402/1000 | BATCH 6/8 | LOSS: 5.834119812269429e-06\n",
      "VAL: EPOCH 402/1000 | BATCH 7/8 | LOSS: 5.7535735322744586e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 0/71 | LOSS: 6.846241831226507e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 1/71 | LOSS: 5.552475840886473e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 2/71 | LOSS: 5.111695524343911e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 3/71 | LOSS: 4.884205736743752e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 4/71 | LOSS: 5.013250483898446e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 5/71 | LOSS: 4.9401182119860705e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 6/71 | LOSS: 4.791020793553409e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 7/71 | LOSS: 5.050554534591356e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 8/71 | LOSS: 5.042228773769845e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 9/71 | LOSS: 5.073859165349859e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 10/71 | LOSS: 5.132544960030248e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 11/71 | LOSS: 5.01888655435323e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 12/71 | LOSS: 4.994200480723521e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 13/71 | LOSS: 5.000831281774611e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 14/71 | LOSS: 4.995655520663907e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 15/71 | LOSS: 5.036544678205246e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 16/71 | LOSS: 5.105063524645041e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 17/71 | LOSS: 5.046592276711534e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 18/71 | LOSS: 4.965705293694842e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 19/71 | LOSS: 4.9891554454006835e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 20/71 | LOSS: 4.930330695580258e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 21/71 | LOSS: 4.863540890586261e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 22/71 | LOSS: 4.84394178349011e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 23/71 | LOSS: 4.800055241579078e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 24/71 | LOSS: 4.801094282811391e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 25/71 | LOSS: 4.8018065924155125e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 26/71 | LOSS: 4.809754323610428e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 27/71 | LOSS: 4.834562616906624e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 28/71 | LOSS: 4.8148367176477694e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 29/71 | LOSS: 4.867501161243126e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 30/71 | LOSS: 4.8803853822450505e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 31/71 | LOSS: 4.90683969900374e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 32/71 | LOSS: 4.907952030187131e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 33/71 | LOSS: 4.989458796658523e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 34/71 | LOSS: 5.0171714389892125e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 35/71 | LOSS: 4.976247579533164e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 36/71 | LOSS: 4.970514885064551e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 37/71 | LOSS: 4.9755678482573495e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 38/71 | LOSS: 4.961920863840929e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 39/71 | LOSS: 4.9747009768452696e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 40/71 | LOSS: 4.974723368854384e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 41/71 | LOSS: 5.011343974036295e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 42/71 | LOSS: 4.982636430086555e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 43/71 | LOSS: 4.997342387014214e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 44/71 | LOSS: 5.011091914437e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 45/71 | LOSS: 4.9897011560618845e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 46/71 | LOSS: 5.01662135862484e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 47/71 | LOSS: 4.995614114970219e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 48/71 | LOSS: 4.986835165254888e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 49/71 | LOSS: 4.97937506679591e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 50/71 | LOSS: 4.983380503129892e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 51/71 | LOSS: 4.994521816570341e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 52/71 | LOSS: 4.990975330999716e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 53/71 | LOSS: 5.014759688458946e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 54/71 | LOSS: 5.037293236786023e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 55/71 | LOSS: 5.018071456268964e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 56/71 | LOSS: 5.0200975565211705e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 57/71 | LOSS: 5.0284241116718605e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 58/71 | LOSS: 5.024897526231667e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 59/71 | LOSS: 5.0376250214867465e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 60/71 | LOSS: 5.057335094355293e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 61/71 | LOSS: 5.055858533494522e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 62/71 | LOSS: 5.065974391982309e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 63/71 | LOSS: 5.068089638626816e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 64/71 | LOSS: 5.0866798014444965e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 65/71 | LOSS: 5.103007007771036e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 66/71 | LOSS: 5.0936487343224415e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 67/71 | LOSS: 5.105172948226348e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 68/71 | LOSS: 5.104272633906331e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 69/71 | LOSS: 5.0889005738749155e-06\n",
      "TRAIN: EPOCH 403/1000 | BATCH 70/71 | LOSS: 5.083451600845544e-06\n",
      "VAL: EPOCH 403/1000 | BATCH 0/8 | LOSS: 4.830943453271175e-06\n",
      "VAL: EPOCH 403/1000 | BATCH 1/8 | LOSS: 4.536198730420438e-06\n",
      "VAL: EPOCH 403/1000 | BATCH 2/8 | LOSS: 4.54041401098948e-06\n",
      "VAL: EPOCH 403/1000 | BATCH 3/8 | LOSS: 4.6090622163319495e-06\n",
      "VAL: EPOCH 403/1000 | BATCH 4/8 | LOSS: 4.490346873353701e-06\n",
      "VAL: EPOCH 403/1000 | BATCH 5/8 | LOSS: 4.365441517014308e-06\n",
      "VAL: EPOCH 403/1000 | BATCH 6/8 | LOSS: 4.25320068383631e-06\n",
      "VAL: EPOCH 403/1000 | BATCH 7/8 | LOSS: 4.085811696086239e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 0/71 | LOSS: 4.794836058863439e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 1/71 | LOSS: 5.3520807341556065e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 2/71 | LOSS: 5.722936293750536e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 3/71 | LOSS: 5.355183475330705e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 4/71 | LOSS: 5.495418736245483e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 5/71 | LOSS: 5.497706752066733e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 6/71 | LOSS: 5.507048367040365e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 7/71 | LOSS: 5.483858558363863e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 8/71 | LOSS: 5.4835280833584775e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 9/71 | LOSS: 5.534856472877436e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 10/71 | LOSS: 5.380773845073153e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 11/71 | LOSS: 5.301496344145562e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 12/71 | LOSS: 5.2962386689614505e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 13/71 | LOSS: 5.227698336187002e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 14/71 | LOSS: 5.157574044763654e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 15/71 | LOSS: 5.066760380145752e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 16/71 | LOSS: 4.95280446824943e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 17/71 | LOSS: 4.943720025746896e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 18/71 | LOSS: 4.907843156998799e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 19/71 | LOSS: 4.845338571612956e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 20/71 | LOSS: 4.8426662553018625e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 21/71 | LOSS: 4.81745463399751e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 22/71 | LOSS: 4.800640276105538e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 23/71 | LOSS: 4.78445239574891e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 24/71 | LOSS: 4.805002245120704e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 25/71 | LOSS: 4.793391134136562e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 26/71 | LOSS: 4.773239888217412e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 27/71 | LOSS: 4.743873140406711e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 28/71 | LOSS: 4.715361777424073e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 29/71 | LOSS: 4.6896476760593945e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 30/71 | LOSS: 4.6497223651855284e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 31/71 | LOSS: 4.62478929819099e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 32/71 | LOSS: 4.60374083173299e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 33/71 | LOSS: 4.606353018864277e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 34/71 | LOSS: 4.584010548569495e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 35/71 | LOSS: 4.59598817946648e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 36/71 | LOSS: 4.591482531789036e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 37/71 | LOSS: 4.595896538567583e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 38/71 | LOSS: 4.626202075413834e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 39/71 | LOSS: 4.6092426600807815e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 40/71 | LOSS: 4.63563426864112e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 41/71 | LOSS: 4.616492982937156e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 42/71 | LOSS: 4.637437636712241e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 43/71 | LOSS: 4.644616327905996e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 44/71 | LOSS: 4.666456657610575e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 45/71 | LOSS: 4.677987530869912e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 46/71 | LOSS: 4.686029772896523e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 47/71 | LOSS: 4.665955065282408e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 48/71 | LOSS: 4.714578510954327e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 49/71 | LOSS: 4.707427265202568e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 50/71 | LOSS: 4.705077255458295e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 51/71 | LOSS: 4.689787142699341e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 52/71 | LOSS: 4.7348714214988505e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 53/71 | LOSS: 4.741012730846705e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 54/71 | LOSS: 4.78149869526202e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 55/71 | LOSS: 4.788960053149692e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 56/71 | LOSS: 4.830021724270059e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 57/71 | LOSS: 4.819122276283702e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 58/71 | LOSS: 4.8240996174002755e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 59/71 | LOSS: 4.848805023508854e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 60/71 | LOSS: 4.848244215594843e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 61/71 | LOSS: 4.903272328661959e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 62/71 | LOSS: 4.886912750277718e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 63/71 | LOSS: 4.915966623997292e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 64/71 | LOSS: 4.934937977360767e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 65/71 | LOSS: 4.9397686800809675e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 66/71 | LOSS: 4.921148871941493e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 67/71 | LOSS: 4.9249184545421005e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 68/71 | LOSS: 4.933480067188941e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 69/71 | LOSS: 4.909576208384741e-06\n",
      "TRAIN: EPOCH 404/1000 | BATCH 70/71 | LOSS: 4.924203814982935e-06\n",
      "VAL: EPOCH 404/1000 | BATCH 0/8 | LOSS: 6.944948836462572e-06\n",
      "VAL: EPOCH 404/1000 | BATCH 1/8 | LOSS: 6.690689815513906e-06\n",
      "VAL: EPOCH 404/1000 | BATCH 2/8 | LOSS: 6.610219164334315e-06\n",
      "VAL: EPOCH 404/1000 | BATCH 3/8 | LOSS: 6.741994070580404e-06\n",
      "VAL: EPOCH 404/1000 | BATCH 4/8 | LOSS: 6.625710466323653e-06\n",
      "VAL: EPOCH 404/1000 | BATCH 5/8 | LOSS: 6.322233806107154e-06\n",
      "VAL: EPOCH 404/1000 | BATCH 6/8 | LOSS: 6.209558770413943e-06\n",
      "VAL: EPOCH 404/1000 | BATCH 7/8 | LOSS: 6.058333838154795e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 0/71 | LOSS: 4.74403532280121e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 1/71 | LOSS: 5.166140908841044e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 2/71 | LOSS: 4.968204999992547e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 3/71 | LOSS: 5.161098783901252e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 4/71 | LOSS: 5.523837808141252e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 5/71 | LOSS: 5.566261809993496e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 6/71 | LOSS: 5.6868895756321895e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 7/71 | LOSS: 5.583400422892737e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 8/71 | LOSS: 5.62663298195629e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 9/71 | LOSS: 5.593474224951933e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 10/71 | LOSS: 5.653950665873708e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 11/71 | LOSS: 5.550200739889988e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 12/71 | LOSS: 5.503476411551954e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 13/71 | LOSS: 5.55322594664176e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 14/71 | LOSS: 5.499655162566341e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 15/71 | LOSS: 5.458984219330887e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 16/71 | LOSS: 5.416102018952027e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 17/71 | LOSS: 5.355079237132385e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 18/71 | LOSS: 5.28054167596948e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 19/71 | LOSS: 5.217787088440673e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 20/71 | LOSS: 5.131942392229878e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 21/71 | LOSS: 5.094663265481358e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 22/71 | LOSS: 5.110176437523004e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 23/71 | LOSS: 5.065016997226242e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 24/71 | LOSS: 5.042053471697727e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 25/71 | LOSS: 5.030694882617144e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 26/71 | LOSS: 5.001383501453178e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 27/71 | LOSS: 4.998971527519254e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 28/71 | LOSS: 4.980786798114423e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 29/71 | LOSS: 4.974810372004867e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 30/71 | LOSS: 4.9402077116434565e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 31/71 | LOSS: 4.960765963346603e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 32/71 | LOSS: 4.938619743673674e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 33/71 | LOSS: 4.93657490979411e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 34/71 | LOSS: 4.935257415386982e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 35/71 | LOSS: 4.961423049583876e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 36/71 | LOSS: 4.946641515621073e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 37/71 | LOSS: 4.936903654819808e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 38/71 | LOSS: 4.984597767477867e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 39/71 | LOSS: 4.986845829080266e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 40/71 | LOSS: 5.00831320983352e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 41/71 | LOSS: 4.996168627258157e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 42/71 | LOSS: 4.986422143617326e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 43/71 | LOSS: 4.975044796331283e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 44/71 | LOSS: 4.963799347024178e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 45/71 | LOSS: 4.986774110480126e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 46/71 | LOSS: 4.986194024705253e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 47/71 | LOSS: 5.017317230719224e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 48/71 | LOSS: 5.039832354223294e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 49/71 | LOSS: 5.043105984441354e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 50/71 | LOSS: 5.038759532979777e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 51/71 | LOSS: 5.072839791444914e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 52/71 | LOSS: 5.067197126097524e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 53/71 | LOSS: 5.107172270342104e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 54/71 | LOSS: 5.103421302589016e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 55/71 | LOSS: 5.1116352243038165e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 56/71 | LOSS: 5.16559493278915e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 57/71 | LOSS: 5.143541502234414e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 58/71 | LOSS: 5.2316008805064484e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 59/71 | LOSS: 5.238308449406759e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 60/71 | LOSS: 5.282161912530638e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 61/71 | LOSS: 5.303134120397097e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 62/71 | LOSS: 5.293939581846759e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 63/71 | LOSS: 5.297335953002857e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 64/71 | LOSS: 5.287264698731283e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 65/71 | LOSS: 5.285082269520816e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 66/71 | LOSS: 5.266835383980521e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 67/71 | LOSS: 5.2538836210741836e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 68/71 | LOSS: 5.2330286685584886e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 69/71 | LOSS: 5.214235787726855e-06\n",
      "TRAIN: EPOCH 405/1000 | BATCH 70/71 | LOSS: 5.249758910202883e-06\n",
      "VAL: EPOCH 405/1000 | BATCH 0/8 | LOSS: 7.945091056171805e-06\n",
      "VAL: EPOCH 405/1000 | BATCH 1/8 | LOSS: 8.270342732430436e-06\n",
      "VAL: EPOCH 405/1000 | BATCH 2/8 | LOSS: 8.77575863948247e-06\n",
      "VAL: EPOCH 405/1000 | BATCH 3/8 | LOSS: 8.686325145390583e-06\n",
      "VAL: EPOCH 405/1000 | BATCH 4/8 | LOSS: 8.728144348424393e-06\n",
      "VAL: EPOCH 405/1000 | BATCH 5/8 | LOSS: 8.782728097382156e-06\n",
      "VAL: EPOCH 405/1000 | BATCH 6/8 | LOSS: 8.662694589085212e-06\n",
      "VAL: EPOCH 405/1000 | BATCH 7/8 | LOSS: 8.848486686474644e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 0/71 | LOSS: 8.422727660217788e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 1/71 | LOSS: 6.8272254338808125e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 2/71 | LOSS: 6.1455919725024915e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 3/71 | LOSS: 6.1509866782216704e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 4/71 | LOSS: 6.037040566297946e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 5/71 | LOSS: 5.933362444920931e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 6/71 | LOSS: 5.665065990407518e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 7/71 | LOSS: 5.5319057423730555e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 8/71 | LOSS: 5.544588980733857e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 9/71 | LOSS: 5.6572939229226906e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 10/71 | LOSS: 5.637064201767895e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 11/71 | LOSS: 5.554738663704484e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 12/71 | LOSS: 5.600144294686078e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 13/71 | LOSS: 5.491553500697981e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 14/71 | LOSS: 5.6620748485632555e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 15/71 | LOSS: 5.600423889973172e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 16/71 | LOSS: 5.593090677661273e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 17/71 | LOSS: 5.592942165498647e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 18/71 | LOSS: 5.575122486334294e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 19/71 | LOSS: 5.48896628060902e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 20/71 | LOSS: 5.49651169768324e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 21/71 | LOSS: 5.510653202443129e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 22/71 | LOSS: 5.500884229980369e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 23/71 | LOSS: 5.533298425082951e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 24/71 | LOSS: 5.495409550348995e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 25/71 | LOSS: 5.4656073084166e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 26/71 | LOSS: 5.431747568865022e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 27/71 | LOSS: 5.383917190166747e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 28/71 | LOSS: 5.351592225734153e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 29/71 | LOSS: 5.348498765063899e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 30/71 | LOSS: 5.320475074474798e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 31/71 | LOSS: 5.279942953961836e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 32/71 | LOSS: 5.251930994718075e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 33/71 | LOSS: 5.241255795387106e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 34/71 | LOSS: 5.231325342590156e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 35/71 | LOSS: 5.215608969895887e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 36/71 | LOSS: 5.19305151740539e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 37/71 | LOSS: 5.173410607695635e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 38/71 | LOSS: 5.151820425033713e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 39/71 | LOSS: 5.1272549171699214e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 40/71 | LOSS: 5.0825113403999885e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 41/71 | LOSS: 5.066590617952031e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 42/71 | LOSS: 5.064074758952619e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 43/71 | LOSS: 5.070553284399053e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 44/71 | LOSS: 5.0439936785551255e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 45/71 | LOSS: 5.009577544905883e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 46/71 | LOSS: 5.036182436866666e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 47/71 | LOSS: 5.019724483190657e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 48/71 | LOSS: 5.0232262060026e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 49/71 | LOSS: 5.003719115848071e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 50/71 | LOSS: 4.9820524313248575e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 51/71 | LOSS: 4.958805964004288e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 52/71 | LOSS: 4.944970090225567e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 53/71 | LOSS: 4.926222328760088e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 54/71 | LOSS: 4.9335928782635495e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 55/71 | LOSS: 4.908713654978263e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 56/71 | LOSS: 4.925402846997154e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 57/71 | LOSS: 4.9289025167221055e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 58/71 | LOSS: 4.900907635786452e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 59/71 | LOSS: 4.887587169832841e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 60/71 | LOSS: 4.867624828294225e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 61/71 | LOSS: 4.867678567719732e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 62/71 | LOSS: 4.846204731057653e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 63/71 | LOSS: 4.851537333649958e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 64/71 | LOSS: 4.840928599710773e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 65/71 | LOSS: 4.837212552019716e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 66/71 | LOSS: 4.8321580495090234e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 67/71 | LOSS: 4.836270397176764e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 68/71 | LOSS: 4.821827601056388e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 69/71 | LOSS: 4.809445213790501e-06\n",
      "TRAIN: EPOCH 406/1000 | BATCH 70/71 | LOSS: 4.804274327321303e-06\n",
      "VAL: EPOCH 406/1000 | BATCH 0/8 | LOSS: 5.230744136497378e-06\n",
      "VAL: EPOCH 406/1000 | BATCH 1/8 | LOSS: 4.848446224059444e-06\n",
      "VAL: EPOCH 406/1000 | BATCH 2/8 | LOSS: 4.8584317179726595e-06\n",
      "VAL: EPOCH 406/1000 | BATCH 3/8 | LOSS: 4.814404178432596e-06\n",
      "VAL: EPOCH 406/1000 | BATCH 4/8 | LOSS: 4.7038736738613805e-06\n",
      "VAL: EPOCH 406/1000 | BATCH 5/8 | LOSS: 4.489974836966819e-06\n",
      "VAL: EPOCH 406/1000 | BATCH 6/8 | LOSS: 4.335761494595707e-06\n",
      "VAL: EPOCH 406/1000 | BATCH 7/8 | LOSS: 4.190821357497043e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 0/71 | LOSS: 4.857949534198269e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 1/71 | LOSS: 4.916000762023032e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 2/71 | LOSS: 5.201438549799302e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 3/71 | LOSS: 4.937893436363083e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 4/71 | LOSS: 4.986063049727818e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 5/71 | LOSS: 4.905100846978409e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 6/71 | LOSS: 4.903818080492783e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 7/71 | LOSS: 4.908144148885185e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 8/71 | LOSS: 5.218477427762183e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 9/71 | LOSS: 5.114483974466566e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 10/71 | LOSS: 5.1797065184060065e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 11/71 | LOSS: 5.2287626507071154e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 12/71 | LOSS: 5.225892402030206e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 13/71 | LOSS: 5.271714599075494e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 14/71 | LOSS: 5.316570453336074e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 15/71 | LOSS: 5.2419800624647905e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 16/71 | LOSS: 5.197916272984858e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 17/71 | LOSS: 5.182278881774336e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 18/71 | LOSS: 5.157996527391094e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 19/71 | LOSS: 5.129798000780283e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 20/71 | LOSS: 5.051348991747502e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 21/71 | LOSS: 5.1271397503362755e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 22/71 | LOSS: 5.1241342352232495e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 23/71 | LOSS: 5.189271405470208e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 24/71 | LOSS: 5.201808453421108e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 25/71 | LOSS: 5.3570588184251955e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 26/71 | LOSS: 5.4076141606384666e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 27/71 | LOSS: 5.433045982629535e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 28/71 | LOSS: 5.547096516865763e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 29/71 | LOSS: 5.539340327231912e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 30/71 | LOSS: 5.511240517315004e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 31/71 | LOSS: 5.546342123352588e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 32/71 | LOSS: 5.591452022737499e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 33/71 | LOSS: 5.53582003703923e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 34/71 | LOSS: 5.5576940342559415e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 35/71 | LOSS: 5.6054490692582076e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 36/71 | LOSS: 5.556839436689483e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 37/71 | LOSS: 5.532737980885079e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 38/71 | LOSS: 5.569227893759559e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 39/71 | LOSS: 5.571982143237619e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 40/71 | LOSS: 5.533286179254391e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 41/71 | LOSS: 5.5632523407482605e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 42/71 | LOSS: 5.568991809923318e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 43/71 | LOSS: 5.535934524852399e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 44/71 | LOSS: 5.5444556841798156e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 45/71 | LOSS: 5.547848366444194e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 46/71 | LOSS: 5.512498198205573e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 47/71 | LOSS: 5.5049597970461645e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 48/71 | LOSS: 5.5039486095647215e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 49/71 | LOSS: 5.512527827704617e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 50/71 | LOSS: 5.506559840368402e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 51/71 | LOSS: 5.566913356548097e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 52/71 | LOSS: 5.5482654724468344e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 53/71 | LOSS: 5.533663393517027e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 54/71 | LOSS: 5.5236940873080375e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 55/71 | LOSS: 5.535695927554636e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 56/71 | LOSS: 5.51580303596111e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 57/71 | LOSS: 5.510937151892245e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 58/71 | LOSS: 5.492242010759429e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 59/71 | LOSS: 5.474278346658442e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 60/71 | LOSS: 5.439999913356647e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 61/71 | LOSS: 5.412930638539688e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 62/71 | LOSS: 5.380461693170622e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 63/71 | LOSS: 5.375158369957944e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 64/71 | LOSS: 5.344326693655323e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 65/71 | LOSS: 5.335696479932901e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 66/71 | LOSS: 5.33839250971114e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 67/71 | LOSS: 5.3325272768447845e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 68/71 | LOSS: 5.354905331420224e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 69/71 | LOSS: 5.336399619539277e-06\n",
      "TRAIN: EPOCH 407/1000 | BATCH 70/71 | LOSS: 5.3427700543541274e-06\n",
      "VAL: EPOCH 407/1000 | BATCH 0/8 | LOSS: 4.4839516704087146e-06\n",
      "VAL: EPOCH 407/1000 | BATCH 1/8 | LOSS: 4.52112590210163e-06\n",
      "VAL: EPOCH 407/1000 | BATCH 2/8 | LOSS: 4.616272841910056e-06\n",
      "VAL: EPOCH 407/1000 | BATCH 3/8 | LOSS: 4.6537054458895e-06\n",
      "VAL: EPOCH 407/1000 | BATCH 4/8 | LOSS: 4.584863745549228e-06\n",
      "VAL: EPOCH 407/1000 | BATCH 5/8 | LOSS: 4.443705241404435e-06\n",
      "VAL: EPOCH 407/1000 | BATCH 6/8 | LOSS: 4.317166648044284e-06\n",
      "VAL: EPOCH 407/1000 | BATCH 7/8 | LOSS: 4.2499381152083515e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 0/71 | LOSS: 5.24582173966337e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 1/71 | LOSS: 5.088761099614203e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 2/71 | LOSS: 4.730448078286524e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 3/71 | LOSS: 4.998589361093764e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 4/71 | LOSS: 4.807693403563462e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 5/71 | LOSS: 4.880502425900583e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 6/71 | LOSS: 4.6803587403181675e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 7/71 | LOSS: 4.710356392934045e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 8/71 | LOSS: 4.689007457475074e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 9/71 | LOSS: 4.622058577297139e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 10/71 | LOSS: 4.65235466022436e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 11/71 | LOSS: 4.672880739538717e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 12/71 | LOSS: 4.583529446860596e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 13/71 | LOSS: 4.556514146055893e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 14/71 | LOSS: 4.542707423145961e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 15/71 | LOSS: 4.573980532995847e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 16/71 | LOSS: 4.5700357451791846e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 17/71 | LOSS: 4.599439838178417e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 18/71 | LOSS: 4.549672690987599e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 19/71 | LOSS: 4.5847611318095e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 20/71 | LOSS: 4.60220055520906e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 21/71 | LOSS: 4.645447668074419e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 22/71 | LOSS: 4.595202812118299e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 23/71 | LOSS: 4.612382658327381e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 24/71 | LOSS: 4.617437471097219e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 25/71 | LOSS: 4.5956910088744526e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 26/71 | LOSS: 4.576225457505508e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 27/71 | LOSS: 4.586993725297361e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 28/71 | LOSS: 4.598693090758451e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 29/71 | LOSS: 4.618998574793901e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 30/71 | LOSS: 4.7203810452394404e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 31/71 | LOSS: 4.707891712030232e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 32/71 | LOSS: 4.74845828510738e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 33/71 | LOSS: 4.745602081236618e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 34/71 | LOSS: 4.774381477545831e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 35/71 | LOSS: 4.743204480443334e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 36/71 | LOSS: 4.758796600022784e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 37/71 | LOSS: 4.798316988279494e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 38/71 | LOSS: 4.7860404813549485e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 39/71 | LOSS: 4.759687249134003e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 40/71 | LOSS: 4.752477895924197e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 41/71 | LOSS: 4.744465925233228e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 42/71 | LOSS: 4.71643721370086e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 43/71 | LOSS: 4.730919127640928e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 44/71 | LOSS: 4.7035445226760605e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 45/71 | LOSS: 4.695411793482376e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 46/71 | LOSS: 4.678364759572705e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 47/71 | LOSS: 4.691721836517597e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 48/71 | LOSS: 4.6670047827412455e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 49/71 | LOSS: 4.657940849028819e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 50/71 | LOSS: 4.647180989480808e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 51/71 | LOSS: 4.635372258340235e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 52/71 | LOSS: 4.6245840604689945e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 53/71 | LOSS: 4.629696886863881e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 54/71 | LOSS: 4.61363289435791e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 55/71 | LOSS: 4.602763628521903e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 56/71 | LOSS: 4.60327663731413e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 57/71 | LOSS: 4.594804863700119e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 58/71 | LOSS: 4.591828305280414e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 59/71 | LOSS: 4.571836393552076e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 60/71 | LOSS: 4.558013888974349e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 61/71 | LOSS: 4.551116658589191e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 62/71 | LOSS: 4.556886764591567e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 63/71 | LOSS: 4.569118658537263e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 64/71 | LOSS: 4.561192256985053e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 65/71 | LOSS: 4.551509405383982e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 66/71 | LOSS: 4.557617340828101e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 67/71 | LOSS: 4.558229514205938e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 68/71 | LOSS: 4.556881236382743e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 69/71 | LOSS: 4.569167809417455e-06\n",
      "TRAIN: EPOCH 408/1000 | BATCH 70/71 | LOSS: 4.543502747867638e-06\n",
      "VAL: EPOCH 408/1000 | BATCH 0/8 | LOSS: 4.919233106193133e-06\n",
      "VAL: EPOCH 408/1000 | BATCH 1/8 | LOSS: 4.899988880424644e-06\n",
      "VAL: EPOCH 408/1000 | BATCH 2/8 | LOSS: 5.110809221757033e-06\n",
      "VAL: EPOCH 408/1000 | BATCH 3/8 | LOSS: 5.117006935506652e-06\n",
      "VAL: EPOCH 408/1000 | BATCH 4/8 | LOSS: 5.068205155112082e-06\n",
      "VAL: EPOCH 408/1000 | BATCH 5/8 | LOSS: 4.974677040081588e-06\n",
      "VAL: EPOCH 408/1000 | BATCH 6/8 | LOSS: 4.830913894693367e-06\n",
      "VAL: EPOCH 408/1000 | BATCH 7/8 | LOSS: 4.796668577000673e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 0/71 | LOSS: 4.155878286837833e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 1/71 | LOSS: 4.566255483950954e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 2/71 | LOSS: 4.5564708367843805e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 3/71 | LOSS: 4.572425041260431e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 4/71 | LOSS: 4.636996163753793e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 5/71 | LOSS: 4.4991262863428956e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 6/71 | LOSS: 4.3439415873893135e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 7/71 | LOSS: 4.7598031187590095e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 8/71 | LOSS: 4.697561659365117e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 9/71 | LOSS: 4.715041995950742e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 10/71 | LOSS: 4.689718314859254e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 11/71 | LOSS: 4.764641062138253e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 12/71 | LOSS: 4.772637951716136e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 13/71 | LOSS: 4.7541867453609095e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 14/71 | LOSS: 4.890708593544938e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 15/71 | LOSS: 4.899800416069411e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 16/71 | LOSS: 4.886852286076378e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 17/71 | LOSS: 4.944319016431109e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 18/71 | LOSS: 4.940490797659884e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 19/71 | LOSS: 4.992776644030527e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 20/71 | LOSS: 5.037329455739209e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 21/71 | LOSS: 5.1396667376221474e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 22/71 | LOSS: 5.112524120151015e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 23/71 | LOSS: 5.20261822127092e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 24/71 | LOSS: 5.3001061860413755e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 25/71 | LOSS: 5.334925512775394e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 26/71 | LOSS: 5.361822315063802e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 27/71 | LOSS: 5.30682624295358e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 28/71 | LOSS: 5.321405202768551e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 29/71 | LOSS: 5.291825588453018e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 30/71 | LOSS: 5.332817915088174e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 31/71 | LOSS: 5.300351190840047e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 32/71 | LOSS: 5.348100950703321e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 33/71 | LOSS: 5.344516867968623e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 34/71 | LOSS: 5.314926888136792e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 35/71 | LOSS: 5.340084019028129e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 36/71 | LOSS: 5.393446937543214e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 37/71 | LOSS: 5.415696059093247e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 38/71 | LOSS: 5.425159589578517e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 39/71 | LOSS: 5.445230362965958e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 40/71 | LOSS: 5.4361685548295695e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 41/71 | LOSS: 5.470197872152009e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 42/71 | LOSS: 5.420168479251388e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 43/71 | LOSS: 5.51939248535663e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 44/71 | LOSS: 5.521267727696492e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 45/71 | LOSS: 5.529407130582157e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 46/71 | LOSS: 5.535261558914963e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 47/71 | LOSS: 5.508664931615688e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 48/71 | LOSS: 5.546579238713947e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 49/71 | LOSS: 5.518660291272681e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 50/71 | LOSS: 5.539195091627976e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 51/71 | LOSS: 5.528963662562512e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 52/71 | LOSS: 5.527033048359576e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 53/71 | LOSS: 5.4858147004105616e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 54/71 | LOSS: 5.457042966967988e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 55/71 | LOSS: 5.419522612523256e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 56/71 | LOSS: 5.3974513440679355e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 57/71 | LOSS: 5.405334702808087e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 58/71 | LOSS: 5.37884155596272e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 59/71 | LOSS: 5.368505575612895e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 60/71 | LOSS: 5.345161108365215e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 61/71 | LOSS: 5.323927542033949e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 62/71 | LOSS: 5.312229184652559e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 63/71 | LOSS: 5.2841544011528185e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 64/71 | LOSS: 5.271833193686549e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 65/71 | LOSS: 5.262059627250093e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 66/71 | LOSS: 5.250824248982261e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 67/71 | LOSS: 5.265698910213413e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 68/71 | LOSS: 5.2364656589223e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 69/71 | LOSS: 5.213309626534672e-06\n",
      "TRAIN: EPOCH 409/1000 | BATCH 70/71 | LOSS: 5.214284552929597e-06\n",
      "VAL: EPOCH 409/1000 | BATCH 0/8 | LOSS: 4.638778591470327e-06\n",
      "VAL: EPOCH 409/1000 | BATCH 1/8 | LOSS: 5.000181090508704e-06\n",
      "VAL: EPOCH 409/1000 | BATCH 2/8 | LOSS: 5.4237251182106165e-06\n",
      "VAL: EPOCH 409/1000 | BATCH 3/8 | LOSS: 5.5499754125776235e-06\n",
      "VAL: EPOCH 409/1000 | BATCH 4/8 | LOSS: 5.560180397878867e-06\n",
      "VAL: EPOCH 409/1000 | BATCH 5/8 | LOSS: 5.5599513567964704e-06\n",
      "VAL: EPOCH 409/1000 | BATCH 6/8 | LOSS: 5.4864687107120905e-06\n",
      "VAL: EPOCH 409/1000 | BATCH 7/8 | LOSS: 5.487574071594281e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 0/71 | LOSS: 6.04543993176776e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 1/71 | LOSS: 5.330543217496597e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 2/71 | LOSS: 5.128544368441605e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 3/71 | LOSS: 5.582652761404461e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 4/71 | LOSS: 5.3345750529842915e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 5/71 | LOSS: 5.212752209142006e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 6/71 | LOSS: 5.09713806552879e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 7/71 | LOSS: 4.9815395186669775e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 8/71 | LOSS: 4.887721944315773e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 9/71 | LOSS: 4.68975354124268e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 10/71 | LOSS: 4.6957471485339655e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 11/71 | LOSS: 4.637961486272009e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 12/71 | LOSS: 4.594988135627668e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 13/71 | LOSS: 4.569301933900403e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 14/71 | LOSS: 4.502842284637154e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 15/71 | LOSS: 4.458484582414712e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 16/71 | LOSS: 4.482120282452396e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 17/71 | LOSS: 4.472520547298902e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 18/71 | LOSS: 4.482092579469042e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 19/71 | LOSS: 4.433452636476431e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 20/71 | LOSS: 4.421045971303968e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 21/71 | LOSS: 4.379300218137029e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 22/71 | LOSS: 4.36189974528173e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 23/71 | LOSS: 4.356978829870665e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 24/71 | LOSS: 4.340560999480658e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 25/71 | LOSS: 4.330971644496738e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 26/71 | LOSS: 4.359201663343508e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 27/71 | LOSS: 4.323012108540882e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 28/71 | LOSS: 4.349512209955719e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 29/71 | LOSS: 4.3636796741945245e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 30/71 | LOSS: 4.384618412058767e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 31/71 | LOSS: 4.372368344718325e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 32/71 | LOSS: 4.4043039741320005e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 33/71 | LOSS: 4.408708270923799e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 34/71 | LOSS: 4.4312165755400205e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 35/71 | LOSS: 4.423159863462287e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 36/71 | LOSS: 4.45173972169672e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 37/71 | LOSS: 4.4691383652063544e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 38/71 | LOSS: 4.475656723092797e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 39/71 | LOSS: 4.51652572905914e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 40/71 | LOSS: 4.501525697888588e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 41/71 | LOSS: 4.567128301113414e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 42/71 | LOSS: 4.568365104501555e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 43/71 | LOSS: 4.628887573041786e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 44/71 | LOSS: 4.637526778121052e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 45/71 | LOSS: 4.646275076835161e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 46/71 | LOSS: 4.652889184070134e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 47/71 | LOSS: 4.662156257495553e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 48/71 | LOSS: 4.689101169973657e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 49/71 | LOSS: 4.68091554921557e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 50/71 | LOSS: 4.713648404860721e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 51/71 | LOSS: 4.698760602213252e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 52/71 | LOSS: 4.699460296766154e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 53/71 | LOSS: 4.755428474507224e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 54/71 | LOSS: 4.756587106219906e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 55/71 | LOSS: 4.807067296854127e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 56/71 | LOSS: 4.784175189342932e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 57/71 | LOSS: 4.787965724125538e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 58/71 | LOSS: 4.775487033099526e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 59/71 | LOSS: 4.767428113912804e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 60/71 | LOSS: 4.7792415607844185e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 61/71 | LOSS: 4.765728586760581e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 62/71 | LOSS: 4.7740454394645185e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 63/71 | LOSS: 4.767160110219493e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 64/71 | LOSS: 4.751335906733472e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 65/71 | LOSS: 4.74454077448172e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 66/71 | LOSS: 4.738552949824388e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 67/71 | LOSS: 4.741875861381071e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 68/71 | LOSS: 4.737357054788542e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 69/71 | LOSS: 4.750693337233055e-06\n",
      "TRAIN: EPOCH 410/1000 | BATCH 70/71 | LOSS: 4.708642927333028e-06\n",
      "VAL: EPOCH 410/1000 | BATCH 0/8 | LOSS: 5.281921403366141e-06\n",
      "VAL: EPOCH 410/1000 | BATCH 1/8 | LOSS: 5.333805802365532e-06\n",
      "VAL: EPOCH 410/1000 | BATCH 2/8 | LOSS: 5.4257078166604815e-06\n",
      "VAL: EPOCH 410/1000 | BATCH 3/8 | LOSS: 5.441576035991602e-06\n",
      "VAL: EPOCH 410/1000 | BATCH 4/8 | LOSS: 5.408954439189983e-06\n",
      "VAL: EPOCH 410/1000 | BATCH 5/8 | LOSS: 5.319848545089674e-06\n",
      "VAL: EPOCH 410/1000 | BATCH 6/8 | LOSS: 5.1540673017111005e-06\n",
      "VAL: EPOCH 410/1000 | BATCH 7/8 | LOSS: 5.076233208001213e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 0/71 | LOSS: 4.815353804588085e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 1/71 | LOSS: 4.531073045654921e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 2/71 | LOSS: 4.66504009940157e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 3/71 | LOSS: 4.889688284492877e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 4/71 | LOSS: 5.0426211601006795e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 5/71 | LOSS: 5.2523024957433035e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 6/71 | LOSS: 5.119733032188378e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 7/71 | LOSS: 5.2755712545149436e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 8/71 | LOSS: 5.1535480957277796e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 9/71 | LOSS: 5.483668473971193e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 10/71 | LOSS: 5.514926049892727e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 11/71 | LOSS: 5.5256898197815945e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 12/71 | LOSS: 5.532329151509867e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 13/71 | LOSS: 5.638747779812547e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 14/71 | LOSS: 5.610342092647139e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 15/71 | LOSS: 5.6936333976409514e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 16/71 | LOSS: 5.7249352828555e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 17/71 | LOSS: 5.696964661991741e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 18/71 | LOSS: 5.707668845532585e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 19/71 | LOSS: 5.686339864041656e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 20/71 | LOSS: 5.706045526278017e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 21/71 | LOSS: 5.70388612107887e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 22/71 | LOSS: 5.726238965587032e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 23/71 | LOSS: 5.694721172252078e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 24/71 | LOSS: 5.683503623004072e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 25/71 | LOSS: 5.605687218891175e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 26/71 | LOSS: 5.568175083681126e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 27/71 | LOSS: 5.531053399668703e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 28/71 | LOSS: 5.480995672894943e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 29/71 | LOSS: 5.423557581707428e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 30/71 | LOSS: 5.33382358885466e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 31/71 | LOSS: 5.366725154942742e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 32/71 | LOSS: 5.341686277681605e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 33/71 | LOSS: 5.327966732693312e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 34/71 | LOSS: 5.278469929440429e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 35/71 | LOSS: 5.294054978498429e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 36/71 | LOSS: 5.281635361137199e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 37/71 | LOSS: 5.3048377678446984e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 38/71 | LOSS: 5.312468036395447e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 39/71 | LOSS: 5.323737372009418e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 40/71 | LOSS: 5.340615749313346e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 41/71 | LOSS: 5.348804620552983e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 42/71 | LOSS: 5.36424921805054e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 43/71 | LOSS: 5.348119197440775e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 44/71 | LOSS: 5.408810946189786e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 45/71 | LOSS: 5.399228084372779e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 46/71 | LOSS: 5.418985007596115e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 47/71 | LOSS: 5.409394821261533e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 48/71 | LOSS: 5.426743423320621e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 49/71 | LOSS: 5.392744005803252e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 50/71 | LOSS: 5.38889691918431e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 51/71 | LOSS: 5.381759417734709e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 52/71 | LOSS: 5.362595745608351e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 53/71 | LOSS: 5.35034860149608e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 54/71 | LOSS: 5.327177909335164e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 55/71 | LOSS: 5.311487809324587e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 56/71 | LOSS: 5.28441189105746e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 57/71 | LOSS: 5.273942182894048e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 58/71 | LOSS: 5.260424929582445e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 59/71 | LOSS: 5.2890642526411584e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 60/71 | LOSS: 5.2656938671551884e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 61/71 | LOSS: 5.253271033031906e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 62/71 | LOSS: 5.260055754011514e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 63/71 | LOSS: 5.238059120671323e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 64/71 | LOSS: 5.239344748658298e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 65/71 | LOSS: 5.236659693295995e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 66/71 | LOSS: 5.241016692503871e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 67/71 | LOSS: 5.240653071242003e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 68/71 | LOSS: 5.230285464479137e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 69/71 | LOSS: 5.2252652527126236e-06\n",
      "TRAIN: EPOCH 411/1000 | BATCH 70/71 | LOSS: 5.202406430431489e-06\n",
      "VAL: EPOCH 411/1000 | BATCH 0/8 | LOSS: 4.919596449326491e-06\n",
      "VAL: EPOCH 411/1000 | BATCH 1/8 | LOSS: 4.698165184890968e-06\n",
      "VAL: EPOCH 411/1000 | BATCH 2/8 | LOSS: 4.699004724291929e-06\n",
      "VAL: EPOCH 411/1000 | BATCH 3/8 | LOSS: 4.66728420178697e-06\n",
      "VAL: EPOCH 411/1000 | BATCH 4/8 | LOSS: 4.608688504958991e-06\n",
      "VAL: EPOCH 411/1000 | BATCH 5/8 | LOSS: 4.441096431643625e-06\n",
      "VAL: EPOCH 411/1000 | BATCH 6/8 | LOSS: 4.3286549303697285e-06\n",
      "VAL: EPOCH 411/1000 | BATCH 7/8 | LOSS: 4.19657627048764e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 0/71 | LOSS: 2.977372105306131e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 1/71 | LOSS: 3.674330059766362e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 2/71 | LOSS: 4.798196565995265e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 3/71 | LOSS: 4.653472558402427e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 4/71 | LOSS: 4.719465096059139e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 5/71 | LOSS: 4.838438106465522e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 6/71 | LOSS: 5.078893699257085e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 7/71 | LOSS: 5.13631707121931e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 8/71 | LOSS: 5.127884605826694e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 9/71 | LOSS: 5.3122023246032765e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 10/71 | LOSS: 5.373886469070302e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 11/71 | LOSS: 5.444884114543432e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 12/71 | LOSS: 5.461014325192082e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 13/71 | LOSS: 5.416763786212998e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 14/71 | LOSS: 5.417028281347787e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 15/71 | LOSS: 5.340361170169672e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 16/71 | LOSS: 5.3789737033666286e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 17/71 | LOSS: 5.42671175986066e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 18/71 | LOSS: 5.4674309539175166e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 19/71 | LOSS: 5.46731197346162e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 20/71 | LOSS: 5.454977034397805e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 21/71 | LOSS: 5.4545954733178155e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 22/71 | LOSS: 5.439445678111161e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 23/71 | LOSS: 5.552896510607752e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 24/71 | LOSS: 5.500128982021124e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 25/71 | LOSS: 5.511583708539547e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 26/71 | LOSS: 5.45686662513213e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 27/71 | LOSS: 5.453895523390072e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 28/71 | LOSS: 5.437595212964064e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 29/71 | LOSS: 5.405218318325448e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 30/71 | LOSS: 5.397196020801916e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 31/71 | LOSS: 5.385113560407717e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 32/71 | LOSS: 5.336250703266794e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 33/71 | LOSS: 5.31591463721484e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 34/71 | LOSS: 5.290073035471973e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 35/71 | LOSS: 5.253164456715038e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 36/71 | LOSS: 5.219563902485518e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 37/71 | LOSS: 5.2064126880156296e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 38/71 | LOSS: 5.213307630560265e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 39/71 | LOSS: 5.22186398939084e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 40/71 | LOSS: 5.212520879534323e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 41/71 | LOSS: 5.2126789190272875e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 42/71 | LOSS: 5.214986918703247e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 43/71 | LOSS: 5.199891564767643e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 44/71 | LOSS: 5.193041751125646e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 45/71 | LOSS: 5.17134209155262e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 46/71 | LOSS: 5.133056674822001e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 47/71 | LOSS: 5.1290657173315895e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 48/71 | LOSS: 5.116909830706261e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 49/71 | LOSS: 5.102454178995686e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 50/71 | LOSS: 5.105120002482698e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 51/71 | LOSS: 5.0970625474852004e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 52/71 | LOSS: 5.079678233431757e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 53/71 | LOSS: 5.0440247479046875e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 54/71 | LOSS: 5.064673785620305e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 55/71 | LOSS: 5.052757980398123e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 56/71 | LOSS: 5.033019501672498e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 57/71 | LOSS: 5.033735305914888e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 58/71 | LOSS: 5.02976362733565e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 59/71 | LOSS: 5.015423009050816e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 60/71 | LOSS: 5.008603559380405e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 61/71 | LOSS: 4.9897155276407685e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 62/71 | LOSS: 4.995610969859944e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 63/71 | LOSS: 4.989391527487896e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 64/71 | LOSS: 4.966515619414097e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 65/71 | LOSS: 4.982032016938099e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 66/71 | LOSS: 4.94922545528167e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 67/71 | LOSS: 4.9353000088772475e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 68/71 | LOSS: 4.937780309721374e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 69/71 | LOSS: 4.942737005616696e-06\n",
      "TRAIN: EPOCH 412/1000 | BATCH 70/71 | LOSS: 4.933832600163697e-06\n",
      "VAL: EPOCH 412/1000 | BATCH 0/8 | LOSS: 5.104822776047513e-06\n",
      "VAL: EPOCH 412/1000 | BATCH 1/8 | LOSS: 4.9519612730364315e-06\n",
      "VAL: EPOCH 412/1000 | BATCH 2/8 | LOSS: 5.145552222529659e-06\n",
      "VAL: EPOCH 412/1000 | BATCH 3/8 | LOSS: 5.139122777109151e-06\n",
      "VAL: EPOCH 412/1000 | BATCH 4/8 | LOSS: 5.096630957268644e-06\n",
      "VAL: EPOCH 412/1000 | BATCH 5/8 | LOSS: 5.015944907427183e-06\n",
      "VAL: EPOCH 412/1000 | BATCH 6/8 | LOSS: 4.918500638138669e-06\n",
      "VAL: EPOCH 412/1000 | BATCH 7/8 | LOSS: 4.88379617991086e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 0/71 | LOSS: 3.7309309846023098e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 1/71 | LOSS: 4.2849042074522e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 2/71 | LOSS: 4.769388093942932e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 3/71 | LOSS: 4.531998001766624e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 4/71 | LOSS: 4.360444927442586e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 5/71 | LOSS: 4.546048406458188e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 6/71 | LOSS: 4.471942182655246e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 7/71 | LOSS: 4.540577776879218e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 8/71 | LOSS: 4.525604051903227e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 9/71 | LOSS: 4.457012983039021e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 10/71 | LOSS: 4.347121045033088e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 11/71 | LOSS: 4.298084225714168e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 12/71 | LOSS: 4.290626538232363e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 13/71 | LOSS: 4.295341178866303e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 14/71 | LOSS: 4.274187176633859e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 15/71 | LOSS: 4.251692899970294e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 16/71 | LOSS: 4.3373345172714085e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 17/71 | LOSS: 4.302122584299974e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 18/71 | LOSS: 4.2880868884247705e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 19/71 | LOSS: 4.332752428126696e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 20/71 | LOSS: 4.37595162154702e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 21/71 | LOSS: 4.420752289165235e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 22/71 | LOSS: 4.4920495937454374e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 23/71 | LOSS: 4.540802327331524e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 24/71 | LOSS: 4.539526671578642e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 25/71 | LOSS: 4.563007800802902e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 26/71 | LOSS: 4.578178639483802e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 27/71 | LOSS: 4.608457873343598e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 28/71 | LOSS: 4.615475878883194e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 29/71 | LOSS: 4.629257288494652e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 30/71 | LOSS: 4.632272491782656e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 31/71 | LOSS: 4.622028697554015e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 32/71 | LOSS: 4.604898439345451e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 33/71 | LOSS: 4.664107457167975e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 34/71 | LOSS: 4.646918660000665e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 35/71 | LOSS: 4.675193003499266e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 36/71 | LOSS: 4.6840979258696595e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 37/71 | LOSS: 4.677150795223976e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 38/71 | LOSS: 4.681112246684139e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 39/71 | LOSS: 4.6973824169072035e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 40/71 | LOSS: 4.697847909882534e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 41/71 | LOSS: 4.686113849553901e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 42/71 | LOSS: 4.670906555263574e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 43/71 | LOSS: 4.638416219842673e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 44/71 | LOSS: 4.63910831361621e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 45/71 | LOSS: 4.6250058462844565e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 46/71 | LOSS: 4.640876882826597e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 47/71 | LOSS: 4.663930601130535e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 48/71 | LOSS: 4.636825126778733e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 49/71 | LOSS: 4.652169127439265e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 50/71 | LOSS: 4.650901504932085e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 51/71 | LOSS: 4.660943685708984e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 52/71 | LOSS: 4.6428317975094716e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 53/71 | LOSS: 4.658152723398719e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 54/71 | LOSS: 4.652212845195953e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 55/71 | LOSS: 4.6452041390563996e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 56/71 | LOSS: 4.635451939082072e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 57/71 | LOSS: 4.6272142487302105e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 58/71 | LOSS: 4.619420035281002e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 59/71 | LOSS: 4.612667218376979e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 60/71 | LOSS: 4.640227304707133e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 61/71 | LOSS: 4.6328834741871106e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 62/71 | LOSS: 4.652023165810591e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 63/71 | LOSS: 4.6490990825986955e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 64/71 | LOSS: 4.661879295151896e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 65/71 | LOSS: 4.6572659453941565e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 66/71 | LOSS: 4.654078559303628e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 67/71 | LOSS: 4.658067215292677e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 68/71 | LOSS: 4.68853427198643e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 69/71 | LOSS: 4.6816827632158365e-06\n",
      "TRAIN: EPOCH 413/1000 | BATCH 70/71 | LOSS: 4.656056758699058e-06\n",
      "VAL: EPOCH 413/1000 | BATCH 0/8 | LOSS: 7.891231689427514e-06\n",
      "VAL: EPOCH 413/1000 | BATCH 1/8 | LOSS: 7.633971108589321e-06\n",
      "VAL: EPOCH 413/1000 | BATCH 2/8 | LOSS: 7.65272549566968e-06\n",
      "VAL: EPOCH 413/1000 | BATCH 3/8 | LOSS: 7.416380185532034e-06\n",
      "VAL: EPOCH 413/1000 | BATCH 4/8 | LOSS: 7.4465174293436576e-06\n",
      "VAL: EPOCH 413/1000 | BATCH 5/8 | LOSS: 7.153987932421539e-06\n",
      "VAL: EPOCH 413/1000 | BATCH 6/8 | LOSS: 7.20313145003664e-06\n",
      "VAL: EPOCH 413/1000 | BATCH 7/8 | LOSS: 7.170915296228486e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 0/71 | LOSS: 5.6054050219245255e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 1/71 | LOSS: 6.487333394034067e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 2/71 | LOSS: 5.920277847811424e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 3/71 | LOSS: 6.131831696620793e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 4/71 | LOSS: 5.564198363572359e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 5/71 | LOSS: 5.637466908107551e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 6/71 | LOSS: 5.5066121344030506e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 7/71 | LOSS: 5.483732309130573e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 8/71 | LOSS: 5.5655355532103895e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 9/71 | LOSS: 5.54378443666792e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 10/71 | LOSS: 5.5026042570137754e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 11/71 | LOSS: 5.443669427525795e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 12/71 | LOSS: 5.431556229058725e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 13/71 | LOSS: 5.3483058439139445e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 14/71 | LOSS: 5.400810490148918e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 15/71 | LOSS: 5.538593939036218e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 16/71 | LOSS: 5.470000306719585e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 17/71 | LOSS: 5.621317692809195e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 18/71 | LOSS: 5.66967274177219e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 19/71 | LOSS: 5.587579425991862e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 20/71 | LOSS: 5.5477646258493356e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 21/71 | LOSS: 5.599617390378791e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 22/71 | LOSS: 5.704499827792777e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 23/71 | LOSS: 5.675834264214548e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 24/71 | LOSS: 5.771963606093777e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 25/71 | LOSS: 5.807129774928819e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 26/71 | LOSS: 5.82714402399789e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 27/71 | LOSS: 5.778963392393572e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 28/71 | LOSS: 5.849160782512125e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 29/71 | LOSS: 5.799986456622719e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 30/71 | LOSS: 5.795252109824252e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 31/71 | LOSS: 5.759406676020262e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 32/71 | LOSS: 5.737492577309515e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 33/71 | LOSS: 5.694491780077977e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 34/71 | LOSS: 5.657089020262772e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 35/71 | LOSS: 5.699479187468468e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 36/71 | LOSS: 5.681064288404053e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 37/71 | LOSS: 5.654858808300464e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 38/71 | LOSS: 5.6790513437404e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 39/71 | LOSS: 5.6900845834206845e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 40/71 | LOSS: 5.6690239877597624e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 41/71 | LOSS: 5.6844254621475036e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 42/71 | LOSS: 5.670171144312155e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 43/71 | LOSS: 5.696436853048164e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 44/71 | LOSS: 5.685590026082031e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 45/71 | LOSS: 5.706124422589631e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 46/71 | LOSS: 5.68825015897288e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 47/71 | LOSS: 5.6647818477510254e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 48/71 | LOSS: 5.678755027177858e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 49/71 | LOSS: 5.703515089408029e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 50/71 | LOSS: 5.70099774788604e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 51/71 | LOSS: 5.6968140667707375e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 52/71 | LOSS: 5.716205981476584e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 53/71 | LOSS: 5.685034432611212e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 54/71 | LOSS: 5.68387526568999e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 55/71 | LOSS: 5.715784433440214e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 56/71 | LOSS: 5.700993688949159e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 57/71 | LOSS: 5.722938246027956e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 58/71 | LOSS: 5.7130537397362355e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 59/71 | LOSS: 5.738502522945055e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 60/71 | LOSS: 5.700330959490336e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 61/71 | LOSS: 5.703551088968638e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 62/71 | LOSS: 5.699427732444804e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 63/71 | LOSS: 5.699853517171505e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 64/71 | LOSS: 5.704664272343507e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 65/71 | LOSS: 5.676422095789211e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 66/71 | LOSS: 5.686892720079223e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 67/71 | LOSS: 5.677898223118117e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 68/71 | LOSS: 5.694654448787965e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 69/71 | LOSS: 5.717359103333105e-06\n",
      "TRAIN: EPOCH 414/1000 | BATCH 70/71 | LOSS: 5.7032284210333565e-06\n",
      "VAL: EPOCH 414/1000 | BATCH 0/8 | LOSS: 7.312476554943714e-06\n",
      "VAL: EPOCH 414/1000 | BATCH 1/8 | LOSS: 7.068711738611455e-06\n",
      "VAL: EPOCH 414/1000 | BATCH 2/8 | LOSS: 7.091263341862941e-06\n",
      "VAL: EPOCH 414/1000 | BATCH 3/8 | LOSS: 7.017443067525164e-06\n",
      "VAL: EPOCH 414/1000 | BATCH 4/8 | LOSS: 6.989719804550987e-06\n",
      "VAL: EPOCH 414/1000 | BATCH 5/8 | LOSS: 6.702727660012897e-06\n",
      "VAL: EPOCH 414/1000 | BATCH 6/8 | LOSS: 6.681404946513274e-06\n",
      "VAL: EPOCH 414/1000 | BATCH 7/8 | LOSS: 6.498081972949876e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 0/71 | LOSS: 6.568732715095393e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 1/71 | LOSS: 6.007377123751212e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 2/71 | LOSS: 5.9191367351256e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 3/71 | LOSS: 5.6567173487565015e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 4/71 | LOSS: 5.702081853087293e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 5/71 | LOSS: 5.875953926685422e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 6/71 | LOSS: 5.8769657828504156e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 7/71 | LOSS: 6.422678040962637e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 8/71 | LOSS: 6.347292482031561e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 9/71 | LOSS: 6.4592472426738825e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 10/71 | LOSS: 6.4017501079599075e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 11/71 | LOSS: 6.32826538549125e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 12/71 | LOSS: 6.282964897344265e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 13/71 | LOSS: 6.13554282478747e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 14/71 | LOSS: 6.2218763559940274e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 15/71 | LOSS: 6.06774408140609e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 16/71 | LOSS: 6.082518225144561e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 17/71 | LOSS: 6.021534709867006e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 18/71 | LOSS: 5.9245523492503895e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 19/71 | LOSS: 5.88565052339618e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 20/71 | LOSS: 5.8246586961883755e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 21/71 | LOSS: 5.7845942158316825e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 22/71 | LOSS: 5.686368171075668e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 23/71 | LOSS: 5.694495210188204e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 24/71 | LOSS: 5.705050853066496e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 25/71 | LOSS: 5.718328994729605e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 26/71 | LOSS: 5.691902220792987e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 27/71 | LOSS: 5.67775237771327e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 28/71 | LOSS: 5.6417538009303954e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 29/71 | LOSS: 5.598673040670595e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 30/71 | LOSS: 5.609198326290484e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 31/71 | LOSS: 5.609409733153825e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 32/71 | LOSS: 5.6094229834839515e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 33/71 | LOSS: 5.62175014639412e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 34/71 | LOSS: 5.60666305448519e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 35/71 | LOSS: 5.63422743324635e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 36/71 | LOSS: 5.613558611199747e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 37/71 | LOSS: 5.633750088475752e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 38/71 | LOSS: 5.6262031543715584e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 39/71 | LOSS: 5.628268257851232e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 40/71 | LOSS: 5.584832301681716e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 41/71 | LOSS: 5.596710290471189e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 42/71 | LOSS: 5.5552765452164245e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 43/71 | LOSS: 5.533132144302561e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 44/71 | LOSS: 5.4963300650949044e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 45/71 | LOSS: 5.484125282513876e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 46/71 | LOSS: 5.455796312201122e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 47/71 | LOSS: 5.435425327012429e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 48/71 | LOSS: 5.413840765367222e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 49/71 | LOSS: 5.37950579655444e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 50/71 | LOSS: 5.348265042758773e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 51/71 | LOSS: 5.346482160243445e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 52/71 | LOSS: 5.330477013146419e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 53/71 | LOSS: 5.326148821209413e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 54/71 | LOSS: 5.305353314807607e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 55/71 | LOSS: 5.325882142415139e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 56/71 | LOSS: 5.319116653162494e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 57/71 | LOSS: 5.3489205340846295e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 58/71 | LOSS: 5.343593079102239e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 59/71 | LOSS: 5.362197267307541e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 60/71 | LOSS: 5.362167265226818e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 61/71 | LOSS: 5.348112414403952e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 62/71 | LOSS: 5.3795934649863895e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 63/71 | LOSS: 5.384845994882426e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 64/71 | LOSS: 5.393258243202581e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 65/71 | LOSS: 5.387162986787161e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 66/71 | LOSS: 5.381821669044569e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 67/71 | LOSS: 5.389264458699472e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 68/71 | LOSS: 5.36900322425725e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 69/71 | LOSS: 5.384963096081005e-06\n",
      "TRAIN: EPOCH 415/1000 | BATCH 70/71 | LOSS: 5.3661534769764305e-06\n",
      "VAL: EPOCH 415/1000 | BATCH 0/8 | LOSS: 6.096237484598532e-06\n",
      "VAL: EPOCH 415/1000 | BATCH 1/8 | LOSS: 6.4336768446082715e-06\n",
      "VAL: EPOCH 415/1000 | BATCH 2/8 | LOSS: 6.972080579240962e-06\n",
      "VAL: EPOCH 415/1000 | BATCH 3/8 | LOSS: 7.172902769525535e-06\n",
      "VAL: EPOCH 415/1000 | BATCH 4/8 | LOSS: 7.126511263777502e-06\n",
      "VAL: EPOCH 415/1000 | BATCH 5/8 | LOSS: 7.141660262277583e-06\n",
      "VAL: EPOCH 415/1000 | BATCH 6/8 | LOSS: 7.002696422984757e-06\n",
      "VAL: EPOCH 415/1000 | BATCH 7/8 | LOSS: 7.0346913503271935e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 0/71 | LOSS: 8.277686902147252e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 1/71 | LOSS: 6.647597274422878e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 2/71 | LOSS: 6.5806279962998815e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 3/71 | LOSS: 6.396074240910821e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 4/71 | LOSS: 5.941407653153874e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 5/71 | LOSS: 5.608238931624025e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 6/71 | LOSS: 5.630097218402495e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 7/71 | LOSS: 5.386099815041234e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 8/71 | LOSS: 5.219095732576938e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 9/71 | LOSS: 5.085896100354148e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 10/71 | LOSS: 5.0343526461140504e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 11/71 | LOSS: 4.966036650936682e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 12/71 | LOSS: 4.924823245416449e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 13/71 | LOSS: 4.856613908746762e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 14/71 | LOSS: 4.852678982085005e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 15/71 | LOSS: 4.819190934313156e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 16/71 | LOSS: 4.76986194959917e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 17/71 | LOSS: 4.813036841571577e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 18/71 | LOSS: 4.766807514418983e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 19/71 | LOSS: 4.701753562130762e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 20/71 | LOSS: 4.7332107009251126e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 21/71 | LOSS: 4.721606904405979e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 22/71 | LOSS: 4.732322978547334e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 23/71 | LOSS: 4.754108213470924e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 24/71 | LOSS: 4.7781570629013e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 25/71 | LOSS: 4.7618439723989395e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 26/71 | LOSS: 4.684813482661546e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 27/71 | LOSS: 4.624496138408826e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 28/71 | LOSS: 4.591914054391713e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 29/71 | LOSS: 4.5855545825664494e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 30/71 | LOSS: 4.596014693355931e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 31/71 | LOSS: 4.5790477116725015e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 32/71 | LOSS: 4.635608299643288e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 33/71 | LOSS: 4.633111195610528e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 34/71 | LOSS: 4.642409995930003e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 35/71 | LOSS: 4.708435956975639e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 36/71 | LOSS: 4.710668973358052e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 37/71 | LOSS: 4.698240577214931e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 38/71 | LOSS: 4.7803091547393706e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 39/71 | LOSS: 4.7698382559246966e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 40/71 | LOSS: 4.769057585047161e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 41/71 | LOSS: 4.775311979409751e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 42/71 | LOSS: 4.788026312463592e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 43/71 | LOSS: 4.767606603970555e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 44/71 | LOSS: 4.773416892728872e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 45/71 | LOSS: 4.753962655187301e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 46/71 | LOSS: 4.743079811537718e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 47/71 | LOSS: 4.7509124196191505e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 48/71 | LOSS: 4.760612552802611e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 49/71 | LOSS: 4.762658436447964e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 50/71 | LOSS: 4.733259959489349e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 51/71 | LOSS: 4.7247608825693114e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 52/71 | LOSS: 4.719689582517911e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 53/71 | LOSS: 4.704741555812049e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 54/71 | LOSS: 4.708374787944856e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 55/71 | LOSS: 4.691711865234538e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 56/71 | LOSS: 4.674199453004628e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 57/71 | LOSS: 4.649056112894273e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 58/71 | LOSS: 4.648966886002276e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 59/71 | LOSS: 4.653381245134369e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 60/71 | LOSS: 4.64386209655533e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 61/71 | LOSS: 4.633957297711834e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 62/71 | LOSS: 4.62239888422898e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 63/71 | LOSS: 4.611927334252641e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 64/71 | LOSS: 4.618671318194873e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 65/71 | LOSS: 4.618172063215532e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 66/71 | LOSS: 4.637875086537766e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 67/71 | LOSS: 4.650734798788642e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 68/71 | LOSS: 4.670898988479166e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 69/71 | LOSS: 4.677582082227413e-06\n",
      "TRAIN: EPOCH 416/1000 | BATCH 70/71 | LOSS: 4.6537044499287525e-06\n",
      "VAL: EPOCH 416/1000 | BATCH 0/8 | LOSS: 9.144161595031619e-06\n",
      "VAL: EPOCH 416/1000 | BATCH 1/8 | LOSS: 9.362965101900045e-06\n",
      "VAL: EPOCH 416/1000 | BATCH 2/8 | LOSS: 8.979028886339316e-06\n",
      "VAL: EPOCH 416/1000 | BATCH 3/8 | LOSS: 8.992916718852939e-06\n",
      "VAL: EPOCH 416/1000 | BATCH 4/8 | LOSS: 8.784410420048517e-06\n",
      "VAL: EPOCH 416/1000 | BATCH 5/8 | LOSS: 8.343972240254516e-06\n",
      "VAL: EPOCH 416/1000 | BATCH 6/8 | LOSS: 8.140607146093057e-06\n",
      "VAL: EPOCH 416/1000 | BATCH 7/8 | LOSS: 7.820811163128383e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 0/71 | LOSS: 1.0969069080601912e-05\n",
      "TRAIN: EPOCH 417/1000 | BATCH 1/71 | LOSS: 8.814420880298712e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 2/71 | LOSS: 8.295655334222829e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 3/71 | LOSS: 7.787031790940091e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 4/71 | LOSS: 7.473195728380233e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 5/71 | LOSS: 7.014398609802204e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 6/71 | LOSS: 6.666238082938694e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 7/71 | LOSS: 6.831257906014798e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 8/71 | LOSS: 6.57266051954745e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 9/71 | LOSS: 6.409447814803571e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 10/71 | LOSS: 6.378858581063634e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 11/71 | LOSS: 6.283688738524991e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 12/71 | LOSS: 6.25139385262558e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 13/71 | LOSS: 6.036235894108748e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 14/71 | LOSS: 6.083504664881426e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 15/71 | LOSS: 6.200596416761073e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 16/71 | LOSS: 6.1563019993199675e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 17/71 | LOSS: 6.128074687694607e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 18/71 | LOSS: 6.060966621876823e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 19/71 | LOSS: 5.949308513208962e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 20/71 | LOSS: 5.9646160934789805e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 21/71 | LOSS: 6.065917649390361e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 22/71 | LOSS: 6.0375083965324015e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 23/71 | LOSS: 5.992547992642964e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 24/71 | LOSS: 5.985495872664614e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 25/71 | LOSS: 6.002101048789672e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 26/71 | LOSS: 5.908135664059147e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 27/71 | LOSS: 5.932946050636799e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 28/71 | LOSS: 5.903634975515286e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 29/71 | LOSS: 5.916944026769974e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 30/71 | LOSS: 5.903222325932509e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 31/71 | LOSS: 5.934944347529836e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 32/71 | LOSS: 5.861616824459458e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 33/71 | LOSS: 5.842443128505479e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 34/71 | LOSS: 5.797319090561359e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 35/71 | LOSS: 5.740391025533932e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 36/71 | LOSS: 5.702376416298353e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 37/71 | LOSS: 5.657592881011385e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 38/71 | LOSS: 5.659728300927852e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 39/71 | LOSS: 5.664132618221629e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 40/71 | LOSS: 5.631356077906165e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 41/71 | LOSS: 5.581538283920617e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 42/71 | LOSS: 5.553866204473009e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 43/71 | LOSS: 5.500933401559781e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 44/71 | LOSS: 5.448681405242597e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 45/71 | LOSS: 5.407008292541318e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 46/71 | LOSS: 5.391726353217506e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 47/71 | LOSS: 5.3682563248003135e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 48/71 | LOSS: 5.333728226179694e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 49/71 | LOSS: 5.331152115104487e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 50/71 | LOSS: 5.304006574043146e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 51/71 | LOSS: 5.286687033171452e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 52/71 | LOSS: 5.272889494627411e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 53/71 | LOSS: 5.2739604867653085e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 54/71 | LOSS: 5.269677289602854e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 55/71 | LOSS: 5.223133606640269e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 56/71 | LOSS: 5.205924847822436e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 57/71 | LOSS: 5.197140000808727e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 58/71 | LOSS: 5.188689517696838e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 59/71 | LOSS: 5.183193237220015e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 60/71 | LOSS: 5.179183897810423e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 61/71 | LOSS: 5.152687201130433e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 62/71 | LOSS: 5.1254989976148375e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 63/71 | LOSS: 5.136941339856094e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 64/71 | LOSS: 5.143089863570192e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 65/71 | LOSS: 5.120636277559132e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 66/71 | LOSS: 5.126800749452225e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 67/71 | LOSS: 5.130878709197157e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 68/71 | LOSS: 5.12645529865488e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 69/71 | LOSS: 5.115735679542662e-06\n",
      "TRAIN: EPOCH 417/1000 | BATCH 70/71 | LOSS: 5.134092640526458e-06\n",
      "VAL: EPOCH 417/1000 | BATCH 0/8 | LOSS: 5.29563840245828e-06\n",
      "VAL: EPOCH 417/1000 | BATCH 1/8 | LOSS: 4.9112650231109e-06\n",
      "VAL: EPOCH 417/1000 | BATCH 2/8 | LOSS: 4.782885904811944e-06\n",
      "VAL: EPOCH 417/1000 | BATCH 3/8 | LOSS: 4.804749437425926e-06\n",
      "VAL: EPOCH 417/1000 | BATCH 4/8 | LOSS: 4.664812422561226e-06\n",
      "VAL: EPOCH 417/1000 | BATCH 5/8 | LOSS: 4.468566885407199e-06\n",
      "VAL: EPOCH 417/1000 | BATCH 6/8 | LOSS: 4.353786672644284e-06\n",
      "VAL: EPOCH 417/1000 | BATCH 7/8 | LOSS: 4.218921134224729e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 0/71 | LOSS: 3.383217745067668e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 1/71 | LOSS: 4.244525939611776e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 2/71 | LOSS: 4.263403828493513e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 3/71 | LOSS: 4.240587998083356e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 4/71 | LOSS: 4.3467719024192775e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 5/71 | LOSS: 4.43822019254488e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 6/71 | LOSS: 4.4878244612586315e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 7/71 | LOSS: 4.542817151786949e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 8/71 | LOSS: 4.782355088334751e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 9/71 | LOSS: 4.792710137735412e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 10/71 | LOSS: 5.070536665450411e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 11/71 | LOSS: 5.096136590054812e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 12/71 | LOSS: 5.191928179416349e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 13/71 | LOSS: 5.21678509812773e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 14/71 | LOSS: 5.250404698623849e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 15/71 | LOSS: 5.2118232503062245e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 16/71 | LOSS: 5.25062591805181e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 17/71 | LOSS: 5.190824077140456e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 18/71 | LOSS: 5.229345154140519e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 19/71 | LOSS: 5.231072293554462e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 20/71 | LOSS: 5.205131996955883e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 21/71 | LOSS: 5.251146097345389e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 22/71 | LOSS: 5.223224404645684e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 23/71 | LOSS: 5.287296810744617e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 24/71 | LOSS: 5.275109897411312e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 25/71 | LOSS: 5.368188471248476e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 26/71 | LOSS: 5.361521112313818e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 27/71 | LOSS: 5.3711525538346905e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 28/71 | LOSS: 5.4336766236795895e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 29/71 | LOSS: 5.456583653540292e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 30/71 | LOSS: 5.478476535450506e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 31/71 | LOSS: 5.477008052423571e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 32/71 | LOSS: 5.45843768536129e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 33/71 | LOSS: 5.41369154368166e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 34/71 | LOSS: 5.386428665588028e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 35/71 | LOSS: 5.352941109827549e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 36/71 | LOSS: 5.352517973733484e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 37/71 | LOSS: 5.319517562814949e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 38/71 | LOSS: 5.281926434737472e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 39/71 | LOSS: 5.246707002015683e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 40/71 | LOSS: 5.271880021742928e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 41/71 | LOSS: 5.2793578030711314e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 42/71 | LOSS: 5.248604354566724e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 43/71 | LOSS: 5.237940611509244e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 44/71 | LOSS: 5.249144603820039e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 45/71 | LOSS: 5.248757623161035e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 46/71 | LOSS: 5.250603103280461e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 47/71 | LOSS: 5.240446109648171e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 48/71 | LOSS: 5.237964851588987e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 49/71 | LOSS: 5.214287971284648e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 50/71 | LOSS: 5.20933159511033e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 51/71 | LOSS: 5.188283165143031e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 52/71 | LOSS: 5.173698359355495e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 53/71 | LOSS: 5.178691998884864e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 54/71 | LOSS: 5.197315993593127e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 55/71 | LOSS: 5.202066936200416e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 56/71 | LOSS: 5.191702117818957e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 57/71 | LOSS: 5.1674472646921235e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 58/71 | LOSS: 5.164359755958608e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 59/71 | LOSS: 5.145854515831161e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 60/71 | LOSS: 5.136743363779759e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 61/71 | LOSS: 5.1230975098024505e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 62/71 | LOSS: 5.105610319058917e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 63/71 | LOSS: 5.082877752471404e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 64/71 | LOSS: 5.072314472552255e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 65/71 | LOSS: 5.076811293513332e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 66/71 | LOSS: 5.064285446746569e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 67/71 | LOSS: 5.069956002631336e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 68/71 | LOSS: 5.064653344337442e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 69/71 | LOSS: 5.0928215484548546e-06\n",
      "TRAIN: EPOCH 418/1000 | BATCH 70/71 | LOSS: 5.075760695881015e-06\n",
      "VAL: EPOCH 418/1000 | BATCH 0/8 | LOSS: 5.037942173657939e-06\n",
      "VAL: EPOCH 418/1000 | BATCH 1/8 | LOSS: 4.835440904571442e-06\n",
      "VAL: EPOCH 418/1000 | BATCH 2/8 | LOSS: 5.1065602140927995e-06\n",
      "VAL: EPOCH 418/1000 | BATCH 3/8 | LOSS: 5.054213261246332e-06\n",
      "VAL: EPOCH 418/1000 | BATCH 4/8 | LOSS: 5.031874388805591e-06\n",
      "VAL: EPOCH 418/1000 | BATCH 5/8 | LOSS: 5.024340377228024e-06\n",
      "VAL: EPOCH 418/1000 | BATCH 6/8 | LOSS: 4.984981452642907e-06\n",
      "VAL: EPOCH 418/1000 | BATCH 7/8 | LOSS: 4.97246043096311e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 0/71 | LOSS: 4.9449122343503404e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 1/71 | LOSS: 4.572093985188985e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 2/71 | LOSS: 4.476158816639024e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 3/71 | LOSS: 4.801787667929602e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 4/71 | LOSS: 4.62987472928944e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 5/71 | LOSS: 4.472994684571556e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 6/71 | LOSS: 4.480324800429766e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 7/71 | LOSS: 4.540539009667555e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 8/71 | LOSS: 4.547449609364978e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 9/71 | LOSS: 4.682294002122944e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 10/71 | LOSS: 4.826326651461634e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 11/71 | LOSS: 4.892012611890095e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 12/71 | LOSS: 4.8099527703016065e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 13/71 | LOSS: 4.7674582219769945e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 14/71 | LOSS: 4.889718487296098e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 15/71 | LOSS: 4.923273820622853e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 16/71 | LOSS: 5.028454003930913e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 17/71 | LOSS: 5.058769526537314e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 18/71 | LOSS: 5.082131074710847e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 19/71 | LOSS: 5.003471187592368e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 20/71 | LOSS: 5.0293181283493706e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 21/71 | LOSS: 5.022248677216174e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 22/71 | LOSS: 4.989871483659321e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 23/71 | LOSS: 4.953576706157037e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 24/71 | LOSS: 4.939526243106229e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 25/71 | LOSS: 4.9256767537232145e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 26/71 | LOSS: 4.898112803386067e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 27/71 | LOSS: 4.8579378894178916e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 28/71 | LOSS: 4.849596060576632e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 29/71 | LOSS: 4.861720738820926e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 30/71 | LOSS: 4.842949986214667e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 31/71 | LOSS: 4.8666348675396875e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 32/71 | LOSS: 4.823202826545722e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 33/71 | LOSS: 4.834872697756509e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 34/71 | LOSS: 4.798425002913323e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 35/71 | LOSS: 4.793154895802824e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 36/71 | LOSS: 4.785228955714391e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 37/71 | LOSS: 4.772771059211472e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 38/71 | LOSS: 4.761821972538317e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 39/71 | LOSS: 4.791078708876739e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 40/71 | LOSS: 4.7627389482463845e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 41/71 | LOSS: 4.741158190448741e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 42/71 | LOSS: 4.744911398860453e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 43/71 | LOSS: 4.72360444415575e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 44/71 | LOSS: 4.685247874173607e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 45/71 | LOSS: 4.6720892550697695e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 46/71 | LOSS: 4.663728396793731e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 47/71 | LOSS: 4.645024967923443e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 48/71 | LOSS: 4.6347753902168455e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 49/71 | LOSS: 4.600755069077423e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 50/71 | LOSS: 4.570427563521662e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 51/71 | LOSS: 4.586566437690355e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 52/71 | LOSS: 4.57586242071488e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 53/71 | LOSS: 4.557483041228099e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 54/71 | LOSS: 4.551502938325443e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 55/71 | LOSS: 4.541544273349375e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 56/71 | LOSS: 4.542542934646726e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 57/71 | LOSS: 4.555671431450625e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 58/71 | LOSS: 4.545665522034712e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 59/71 | LOSS: 4.535149154586786e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 60/71 | LOSS: 4.552715221298505e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 61/71 | LOSS: 4.568147390625288e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 62/71 | LOSS: 4.563538841322288e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 63/71 | LOSS: 4.549139429599336e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 64/71 | LOSS: 4.532098994157143e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 65/71 | LOSS: 4.540886648514557e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 66/71 | LOSS: 4.52418780424971e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 67/71 | LOSS: 4.531146587664005e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 68/71 | LOSS: 4.537480165616717e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 69/71 | LOSS: 4.534988888086186e-06\n",
      "TRAIN: EPOCH 419/1000 | BATCH 70/71 | LOSS: 4.539174273637579e-06\n",
      "VAL: EPOCH 419/1000 | BATCH 0/8 | LOSS: 5.909746960242046e-06\n",
      "VAL: EPOCH 419/1000 | BATCH 1/8 | LOSS: 5.8925240864482475e-06\n",
      "VAL: EPOCH 419/1000 | BATCH 2/8 | LOSS: 6.283514115542251e-06\n",
      "VAL: EPOCH 419/1000 | BATCH 3/8 | LOSS: 6.517472911582445e-06\n",
      "VAL: EPOCH 419/1000 | BATCH 4/8 | LOSS: 6.4314540395571385e-06\n",
      "VAL: EPOCH 419/1000 | BATCH 5/8 | LOSS: 6.6365304671004805e-06\n",
      "VAL: EPOCH 419/1000 | BATCH 6/8 | LOSS: 6.543569660217534e-06\n",
      "VAL: EPOCH 419/1000 | BATCH 7/8 | LOSS: 6.584633524653327e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 0/71 | LOSS: 7.574873052362818e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 1/71 | LOSS: 5.834756848344114e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 2/71 | LOSS: 6.3160523495753296e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 3/71 | LOSS: 5.704442742171523e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 4/71 | LOSS: 5.977350338071119e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 5/71 | LOSS: 5.774650010910894e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 6/71 | LOSS: 6.343737108441669e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 7/71 | LOSS: 6.060306134259008e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 8/71 | LOSS: 6.027032365333677e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 9/71 | LOSS: 5.912823462494999e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 10/71 | LOSS: 5.879972708168101e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 11/71 | LOSS: 6.0062681086492375e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 12/71 | LOSS: 5.883124826001362e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 13/71 | LOSS: 6.023626935659974e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 14/71 | LOSS: 5.821258855576161e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 15/71 | LOSS: 5.98472399815364e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 16/71 | LOSS: 6.067619270812857e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 17/71 | LOSS: 6.0672259678540286e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 18/71 | LOSS: 6.02667827670781e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 19/71 | LOSS: 6.033316640241537e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 20/71 | LOSS: 6.048073633532372e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 21/71 | LOSS: 5.982536425687034e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 22/71 | LOSS: 6.068581095188046e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 23/71 | LOSS: 5.964330853203137e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 24/71 | LOSS: 6.107009467086755e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 25/71 | LOSS: 6.0688483314991645e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 26/71 | LOSS: 6.137452887573831e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 27/71 | LOSS: 6.1117590511068035e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 28/71 | LOSS: 6.0894124481851105e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 29/71 | LOSS: 6.103044385478521e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 30/71 | LOSS: 6.0524452413618186e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 31/71 | LOSS: 6.105123233623999e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 32/71 | LOSS: 6.1281569590695044e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 33/71 | LOSS: 6.129239128473701e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 34/71 | LOSS: 6.155664673964826e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 35/71 | LOSS: 6.164589118876999e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 36/71 | LOSS: 6.155621540037336e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 37/71 | LOSS: 6.098333319402484e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 38/71 | LOSS: 6.090072393141245e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 39/71 | LOSS: 6.042907943992759e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 40/71 | LOSS: 6.003169779078013e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 41/71 | LOSS: 5.9704382779489694e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 42/71 | LOSS: 5.941194209553095e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 43/71 | LOSS: 5.932347996597607e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 44/71 | LOSS: 5.9158557329889545e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 45/71 | LOSS: 5.915443214531558e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 46/71 | LOSS: 5.871658115520791e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 47/71 | LOSS: 5.919668363200496e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 48/71 | LOSS: 5.9014406100946615e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 49/71 | LOSS: 5.8996359803131785e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 50/71 | LOSS: 5.887912649603666e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 51/71 | LOSS: 5.84827187839647e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 52/71 | LOSS: 5.826922709807064e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 53/71 | LOSS: 5.786122848668903e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 54/71 | LOSS: 5.788648633659415e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 55/71 | LOSS: 5.760149820486861e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 56/71 | LOSS: 5.750420513594004e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 57/71 | LOSS: 5.743989034052557e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 58/71 | LOSS: 5.718769412944034e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 59/71 | LOSS: 5.708076840467887e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 60/71 | LOSS: 5.6887031666415395e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 61/71 | LOSS: 5.677822217318951e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 62/71 | LOSS: 5.657461510314274e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 63/71 | LOSS: 5.625593445302002e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 64/71 | LOSS: 5.618303063014397e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 65/71 | LOSS: 5.587246953313517e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 66/71 | LOSS: 5.588335503190721e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 67/71 | LOSS: 5.571210368436689e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 68/71 | LOSS: 5.55228439554702e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 69/71 | LOSS: 5.531515625989414e-06\n",
      "TRAIN: EPOCH 420/1000 | BATCH 70/71 | LOSS: 5.508890468955145e-06\n",
      "VAL: EPOCH 420/1000 | BATCH 0/8 | LOSS: 5.948922989773564e-06\n",
      "VAL: EPOCH 420/1000 | BATCH 1/8 | LOSS: 5.550759624384227e-06\n",
      "VAL: EPOCH 420/1000 | BATCH 2/8 | LOSS: 5.483299749660849e-06\n",
      "VAL: EPOCH 420/1000 | BATCH 3/8 | LOSS: 5.572601025960466e-06\n",
      "VAL: EPOCH 420/1000 | BATCH 4/8 | LOSS: 5.395359130488941e-06\n",
      "VAL: EPOCH 420/1000 | BATCH 5/8 | LOSS: 5.219134664002922e-06\n",
      "VAL: EPOCH 420/1000 | BATCH 6/8 | LOSS: 5.076162714041337e-06\n",
      "VAL: EPOCH 420/1000 | BATCH 7/8 | LOSS: 4.828058621342279e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 0/71 | LOSS: 4.893320237897569e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 1/71 | LOSS: 5.477011427501566e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 2/71 | LOSS: 5.386167837665805e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 3/71 | LOSS: 5.335144578566542e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 4/71 | LOSS: 5.1425463425402995e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 5/71 | LOSS: 4.884870046832172e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 6/71 | LOSS: 4.916114253969031e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 7/71 | LOSS: 4.8874284743760654e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 8/71 | LOSS: 4.8718212989721605e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 9/71 | LOSS: 4.9769522320275424e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 10/71 | LOSS: 4.9131793027547905e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 11/71 | LOSS: 4.906400098055504e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 12/71 | LOSS: 4.933076979796169e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 13/71 | LOSS: 4.936701218736873e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 14/71 | LOSS: 4.7842814941153245e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 15/71 | LOSS: 4.763631352489028e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 16/71 | LOSS: 4.757052144722205e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 17/71 | LOSS: 4.780604361561321e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 18/71 | LOSS: 4.757227136571218e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 19/71 | LOSS: 4.770783129970369e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 20/71 | LOSS: 4.779611095901124e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 21/71 | LOSS: 4.790443488838553e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 22/71 | LOSS: 4.798543910589575e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 23/71 | LOSS: 4.806363790521573e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 24/71 | LOSS: 4.841091176785994e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 25/71 | LOSS: 4.835708960644507e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 26/71 | LOSS: 4.979650569751996e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 27/71 | LOSS: 4.97516722524389e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 28/71 | LOSS: 5.0496464139322254e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 29/71 | LOSS: 5.057822409071377e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 30/71 | LOSS: 5.117705959198629e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 31/71 | LOSS: 5.114654072713165e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 32/71 | LOSS: 5.147027877683286e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 33/71 | LOSS: 5.190687996967908e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 34/71 | LOSS: 5.216220558525362e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 35/71 | LOSS: 5.234878901521572e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 36/71 | LOSS: 5.2487359327544155e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 37/71 | LOSS: 5.2670306774272925e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 38/71 | LOSS: 5.300295096952677e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 39/71 | LOSS: 5.340749510196474e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 40/71 | LOSS: 5.317636994111192e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 41/71 | LOSS: 5.27490420782193e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 42/71 | LOSS: 5.274412519928626e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 43/71 | LOSS: 5.26815038235575e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 44/71 | LOSS: 5.264549721080888e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 45/71 | LOSS: 5.234966811653778e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 46/71 | LOSS: 5.212994524976858e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 47/71 | LOSS: 5.188141888841831e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 48/71 | LOSS: 5.157369092298073e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 49/71 | LOSS: 5.144635615579318e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 50/71 | LOSS: 5.134874817147148e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 51/71 | LOSS: 5.113749344113435e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 52/71 | LOSS: 5.0894837532157325e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 53/71 | LOSS: 5.065152048768208e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 54/71 | LOSS: 5.0448971044467974e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 55/71 | LOSS: 5.033187903791259e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 56/71 | LOSS: 5.012353133344349e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 57/71 | LOSS: 5.016773974571622e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 58/71 | LOSS: 5.004716937129792e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 59/71 | LOSS: 5.003925548408006e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 60/71 | LOSS: 4.989975191985322e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 61/71 | LOSS: 4.983130455941371e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 62/71 | LOSS: 4.9776679675449574e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 63/71 | LOSS: 4.965902959952473e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 64/71 | LOSS: 4.961195883459788e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 65/71 | LOSS: 4.977436605479328e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 66/71 | LOSS: 4.966848893618861e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 67/71 | LOSS: 4.9475842395034255e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 68/71 | LOSS: 4.934823924697306e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 69/71 | LOSS: 4.952337332854638e-06\n",
      "TRAIN: EPOCH 421/1000 | BATCH 70/71 | LOSS: 4.93360579332122e-06\n",
      "VAL: EPOCH 421/1000 | BATCH 0/8 | LOSS: 7.0902283368923236e-06\n",
      "VAL: EPOCH 421/1000 | BATCH 1/8 | LOSS: 6.53646634418692e-06\n",
      "VAL: EPOCH 421/1000 | BATCH 2/8 | LOSS: 6.41018126164757e-06\n",
      "VAL: EPOCH 421/1000 | BATCH 3/8 | LOSS: 6.43807641154126e-06\n",
      "VAL: EPOCH 421/1000 | BATCH 4/8 | LOSS: 6.2250494920590425e-06\n",
      "VAL: EPOCH 421/1000 | BATCH 5/8 | LOSS: 5.949146649678975e-06\n",
      "VAL: EPOCH 421/1000 | BATCH 6/8 | LOSS: 5.732214114167229e-06\n",
      "VAL: EPOCH 421/1000 | BATCH 7/8 | LOSS: 5.5023504046403104e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 0/71 | LOSS: 6.231372935872059e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 1/71 | LOSS: 5.585890448855935e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 2/71 | LOSS: 5.072360333239582e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 3/71 | LOSS: 5.020936441724189e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 4/71 | LOSS: 4.885335329163354e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 5/71 | LOSS: 4.9910393045138335e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 6/71 | LOSS: 4.886021088168491e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 7/71 | LOSS: 4.851203982525476e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 8/71 | LOSS: 4.771168227711718e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 9/71 | LOSS: 4.672739305533469e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 10/71 | LOSS: 4.740228426023598e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 11/71 | LOSS: 4.773871599657771e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 12/71 | LOSS: 4.784613933085124e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 13/71 | LOSS: 4.7603930722419e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 14/71 | LOSS: 4.6804146222712005e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 15/71 | LOSS: 4.754663365247325e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 16/71 | LOSS: 4.728317248116261e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 17/71 | LOSS: 4.7453958763475785e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 18/71 | LOSS: 4.769521638081642e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 19/71 | LOSS: 4.791520223079715e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 20/71 | LOSS: 4.814211890680738e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 21/71 | LOSS: 4.781859946557564e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 22/71 | LOSS: 4.780801626348756e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 23/71 | LOSS: 4.7420891936174785e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 24/71 | LOSS: 4.819104069611058e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 25/71 | LOSS: 4.778998450442136e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 26/71 | LOSS: 4.818912185766277e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 27/71 | LOSS: 4.777685181319871e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 28/71 | LOSS: 4.778017510347879e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 29/71 | LOSS: 4.81338903834209e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 30/71 | LOSS: 4.7985976262650855e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 31/71 | LOSS: 4.812067103898698e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 32/71 | LOSS: 4.8431226346248435e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 33/71 | LOSS: 4.852101148892817e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 34/71 | LOSS: 4.8679712173387606e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 35/71 | LOSS: 4.877186319794822e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 36/71 | LOSS: 4.8757288405377535e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 37/71 | LOSS: 4.849266528445266e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 38/71 | LOSS: 4.828245413917988e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 39/71 | LOSS: 4.832740251003997e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 40/71 | LOSS: 4.8249992953681545e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 41/71 | LOSS: 4.813497320182727e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 42/71 | LOSS: 4.815568043467951e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 43/71 | LOSS: 4.786268365561145e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 44/71 | LOSS: 4.7956418256944746e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 45/71 | LOSS: 4.818158048613481e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 46/71 | LOSS: 4.829728024870224e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 47/71 | LOSS: 4.852639217271341e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 48/71 | LOSS: 4.877959276088905e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 49/71 | LOSS: 4.878384174844541e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 50/71 | LOSS: 4.874663625955959e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 51/71 | LOSS: 4.878252416172192e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 52/71 | LOSS: 4.878383258914735e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 53/71 | LOSS: 4.901094122807855e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 54/71 | LOSS: 4.909683854169667e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 55/71 | LOSS: 4.940165723610335e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 56/71 | LOSS: 4.933445887601058e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 57/71 | LOSS: 4.931122722656914e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 58/71 | LOSS: 4.970505496187844e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 59/71 | LOSS: 4.979973190681145e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 60/71 | LOSS: 5.071671332606471e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 61/71 | LOSS: 5.072536201887781e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 62/71 | LOSS: 5.0697593047112204e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 63/71 | LOSS: 5.0730052159053685e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 64/71 | LOSS: 5.071832377395297e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 65/71 | LOSS: 5.078519524376492e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 66/71 | LOSS: 5.075188218102288e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 67/71 | LOSS: 5.0640698331709944e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 68/71 | LOSS: 5.047780398630151e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 69/71 | LOSS: 5.039803847596756e-06\n",
      "TRAIN: EPOCH 422/1000 | BATCH 70/71 | LOSS: 5.015334354667034e-06\n",
      "VAL: EPOCH 422/1000 | BATCH 0/8 | LOSS: 5.3600870160153136e-06\n",
      "VAL: EPOCH 422/1000 | BATCH 1/8 | LOSS: 5.161618446436478e-06\n",
      "VAL: EPOCH 422/1000 | BATCH 2/8 | LOSS: 5.159462943993276e-06\n",
      "VAL: EPOCH 422/1000 | BATCH 3/8 | LOSS: 5.183545681575197e-06\n",
      "VAL: EPOCH 422/1000 | BATCH 4/8 | LOSS: 5.042078919359483e-06\n",
      "VAL: EPOCH 422/1000 | BATCH 5/8 | LOSS: 4.833429178082345e-06\n",
      "VAL: EPOCH 422/1000 | BATCH 6/8 | LOSS: 4.708631682400924e-06\n",
      "VAL: EPOCH 422/1000 | BATCH 7/8 | LOSS: 4.543265049505862e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 0/71 | LOSS: 4.36638219980523e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 1/71 | LOSS: 5.084266831545392e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 2/71 | LOSS: 4.810433741416394e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 3/71 | LOSS: 4.921060167362157e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 4/71 | LOSS: 4.560358866001479e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 5/71 | LOSS: 4.871112802599479e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 6/71 | LOSS: 4.6952940725272385e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 7/71 | LOSS: 4.734766832825699e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 8/71 | LOSS: 4.692208120913064e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 9/71 | LOSS: 4.619838637154316e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 10/71 | LOSS: 4.656868110363245e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 11/71 | LOSS: 4.649292539700885e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 12/71 | LOSS: 4.6842136935783155e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 13/71 | LOSS: 4.6454285893560154e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 14/71 | LOSS: 4.610899365313041e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 15/71 | LOSS: 4.616259417389301e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 16/71 | LOSS: 4.666203708706341e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 17/71 | LOSS: 4.6950268723675335e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 18/71 | LOSS: 4.71218455507034e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 19/71 | LOSS: 4.654485951505194e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 20/71 | LOSS: 4.609437621097424e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 21/71 | LOSS: 4.617746919228177e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 22/71 | LOSS: 4.582508510160867e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 23/71 | LOSS: 4.5473598030791136e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 24/71 | LOSS: 4.5154602230468295e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 25/71 | LOSS: 4.49901159089887e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 26/71 | LOSS: 4.497793337644974e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 27/71 | LOSS: 4.4945728632228565e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 28/71 | LOSS: 4.478043467378452e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 29/71 | LOSS: 4.485731157425713e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 30/71 | LOSS: 4.511678188493768e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 31/71 | LOSS: 4.5163547923721126e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 32/71 | LOSS: 4.5268326368496865e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 33/71 | LOSS: 4.542509981654482e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 34/71 | LOSS: 4.5084679576185795e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 35/71 | LOSS: 4.558802225397409e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 36/71 | LOSS: 4.613962917918232e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 37/71 | LOSS: 4.63696533427681e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 38/71 | LOSS: 4.656693200115861e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 39/71 | LOSS: 4.715531179044774e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 40/71 | LOSS: 4.713003552130497e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 41/71 | LOSS: 4.729946123829688e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 42/71 | LOSS: 4.7939437599568414e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 43/71 | LOSS: 4.801654230587618e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 44/71 | LOSS: 4.859939953247602e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 45/71 | LOSS: 4.858466403991612e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 46/71 | LOSS: 4.9462448859753265e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 47/71 | LOSS: 5.0191114079704375e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 48/71 | LOSS: 5.105407933801964e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 49/71 | LOSS: 5.112683138577267e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 50/71 | LOSS: 5.114433888058343e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 51/71 | LOSS: 5.287833188888796e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 52/71 | LOSS: 5.3077417863467005e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 53/71 | LOSS: 5.37892137125945e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 54/71 | LOSS: 5.496225755698767e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 55/71 | LOSS: 5.533915652579578e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 56/71 | LOSS: 5.545579258244272e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 57/71 | LOSS: 5.577187588140328e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 58/71 | LOSS: 5.630209069734565e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 59/71 | LOSS: 5.622443965573135e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 60/71 | LOSS: 5.64978204163738e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 61/71 | LOSS: 5.682155548833356e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 62/71 | LOSS: 5.732429108609231e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 63/71 | LOSS: 5.73416511429059e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 64/71 | LOSS: 5.766315539054071e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 65/71 | LOSS: 5.780742653907069e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 66/71 | LOSS: 5.775229924642081e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 67/71 | LOSS: 5.769736838053231e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 68/71 | LOSS: 5.7784191942145284e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 69/71 | LOSS: 5.789259064061168e-06\n",
      "TRAIN: EPOCH 423/1000 | BATCH 70/71 | LOSS: 5.781989233915917e-06\n",
      "VAL: EPOCH 423/1000 | BATCH 0/8 | LOSS: 6.736237992299721e-06\n",
      "VAL: EPOCH 423/1000 | BATCH 1/8 | LOSS: 6.213197366378154e-06\n",
      "VAL: EPOCH 423/1000 | BATCH 2/8 | LOSS: 6.399523196402394e-06\n",
      "VAL: EPOCH 423/1000 | BATCH 3/8 | LOSS: 6.298671223703423e-06\n",
      "VAL: EPOCH 423/1000 | BATCH 4/8 | LOSS: 6.314727852441138e-06\n",
      "VAL: EPOCH 423/1000 | BATCH 5/8 | LOSS: 6.123540818710656e-06\n",
      "VAL: EPOCH 423/1000 | BATCH 6/8 | LOSS: 6.111307681879095e-06\n",
      "VAL: EPOCH 423/1000 | BATCH 7/8 | LOSS: 6.064930289539916e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 0/71 | LOSS: 4.701810667029349e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 1/71 | LOSS: 5.29801604898239e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 2/71 | LOSS: 5.808970551394547e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 3/71 | LOSS: 5.519580554391723e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 4/71 | LOSS: 5.683798099198611e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 5/71 | LOSS: 6.216668604489921e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 6/71 | LOSS: 6.073216159815534e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 7/71 | LOSS: 6.063366072339704e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 8/71 | LOSS: 6.035029703828816e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 9/71 | LOSS: 6.227151698112721e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 10/71 | LOSS: 6.207097240721024e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 11/71 | LOSS: 6.296865118808152e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 12/71 | LOSS: 6.6040792472239655e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 13/71 | LOSS: 6.464937996497611e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 14/71 | LOSS: 6.399083758878987e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 15/71 | LOSS: 6.4747963506306405e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 16/71 | LOSS: 6.3697072405107865e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 17/71 | LOSS: 6.313330181405439e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 18/71 | LOSS: 6.263608300191424e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 19/71 | LOSS: 6.306167961156461e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 20/71 | LOSS: 6.295936968637675e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 21/71 | LOSS: 6.293099865863703e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 22/71 | LOSS: 6.229200627548499e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 23/71 | LOSS: 6.206559855096809e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 24/71 | LOSS: 6.147539006633451e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 25/71 | LOSS: 6.054483737198797e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 26/71 | LOSS: 6.097216639857463e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 27/71 | LOSS: 6.087625917773819e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 28/71 | LOSS: 6.052392171991884e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 29/71 | LOSS: 5.963286616861296e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 30/71 | LOSS: 5.948830698065238e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 31/71 | LOSS: 5.888261149777918e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 32/71 | LOSS: 5.833546979236857e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 33/71 | LOSS: 5.845057858898465e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 34/71 | LOSS: 5.810871691989763e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 35/71 | LOSS: 5.771998764379936e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 36/71 | LOSS: 5.731655637892136e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 37/71 | LOSS: 5.696624318835874e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 38/71 | LOSS: 5.628723096005837e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 39/71 | LOSS: 5.596541114982756e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 40/71 | LOSS: 5.6195324526836375e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 41/71 | LOSS: 5.580835157897604e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 42/71 | LOSS: 5.550085942622688e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 43/71 | LOSS: 5.518607978997765e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 44/71 | LOSS: 5.503899850130741e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 45/71 | LOSS: 5.464542428141969e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 46/71 | LOSS: 5.434347766377539e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 47/71 | LOSS: 5.401648020135023e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 48/71 | LOSS: 5.371885951105042e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 49/71 | LOSS: 5.368548359001579e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 50/71 | LOSS: 5.362052349358005e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 51/71 | LOSS: 5.340991007275219e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 52/71 | LOSS: 5.352714295886986e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 53/71 | LOSS: 5.317762471619332e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 54/71 | LOSS: 5.338346450530066e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 55/71 | LOSS: 5.344953182527336e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 56/71 | LOSS: 5.341604531949678e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 57/71 | LOSS: 5.332581595055295e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 58/71 | LOSS: 5.312448951433723e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 59/71 | LOSS: 5.340197886501604e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 60/71 | LOSS: 5.327843330290262e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 61/71 | LOSS: 5.3177939904461735e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 62/71 | LOSS: 5.304602768413292e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 63/71 | LOSS: 5.3158626975857715e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 64/71 | LOSS: 5.297961780133147e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 65/71 | LOSS: 5.288526894577389e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 66/71 | LOSS: 5.273554876763608e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 67/71 | LOSS: 5.272716979267416e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 68/71 | LOSS: 5.258475812956601e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 69/71 | LOSS: 5.255153489867683e-06\n",
      "TRAIN: EPOCH 424/1000 | BATCH 70/71 | LOSS: 5.233165589282141e-06\n",
      "VAL: EPOCH 424/1000 | BATCH 0/8 | LOSS: 5.44229715160327e-06\n",
      "VAL: EPOCH 424/1000 | BATCH 1/8 | LOSS: 5.103805733597255e-06\n",
      "VAL: EPOCH 424/1000 | BATCH 2/8 | LOSS: 5.180280671387057e-06\n",
      "VAL: EPOCH 424/1000 | BATCH 3/8 | LOSS: 5.1208437525929185e-06\n",
      "VAL: EPOCH 424/1000 | BATCH 4/8 | LOSS: 5.025435075367568e-06\n",
      "VAL: EPOCH 424/1000 | BATCH 5/8 | LOSS: 4.955810330405559e-06\n",
      "VAL: EPOCH 424/1000 | BATCH 6/8 | LOSS: 4.8395318117400166e-06\n",
      "VAL: EPOCH 424/1000 | BATCH 7/8 | LOSS: 4.790485263583832e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 0/71 | LOSS: 4.258010449120775e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 1/71 | LOSS: 4.228073748890893e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 2/71 | LOSS: 4.6471404857584275e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 3/71 | LOSS: 4.3321101657056715e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 4/71 | LOSS: 4.4466538383858275e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 5/71 | LOSS: 4.476028304149319e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 6/71 | LOSS: 4.438007895909582e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 7/71 | LOSS: 4.469004409202171e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 8/71 | LOSS: 4.492869720706949e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 9/71 | LOSS: 4.54500282103254e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 10/71 | LOSS: 4.54648046467231e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 11/71 | LOSS: 4.496373662732367e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 12/71 | LOSS: 4.492626360913094e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 13/71 | LOSS: 4.485362913848283e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 14/71 | LOSS: 4.446691976530322e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 15/71 | LOSS: 4.395541452595353e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 16/71 | LOSS: 4.404525123605067e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 17/71 | LOSS: 4.384773774290807e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 18/71 | LOSS: 4.436588991857686e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 19/71 | LOSS: 4.412984731061443e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 20/71 | LOSS: 4.359869210799973e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 21/71 | LOSS: 4.415370426613663e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 22/71 | LOSS: 4.399511898673549e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 23/71 | LOSS: 4.428799750636851e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 24/71 | LOSS: 4.432008745425265e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 25/71 | LOSS: 4.4775081715460356e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 26/71 | LOSS: 4.473944171776465e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 27/71 | LOSS: 4.448846466077937e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 28/71 | LOSS: 4.443296089009201e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 29/71 | LOSS: 4.470539768893407e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 30/71 | LOSS: 4.442611901639236e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 31/71 | LOSS: 4.473533515181316e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 32/71 | LOSS: 4.49841951938817e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 33/71 | LOSS: 4.505452145221009e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 34/71 | LOSS: 4.600460845592481e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 35/71 | LOSS: 4.595388083493566e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 36/71 | LOSS: 4.628462334408509e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 37/71 | LOSS: 4.609808426817165e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 38/71 | LOSS: 4.592290521871361e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 39/71 | LOSS: 4.55627080668819e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 40/71 | LOSS: 4.564109481290826e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 41/71 | LOSS: 4.608291533259035e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 42/71 | LOSS: 4.620948027158467e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 43/71 | LOSS: 4.616054706151441e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 44/71 | LOSS: 4.676913860950865e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 45/71 | LOSS: 4.6884475948346615e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 46/71 | LOSS: 4.677429266868127e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 47/71 | LOSS: 4.6673229642616816e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 48/71 | LOSS: 4.661266956998365e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 49/71 | LOSS: 4.667408898058057e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 50/71 | LOSS: 4.674614122753458e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 51/71 | LOSS: 4.691853339758941e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 52/71 | LOSS: 4.680650796594904e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 53/71 | LOSS: 4.6713347585494005e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 54/71 | LOSS: 4.694637800639611e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 55/71 | LOSS: 4.703224326314219e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 56/71 | LOSS: 4.723589279564565e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 57/71 | LOSS: 4.740499916963394e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 58/71 | LOSS: 4.747724738158236e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 59/71 | LOSS: 4.743427814446477e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 60/71 | LOSS: 4.761287099143851e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 61/71 | LOSS: 4.798398934678605e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 62/71 | LOSS: 4.796735910286005e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 63/71 | LOSS: 4.818793154726109e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 64/71 | LOSS: 4.81632787691174e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 65/71 | LOSS: 4.832397718074032e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 66/71 | LOSS: 4.806425875412698e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 67/71 | LOSS: 4.8698315775736235e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 68/71 | LOSS: 4.887744438931712e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 69/71 | LOSS: 4.932512628589133e-06\n",
      "TRAIN: EPOCH 425/1000 | BATCH 70/71 | LOSS: 4.915538386445456e-06\n",
      "VAL: EPOCH 425/1000 | BATCH 0/8 | LOSS: 7.984894182300195e-06\n",
      "VAL: EPOCH 425/1000 | BATCH 1/8 | LOSS: 7.480812428184436e-06\n",
      "VAL: EPOCH 425/1000 | BATCH 2/8 | LOSS: 7.463065988607316e-06\n",
      "VAL: EPOCH 425/1000 | BATCH 3/8 | LOSS: 7.320949293898593e-06\n",
      "VAL: EPOCH 425/1000 | BATCH 4/8 | LOSS: 7.2524905590398704e-06\n",
      "VAL: EPOCH 425/1000 | BATCH 5/8 | LOSS: 6.899444315422443e-06\n",
      "VAL: EPOCH 425/1000 | BATCH 6/8 | LOSS: 6.820727646429857e-06\n",
      "VAL: EPOCH 425/1000 | BATCH 7/8 | LOSS: 6.6314208311268885e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 0/71 | LOSS: 7.050478870951338e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 1/71 | LOSS: 7.181885848694947e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 2/71 | LOSS: 6.73001522955019e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 3/71 | LOSS: 7.591181883981335e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 4/71 | LOSS: 7.177339648478665e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 5/71 | LOSS: 7.291945773128343e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 6/71 | LOSS: 6.976553257637923e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 7/71 | LOSS: 6.735547685821075e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 8/71 | LOSS: 6.678350574576244e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 9/71 | LOSS: 6.643274355155882e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 10/71 | LOSS: 6.588216902772811e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 11/71 | LOSS: 6.399268765259573e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 12/71 | LOSS: 6.441224881060547e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 13/71 | LOSS: 6.3468088943149525e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 14/71 | LOSS: 6.391134714552512e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 15/71 | LOSS: 6.301722464741033e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 16/71 | LOSS: 6.165185181196327e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 17/71 | LOSS: 6.194113944325687e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 18/71 | LOSS: 6.1207341260390084e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 19/71 | LOSS: 6.158766245789593e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 20/71 | LOSS: 6.066452096135999e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 21/71 | LOSS: 6.07310704998037e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 22/71 | LOSS: 5.979636230419957e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 23/71 | LOSS: 5.9619873127303435e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 24/71 | LOSS: 5.899417956243269e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 25/71 | LOSS: 5.881603468738971e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 26/71 | LOSS: 5.866182349294562e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 27/71 | LOSS: 5.829597187455095e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 28/71 | LOSS: 5.800078160346261e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 29/71 | LOSS: 5.751580162420093e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 30/71 | LOSS: 5.762022650038313e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 31/71 | LOSS: 5.689014635379408e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 32/71 | LOSS: 5.682906296924832e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 33/71 | LOSS: 5.637630311759815e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 34/71 | LOSS: 5.6097645483532684e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 35/71 | LOSS: 5.557248029061965e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 36/71 | LOSS: 5.516273859029752e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 37/71 | LOSS: 5.476914488530096e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 38/71 | LOSS: 5.452074202157047e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 39/71 | LOSS: 5.468037824130079e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 40/71 | LOSS: 5.478774205695983e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 41/71 | LOSS: 5.426801045532989e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 42/71 | LOSS: 5.426032515081948e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 43/71 | LOSS: 5.4035403836265194e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 44/71 | LOSS: 5.39219747932091e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 45/71 | LOSS: 5.375666783829334e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 46/71 | LOSS: 5.344736491872724e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 47/71 | LOSS: 5.344107615693853e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 48/71 | LOSS: 5.3063976598163464e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 49/71 | LOSS: 5.2902701190760124e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 50/71 | LOSS: 5.256993584707208e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 51/71 | LOSS: 5.236094339649683e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 52/71 | LOSS: 5.218739334112506e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 53/71 | LOSS: 5.191216823058548e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 54/71 | LOSS: 5.186165171008642e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 55/71 | LOSS: 5.178048541308401e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 56/71 | LOSS: 5.162461747475614e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 57/71 | LOSS: 5.168974557191491e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 58/71 | LOSS: 5.1592786248721045e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 59/71 | LOSS: 5.15832224815919e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 60/71 | LOSS: 5.151441603286868e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 61/71 | LOSS: 5.168008108798418e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 62/71 | LOSS: 5.15572484858687e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 63/71 | LOSS: 5.137373157992897e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 64/71 | LOSS: 5.124515302248675e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 65/71 | LOSS: 5.128354180697263e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 66/71 | LOSS: 5.141463229372323e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 67/71 | LOSS: 5.1199939334497576e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 68/71 | LOSS: 5.097948521812424e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 69/71 | LOSS: 5.094669108984817e-06\n",
      "TRAIN: EPOCH 426/1000 | BATCH 70/71 | LOSS: 5.094955197268397e-06\n",
      "VAL: EPOCH 426/1000 | BATCH 0/8 | LOSS: 5.3936810218147e-06\n",
      "VAL: EPOCH 426/1000 | BATCH 1/8 | LOSS: 5.583018491961411e-06\n",
      "VAL: EPOCH 426/1000 | BATCH 2/8 | LOSS: 6.054483340752388e-06\n",
      "VAL: EPOCH 426/1000 | BATCH 3/8 | LOSS: 6.128390054982447e-06\n",
      "VAL: EPOCH 426/1000 | BATCH 4/8 | LOSS: 6.13885558777838e-06\n",
      "VAL: EPOCH 426/1000 | BATCH 5/8 | LOSS: 6.190019576024497e-06\n",
      "VAL: EPOCH 426/1000 | BATCH 6/8 | LOSS: 6.092958496343012e-06\n",
      "VAL: EPOCH 426/1000 | BATCH 7/8 | LOSS: 6.141270773696306e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 0/71 | LOSS: 5.570690063905204e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 1/71 | LOSS: 4.976484660801361e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 2/71 | LOSS: 5.3776707318320405e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 3/71 | LOSS: 5.394379400058824e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 4/71 | LOSS: 5.312113989930367e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 5/71 | LOSS: 5.795736266615374e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 6/71 | LOSS: 5.71729989522802e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 7/71 | LOSS: 5.750122113568068e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 8/71 | LOSS: 5.539025146895761e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 9/71 | LOSS: 5.641417055812781e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 10/71 | LOSS: 5.484331640301124e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 11/71 | LOSS: 5.472594580169243e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 12/71 | LOSS: 5.477500088510658e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 13/71 | LOSS: 5.3684165907595475e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 14/71 | LOSS: 5.294279101993501e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 15/71 | LOSS: 5.2925238094303495e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 16/71 | LOSS: 5.274818811492022e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 17/71 | LOSS: 5.2461098473738984e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 18/71 | LOSS: 5.234532421270974e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 19/71 | LOSS: 5.233899469203607e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 20/71 | LOSS: 5.195481792595404e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 21/71 | LOSS: 5.185293647710668e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 22/71 | LOSS: 5.255912603151165e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 23/71 | LOSS: 5.2863354274753265e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 24/71 | LOSS: 5.235374501353362e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 25/71 | LOSS: 5.257443805161389e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 26/71 | LOSS: 5.238107826665294e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 27/71 | LOSS: 5.288254426107285e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 28/71 | LOSS: 5.253564769849743e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 29/71 | LOSS: 5.223293131469594e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 30/71 | LOSS: 5.246226992899357e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 31/71 | LOSS: 5.1902531126302165e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 32/71 | LOSS: 5.2293983530530985e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 33/71 | LOSS: 5.193291619488103e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 34/71 | LOSS: 5.1827513418954495e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 35/71 | LOSS: 5.19111279749672e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 36/71 | LOSS: 5.201953719143923e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 37/71 | LOSS: 5.182852544892644e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 38/71 | LOSS: 5.1561463841118475e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 39/71 | LOSS: 5.113056062100441e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 40/71 | LOSS: 5.12447691773293e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 41/71 | LOSS: 5.093874966362255e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 42/71 | LOSS: 5.0727639285638406e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 43/71 | LOSS: 5.076530189026843e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 44/71 | LOSS: 5.060830391004048e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 45/71 | LOSS: 5.029656197462907e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 46/71 | LOSS: 5.025956770236433e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 47/71 | LOSS: 5.014659114029503e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 48/71 | LOSS: 5.013396698164538e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 49/71 | LOSS: 5.002663369850779e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 50/71 | LOSS: 4.9795467154634775e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 51/71 | LOSS: 4.997736792889695e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 52/71 | LOSS: 5.005174928572949e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 53/71 | LOSS: 5.009832096692224e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 54/71 | LOSS: 5.000600880190508e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 55/71 | LOSS: 4.971858138398082e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 56/71 | LOSS: 4.982531380479932e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 57/71 | LOSS: 4.973527944488963e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 58/71 | LOSS: 4.941850115987324e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 59/71 | LOSS: 4.9305073427300766e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 60/71 | LOSS: 4.934081304230351e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 61/71 | LOSS: 4.927068003311316e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 62/71 | LOSS: 4.903328151577004e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 63/71 | LOSS: 4.890886089015112e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 64/71 | LOSS: 4.908356857833076e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 65/71 | LOSS: 4.90592116985874e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 66/71 | LOSS: 4.9096271656857584e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 67/71 | LOSS: 4.914367911410141e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 68/71 | LOSS: 4.93040118047814e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 69/71 | LOSS: 4.931956241957128e-06\n",
      "TRAIN: EPOCH 427/1000 | BATCH 70/71 | LOSS: 4.956203342067376e-06\n",
      "VAL: EPOCH 427/1000 | BATCH 0/8 | LOSS: 4.871596956945723e-06\n",
      "VAL: EPOCH 427/1000 | BATCH 1/8 | LOSS: 4.77190087622148e-06\n",
      "VAL: EPOCH 427/1000 | BATCH 2/8 | LOSS: 4.878229750223302e-06\n",
      "VAL: EPOCH 427/1000 | BATCH 3/8 | LOSS: 4.915377985525993e-06\n",
      "VAL: EPOCH 427/1000 | BATCH 4/8 | LOSS: 4.8541057367401665e-06\n",
      "VAL: EPOCH 427/1000 | BATCH 5/8 | LOSS: 4.739140498107493e-06\n",
      "VAL: EPOCH 427/1000 | BATCH 6/8 | LOSS: 4.627765844946095e-06\n",
      "VAL: EPOCH 427/1000 | BATCH 7/8 | LOSS: 4.57931696473679e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 0/71 | LOSS: 4.900411568087293e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 1/71 | LOSS: 4.633200887838029e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 2/71 | LOSS: 4.891419848718215e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 3/71 | LOSS: 4.772610850523051e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 4/71 | LOSS: 5.002221496397397e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 5/71 | LOSS: 4.988316656332851e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 6/71 | LOSS: 4.845705786595188e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 7/71 | LOSS: 4.6876437522769265e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 8/71 | LOSS: 4.6923740531686535e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 9/71 | LOSS: 4.574645981847425e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 10/71 | LOSS: 4.604782010243401e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 11/71 | LOSS: 4.584865716121082e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 12/71 | LOSS: 4.627952316346077e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 13/71 | LOSS: 4.577492745738709e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 14/71 | LOSS: 4.589586023939774e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 15/71 | LOSS: 4.551026847821049e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 16/71 | LOSS: 4.647325728429303e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 17/71 | LOSS: 4.599377727269409e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 18/71 | LOSS: 4.550651654330738e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 19/71 | LOSS: 4.5463392666533766e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 20/71 | LOSS: 4.576006252439886e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 21/71 | LOSS: 4.638106650583557e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 22/71 | LOSS: 4.6235393256210955e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 23/71 | LOSS: 4.659650500116186e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 24/71 | LOSS: 4.630582507161307e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 25/71 | LOSS: 4.6125634823441105e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 26/71 | LOSS: 4.636303005840832e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 27/71 | LOSS: 4.599183528495944e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 28/71 | LOSS: 4.5763995801500075e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 29/71 | LOSS: 4.5625014233034255e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 30/71 | LOSS: 4.547297125569575e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 31/71 | LOSS: 4.5592604038802165e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 32/71 | LOSS: 4.58933500064604e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 33/71 | LOSS: 4.614402742470916e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 34/71 | LOSS: 4.610807445715182e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 35/71 | LOSS: 4.635550087388967e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 36/71 | LOSS: 4.614383927931871e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 37/71 | LOSS: 4.614362763420918e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 38/71 | LOSS: 4.607877960948733e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 39/71 | LOSS: 4.61232951920465e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 40/71 | LOSS: 4.590317530528409e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 41/71 | LOSS: 4.607212185008047e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 42/71 | LOSS: 4.60567162008374e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 43/71 | LOSS: 4.620671120392217e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 44/71 | LOSS: 4.608367412907279e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 45/71 | LOSS: 4.599887668756191e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 46/71 | LOSS: 4.620612315270688e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 47/71 | LOSS: 4.605319427734382e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 48/71 | LOSS: 4.620122952622537e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 49/71 | LOSS: 4.625653300536214e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 50/71 | LOSS: 4.614005462701921e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 51/71 | LOSS: 4.687061837452907e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 52/71 | LOSS: 4.69655537497876e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 53/71 | LOSS: 4.727616636012465e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 54/71 | LOSS: 4.7657400914018085e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 55/71 | LOSS: 4.77341045065519e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 56/71 | LOSS: 4.77228110883101e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 57/71 | LOSS: 4.7870275372180664e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 58/71 | LOSS: 4.78392770273169e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 59/71 | LOSS: 4.7618577013963655e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 60/71 | LOSS: 4.7924443300812005e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 61/71 | LOSS: 4.787605183581388e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 62/71 | LOSS: 4.7824729256443256e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 63/71 | LOSS: 4.787307180009748e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 64/71 | LOSS: 4.774694868431722e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 65/71 | LOSS: 4.779508568583079e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 66/71 | LOSS: 4.752940209181727e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 67/71 | LOSS: 4.741864168355439e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 68/71 | LOSS: 4.757407857365594e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 69/71 | LOSS: 4.7466050773437015e-06\n",
      "TRAIN: EPOCH 428/1000 | BATCH 70/71 | LOSS: 4.741065659275168e-06\n",
      "VAL: EPOCH 428/1000 | BATCH 0/8 | LOSS: 4.941955012327526e-06\n",
      "VAL: EPOCH 428/1000 | BATCH 1/8 | LOSS: 5.280334107737872e-06\n",
      "VAL: EPOCH 428/1000 | BATCH 2/8 | LOSS: 5.471300028148107e-06\n",
      "VAL: EPOCH 428/1000 | BATCH 3/8 | LOSS: 5.650751063512871e-06\n",
      "VAL: EPOCH 428/1000 | BATCH 4/8 | LOSS: 5.5902013627928685e-06\n",
      "VAL: EPOCH 428/1000 | BATCH 5/8 | LOSS: 5.567067167551916e-06\n",
      "VAL: EPOCH 428/1000 | BATCH 6/8 | LOSS: 5.446576324175112e-06\n",
      "VAL: EPOCH 428/1000 | BATCH 7/8 | LOSS: 5.369382392927946e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 0/71 | LOSS: 5.190821866563056e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 1/71 | LOSS: 4.673414878197946e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 2/71 | LOSS: 4.66918997214331e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 3/71 | LOSS: 4.719560593002825e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 4/71 | LOSS: 4.828043711313512e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 5/71 | LOSS: 4.911189004512077e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 6/71 | LOSS: 4.734510282235403e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 7/71 | LOSS: 4.652619367107036e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 8/71 | LOSS: 4.732715675951719e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 9/71 | LOSS: 4.661334082811664e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 10/71 | LOSS: 4.686450190340772e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 11/71 | LOSS: 4.627837843903156e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 12/71 | LOSS: 4.71491073137888e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 13/71 | LOSS: 4.706224168720448e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 14/71 | LOSS: 4.776631537121527e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 15/71 | LOSS: 4.750241132001065e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 16/71 | LOSS: 4.707957553845892e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 17/71 | LOSS: 4.6929980296934245e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 18/71 | LOSS: 4.719786626370287e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 19/71 | LOSS: 4.683139025019045e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 20/71 | LOSS: 4.657718288096173e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 21/71 | LOSS: 4.694928758329876e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 22/71 | LOSS: 4.673588700429916e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 23/71 | LOSS: 4.704334694830929e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 24/71 | LOSS: 4.718834943560068e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 25/71 | LOSS: 4.690952218530024e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 26/71 | LOSS: 4.705540378482611e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 27/71 | LOSS: 4.705596268195222e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 28/71 | LOSS: 4.695936797424079e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 29/71 | LOSS: 4.724899728595725e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 30/71 | LOSS: 4.767203695483923e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 31/71 | LOSS: 4.749414493687709e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 32/71 | LOSS: 4.776293053510017e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 33/71 | LOSS: 4.756017748310542e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 34/71 | LOSS: 4.750815703248788e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 35/71 | LOSS: 4.724007535767467e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 36/71 | LOSS: 4.741978592711668e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 37/71 | LOSS: 4.718080300622923e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 38/71 | LOSS: 4.758809125586338e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 39/71 | LOSS: 4.770416302335434e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 40/71 | LOSS: 4.7602450194972334e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 41/71 | LOSS: 4.77232463037494e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 42/71 | LOSS: 4.759617891173802e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 43/71 | LOSS: 4.788213671376567e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 44/71 | LOSS: 4.775561941035752e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 45/71 | LOSS: 4.803664321274403e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 46/71 | LOSS: 4.807173949534735e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 47/71 | LOSS: 4.839522162569665e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 48/71 | LOSS: 4.868756646754598e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 49/71 | LOSS: 4.890890163551376e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 50/71 | LOSS: 4.920394098471483e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 51/71 | LOSS: 4.920228648713349e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 52/71 | LOSS: 4.933092445496169e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 53/71 | LOSS: 4.913063342563099e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 54/71 | LOSS: 4.9183668248918945e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 55/71 | LOSS: 4.901390464543251e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 56/71 | LOSS: 4.902521105146948e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 57/71 | LOSS: 4.897017894453993e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 58/71 | LOSS: 4.907298489342793e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 59/71 | LOSS: 4.9188966803133855e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 60/71 | LOSS: 4.914831714679584e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 61/71 | LOSS: 4.965307781393706e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 62/71 | LOSS: 4.968924587147841e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 63/71 | LOSS: 4.980970903289972e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 64/71 | LOSS: 4.97879913592232e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 65/71 | LOSS: 4.976521540122507e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 66/71 | LOSS: 4.97831716366723e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 67/71 | LOSS: 4.973853944032753e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 68/71 | LOSS: 4.9651661046864586e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 69/71 | LOSS: 4.952531237125056e-06\n",
      "TRAIN: EPOCH 429/1000 | BATCH 70/71 | LOSS: 4.936367954350073e-06\n",
      "VAL: EPOCH 429/1000 | BATCH 0/8 | LOSS: 6.237366505956743e-06\n",
      "VAL: EPOCH 429/1000 | BATCH 1/8 | LOSS: 6.058219241822371e-06\n",
      "VAL: EPOCH 429/1000 | BATCH 2/8 | LOSS: 6.107989368805041e-06\n",
      "VAL: EPOCH 429/1000 | BATCH 3/8 | LOSS: 6.07348908943095e-06\n",
      "VAL: EPOCH 429/1000 | BATCH 4/8 | LOSS: 5.916042118769837e-06\n",
      "VAL: EPOCH 429/1000 | BATCH 5/8 | LOSS: 5.766591584688285e-06\n",
      "VAL: EPOCH 429/1000 | BATCH 6/8 | LOSS: 5.679702228787521e-06\n",
      "VAL: EPOCH 429/1000 | BATCH 7/8 | LOSS: 5.606878517028235e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 0/71 | LOSS: 5.598018560704077e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 1/71 | LOSS: 5.5384623465215554e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 2/71 | LOSS: 5.400587572997513e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 3/71 | LOSS: 5.427603241514589e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 4/71 | LOSS: 5.330748990672873e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 5/71 | LOSS: 5.103255034555332e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 6/71 | LOSS: 5.206384685152443e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 7/71 | LOSS: 5.269673749808135e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 8/71 | LOSS: 5.135561928505518e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 9/71 | LOSS: 5.152025778443203e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 10/71 | LOSS: 5.121375662301117e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 11/71 | LOSS: 5.182442540293171e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 12/71 | LOSS: 5.169943506189156e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 13/71 | LOSS: 5.029133928603967e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 14/71 | LOSS: 4.990124898540671e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 15/71 | LOSS: 4.975345362367989e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 16/71 | LOSS: 5.026897015876089e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 17/71 | LOSS: 5.025490850130154e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 18/71 | LOSS: 5.0357762479267735e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 19/71 | LOSS: 4.96264601679286e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 20/71 | LOSS: 4.9158295171634696e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 21/71 | LOSS: 4.929562464894843e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 22/71 | LOSS: 4.915258040969291e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 23/71 | LOSS: 4.9317648252629924e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 24/71 | LOSS: 5.111004520585994e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 25/71 | LOSS: 5.175959603757991e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 26/71 | LOSS: 5.195700918260694e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 27/71 | LOSS: 5.20703818957762e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 28/71 | LOSS: 5.249248652975878e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 29/71 | LOSS: 5.2637833808451735e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 30/71 | LOSS: 5.322669626513098e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 31/71 | LOSS: 5.321528661283992e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 32/71 | LOSS: 5.423896268577224e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 33/71 | LOSS: 5.435135268699549e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 34/71 | LOSS: 5.518024993632155e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 35/71 | LOSS: 5.513781287744577e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 36/71 | LOSS: 5.559657463006285e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 37/71 | LOSS: 5.608711729253532e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 38/71 | LOSS: 5.585098617116157e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 39/71 | LOSS: 5.584095924859867e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 40/71 | LOSS: 5.5901516644531155e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 41/71 | LOSS: 5.60348192785958e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 42/71 | LOSS: 5.576652436206513e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 43/71 | LOSS: 5.556990179849725e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 44/71 | LOSS: 5.536296137304615e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 45/71 | LOSS: 5.516945095777096e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 46/71 | LOSS: 5.5308753852590055e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 47/71 | LOSS: 5.5295659440920035e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 48/71 | LOSS: 5.524947603274142e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 49/71 | LOSS: 5.5166467336675855e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 50/71 | LOSS: 5.545104658285352e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 51/71 | LOSS: 5.523617600234642e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 52/71 | LOSS: 5.487828086600375e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 53/71 | LOSS: 5.507719522399755e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 54/71 | LOSS: 5.505062794823475e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 55/71 | LOSS: 5.494099923453177e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 56/71 | LOSS: 5.469990132392891e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 57/71 | LOSS: 5.476300014473462e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 58/71 | LOSS: 5.46249742201634e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 59/71 | LOSS: 5.453305167672321e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 60/71 | LOSS: 5.4432186113787835e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 61/71 | LOSS: 5.48012235412898e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 62/71 | LOSS: 5.46052019166786e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 63/71 | LOSS: 5.4801797659820295e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 64/71 | LOSS: 5.482868422964552e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 65/71 | LOSS: 5.477551088585063e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 66/71 | LOSS: 5.467702616877079e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 67/71 | LOSS: 5.4868866540988056e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 68/71 | LOSS: 5.4766328305603285e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 69/71 | LOSS: 5.4631646500118745e-06\n",
      "TRAIN: EPOCH 430/1000 | BATCH 70/71 | LOSS: 5.513076527203223e-06\n",
      "VAL: EPOCH 430/1000 | BATCH 0/8 | LOSS: 5.113927272759611e-06\n",
      "VAL: EPOCH 430/1000 | BATCH 1/8 | LOSS: 5.116133934279787e-06\n",
      "VAL: EPOCH 430/1000 | BATCH 2/8 | LOSS: 5.111545154553217e-06\n",
      "VAL: EPOCH 430/1000 | BATCH 3/8 | LOSS: 5.317921932146419e-06\n",
      "VAL: EPOCH 430/1000 | BATCH 4/8 | LOSS: 5.188002251088619e-06\n",
      "VAL: EPOCH 430/1000 | BATCH 5/8 | LOSS: 5.173943160722653e-06\n",
      "VAL: EPOCH 430/1000 | BATCH 6/8 | LOSS: 5.037053402442585e-06\n",
      "VAL: EPOCH 430/1000 | BATCH 7/8 | LOSS: 4.8969462795867e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 0/71 | LOSS: 4.381671260489384e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 1/71 | LOSS: 4.8962454002321465e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 2/71 | LOSS: 5.107436057490607e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 3/71 | LOSS: 5.20788978519704e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 4/71 | LOSS: 4.890582022198941e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 5/71 | LOSS: 4.866669845190093e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 6/71 | LOSS: 4.866073335766227e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 7/71 | LOSS: 4.845261344144092e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 8/71 | LOSS: 4.70825163271608e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 9/71 | LOSS: 4.7678653800176106e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 10/71 | LOSS: 4.876164832074375e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 11/71 | LOSS: 4.8410065005555225e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 12/71 | LOSS: 4.9009358393120165e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 13/71 | LOSS: 4.914978571183123e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 14/71 | LOSS: 4.78562333228183e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 15/71 | LOSS: 4.862586621356968e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 16/71 | LOSS: 4.858966937771423e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 17/71 | LOSS: 4.852117070994508e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 18/71 | LOSS: 4.885062265346182e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 19/71 | LOSS: 4.899998089058499e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 20/71 | LOSS: 4.908800162541281e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 21/71 | LOSS: 4.943798186022005e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 22/71 | LOSS: 4.898134648101404e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 23/71 | LOSS: 4.864742190117492e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 24/71 | LOSS: 4.854966591665288e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 25/71 | LOSS: 4.872112797854173e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 26/71 | LOSS: 4.817292198856228e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 27/71 | LOSS: 4.786816451282253e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 28/71 | LOSS: 4.8241999028787494e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 29/71 | LOSS: 4.8396752163171184e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 30/71 | LOSS: 4.851761398259118e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 31/71 | LOSS: 4.823855626057139e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 32/71 | LOSS: 4.817642610016099e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 33/71 | LOSS: 4.817035199480683e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 34/71 | LOSS: 4.821915204697039e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 35/71 | LOSS: 4.808926816066459e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 36/71 | LOSS: 4.809502859250249e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 37/71 | LOSS: 4.838960379809003e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 38/71 | LOSS: 4.898751361300431e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 39/71 | LOSS: 4.896529111420022e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 40/71 | LOSS: 4.907427393579671e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 41/71 | LOSS: 4.946036342733437e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 42/71 | LOSS: 4.929152155878106e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 43/71 | LOSS: 4.93927929441766e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 44/71 | LOSS: 4.957501727807944e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 45/71 | LOSS: 4.963688233463314e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 46/71 | LOSS: 4.933793173009651e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 47/71 | LOSS: 4.918962630995338e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 48/71 | LOSS: 4.884978250406768e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 49/71 | LOSS: 4.850013137911446e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 50/71 | LOSS: 4.835191694106541e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 51/71 | LOSS: 4.8279722827478854e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 52/71 | LOSS: 4.807119977450161e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 53/71 | LOSS: 4.7903920909043845e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 54/71 | LOSS: 4.798631446399006e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 55/71 | LOSS: 4.782630583122227e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 56/71 | LOSS: 4.797096137241297e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 57/71 | LOSS: 4.8072816157243856e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 58/71 | LOSS: 4.78729596240663e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 59/71 | LOSS: 4.778659854309808e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 60/71 | LOSS: 4.775770955094514e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 61/71 | LOSS: 4.791521933516525e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 62/71 | LOSS: 4.782169634425026e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 63/71 | LOSS: 4.79061923641666e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 64/71 | LOSS: 4.7875211748760195e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 65/71 | LOSS: 4.779306615973562e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 66/71 | LOSS: 4.801253568862805e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 67/71 | LOSS: 4.807566563827363e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 68/71 | LOSS: 4.801846312213878e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 69/71 | LOSS: 4.820485914284031e-06\n",
      "TRAIN: EPOCH 431/1000 | BATCH 70/71 | LOSS: 4.785963524932093e-06\n",
      "VAL: EPOCH 431/1000 | BATCH 0/8 | LOSS: 9.674513421487063e-06\n",
      "VAL: EPOCH 431/1000 | BATCH 1/8 | LOSS: 9.232127013092395e-06\n",
      "VAL: EPOCH 431/1000 | BATCH 2/8 | LOSS: 9.08949793180606e-06\n",
      "VAL: EPOCH 431/1000 | BATCH 3/8 | LOSS: 8.968421525423764e-06\n",
      "VAL: EPOCH 431/1000 | BATCH 4/8 | LOSS: 8.90319570316933e-06\n",
      "VAL: EPOCH 431/1000 | BATCH 5/8 | LOSS: 8.454934459223296e-06\n",
      "VAL: EPOCH 431/1000 | BATCH 6/8 | LOSS: 8.332454594242985e-06\n",
      "VAL: EPOCH 431/1000 | BATCH 7/8 | LOSS: 8.125358647248504e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 0/71 | LOSS: 8.534272637916729e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 1/71 | LOSS: 6.22390666649153e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 2/71 | LOSS: 7.20252000974142e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 3/71 | LOSS: 6.5884570403795806e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 4/71 | LOSS: 6.341123844322283e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 5/71 | LOSS: 6.416297613516993e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 6/71 | LOSS: 6.263918619099838e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 7/71 | LOSS: 6.135559090125753e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 8/71 | LOSS: 5.900057759491675e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 9/71 | LOSS: 5.9895775848417545e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 10/71 | LOSS: 5.948354720947629e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 11/71 | LOSS: 5.89161216642727e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 12/71 | LOSS: 5.959138539205574e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 13/71 | LOSS: 5.8743424428290126e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 14/71 | LOSS: 5.787066644794928e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 15/71 | LOSS: 5.698799640185825e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 16/71 | LOSS: 5.723222543832997e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 17/71 | LOSS: 5.708869063406989e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 18/71 | LOSS: 5.651801362354921e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 19/71 | LOSS: 5.619277408186462e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 20/71 | LOSS: 5.658835843427196e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 21/71 | LOSS: 5.650195052195076e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 22/71 | LOSS: 5.613986480096608e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 23/71 | LOSS: 5.596812741259782e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 24/71 | LOSS: 5.5500893722637555e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 25/71 | LOSS: 5.499691531860242e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 26/71 | LOSS: 5.505556482940274e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 27/71 | LOSS: 5.445653024643045e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 28/71 | LOSS: 5.377903852573142e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 29/71 | LOSS: 5.312110586904358e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 30/71 | LOSS: 5.310893819173103e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 31/71 | LOSS: 5.293762832536686e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 32/71 | LOSS: 5.302569544448598e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 33/71 | LOSS: 5.286593417461999e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 34/71 | LOSS: 5.228577687635802e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 35/71 | LOSS: 5.208767289685865e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 36/71 | LOSS: 5.230066151213848e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 37/71 | LOSS: 5.2126205218883115e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 38/71 | LOSS: 5.177793547353269e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 39/71 | LOSS: 5.180041495123078e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 40/71 | LOSS: 5.164747335584252e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 41/71 | LOSS: 5.184360940715817e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 42/71 | LOSS: 5.18534384778028e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 43/71 | LOSS: 5.176037621946637e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 44/71 | LOSS: 5.159125617461137e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 45/71 | LOSS: 5.161127897617521e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 46/71 | LOSS: 5.162564035499793e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 47/71 | LOSS: 5.201695392050472e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 48/71 | LOSS: 5.200010097111345e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 49/71 | LOSS: 5.224180872573925e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 50/71 | LOSS: 5.281796798133691e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 51/71 | LOSS: 5.2776041675665165e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 52/71 | LOSS: 5.368162569624782e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 53/71 | LOSS: 5.361154101727287e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 54/71 | LOSS: 5.434563052569336e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 55/71 | LOSS: 5.40635279630156e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 56/71 | LOSS: 5.448712787064754e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 57/71 | LOSS: 5.489074043970472e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 58/71 | LOSS: 5.513454147981297e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 59/71 | LOSS: 5.514868246336846e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 60/71 | LOSS: 5.546709957581966e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 61/71 | LOSS: 5.5842289010533635e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 62/71 | LOSS: 5.579460483724168e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 63/71 | LOSS: 5.6569881046186765e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 64/71 | LOSS: 5.667163416616789e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 65/71 | LOSS: 5.64956146166823e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 66/71 | LOSS: 5.662601450294189e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 67/71 | LOSS: 5.676228140065873e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 68/71 | LOSS: 5.66267837145289e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 69/71 | LOSS: 5.6558765956132055e-06\n",
      "TRAIN: EPOCH 432/1000 | BATCH 70/71 | LOSS: 5.709170639894298e-06\n",
      "VAL: EPOCH 432/1000 | BATCH 0/8 | LOSS: 5.065113327873405e-06\n",
      "VAL: EPOCH 432/1000 | BATCH 1/8 | LOSS: 4.811644885194255e-06\n",
      "VAL: EPOCH 432/1000 | BATCH 2/8 | LOSS: 4.759613754382978e-06\n",
      "VAL: EPOCH 432/1000 | BATCH 3/8 | LOSS: 4.760336764775275e-06\n",
      "VAL: EPOCH 432/1000 | BATCH 4/8 | LOSS: 4.655081011151197e-06\n",
      "VAL: EPOCH 432/1000 | BATCH 5/8 | LOSS: 4.499067434456568e-06\n",
      "VAL: EPOCH 432/1000 | BATCH 6/8 | LOSS: 4.429789279518965e-06\n",
      "VAL: EPOCH 432/1000 | BATCH 7/8 | LOSS: 4.317994807934156e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 0/71 | LOSS: 4.794557753484696e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 1/71 | LOSS: 5.095990218251245e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 2/71 | LOSS: 5.323044206306804e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 3/71 | LOSS: 4.945326054439647e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 4/71 | LOSS: 5.508705362444743e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 5/71 | LOSS: 5.368109138241077e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 6/71 | LOSS: 5.370533277268155e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 7/71 | LOSS: 5.273129261240683e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 8/71 | LOSS: 5.207134108786704e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 9/71 | LOSS: 5.1730773066083206e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 10/71 | LOSS: 5.318531334937275e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 11/71 | LOSS: 5.3539106753911865e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 12/71 | LOSS: 5.373031784144517e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 13/71 | LOSS: 5.432022102728037e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 14/71 | LOSS: 5.3773415553829786e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 15/71 | LOSS: 5.316986033676585e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 16/71 | LOSS: 5.274544144092087e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 17/71 | LOSS: 5.261644774792431e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 18/71 | LOSS: 5.1688466795128894e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 19/71 | LOSS: 5.067777397016471e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 20/71 | LOSS: 5.0267624265827525e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 21/71 | LOSS: 5.035104033430054e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 22/71 | LOSS: 5.005148523196112e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 23/71 | LOSS: 5.0114305546837086e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 24/71 | LOSS: 5.005721504858229e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 25/71 | LOSS: 4.981417882495757e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 26/71 | LOSS: 4.999795102641521e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 27/71 | LOSS: 5.031437759888442e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 28/71 | LOSS: 5.030543951730139e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 29/71 | LOSS: 5.003418179209499e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 30/71 | LOSS: 4.971259797105543e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 31/71 | LOSS: 4.9526003209621194e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 32/71 | LOSS: 4.9677047227856805e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 33/71 | LOSS: 4.952267411628033e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 34/71 | LOSS: 4.9940701891111014e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 35/71 | LOSS: 4.978474673104453e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 36/71 | LOSS: 4.9671212223443404e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 37/71 | LOSS: 4.976838418339244e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 38/71 | LOSS: 4.995118149217421e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 39/71 | LOSS: 4.969207247995655e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 40/71 | LOSS: 4.938041556448386e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 41/71 | LOSS: 4.964870142776774e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 42/71 | LOSS: 5.003309059586257e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 43/71 | LOSS: 4.965561205691219e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 44/71 | LOSS: 5.002437521600061e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 45/71 | LOSS: 4.988782601013435e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 46/71 | LOSS: 5.020317830519374e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 47/71 | LOSS: 5.063004455981475e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 48/71 | LOSS: 5.06385586792318e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 49/71 | LOSS: 5.135933552082861e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 50/71 | LOSS: 5.142631594293669e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 51/71 | LOSS: 5.142551782018228e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 52/71 | LOSS: 5.1325579199397425e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 53/71 | LOSS: 5.143099525850158e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 54/71 | LOSS: 5.1323355695455e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 55/71 | LOSS: 5.11053545843814e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 56/71 | LOSS: 5.12248401432053e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 57/71 | LOSS: 5.118743470590963e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 58/71 | LOSS: 5.117155015503127e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 59/71 | LOSS: 5.1065120411900955e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 60/71 | LOSS: 5.093117616510068e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 61/71 | LOSS: 5.093702384328935e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 62/71 | LOSS: 5.0616950786553345e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 63/71 | LOSS: 5.057724735024749e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 64/71 | LOSS: 5.039174336091562e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 65/71 | LOSS: 5.019444477676933e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 66/71 | LOSS: 5.008877743326159e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 67/71 | LOSS: 4.999170140335456e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 68/71 | LOSS: 4.982310551038582e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 69/71 | LOSS: 4.959813109702996e-06\n",
      "TRAIN: EPOCH 433/1000 | BATCH 70/71 | LOSS: 4.929693444941683e-06\n",
      "VAL: EPOCH 433/1000 | BATCH 0/8 | LOSS: 4.4632843128056265e-06\n",
      "VAL: EPOCH 433/1000 | BATCH 1/8 | LOSS: 4.444997330210754e-06\n",
      "VAL: EPOCH 433/1000 | BATCH 2/8 | LOSS: 4.4855925504331635e-06\n",
      "VAL: EPOCH 433/1000 | BATCH 3/8 | LOSS: 4.499792112255818e-06\n",
      "VAL: EPOCH 433/1000 | BATCH 4/8 | LOSS: 4.428650572663173e-06\n",
      "VAL: EPOCH 433/1000 | BATCH 5/8 | LOSS: 4.287898264010437e-06\n",
      "VAL: EPOCH 433/1000 | BATCH 6/8 | LOSS: 4.158313426419877e-06\n",
      "VAL: EPOCH 433/1000 | BATCH 7/8 | LOSS: 4.034888831938588e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 0/71 | LOSS: 4.304814865463413e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 1/71 | LOSS: 4.316104423196521e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 2/71 | LOSS: 4.083293940008541e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 3/71 | LOSS: 4.2209034063489526e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 4/71 | LOSS: 4.264405379217351e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 5/71 | LOSS: 4.342677812019247e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 6/71 | LOSS: 4.311932246179952e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 7/71 | LOSS: 4.327761530475982e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 8/71 | LOSS: 4.251162130862616e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 9/71 | LOSS: 4.2404301666465475e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 10/71 | LOSS: 4.167234299695172e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 11/71 | LOSS: 4.185861162871636e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 12/71 | LOSS: 4.2746279142682815e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 13/71 | LOSS: 4.2785518904305974e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 14/71 | LOSS: 4.310967976077033e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 15/71 | LOSS: 4.253847308177683e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 16/71 | LOSS: 4.292630324818995e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 17/71 | LOSS: 4.342124624498764e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 18/71 | LOSS: 4.335134774105666e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 19/71 | LOSS: 4.375118544430734e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 20/71 | LOSS: 4.424870732171777e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 21/71 | LOSS: 4.445446816961604e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 22/71 | LOSS: 4.408623483512834e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 23/71 | LOSS: 4.4323265910861664e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 24/71 | LOSS: 4.438583437149645e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 25/71 | LOSS: 4.4930627609574e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 26/71 | LOSS: 4.541705375568536e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 27/71 | LOSS: 4.569627435557777e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 28/71 | LOSS: 4.554772878126924e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 29/71 | LOSS: 4.569985973527461e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 30/71 | LOSS: 4.572271982103684e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 31/71 | LOSS: 4.603627687060907e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 32/71 | LOSS: 4.582810120292848e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 33/71 | LOSS: 4.586223560502677e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 34/71 | LOSS: 4.601114791772229e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 35/71 | LOSS: 4.599045168005331e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 36/71 | LOSS: 4.6279153409429725e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 37/71 | LOSS: 4.601795922098972e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 38/71 | LOSS: 4.635841570724062e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 39/71 | LOSS: 4.588325282384176e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 40/71 | LOSS: 4.584426345146068e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 41/71 | LOSS: 4.5833383063879975e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 42/71 | LOSS: 4.570616162053802e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 43/71 | LOSS: 4.567787486435422e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 44/71 | LOSS: 4.569755199352383e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 45/71 | LOSS: 4.568255146454169e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 46/71 | LOSS: 4.540866803894305e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 47/71 | LOSS: 4.526863572588506e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 48/71 | LOSS: 4.518957434718649e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 49/71 | LOSS: 4.4902317404194034e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 50/71 | LOSS: 4.50049195505391e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 51/71 | LOSS: 4.492144993351898e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 52/71 | LOSS: 4.480313005190527e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 53/71 | LOSS: 4.507391403657729e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 54/71 | LOSS: 4.5043833779345736e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 55/71 | LOSS: 4.500607011388118e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 56/71 | LOSS: 4.501676501508643e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 57/71 | LOSS: 4.5011070455827926e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 58/71 | LOSS: 4.48860900779452e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 59/71 | LOSS: 4.500650106820103e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 60/71 | LOSS: 4.488627587679581e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 61/71 | LOSS: 4.480781131537844e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 62/71 | LOSS: 4.494109336721724e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 63/71 | LOSS: 4.498250316231633e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 64/71 | LOSS: 4.51216017679074e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 65/71 | LOSS: 4.503613977800397e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 66/71 | LOSS: 4.548798001247741e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 67/71 | LOSS: 4.553761216892085e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 68/71 | LOSS: 4.582675853890723e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 69/71 | LOSS: 4.580831510533504e-06\n",
      "TRAIN: EPOCH 434/1000 | BATCH 70/71 | LOSS: 4.596483331368572e-06\n",
      "VAL: EPOCH 434/1000 | BATCH 0/8 | LOSS: 7.789512892486528e-06\n",
      "VAL: EPOCH 434/1000 | BATCH 1/8 | LOSS: 7.474132416973589e-06\n",
      "VAL: EPOCH 434/1000 | BATCH 2/8 | LOSS: 7.527338008609756e-06\n",
      "VAL: EPOCH 434/1000 | BATCH 3/8 | LOSS: 7.50021206386009e-06\n",
      "VAL: EPOCH 434/1000 | BATCH 4/8 | LOSS: 7.457044193870388e-06\n",
      "VAL: EPOCH 434/1000 | BATCH 5/8 | LOSS: 7.201538664958207e-06\n",
      "VAL: EPOCH 434/1000 | BATCH 6/8 | LOSS: 7.241888397402363e-06\n",
      "VAL: EPOCH 434/1000 | BATCH 7/8 | LOSS: 7.041257561013481e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 0/71 | LOSS: 6.693428986181971e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 1/71 | LOSS: 6.928740731382277e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 2/71 | LOSS: 6.789805562827193e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 3/71 | LOSS: 6.151087632133567e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 4/71 | LOSS: 6.890202803333522e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 5/71 | LOSS: 6.7605408275994705e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 6/71 | LOSS: 6.802757980040042e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 7/71 | LOSS: 6.638598222252767e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 8/71 | LOSS: 6.531558281292544e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 9/71 | LOSS: 6.371158633555751e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 10/71 | LOSS: 6.092627762138198e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 11/71 | LOSS: 6.025825295334168e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 12/71 | LOSS: 6.032929185004412e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 13/71 | LOSS: 5.8335945303562246e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 14/71 | LOSS: 5.951584459277607e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 15/71 | LOSS: 5.9112872889954815e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 16/71 | LOSS: 5.9274703236202615e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 17/71 | LOSS: 5.831757322185796e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 18/71 | LOSS: 5.818928349598489e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 19/71 | LOSS: 5.891495163723448e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 20/71 | LOSS: 5.930957123772325e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 21/71 | LOSS: 5.973732506638705e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 22/71 | LOSS: 5.918927805080033e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 23/71 | LOSS: 6.0020276938151556e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 24/71 | LOSS: 5.992237665850553e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 25/71 | LOSS: 6.072809661908161e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 26/71 | LOSS: 6.097620918673648e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 27/71 | LOSS: 6.021267665801133e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 28/71 | LOSS: 5.984959278944659e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 29/71 | LOSS: 5.9163160055201536e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 30/71 | LOSS: 5.8566707197784245e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 31/71 | LOSS: 5.7950843697085475e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 32/71 | LOSS: 5.7450582434367705e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 33/71 | LOSS: 5.705053935986036e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 34/71 | LOSS: 5.679599306469235e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 35/71 | LOSS: 5.619913792997977e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 36/71 | LOSS: 5.550587742176061e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 37/71 | LOSS: 5.513777254188761e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 38/71 | LOSS: 5.476408304252217e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 39/71 | LOSS: 5.451026203218134e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 40/71 | LOSS: 5.428151031837177e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 41/71 | LOSS: 5.39711045023848e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 42/71 | LOSS: 5.373259846448999e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 43/71 | LOSS: 5.38444140829597e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 44/71 | LOSS: 5.3574166435686895e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 45/71 | LOSS: 5.328056693582292e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 46/71 | LOSS: 5.308927460104547e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 47/71 | LOSS: 5.31363719365648e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 48/71 | LOSS: 5.2826814625200775e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 49/71 | LOSS: 5.292956320772646e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 50/71 | LOSS: 5.294376202144514e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 51/71 | LOSS: 5.320530581579637e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 52/71 | LOSS: 5.293097994993177e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 53/71 | LOSS: 5.29569650064298e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 54/71 | LOSS: 5.285268269455462e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 55/71 | LOSS: 5.267002433941213e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 56/71 | LOSS: 5.293556792470586e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 57/71 | LOSS: 5.323304251227267e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 58/71 | LOSS: 5.304018740990629e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 59/71 | LOSS: 5.297177987510319e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 60/71 | LOSS: 5.3053320131811395e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 61/71 | LOSS: 5.309217525087485e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 62/71 | LOSS: 5.293448050487935e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 63/71 | LOSS: 5.291195414258709e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 64/71 | LOSS: 5.306540123666099e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 65/71 | LOSS: 5.283067962685009e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 66/71 | LOSS: 5.2720706221339585e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 67/71 | LOSS: 5.277706814949317e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 68/71 | LOSS: 5.274089438029003e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 69/71 | LOSS: 5.2644534629767545e-06\n",
      "TRAIN: EPOCH 435/1000 | BATCH 70/71 | LOSS: 5.260345668714981e-06\n",
      "VAL: EPOCH 435/1000 | BATCH 0/8 | LOSS: 7.519529390265234e-06\n",
      "VAL: EPOCH 435/1000 | BATCH 1/8 | LOSS: 7.031191216810839e-06\n",
      "VAL: EPOCH 435/1000 | BATCH 2/8 | LOSS: 6.813243089709431e-06\n",
      "VAL: EPOCH 435/1000 | BATCH 3/8 | LOSS: 6.881121748847363e-06\n",
      "VAL: EPOCH 435/1000 | BATCH 4/8 | LOSS: 6.6127328864240555e-06\n",
      "VAL: EPOCH 435/1000 | BATCH 5/8 | LOSS: 6.38575784250861e-06\n",
      "VAL: EPOCH 435/1000 | BATCH 6/8 | LOSS: 6.155422072749518e-06\n",
      "VAL: EPOCH 435/1000 | BATCH 7/8 | LOSS: 5.89321405186638e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 0/71 | LOSS: 5.868788321095053e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 1/71 | LOSS: 6.112810979175265e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 2/71 | LOSS: 5.91859300887639e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 3/71 | LOSS: 5.98302051457722e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 4/71 | LOSS: 5.515831162483664e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 5/71 | LOSS: 5.8988344638540484e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 6/71 | LOSS: 5.752034862130781e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 7/71 | LOSS: 5.820247338306217e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 8/71 | LOSS: 5.795498509542085e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 9/71 | LOSS: 6.047077840776183e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 10/71 | LOSS: 6.154580329497218e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 11/71 | LOSS: 6.203093410780032e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 12/71 | LOSS: 6.290524722247098e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 13/71 | LOSS: 6.211374966369476e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 14/71 | LOSS: 6.136842269673556e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 15/71 | LOSS: 6.104672962692348e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 16/71 | LOSS: 6.093828794880264e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 17/71 | LOSS: 6.040455673428369e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 18/71 | LOSS: 6.015952701426086e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 19/71 | LOSS: 5.948912917119742e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 20/71 | LOSS: 5.840277513665829e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 21/71 | LOSS: 5.83449587504054e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 22/71 | LOSS: 5.758281917144854e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 23/71 | LOSS: 5.782657742277782e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 24/71 | LOSS: 5.748419143856154e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 25/71 | LOSS: 5.770520767957631e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 26/71 | LOSS: 5.741764408850469e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 27/71 | LOSS: 5.751003162198945e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 28/71 | LOSS: 5.770328831705887e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 29/71 | LOSS: 5.727501040079611e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 30/71 | LOSS: 5.7411271046153305e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 31/71 | LOSS: 5.66589103101478e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 32/71 | LOSS: 5.655890073753791e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 33/71 | LOSS: 5.633305236455378e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 34/71 | LOSS: 5.6021252216201645e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 35/71 | LOSS: 5.659069497849285e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 36/71 | LOSS: 5.6436108602733965e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 37/71 | LOSS: 5.704948624665313e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 38/71 | LOSS: 5.7359674230657474e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 39/71 | LOSS: 5.776701897275416e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 40/71 | LOSS: 5.752593756924176e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 41/71 | LOSS: 5.715865735770162e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 42/71 | LOSS: 5.749482524638446e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 43/71 | LOSS: 5.714942749984237e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 44/71 | LOSS: 5.70566531779251e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 45/71 | LOSS: 5.694844043832943e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 46/71 | LOSS: 5.70124486930194e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 47/71 | LOSS: 5.6652767028708695e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 48/71 | LOSS: 5.629123339828339e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 49/71 | LOSS: 5.6195612296505715e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 50/71 | LOSS: 5.589341218878604e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 51/71 | LOSS: 5.566591614052203e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 52/71 | LOSS: 5.572539534251467e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 53/71 | LOSS: 5.558679912677757e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 54/71 | LOSS: 5.576181489562838e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 55/71 | LOSS: 5.567954949421099e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 56/71 | LOSS: 5.564948084786904e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 57/71 | LOSS: 5.533991180402217e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 58/71 | LOSS: 5.532595658655351e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 59/71 | LOSS: 5.490332803977557e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 60/71 | LOSS: 5.485450379101018e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 61/71 | LOSS: 5.455492944219582e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 62/71 | LOSS: 5.4337607570420325e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 63/71 | LOSS: 5.418511200616649e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 64/71 | LOSS: 5.3930230268843636e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 65/71 | LOSS: 5.367750094537217e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 66/71 | LOSS: 5.354176997272263e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 67/71 | LOSS: 5.321263843678239e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 68/71 | LOSS: 5.312539292060757e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 69/71 | LOSS: 5.322841707311454e-06\n",
      "TRAIN: EPOCH 436/1000 | BATCH 70/71 | LOSS: 5.296550122860116e-06\n",
      "VAL: EPOCH 436/1000 | BATCH 0/8 | LOSS: 5.442133442556951e-06\n",
      "VAL: EPOCH 436/1000 | BATCH 1/8 | LOSS: 5.309715788825997e-06\n",
      "VAL: EPOCH 436/1000 | BATCH 2/8 | LOSS: 5.383158319697638e-06\n",
      "VAL: EPOCH 436/1000 | BATCH 3/8 | LOSS: 5.26004566836491e-06\n",
      "VAL: EPOCH 436/1000 | BATCH 4/8 | LOSS: 5.24905372003559e-06\n",
      "VAL: EPOCH 436/1000 | BATCH 5/8 | LOSS: 5.098993369756499e-06\n",
      "VAL: EPOCH 436/1000 | BATCH 6/8 | LOSS: 5.049536282188326e-06\n",
      "VAL: EPOCH 436/1000 | BATCH 7/8 | LOSS: 5.019865511712851e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 0/71 | LOSS: 4.6709965317859314e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 1/71 | LOSS: 4.327161150285974e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 2/71 | LOSS: 4.149014027158652e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 3/71 | LOSS: 4.846064825869689e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 4/71 | LOSS: 4.858089323533932e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 5/71 | LOSS: 4.818430928329083e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 6/71 | LOSS: 4.8649059993165015e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 7/71 | LOSS: 4.7334874579973985e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 8/71 | LOSS: 4.663699352628707e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 9/71 | LOSS: 4.584087992043351e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 10/71 | LOSS: 4.518206589471612e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 11/71 | LOSS: 4.4926151000860654e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 12/71 | LOSS: 4.5639380564702714e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 13/71 | LOSS: 4.510995722739608e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 14/71 | LOSS: 4.509153404796961e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 15/71 | LOSS: 4.48979045586384e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 16/71 | LOSS: 4.439401289560278e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 17/71 | LOSS: 4.47092319670143e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 18/71 | LOSS: 4.540946197027408e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 19/71 | LOSS: 4.565757160435169e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 20/71 | LOSS: 4.529423460466898e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 21/71 | LOSS: 4.572086998616049e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 22/71 | LOSS: 4.512223095725463e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 23/71 | LOSS: 4.509029812046113e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 24/71 | LOSS: 4.469407695069095e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 25/71 | LOSS: 4.483020968739136e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 26/71 | LOSS: 4.514007127067156e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 27/71 | LOSS: 4.525737551830389e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 28/71 | LOSS: 4.490401085510304e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 29/71 | LOSS: 4.514022900063234e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 30/71 | LOSS: 4.5619434522156935e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 31/71 | LOSS: 4.539989319596316e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 32/71 | LOSS: 4.5279045728172855e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 33/71 | LOSS: 4.559192776094492e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 34/71 | LOSS: 4.581271579289543e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 35/71 | LOSS: 4.569086879908153e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 36/71 | LOSS: 4.561374139853256e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 37/71 | LOSS: 4.572665829982725e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 38/71 | LOSS: 4.570597796443438e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 39/71 | LOSS: 4.587193609495444e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 40/71 | LOSS: 4.609286695152306e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 41/71 | LOSS: 4.605134628426251e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 42/71 | LOSS: 4.5718331611444525e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 43/71 | LOSS: 4.609934669794959e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 44/71 | LOSS: 4.597704678922633e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 45/71 | LOSS: 4.598715013739622e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 46/71 | LOSS: 4.595866231922744e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 47/71 | LOSS: 4.6323689465073885e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 48/71 | LOSS: 4.608119719965284e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 49/71 | LOSS: 4.603826764650875e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 50/71 | LOSS: 4.608212899862944e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 51/71 | LOSS: 4.594146748086831e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 52/71 | LOSS: 4.586767798447806e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 53/71 | LOSS: 4.595277737785687e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 54/71 | LOSS: 4.5797847509344465e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 55/71 | LOSS: 4.5652858854542245e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 56/71 | LOSS: 4.578287209305767e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 57/71 | LOSS: 4.564968725358555e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 58/71 | LOSS: 4.5489425432051425e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 59/71 | LOSS: 4.549410899320113e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 60/71 | LOSS: 4.55396202298408e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 61/71 | LOSS: 4.544824627549632e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 62/71 | LOSS: 4.521785776598827e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 63/71 | LOSS: 4.50813664443217e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 64/71 | LOSS: 4.498588326056891e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 65/71 | LOSS: 4.491908091977441e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 66/71 | LOSS: 4.513320784859226e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 67/71 | LOSS: 4.523772080280169e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 68/71 | LOSS: 4.514373803549856e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 69/71 | LOSS: 4.524463452071359e-06\n",
      "TRAIN: EPOCH 437/1000 | BATCH 70/71 | LOSS: 4.5117649791227705e-06\n",
      "VAL: EPOCH 437/1000 | BATCH 0/8 | LOSS: 5.946297278569546e-06\n",
      "VAL: EPOCH 437/1000 | BATCH 1/8 | LOSS: 5.659079306497006e-06\n",
      "VAL: EPOCH 437/1000 | BATCH 2/8 | LOSS: 5.6480457715224475e-06\n",
      "VAL: EPOCH 437/1000 | BATCH 3/8 | LOSS: 5.546550255530747e-06\n",
      "VAL: EPOCH 437/1000 | BATCH 4/8 | LOSS: 5.504198452399578e-06\n",
      "VAL: EPOCH 437/1000 | BATCH 5/8 | LOSS: 5.236009201325942e-06\n",
      "VAL: EPOCH 437/1000 | BATCH 6/8 | LOSS: 5.0759355352576156e-06\n",
      "VAL: EPOCH 437/1000 | BATCH 7/8 | LOSS: 4.9928888756767265e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 0/71 | LOSS: 4.100808382645482e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 1/71 | LOSS: 4.02588761971856e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 2/71 | LOSS: 4.245202793147958e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 3/71 | LOSS: 4.350198423708207e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 4/71 | LOSS: 4.597775387082947e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 5/71 | LOSS: 4.3668075022651465e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 6/71 | LOSS: 4.336016218076111e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 7/71 | LOSS: 4.556687002832405e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 8/71 | LOSS: 4.58322243604117e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 9/71 | LOSS: 4.582843826028693e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 10/71 | LOSS: 4.499525175560848e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 11/71 | LOSS: 4.581596670050203e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 12/71 | LOSS: 4.556679571648531e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 13/71 | LOSS: 4.646491252710153e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 14/71 | LOSS: 4.660715406619905e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 15/71 | LOSS: 4.661546697093399e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 16/71 | LOSS: 4.693326913826133e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 17/71 | LOSS: 4.727438421367778e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 18/71 | LOSS: 4.722404654737482e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 19/71 | LOSS: 4.682895530550013e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 20/71 | LOSS: 4.672348484832799e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 21/71 | LOSS: 4.673415694676144e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 22/71 | LOSS: 4.716833631312551e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 23/71 | LOSS: 4.7109547930782964e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 24/71 | LOSS: 4.711316696557333e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 25/71 | LOSS: 4.6844809538435165e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 26/71 | LOSS: 4.660477459226429e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 27/71 | LOSS: 4.632639932619245e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 28/71 | LOSS: 4.629278064698908e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 29/71 | LOSS: 4.597808295632907e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 30/71 | LOSS: 4.5688303221287506e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 31/71 | LOSS: 4.573040200739342e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 32/71 | LOSS: 4.570356051055679e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 33/71 | LOSS: 4.564885316807811e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 34/71 | LOSS: 4.55881008747383e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 35/71 | LOSS: 4.569768937775128e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 36/71 | LOSS: 4.5722443344955635e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 37/71 | LOSS: 4.5646261002586565e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 38/71 | LOSS: 4.550231120120794e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 39/71 | LOSS: 4.568752410705202e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 40/71 | LOSS: 4.556257042678204e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 41/71 | LOSS: 4.56877950498546e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 42/71 | LOSS: 4.5897411964265194e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 43/71 | LOSS: 4.575970437640948e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 44/71 | LOSS: 4.586293531853395e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 45/71 | LOSS: 4.577780155950282e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 46/71 | LOSS: 4.56632279623436e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 47/71 | LOSS: 4.569396149160336e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 48/71 | LOSS: 4.571870565031328e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 49/71 | LOSS: 4.576276087391306e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 50/71 | LOSS: 4.562546662639886e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 51/71 | LOSS: 4.5621626965769865e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 52/71 | LOSS: 4.546706478785776e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 53/71 | LOSS: 4.5334596523860925e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 54/71 | LOSS: 4.527197374541587e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 55/71 | LOSS: 4.503024233112488e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 56/71 | LOSS: 4.49993583635277e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 57/71 | LOSS: 4.4937650437563644e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 58/71 | LOSS: 4.478549406762872e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 59/71 | LOSS: 4.487798980790103e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 60/71 | LOSS: 4.4947061135152594e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 61/71 | LOSS: 4.487193824652716e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 62/71 | LOSS: 4.499284036755275e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 63/71 | LOSS: 4.4983327178726995e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 64/71 | LOSS: 4.4992426165453466e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 65/71 | LOSS: 4.504848389484378e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 66/71 | LOSS: 4.516554656165952e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 67/71 | LOSS: 4.507254492877751e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 68/71 | LOSS: 4.526009865583257e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 69/71 | LOSS: 4.529735175700418e-06\n",
      "TRAIN: EPOCH 438/1000 | BATCH 70/71 | LOSS: 4.528147050684349e-06\n",
      "VAL: EPOCH 438/1000 | BATCH 0/8 | LOSS: 4.5684923861699644e-06\n",
      "VAL: EPOCH 438/1000 | BATCH 1/8 | LOSS: 4.5258896079758415e-06\n",
      "VAL: EPOCH 438/1000 | BATCH 2/8 | LOSS: 4.669717782235239e-06\n",
      "VAL: EPOCH 438/1000 | BATCH 3/8 | LOSS: 4.655058774005738e-06\n",
      "VAL: EPOCH 438/1000 | BATCH 4/8 | LOSS: 4.6004040996194815e-06\n",
      "VAL: EPOCH 438/1000 | BATCH 5/8 | LOSS: 4.555707315982242e-06\n",
      "VAL: EPOCH 438/1000 | BATCH 6/8 | LOSS: 4.432784862729022e-06\n",
      "VAL: EPOCH 438/1000 | BATCH 7/8 | LOSS: 4.419448487169575e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 0/71 | LOSS: 3.431740424275631e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 1/71 | LOSS: 3.95305346501118e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 2/71 | LOSS: 3.951683235451735e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 3/71 | LOSS: 4.011166765849339e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 4/71 | LOSS: 4.47122711193515e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 5/71 | LOSS: 4.322244990362378e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 6/71 | LOSS: 4.344037181778861e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 7/71 | LOSS: 4.245914311695742e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 8/71 | LOSS: 4.345196051266005e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 9/71 | LOSS: 4.436305175659072e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 10/71 | LOSS: 4.4117738194803344e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 11/71 | LOSS: 4.466633508097099e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 12/71 | LOSS: 4.4749624871306314e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 13/71 | LOSS: 4.49549067330476e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 14/71 | LOSS: 4.58938679912535e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 15/71 | LOSS: 4.531108501737435e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 16/71 | LOSS: 4.5004946701630935e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 17/71 | LOSS: 4.532700902220515e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 18/71 | LOSS: 4.541441608365092e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 19/71 | LOSS: 4.559245201107842e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 20/71 | LOSS: 4.544569156635026e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 21/71 | LOSS: 4.609867439533198e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 22/71 | LOSS: 4.622481346023446e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 23/71 | LOSS: 4.616388830906241e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 24/71 | LOSS: 4.634441802409129e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 25/71 | LOSS: 4.665885180307109e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 26/71 | LOSS: 4.682902449025682e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 27/71 | LOSS: 4.7426636992245875e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 28/71 | LOSS: 4.702705962637115e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 29/71 | LOSS: 4.741265388474858e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 30/71 | LOSS: 4.700925119909845e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 31/71 | LOSS: 4.701680914820372e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 32/71 | LOSS: 4.677391871155123e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 33/71 | LOSS: 4.682084335506849e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 34/71 | LOSS: 4.6676845873402535e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 35/71 | LOSS: 4.671192963693708e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 36/71 | LOSS: 4.678016485077269e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 37/71 | LOSS: 4.673610473410487e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 38/71 | LOSS: 4.702049042092627e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 39/71 | LOSS: 4.736161082519175e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 40/71 | LOSS: 4.734396481314722e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 41/71 | LOSS: 4.745905465868072e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 42/71 | LOSS: 4.79298575005804e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 43/71 | LOSS: 4.759835762384186e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 44/71 | LOSS: 4.797179942923119e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 45/71 | LOSS: 4.80169398997532e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 46/71 | LOSS: 4.796449526653072e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 47/71 | LOSS: 4.840206301537364e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 48/71 | LOSS: 4.8402604608447885e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 49/71 | LOSS: 4.914723181173031e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 50/71 | LOSS: 4.882389542698922e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 51/71 | LOSS: 4.956758853215713e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 52/71 | LOSS: 4.964057376677168e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 53/71 | LOSS: 4.988206064303489e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 54/71 | LOSS: 5.003912441349134e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 55/71 | LOSS: 5.007253520261656e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 56/71 | LOSS: 5.045634699790594e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 57/71 | LOSS: 5.039519821585945e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 58/71 | LOSS: 5.036371117905404e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 59/71 | LOSS: 5.023858955155447e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 60/71 | LOSS: 5.030235335473916e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 61/71 | LOSS: 5.0250501192661636e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 62/71 | LOSS: 5.0291095978161415e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 63/71 | LOSS: 5.025037506101171e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 64/71 | LOSS: 5.029567710852672e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 65/71 | LOSS: 5.0130234140769145e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 66/71 | LOSS: 4.99741588725162e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 67/71 | LOSS: 4.993277641193238e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 68/71 | LOSS: 4.987703956773486e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 69/71 | LOSS: 4.9688072684667925e-06\n",
      "TRAIN: EPOCH 439/1000 | BATCH 70/71 | LOSS: 4.937620635989243e-06\n",
      "VAL: EPOCH 439/1000 | BATCH 0/8 | LOSS: 6.853054856037488e-06\n",
      "VAL: EPOCH 439/1000 | BATCH 1/8 | LOSS: 6.902652557982947e-06\n",
      "VAL: EPOCH 439/1000 | BATCH 2/8 | LOSS: 6.697095614072168e-06\n",
      "VAL: EPOCH 439/1000 | BATCH 3/8 | LOSS: 6.662823693659448e-06\n",
      "VAL: EPOCH 439/1000 | BATCH 4/8 | LOSS: 6.5409716626163575e-06\n",
      "VAL: EPOCH 439/1000 | BATCH 5/8 | LOSS: 6.199919804809421e-06\n",
      "VAL: EPOCH 439/1000 | BATCH 6/8 | LOSS: 6.030430150920958e-06\n",
      "VAL: EPOCH 439/1000 | BATCH 7/8 | LOSS: 5.8404481251272955e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 0/71 | LOSS: 7.252177056216169e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 1/71 | LOSS: 6.080402272345964e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 2/71 | LOSS: 7.2950406320160255e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 3/71 | LOSS: 6.441954951696971e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 4/71 | LOSS: 6.617994131374871e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 5/71 | LOSS: 6.255792944405887e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 6/71 | LOSS: 6.1219165477918324e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 7/71 | LOSS: 5.9347650562813214e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 8/71 | LOSS: 5.748314075996556e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 9/71 | LOSS: 5.695214667866821e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 10/71 | LOSS: 5.660143456638749e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 11/71 | LOSS: 5.552064862968109e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 12/71 | LOSS: 5.512041437983638e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 13/71 | LOSS: 5.592223357910241e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 14/71 | LOSS: 5.665146606285513e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 15/71 | LOSS: 5.607397412177306e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 16/71 | LOSS: 5.553234988089089e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 17/71 | LOSS: 5.604673459755658e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 18/71 | LOSS: 5.601518774997921e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 19/71 | LOSS: 5.554911422223086e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 20/71 | LOSS: 5.5491100283688866e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 21/71 | LOSS: 5.601889804231839e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 22/71 | LOSS: 5.554963239695138e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 23/71 | LOSS: 5.591546880623355e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 24/71 | LOSS: 5.616354737867368e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 25/71 | LOSS: 5.599486572604152e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 26/71 | LOSS: 5.61482474276326e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 27/71 | LOSS: 5.586778895251752e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 28/71 | LOSS: 5.611284152486217e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 29/71 | LOSS: 5.6014240878236405e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 30/71 | LOSS: 5.561288411803206e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 31/71 | LOSS: 5.564213196862511e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 32/71 | LOSS: 5.561224401334942e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 33/71 | LOSS: 5.621271571619539e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 34/71 | LOSS: 5.564717418695051e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 35/71 | LOSS: 5.5956846772965546e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 36/71 | LOSS: 5.561409586113547e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 37/71 | LOSS: 5.56576045303365e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 38/71 | LOSS: 5.55757248595257e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 39/71 | LOSS: 5.57089641120001e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 40/71 | LOSS: 5.56194089679083e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 41/71 | LOSS: 5.534724512775331e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 42/71 | LOSS: 5.512829644896217e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 43/71 | LOSS: 5.534595293904038e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 44/71 | LOSS: 5.515831167536412e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 45/71 | LOSS: 5.511497158255699e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 46/71 | LOSS: 5.499065387181311e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 47/71 | LOSS: 5.599454648101225e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 48/71 | LOSS: 5.580569334096235e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 49/71 | LOSS: 5.629592901641445e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 50/71 | LOSS: 5.626092134287533e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 51/71 | LOSS: 5.661175626196032e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 52/71 | LOSS: 5.711104389740525e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 53/71 | LOSS: 5.7093834458201955e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 54/71 | LOSS: 5.806573017252958e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 55/71 | LOSS: 5.769639784602987e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 56/71 | LOSS: 5.833513565097778e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 57/71 | LOSS: 5.790993100163178e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 58/71 | LOSS: 5.8128266111021205e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 59/71 | LOSS: 5.796089491620175e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 60/71 | LOSS: 5.766161587521087e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 61/71 | LOSS: 5.7606290621122515e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 62/71 | LOSS: 5.752161224148632e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 63/71 | LOSS: 5.734102202836766e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 64/71 | LOSS: 5.7114948993750235e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 65/71 | LOSS: 5.730304048318596e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 66/71 | LOSS: 5.709000391500773e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 67/71 | LOSS: 5.676538107228921e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 68/71 | LOSS: 5.653388215896616e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 69/71 | LOSS: 5.62888359516884e-06\n",
      "TRAIN: EPOCH 440/1000 | BATCH 70/71 | LOSS: 5.6242636699761546e-06\n",
      "VAL: EPOCH 440/1000 | BATCH 0/8 | LOSS: 4.246197931934148e-06\n",
      "VAL: EPOCH 440/1000 | BATCH 1/8 | LOSS: 4.340020723248017e-06\n",
      "VAL: EPOCH 440/1000 | BATCH 2/8 | LOSS: 4.435485152498586e-06\n",
      "VAL: EPOCH 440/1000 | BATCH 3/8 | LOSS: 4.51118125965877e-06\n",
      "VAL: EPOCH 440/1000 | BATCH 4/8 | LOSS: 4.452506436791736e-06\n",
      "VAL: EPOCH 440/1000 | BATCH 5/8 | LOSS: 4.343997034084168e-06\n",
      "VAL: EPOCH 440/1000 | BATCH 6/8 | LOSS: 4.197589987597894e-06\n",
      "VAL: EPOCH 440/1000 | BATCH 7/8 | LOSS: 4.101984444560003e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 0/71 | LOSS: 4.474354227568256e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 1/71 | LOSS: 4.253627594152931e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 2/71 | LOSS: 4.203242648751863e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 3/71 | LOSS: 4.277312427802826e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 4/71 | LOSS: 4.279897893866291e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 5/71 | LOSS: 4.439520732072803e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 6/71 | LOSS: 4.370218188601679e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 7/71 | LOSS: 4.266779939143817e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 8/71 | LOSS: 4.15547747757551e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 9/71 | LOSS: 4.098011936548574e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 10/71 | LOSS: 4.151811625971194e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 11/71 | LOSS: 4.070188310834055e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 12/71 | LOSS: 4.074112999100739e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 13/71 | LOSS: 4.1657034281732715e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 14/71 | LOSS: 4.229467958793976e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 15/71 | LOSS: 4.219324466703256e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 16/71 | LOSS: 4.308157365491806e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 17/71 | LOSS: 4.30228700073106e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 18/71 | LOSS: 4.3608907904662805e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 19/71 | LOSS: 4.441896066964546e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 20/71 | LOSS: 4.446546210515191e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 21/71 | LOSS: 4.480279812923982e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 22/71 | LOSS: 4.441664714249782e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 23/71 | LOSS: 4.458005889773631e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 24/71 | LOSS: 4.463577752176206e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 25/71 | LOSS: 4.4698349309048635e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 26/71 | LOSS: 4.434356739053598e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 27/71 | LOSS: 4.464926941441913e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 28/71 | LOSS: 4.497395617238061e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 29/71 | LOSS: 4.480912790919926e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 30/71 | LOSS: 4.454380183644037e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 31/71 | LOSS: 4.4777075345336925e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 32/71 | LOSS: 4.471452953249823e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 33/71 | LOSS: 4.506504143030136e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 34/71 | LOSS: 4.496353176364209e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 35/71 | LOSS: 4.541313981260626e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 36/71 | LOSS: 4.55156517804229e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 37/71 | LOSS: 4.591793975506382e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 38/71 | LOSS: 4.6226384652888955e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 39/71 | LOSS: 4.616841602000932e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 40/71 | LOSS: 4.661986655797014e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 41/71 | LOSS: 4.6660710982645725e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 42/71 | LOSS: 4.689745067078674e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 43/71 | LOSS: 4.6948578280783044e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 44/71 | LOSS: 4.697363096460726e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 45/71 | LOSS: 4.7279959784171535e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 46/71 | LOSS: 4.744323622946197e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 47/71 | LOSS: 4.755933341963707e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 48/71 | LOSS: 4.74760656511442e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 49/71 | LOSS: 4.7693249598523835e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 50/71 | LOSS: 4.75638018863666e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 51/71 | LOSS: 4.771985205619645e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 52/71 | LOSS: 4.748869793369316e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 53/71 | LOSS: 4.773241033507037e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 54/71 | LOSS: 4.7936896077193195e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 55/71 | LOSS: 4.833560776010667e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 56/71 | LOSS: 4.837164987405056e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 57/71 | LOSS: 4.8567866432959226e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 58/71 | LOSS: 4.830900475792725e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 59/71 | LOSS: 4.826808026336948e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 60/71 | LOSS: 4.8139865134917125e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 61/71 | LOSS: 4.816586624656338e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 62/71 | LOSS: 4.839336573544036e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 63/71 | LOSS: 4.825852883527659e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 64/71 | LOSS: 4.8183486796915535e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 65/71 | LOSS: 4.8420953879196365e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 66/71 | LOSS: 4.835447196374341e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 67/71 | LOSS: 4.851165506218368e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 68/71 | LOSS: 4.859881742511103e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 69/71 | LOSS: 4.8600469459155905e-06\n",
      "TRAIN: EPOCH 441/1000 | BATCH 70/71 | LOSS: 4.842516606824036e-06\n",
      "VAL: EPOCH 441/1000 | BATCH 0/8 | LOSS: 6.561635473190108e-06\n",
      "VAL: EPOCH 441/1000 | BATCH 1/8 | LOSS: 6.442703352149692e-06\n",
      "VAL: EPOCH 441/1000 | BATCH 2/8 | LOSS: 6.435716992806799e-06\n",
      "VAL: EPOCH 441/1000 | BATCH 3/8 | LOSS: 6.381046432579751e-06\n",
      "VAL: EPOCH 441/1000 | BATCH 4/8 | LOSS: 6.34199486739817e-06\n",
      "VAL: EPOCH 441/1000 | BATCH 5/8 | LOSS: 6.185276030616175e-06\n",
      "VAL: EPOCH 441/1000 | BATCH 6/8 | LOSS: 6.153575668577105e-06\n",
      "VAL: EPOCH 441/1000 | BATCH 7/8 | LOSS: 6.054286245671392e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 0/71 | LOSS: 5.574182978307363e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 1/71 | LOSS: 6.268840024858946e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 2/71 | LOSS: 5.7597580962465145e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 3/71 | LOSS: 5.769087806584139e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 4/71 | LOSS: 5.6094831052178055e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 5/71 | LOSS: 5.506182939522357e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 6/71 | LOSS: 5.231707713651953e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 7/71 | LOSS: 5.255982529206449e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 8/71 | LOSS: 5.2838863414055795e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 9/71 | LOSS: 5.307595142767241e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 10/71 | LOSS: 5.277888352462799e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 11/71 | LOSS: 5.2614310751171916e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 12/71 | LOSS: 5.109677273979357e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 13/71 | LOSS: 5.0299561605373e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 14/71 | LOSS: 5.01394494373623e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 15/71 | LOSS: 4.969007591171248e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 16/71 | LOSS: 4.9247527225388155e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 17/71 | LOSS: 4.881955798434016e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 18/71 | LOSS: 4.8770114421801595e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 19/71 | LOSS: 4.823462268177536e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 20/71 | LOSS: 4.936164129633523e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 21/71 | LOSS: 4.9538908073869114e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 22/71 | LOSS: 4.961861557840956e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 23/71 | LOSS: 4.937262228092247e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 24/71 | LOSS: 4.9752724044083155e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 25/71 | LOSS: 4.981563139293799e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 26/71 | LOSS: 4.970445994937715e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 27/71 | LOSS: 4.922616556411542e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 28/71 | LOSS: 4.88572192713538e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 29/71 | LOSS: 4.853537150969108e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 30/71 | LOSS: 4.819267873714242e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 31/71 | LOSS: 4.805426200960028e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 32/71 | LOSS: 4.784857813347392e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 33/71 | LOSS: 4.770675960732179e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 34/71 | LOSS: 4.749787655912639e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 35/71 | LOSS: 4.759088432769608e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 36/71 | LOSS: 4.751420739331799e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 37/71 | LOSS: 4.781732418211361e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 38/71 | LOSS: 4.780973074211476e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 39/71 | LOSS: 4.776973423759046e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 40/71 | LOSS: 4.796759285232926e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 41/71 | LOSS: 4.816739311493057e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 42/71 | LOSS: 4.7980230340536485e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 43/71 | LOSS: 4.76514928881112e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 44/71 | LOSS: 4.802263098705831e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 45/71 | LOSS: 4.791373518618576e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 46/71 | LOSS: 4.813322027288489e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 47/71 | LOSS: 4.81604169995838e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 48/71 | LOSS: 4.819974237205092e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 49/71 | LOSS: 4.804879722541955e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 50/71 | LOSS: 4.797091494266151e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 51/71 | LOSS: 4.826021027453792e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 52/71 | LOSS: 4.826501288094595e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 53/71 | LOSS: 4.858808926689596e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 54/71 | LOSS: 4.871789253064145e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 55/71 | LOSS: 4.921634533567547e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 56/71 | LOSS: 4.9028158532325365e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 57/71 | LOSS: 4.899088280737864e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 58/71 | LOSS: 4.898929920288492e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 59/71 | LOSS: 4.88045052028004e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 60/71 | LOSS: 4.871174686756052e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 61/71 | LOSS: 4.873611498712102e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 62/71 | LOSS: 4.866658967344732e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 63/71 | LOSS: 4.900266215912552e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 64/71 | LOSS: 4.884773985535363e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 65/71 | LOSS: 4.9050733726592926e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 66/71 | LOSS: 4.898376801065751e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 67/71 | LOSS: 4.899629305675583e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 68/71 | LOSS: 4.909506175692144e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 69/71 | LOSS: 4.9029157415653965e-06\n",
      "TRAIN: EPOCH 442/1000 | BATCH 70/71 | LOSS: 4.89460942422021e-06\n",
      "VAL: EPOCH 442/1000 | BATCH 0/8 | LOSS: 5.872140718565788e-06\n",
      "VAL: EPOCH 442/1000 | BATCH 1/8 | LOSS: 5.868138714504312e-06\n",
      "VAL: EPOCH 442/1000 | BATCH 2/8 | LOSS: 5.952083029114874e-06\n",
      "VAL: EPOCH 442/1000 | BATCH 3/8 | LOSS: 5.886589292458666e-06\n",
      "VAL: EPOCH 442/1000 | BATCH 4/8 | LOSS: 5.8524810810922645e-06\n",
      "VAL: EPOCH 442/1000 | BATCH 5/8 | LOSS: 5.618702440794247e-06\n",
      "VAL: EPOCH 442/1000 | BATCH 6/8 | LOSS: 5.55628106927283e-06\n",
      "VAL: EPOCH 442/1000 | BATCH 7/8 | LOSS: 5.465517347147397e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 0/71 | LOSS: 4.253336555848364e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 1/71 | LOSS: 4.132038611714961e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 2/71 | LOSS: 4.589797451141446e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 3/71 | LOSS: 4.105096991224855e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 4/71 | LOSS: 4.265669940650696e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 5/71 | LOSS: 4.374893478598096e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 6/71 | LOSS: 4.357858805243657e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 7/71 | LOSS: 4.544193529909535e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 8/71 | LOSS: 4.509697545371536e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 9/71 | LOSS: 4.454929330677259e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 10/71 | LOSS: 4.4622980071422224e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 11/71 | LOSS: 4.526651991909603e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 12/71 | LOSS: 4.5105234676157124e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 13/71 | LOSS: 4.544216738265407e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 14/71 | LOSS: 4.571793336557069e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 15/71 | LOSS: 4.50579450728128e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 16/71 | LOSS: 4.5378364093802e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 17/71 | LOSS: 4.499940928326396e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 18/71 | LOSS: 4.487685469269452e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 19/71 | LOSS: 4.491023321406828e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 20/71 | LOSS: 4.487720583970908e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 21/71 | LOSS: 4.496254498256101e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 22/71 | LOSS: 4.464722649019045e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 23/71 | LOSS: 4.470704188482462e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 24/71 | LOSS: 4.451338527360349e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 25/71 | LOSS: 4.465361799842727e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 26/71 | LOSS: 4.450156321009099e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 27/71 | LOSS: 4.413659715802558e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 28/71 | LOSS: 4.391267480216278e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 29/71 | LOSS: 4.389078488505523e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 30/71 | LOSS: 4.38734119715293e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 31/71 | LOSS: 4.406805359735699e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 32/71 | LOSS: 4.4055317988695055e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 33/71 | LOSS: 4.428115797803484e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 34/71 | LOSS: 4.467107373784529e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 35/71 | LOSS: 4.465378386460846e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 36/71 | LOSS: 4.496599951773926e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 37/71 | LOSS: 4.511750687111328e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 38/71 | LOSS: 4.496494430219545e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 39/71 | LOSS: 4.502868898725865e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 40/71 | LOSS: 4.488068986197837e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 41/71 | LOSS: 4.515714549381214e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 42/71 | LOSS: 4.541082753983277e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 43/71 | LOSS: 4.514843417589211e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 44/71 | LOSS: 4.514353688339017e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 45/71 | LOSS: 4.501440161141585e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 46/71 | LOSS: 4.480966152457652e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 47/71 | LOSS: 4.491583093605793e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 48/71 | LOSS: 4.507169701262645e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 49/71 | LOSS: 4.505920319388679e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 50/71 | LOSS: 4.520883445972383e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 51/71 | LOSS: 4.521907547209244e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 52/71 | LOSS: 4.581341877469039e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 53/71 | LOSS: 4.592118418461016e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 54/71 | LOSS: 4.651018856417398e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 55/71 | LOSS: 4.672370863545439e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 56/71 | LOSS: 4.682541414200514e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 57/71 | LOSS: 4.743579850206703e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 58/71 | LOSS: 4.745644662114624e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 59/71 | LOSS: 4.775813367056495e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 60/71 | LOSS: 4.753296830354795e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 61/71 | LOSS: 4.870479866410953e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 62/71 | LOSS: 4.852812675640702e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 63/71 | LOSS: 4.883892589901961e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 64/71 | LOSS: 4.895466718852941e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 65/71 | LOSS: 4.904191727787706e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 66/71 | LOSS: 4.929399400639544e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 67/71 | LOSS: 4.936298079000528e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 68/71 | LOSS: 4.990103243340011e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 69/71 | LOSS: 4.975601128925129e-06\n",
      "TRAIN: EPOCH 443/1000 | BATCH 70/71 | LOSS: 4.990464708818866e-06\n",
      "VAL: EPOCH 443/1000 | BATCH 0/8 | LOSS: 6.201587893883698e-06\n",
      "VAL: EPOCH 443/1000 | BATCH 1/8 | LOSS: 5.913204859098187e-06\n",
      "VAL: EPOCH 443/1000 | BATCH 2/8 | LOSS: 5.791574343068835e-06\n",
      "VAL: EPOCH 443/1000 | BATCH 3/8 | LOSS: 5.672598263117834e-06\n",
      "VAL: EPOCH 443/1000 | BATCH 4/8 | LOSS: 5.572456211666577e-06\n",
      "VAL: EPOCH 443/1000 | BATCH 5/8 | LOSS: 5.394022233910316e-06\n",
      "VAL: EPOCH 443/1000 | BATCH 6/8 | LOSS: 5.316071825031291e-06\n",
      "VAL: EPOCH 443/1000 | BATCH 7/8 | LOSS: 5.204379590395547e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 0/71 | LOSS: 6.6865404733107425e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 1/71 | LOSS: 7.153425030992366e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 2/71 | LOSS: 7.460542595557247e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 3/71 | LOSS: 7.604107167935581e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 4/71 | LOSS: 7.335832924582064e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 5/71 | LOSS: 6.976957365623093e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 6/71 | LOSS: 7.228514277812792e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 7/71 | LOSS: 7.079488284489344e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 8/71 | LOSS: 6.868487818994456e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 9/71 | LOSS: 6.80717653267493e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 10/71 | LOSS: 6.922964489190648e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 11/71 | LOSS: 6.864978179995281e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 12/71 | LOSS: 6.7433958555249355e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 13/71 | LOSS: 6.861740205619883e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 14/71 | LOSS: 6.74400932136147e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 15/71 | LOSS: 6.6637419706694345e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 16/71 | LOSS: 6.63214517343851e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 17/71 | LOSS: 6.759439025295756e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 18/71 | LOSS: 6.599195115229936e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 19/71 | LOSS: 6.651103080912435e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 20/71 | LOSS: 6.598249954203354e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 21/71 | LOSS: 6.478699064742117e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 22/71 | LOSS: 6.44723573002367e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 23/71 | LOSS: 6.380610443557089e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 24/71 | LOSS: 6.3426474298466925e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 25/71 | LOSS: 6.273301236293404e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 26/71 | LOSS: 6.219592688904851e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 27/71 | LOSS: 6.163316584206768e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 28/71 | LOSS: 6.145580896529658e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 29/71 | LOSS: 6.078563137634773e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 30/71 | LOSS: 6.084354158848592e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 31/71 | LOSS: 6.085961786084226e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 32/71 | LOSS: 6.0441774015392484e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 33/71 | LOSS: 6.052677744104157e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 34/71 | LOSS: 6.034696954364855e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 35/71 | LOSS: 6.042777929198766e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 36/71 | LOSS: 5.985188304416747e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 37/71 | LOSS: 5.975650837124129e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 38/71 | LOSS: 5.934145508198969e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 39/71 | LOSS: 5.8966450637853995e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 40/71 | LOSS: 5.8539292961252545e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 41/71 | LOSS: 5.824079488818479e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 42/71 | LOSS: 5.769002475358849e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 43/71 | LOSS: 5.738606660088408e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 44/71 | LOSS: 5.759086454620248e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 45/71 | LOSS: 5.724654952048339e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 46/71 | LOSS: 5.704437906852616e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 47/71 | LOSS: 5.669334816123713e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 48/71 | LOSS: 5.64458846662203e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 49/71 | LOSS: 5.620059137072531e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 50/71 | LOSS: 5.603942286945633e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 51/71 | LOSS: 5.571499496699499e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 52/71 | LOSS: 5.559483776418141e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 53/71 | LOSS: 5.520058906498619e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 54/71 | LOSS: 5.505313922846902e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 55/71 | LOSS: 5.49511326296072e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 56/71 | LOSS: 5.474508302620706e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 57/71 | LOSS: 5.452196695653678e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 58/71 | LOSS: 5.438183776832529e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 59/71 | LOSS: 5.40745914274036e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 60/71 | LOSS: 5.39513884863571e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 61/71 | LOSS: 5.363199669949198e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 62/71 | LOSS: 5.334244521676853e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 63/71 | LOSS: 5.32070011516339e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 64/71 | LOSS: 5.317218434496541e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 65/71 | LOSS: 5.3053822567204465e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 66/71 | LOSS: 5.2911932437097285e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 67/71 | LOSS: 5.27519171435094e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 68/71 | LOSS: 5.261836138848541e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 69/71 | LOSS: 5.248877554160052e-06\n",
      "TRAIN: EPOCH 444/1000 | BATCH 70/71 | LOSS: 5.237436618033467e-06\n",
      "VAL: EPOCH 444/1000 | BATCH 0/8 | LOSS: 4.7101470954658e-06\n",
      "VAL: EPOCH 444/1000 | BATCH 1/8 | LOSS: 4.740237045552931e-06\n",
      "VAL: EPOCH 444/1000 | BATCH 2/8 | LOSS: 4.930490073699427e-06\n",
      "VAL: EPOCH 444/1000 | BATCH 3/8 | LOSS: 4.89068361275713e-06\n",
      "VAL: EPOCH 444/1000 | BATCH 4/8 | LOSS: 4.863336107518989e-06\n",
      "VAL: EPOCH 444/1000 | BATCH 5/8 | LOSS: 4.741338064680652e-06\n",
      "VAL: EPOCH 444/1000 | BATCH 6/8 | LOSS: 4.6452963553227685e-06\n",
      "VAL: EPOCH 444/1000 | BATCH 7/8 | LOSS: 4.605710159921728e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 0/71 | LOSS: 5.5109676395659335e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 1/71 | LOSS: 4.940581902701524e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 2/71 | LOSS: 5.340426014299737e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 3/71 | LOSS: 5.268221798360173e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 4/71 | LOSS: 5.787659847555915e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 5/71 | LOSS: 6.104048604053484e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 6/71 | LOSS: 6.055424624459452e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 7/71 | LOSS: 5.843070255195926e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 8/71 | LOSS: 5.883932064736857e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 9/71 | LOSS: 5.737607943956391e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 10/71 | LOSS: 5.591617661015004e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 11/71 | LOSS: 5.476692952773495e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 12/71 | LOSS: 5.317419243930579e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 13/71 | LOSS: 5.322185107356095e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 14/71 | LOSS: 5.346970904914391e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 15/71 | LOSS: 5.2307560025610655e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 16/71 | LOSS: 5.270660267342464e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 17/71 | LOSS: 5.262134171365081e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 18/71 | LOSS: 5.218711653230314e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 19/71 | LOSS: 5.232560818058119e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 20/71 | LOSS: 5.193044465895149e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 21/71 | LOSS: 5.2096097430297626e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 22/71 | LOSS: 5.161579446908234e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 23/71 | LOSS: 5.153233350559579e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 24/71 | LOSS: 5.141054234627518e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 25/71 | LOSS: 5.107452186446328e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 26/71 | LOSS: 5.07863689933772e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 27/71 | LOSS: 5.0345423444144084e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 28/71 | LOSS: 5.006506108688412e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 29/71 | LOSS: 4.992022991245904e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 30/71 | LOSS: 4.9835163457417714e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 31/71 | LOSS: 4.953441141708481e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 32/71 | LOSS: 4.962117689942578e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 33/71 | LOSS: 4.992159795298957e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 34/71 | LOSS: 4.9603910351184565e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 35/71 | LOSS: 4.956362501060438e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 36/71 | LOSS: 4.935957748956925e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 37/71 | LOSS: 4.92338138931365e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 38/71 | LOSS: 4.92066379382987e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 39/71 | LOSS: 4.953078604330585e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 40/71 | LOSS: 4.988027601540103e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 41/71 | LOSS: 4.9960929280652115e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 42/71 | LOSS: 5.033374485042999e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 43/71 | LOSS: 5.01525570164383e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 44/71 | LOSS: 5.086465459195703e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 45/71 | LOSS: 5.063458042573273e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 46/71 | LOSS: 5.054553625125257e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 47/71 | LOSS: 5.031417245504599e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 48/71 | LOSS: 5.026627498094114e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 49/71 | LOSS: 5.018489041503926e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 50/71 | LOSS: 4.9937868969245045e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 51/71 | LOSS: 4.983397244674192e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 52/71 | LOSS: 4.965175909297991e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 53/71 | LOSS: 4.973664583000452e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 54/71 | LOSS: 4.9451105414019285e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 55/71 | LOSS: 4.948218438422762e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 56/71 | LOSS: 4.928338548301039e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 57/71 | LOSS: 4.909914105939669e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 58/71 | LOSS: 4.887621887480806e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 59/71 | LOSS: 4.883632887716279e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 60/71 | LOSS: 4.851836399107238e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 61/71 | LOSS: 4.845804204295188e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 62/71 | LOSS: 4.855507544533115e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 63/71 | LOSS: 4.859448271332667e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 64/71 | LOSS: 4.847423836578785e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 65/71 | LOSS: 4.84413969430038e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 66/71 | LOSS: 4.852141592660945e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 67/71 | LOSS: 4.8405329079394185e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 68/71 | LOSS: 4.833450973002481e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 69/71 | LOSS: 4.828587624875321e-06\n",
      "TRAIN: EPOCH 445/1000 | BATCH 70/71 | LOSS: 4.821960089084058e-06\n",
      "VAL: EPOCH 445/1000 | BATCH 0/8 | LOSS: 5.42198768016533e-06\n",
      "VAL: EPOCH 445/1000 | BATCH 1/8 | LOSS: 5.338794153431081e-06\n",
      "VAL: EPOCH 445/1000 | BATCH 2/8 | LOSS: 5.262442300590919e-06\n",
      "VAL: EPOCH 445/1000 | BATCH 3/8 | LOSS: 5.176353283786739e-06\n",
      "VAL: EPOCH 445/1000 | BATCH 4/8 | LOSS: 5.1179434194636995e-06\n",
      "VAL: EPOCH 445/1000 | BATCH 5/8 | LOSS: 4.888990133622428e-06\n",
      "VAL: EPOCH 445/1000 | BATCH 6/8 | LOSS: 4.744041299480679e-06\n",
      "VAL: EPOCH 445/1000 | BATCH 7/8 | LOSS: 4.629803981970326e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 0/71 | LOSS: 4.4418002289603464e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 1/71 | LOSS: 4.5494368805520935e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 2/71 | LOSS: 4.412349274692436e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 3/71 | LOSS: 4.571391627905541e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 4/71 | LOSS: 4.568344593280926e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 5/71 | LOSS: 4.513622267647103e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 6/71 | LOSS: 4.365742207872765e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 7/71 | LOSS: 4.491529779215853e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 8/71 | LOSS: 4.4623965196579875e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 9/71 | LOSS: 4.485519252739323e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 10/71 | LOSS: 4.436575946701024e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 11/71 | LOSS: 4.448684137514647e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 12/71 | LOSS: 4.370796939558484e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 13/71 | LOSS: 4.350493733389678e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 14/71 | LOSS: 4.444941517552555e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 15/71 | LOSS: 4.422447588581235e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 16/71 | LOSS: 4.477686576030785e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 17/71 | LOSS: 4.480899343030211e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 18/71 | LOSS: 4.519089076749974e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 19/71 | LOSS: 4.5054636416352874e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 20/71 | LOSS: 4.454913908245674e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 21/71 | LOSS: 4.495213013135733e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 22/71 | LOSS: 4.4596767625277964e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 23/71 | LOSS: 4.463015557121253e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 24/71 | LOSS: 4.4280543261265845e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 25/71 | LOSS: 4.454284040688295e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 26/71 | LOSS: 4.472608317748247e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 27/71 | LOSS: 4.482491086881575e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 28/71 | LOSS: 4.544094610600009e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 29/71 | LOSS: 4.5391668966961635e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 30/71 | LOSS: 4.556908298846176e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 31/71 | LOSS: 4.624703919375861e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 32/71 | LOSS: 4.683846134756812e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 33/71 | LOSS: 4.721428387497058e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 34/71 | LOSS: 4.7614386598330124e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 35/71 | LOSS: 4.748471004252981e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 36/71 | LOSS: 4.764681049178958e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 37/71 | LOSS: 4.8376326551653235e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 38/71 | LOSS: 4.8627866817696486e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 39/71 | LOSS: 4.8672899652046905e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 40/71 | LOSS: 4.8905791539632115e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 41/71 | LOSS: 4.867453136674678e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 42/71 | LOSS: 4.873841441812421e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 43/71 | LOSS: 4.8801171446526e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 44/71 | LOSS: 4.896771901662254e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 45/71 | LOSS: 4.880907750782131e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 46/71 | LOSS: 4.903274163959221e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 47/71 | LOSS: 4.888338082764676e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 48/71 | LOSS: 4.872173548744972e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 49/71 | LOSS: 4.855150928051444e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 50/71 | LOSS: 4.863523292228552e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 51/71 | LOSS: 4.8542120007018985e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 52/71 | LOSS: 4.852019989108535e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 53/71 | LOSS: 4.845362413848138e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 54/71 | LOSS: 4.828470189915441e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 55/71 | LOSS: 4.817839981439777e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 56/71 | LOSS: 4.810937489788844e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 57/71 | LOSS: 4.813983250955814e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 58/71 | LOSS: 4.8126759901282705e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 59/71 | LOSS: 4.790318867738582e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 60/71 | LOSS: 4.788012168572721e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 61/71 | LOSS: 4.784867091104623e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 62/71 | LOSS: 4.785250796755593e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 63/71 | LOSS: 4.767170040054225e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 64/71 | LOSS: 4.766113096803355e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 65/71 | LOSS: 4.75862202620192e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 66/71 | LOSS: 4.754771955266557e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 67/71 | LOSS: 4.752756340434751e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 68/71 | LOSS: 4.738196202052295e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 69/71 | LOSS: 4.725251151934831e-06\n",
      "TRAIN: EPOCH 446/1000 | BATCH 70/71 | LOSS: 4.701059914379943e-06\n",
      "VAL: EPOCH 446/1000 | BATCH 0/8 | LOSS: 4.401441856316524e-06\n",
      "VAL: EPOCH 446/1000 | BATCH 1/8 | LOSS: 4.383131226859405e-06\n",
      "VAL: EPOCH 446/1000 | BATCH 2/8 | LOSS: 4.690055599591385e-06\n",
      "VAL: EPOCH 446/1000 | BATCH 3/8 | LOSS: 4.965265588907641e-06\n",
      "VAL: EPOCH 446/1000 | BATCH 4/8 | LOSS: 4.846977299166611e-06\n",
      "VAL: EPOCH 446/1000 | BATCH 5/8 | LOSS: 4.89607790162457e-06\n",
      "VAL: EPOCH 446/1000 | BATCH 6/8 | LOSS: 4.79803650575507e-06\n",
      "VAL: EPOCH 446/1000 | BATCH 7/8 | LOSS: 4.70916859285353e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 0/71 | LOSS: 4.402833383210236e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 1/71 | LOSS: 4.315205842431169e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 2/71 | LOSS: 4.436930794327054e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 3/71 | LOSS: 4.3613282514343155e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 4/71 | LOSS: 4.25650478064199e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 5/71 | LOSS: 4.25360917688522e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 6/71 | LOSS: 4.224472507173362e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 7/71 | LOSS: 4.197791213300661e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 8/71 | LOSS: 4.301526435786703e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 9/71 | LOSS: 4.236442009641905e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 10/71 | LOSS: 4.164839620815738e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 11/71 | LOSS: 4.0907306318634555e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 12/71 | LOSS: 4.2197233981893915e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 13/71 | LOSS: 4.207286321746194e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 14/71 | LOSS: 4.26933749319384e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 15/71 | LOSS: 4.271105112252371e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 16/71 | LOSS: 4.240523447365644e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 17/71 | LOSS: 4.246481631121747e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 18/71 | LOSS: 4.285283694116515e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 19/71 | LOSS: 4.287037415906525e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 20/71 | LOSS: 4.351214459217902e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 21/71 | LOSS: 4.461061249375317e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 22/71 | LOSS: 4.510579767314915e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 23/71 | LOSS: 4.505947411341064e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 24/71 | LOSS: 4.511505858317833e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 25/71 | LOSS: 4.5177262021612615e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 26/71 | LOSS: 4.5399904812021065e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 27/71 | LOSS: 4.590806570701846e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 28/71 | LOSS: 4.540564054093858e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 29/71 | LOSS: 4.5240535428092695e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 30/71 | LOSS: 4.486849166501235e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 31/71 | LOSS: 4.488392654877771e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 32/71 | LOSS: 4.4927337030182546e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 33/71 | LOSS: 4.503047481354288e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 34/71 | LOSS: 4.5059138171512004e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 35/71 | LOSS: 4.490756339237123e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 36/71 | LOSS: 4.4718918505512535e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 37/71 | LOSS: 4.484573459625321e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 38/71 | LOSS: 4.480469104423835e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 39/71 | LOSS: 4.48558000130106e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 40/71 | LOSS: 4.480986756500831e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 41/71 | LOSS: 4.4740477116127816e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 42/71 | LOSS: 4.448763854460869e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 43/71 | LOSS: 4.452605341240004e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 44/71 | LOSS: 4.458303076009745e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 45/71 | LOSS: 4.450021848519416e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 46/71 | LOSS: 4.429613469217153e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 47/71 | LOSS: 4.44819406671589e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 48/71 | LOSS: 4.4571441980669805e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 49/71 | LOSS: 4.435041319084121e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 50/71 | LOSS: 4.46219320449517e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 51/71 | LOSS: 4.46846906225591e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 52/71 | LOSS: 4.451344796696091e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 53/71 | LOSS: 4.441570088905185e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 54/71 | LOSS: 4.456414696753861e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 55/71 | LOSS: 4.464906603678693e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 56/71 | LOSS: 4.450361470126633e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 57/71 | LOSS: 4.45961073825762e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 58/71 | LOSS: 4.462618508730853e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 59/71 | LOSS: 4.4794118214971e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 60/71 | LOSS: 4.473799487984296e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 61/71 | LOSS: 4.466275479517794e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 62/71 | LOSS: 4.518593442977391e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 63/71 | LOSS: 4.522698610287534e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 64/71 | LOSS: 4.539615383691853e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 65/71 | LOSS: 4.543075309618761e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 66/71 | LOSS: 4.57715220105836e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 67/71 | LOSS: 4.597414886120532e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 68/71 | LOSS: 4.603890750371173e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 69/71 | LOSS: 4.612525162883685e-06\n",
      "TRAIN: EPOCH 447/1000 | BATCH 70/71 | LOSS: 4.6389294651125455e-06\n",
      "VAL: EPOCH 447/1000 | BATCH 0/8 | LOSS: 5.211813004279975e-06\n",
      "VAL: EPOCH 447/1000 | BATCH 1/8 | LOSS: 5.651741503243102e-06\n",
      "VAL: EPOCH 447/1000 | BATCH 2/8 | LOSS: 5.8895372300564e-06\n",
      "VAL: EPOCH 447/1000 | BATCH 3/8 | LOSS: 5.9950584727630485e-06\n",
      "VAL: EPOCH 447/1000 | BATCH 4/8 | LOSS: 5.994819639454363e-06\n",
      "VAL: EPOCH 447/1000 | BATCH 5/8 | LOSS: 5.80734854338516e-06\n",
      "VAL: EPOCH 447/1000 | BATCH 6/8 | LOSS: 5.650448526596717e-06\n",
      "VAL: EPOCH 447/1000 | BATCH 7/8 | LOSS: 5.51703192286368e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 0/71 | LOSS: 5.447903276944999e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 1/71 | LOSS: 5.6303103974642e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 2/71 | LOSS: 5.356967903935583e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 3/71 | LOSS: 5.819610919388651e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 4/71 | LOSS: 5.354921177058713e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 5/71 | LOSS: 5.583969671837015e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 6/71 | LOSS: 5.509522517448724e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 7/71 | LOSS: 5.4897465133763035e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 8/71 | LOSS: 5.418529882364156e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 9/71 | LOSS: 5.3271874094207305e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 10/71 | LOSS: 5.312321759860921e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 11/71 | LOSS: 5.225710234905516e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 12/71 | LOSS: 5.29127783011063e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 13/71 | LOSS: 5.245627302688913e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 14/71 | LOSS: 5.237082647605954e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 15/71 | LOSS: 5.202994486808166e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 16/71 | LOSS: 5.233209455635754e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 17/71 | LOSS: 5.223939170820005e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 18/71 | LOSS: 5.2163794814412925e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 19/71 | LOSS: 5.207526464801049e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 20/71 | LOSS: 5.220872026256984e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 21/71 | LOSS: 5.238289779960971e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 22/71 | LOSS: 5.194167284650535e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 23/71 | LOSS: 5.225930389466764e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 24/71 | LOSS: 5.2444255015871025e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 25/71 | LOSS: 5.216927950011897e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 26/71 | LOSS: 5.151684077915249e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 27/71 | LOSS: 5.162568096953432e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 28/71 | LOSS: 5.165145019983074e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 29/71 | LOSS: 5.145398707403122e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 30/71 | LOSS: 5.1204802627668665e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 31/71 | LOSS: 5.157135667843704e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 32/71 | LOSS: 5.125788702846903e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 33/71 | LOSS: 5.093804695795424e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 34/71 | LOSS: 5.068623620120759e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 35/71 | LOSS: 5.061525503909555e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 36/71 | LOSS: 5.023431148819745e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 37/71 | LOSS: 5.0014822543169814e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 38/71 | LOSS: 4.97087586527647e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 39/71 | LOSS: 4.959814526728224e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 40/71 | LOSS: 4.942745956613202e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 41/71 | LOSS: 4.953265753708062e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 42/71 | LOSS: 4.9569569952392725e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 43/71 | LOSS: 4.958224544473276e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 44/71 | LOSS: 4.9327424070118775e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 45/71 | LOSS: 4.922218206205719e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 46/71 | LOSS: 4.925043906544662e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 47/71 | LOSS: 4.913353507163265e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 48/71 | LOSS: 4.935389871225569e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 49/71 | LOSS: 4.913737402603147e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 50/71 | LOSS: 4.909049565350299e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 51/71 | LOSS: 4.8946269194297765e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 52/71 | LOSS: 4.884113543153464e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 53/71 | LOSS: 4.87083253507013e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 54/71 | LOSS: 4.8606724951903086e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 55/71 | LOSS: 4.841961567438245e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 56/71 | LOSS: 4.839566691659632e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 57/71 | LOSS: 4.8305713052882465e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 58/71 | LOSS: 4.8055251227197004e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 59/71 | LOSS: 4.795889500049573e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 60/71 | LOSS: 4.787391960279984e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 61/71 | LOSS: 4.764238170654637e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 62/71 | LOSS: 4.7843428647967e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 63/71 | LOSS: 4.776525571514867e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 64/71 | LOSS: 4.833994227318236e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 65/71 | LOSS: 4.8337364012585375e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 66/71 | LOSS: 4.864216984359116e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 67/71 | LOSS: 4.837371176670465e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 68/71 | LOSS: 4.849106751177148e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 69/71 | LOSS: 4.827214820579684e-06\n",
      "TRAIN: EPOCH 448/1000 | BATCH 70/71 | LOSS: 4.810227428094204e-06\n",
      "VAL: EPOCH 448/1000 | BATCH 0/8 | LOSS: 6.021045919624157e-06\n",
      "VAL: EPOCH 448/1000 | BATCH 1/8 | LOSS: 5.941988320046221e-06\n",
      "VAL: EPOCH 448/1000 | BATCH 2/8 | LOSS: 6.2429051771080895e-06\n",
      "VAL: EPOCH 448/1000 | BATCH 3/8 | LOSS: 6.3360222384289955e-06\n",
      "VAL: EPOCH 448/1000 | BATCH 4/8 | LOSS: 6.264816329348833e-06\n",
      "VAL: EPOCH 448/1000 | BATCH 5/8 | LOSS: 6.402122380677611e-06\n",
      "VAL: EPOCH 448/1000 | BATCH 6/8 | LOSS: 6.309101146533587e-06\n",
      "VAL: EPOCH 448/1000 | BATCH 7/8 | LOSS: 6.3476643390458776e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 0/71 | LOSS: 6.271808160818182e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 1/71 | LOSS: 5.46189312444767e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 2/71 | LOSS: 6.014823005292176e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 3/71 | LOSS: 5.50923084574606e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 4/71 | LOSS: 5.926263293076773e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 5/71 | LOSS: 5.656426310451934e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 6/71 | LOSS: 5.557987995936335e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 7/71 | LOSS: 5.586567681348242e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 8/71 | LOSS: 5.481206092857367e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 9/71 | LOSS: 5.64859601581702e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 10/71 | LOSS: 5.536209342112257e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 11/71 | LOSS: 5.566264652164439e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 12/71 | LOSS: 5.526136786871715e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 13/71 | LOSS: 5.529522728206107e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 14/71 | LOSS: 5.471374151966302e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 15/71 | LOSS: 5.4254560666322504e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 16/71 | LOSS: 5.3749009994670115e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 17/71 | LOSS: 5.390003151559439e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 18/71 | LOSS: 5.4266220582863565e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 19/71 | LOSS: 5.401850467023906e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 20/71 | LOSS: 5.367301431154677e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 21/71 | LOSS: 5.334538028281796e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 22/71 | LOSS: 5.2617818986935e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 23/71 | LOSS: 5.237116122695322e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 24/71 | LOSS: 5.221982710281736e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 25/71 | LOSS: 5.263100731029296e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 26/71 | LOSS: 5.2143886848541485e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 27/71 | LOSS: 5.225881498712884e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 28/71 | LOSS: 5.2271115597751965e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 29/71 | LOSS: 5.21693898463127e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 30/71 | LOSS: 5.213003760218271e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 31/71 | LOSS: 5.230717100346283e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 32/71 | LOSS: 5.275625212699666e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 33/71 | LOSS: 5.2404896196629335e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 34/71 | LOSS: 5.195553623187672e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 35/71 | LOSS: 5.242253027720128e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 36/71 | LOSS: 5.256011269699917e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 37/71 | LOSS: 5.227336229932246e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 38/71 | LOSS: 5.203070888735685e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 39/71 | LOSS: 5.215546701720087e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 40/71 | LOSS: 5.2125321428498065e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 41/71 | LOSS: 5.2026528497403234e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 42/71 | LOSS: 5.213875003264971e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 43/71 | LOSS: 5.243072238995823e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 44/71 | LOSS: 5.215074336067321e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 45/71 | LOSS: 5.241675822894056e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 46/71 | LOSS: 5.247960150379012e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 47/71 | LOSS: 5.227284233910723e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 48/71 | LOSS: 5.198592742424213e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 49/71 | LOSS: 5.239532288214832e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 50/71 | LOSS: 5.239026561456758e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 51/71 | LOSS: 5.20616738150728e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 52/71 | LOSS: 5.2701158489620664e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 53/71 | LOSS: 5.269669377075506e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 54/71 | LOSS: 5.248112934582009e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 55/71 | LOSS: 5.234295205777276e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 56/71 | LOSS: 5.2534082298830315e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 57/71 | LOSS: 5.263012863467558e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 58/71 | LOSS: 5.269392531926762e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 59/71 | LOSS: 5.24562993859945e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 60/71 | LOSS: 5.237480521469986e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 61/71 | LOSS: 5.243506345851206e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 62/71 | LOSS: 5.229732190104736e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 63/71 | LOSS: 5.239828585246187e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 64/71 | LOSS: 5.285500114601186e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 65/71 | LOSS: 5.264034553654325e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 66/71 | LOSS: 5.246367358430784e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 67/71 | LOSS: 5.243740043383963e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 68/71 | LOSS: 5.2499900494444445e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 69/71 | LOSS: 5.227084885258851e-06\n",
      "TRAIN: EPOCH 449/1000 | BATCH 70/71 | LOSS: 5.2181547090813315e-06\n",
      "VAL: EPOCH 449/1000 | BATCH 0/8 | LOSS: 5.374326974560972e-06\n",
      "VAL: EPOCH 449/1000 | BATCH 1/8 | LOSS: 5.202711918173009e-06\n",
      "VAL: EPOCH 449/1000 | BATCH 2/8 | LOSS: 5.137581410963321e-06\n",
      "VAL: EPOCH 449/1000 | BATCH 3/8 | LOSS: 5.0783410188159905e-06\n",
      "VAL: EPOCH 449/1000 | BATCH 4/8 | LOSS: 4.930235627398361e-06\n",
      "VAL: EPOCH 449/1000 | BATCH 5/8 | LOSS: 4.79520106940375e-06\n",
      "VAL: EPOCH 449/1000 | BATCH 6/8 | LOSS: 4.625152704258133e-06\n",
      "VAL: EPOCH 449/1000 | BATCH 7/8 | LOSS: 4.533243185278479e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 0/71 | LOSS: 6.4742507674964145e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 1/71 | LOSS: 5.257285920379218e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 2/71 | LOSS: 4.936395119633137e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 3/71 | LOSS: 4.692119091487257e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 4/71 | LOSS: 4.7505479415121956e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 5/71 | LOSS: 4.55047893410665e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 6/71 | LOSS: 4.468256520340219e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 7/71 | LOSS: 4.383204100122384e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 8/71 | LOSS: 4.406558825091149e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 9/71 | LOSS: 4.33654574862885e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 10/71 | LOSS: 4.423246185648498e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 11/71 | LOSS: 4.401593571401463e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 12/71 | LOSS: 4.459061546395577e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 13/71 | LOSS: 4.407647111293045e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 14/71 | LOSS: 4.383214976163193e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 15/71 | LOSS: 4.409637398339328e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 16/71 | LOSS: 4.456723512275323e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 17/71 | LOSS: 4.495941513798445e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 18/71 | LOSS: 4.4751621939404254e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 19/71 | LOSS: 4.513512089943106e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 20/71 | LOSS: 4.5345388484531125e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 21/71 | LOSS: 4.494001752225978e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 22/71 | LOSS: 4.476389755797319e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 23/71 | LOSS: 4.46651552010735e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 24/71 | LOSS: 4.461079952307046e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 25/71 | LOSS: 4.47512554903649e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 26/71 | LOSS: 4.451847703253238e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 27/71 | LOSS: 4.4991346937552275e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 28/71 | LOSS: 4.469488964065837e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 29/71 | LOSS: 4.447289021906423e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 30/71 | LOSS: 4.452605722807705e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 31/71 | LOSS: 4.417658409749947e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 32/71 | LOSS: 4.390658590857514e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 33/71 | LOSS: 4.41329029185641e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 34/71 | LOSS: 4.3993592758592314e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 35/71 | LOSS: 4.436480815507417e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 36/71 | LOSS: 4.442297888483547e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 37/71 | LOSS: 4.446866922224121e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 38/71 | LOSS: 4.428261735693713e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 39/71 | LOSS: 4.401266937748005e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 40/71 | LOSS: 4.384726042456062e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 41/71 | LOSS: 4.361386334581073e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 42/71 | LOSS: 4.385592705528875e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 43/71 | LOSS: 4.391145260094411e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 44/71 | LOSS: 4.375578585798697e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 45/71 | LOSS: 4.3758433074127305e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 46/71 | LOSS: 4.425837019841717e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 47/71 | LOSS: 4.414313101127239e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 48/71 | LOSS: 4.419863680781492e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 49/71 | LOSS: 4.427207468324923e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 50/71 | LOSS: 4.445249451950361e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 51/71 | LOSS: 4.427785046783146e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 52/71 | LOSS: 4.422065386571153e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 53/71 | LOSS: 4.408472999852232e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 54/71 | LOSS: 4.390791466655421e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 55/71 | LOSS: 4.384177129525467e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 56/71 | LOSS: 4.373479146523638e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 57/71 | LOSS: 4.3952801551595575e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 58/71 | LOSS: 4.371689980054448e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 59/71 | LOSS: 4.387332182886894e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 60/71 | LOSS: 4.3889127691280185e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 61/71 | LOSS: 4.415504118667506e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 62/71 | LOSS: 4.419948543236676e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 63/71 | LOSS: 4.447605775226293e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 64/71 | LOSS: 4.462309733263772e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 65/71 | LOSS: 4.4835494463768555e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 66/71 | LOSS: 4.5096738366696835e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 67/71 | LOSS: 4.519585391950814e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 68/71 | LOSS: 4.565521192199557e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 69/71 | LOSS: 4.57139429792213e-06\n",
      "TRAIN: EPOCH 450/1000 | BATCH 70/71 | LOSS: 4.597800455831276e-06\n",
      "VAL: EPOCH 450/1000 | BATCH 0/8 | LOSS: 5.61483602723456e-06\n",
      "VAL: EPOCH 450/1000 | BATCH 1/8 | LOSS: 5.541347491089255e-06\n",
      "VAL: EPOCH 450/1000 | BATCH 2/8 | LOSS: 5.974279702058993e-06\n",
      "VAL: EPOCH 450/1000 | BATCH 3/8 | LOSS: 5.870923018846952e-06\n",
      "VAL: EPOCH 450/1000 | BATCH 4/8 | LOSS: 5.890205466130283e-06\n",
      "VAL: EPOCH 450/1000 | BATCH 5/8 | LOSS: 5.872903178290774e-06\n",
      "VAL: EPOCH 450/1000 | BATCH 6/8 | LOSS: 5.82843066305421e-06\n",
      "VAL: EPOCH 450/1000 | BATCH 7/8 | LOSS: 5.897135849863844e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 0/71 | LOSS: 6.7299579313839786e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 1/71 | LOSS: 7.577520591439679e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 2/71 | LOSS: 6.924805044642805e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 3/71 | LOSS: 6.836278089394909e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 4/71 | LOSS: 6.608048897760454e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 5/71 | LOSS: 6.608867427833805e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 6/71 | LOSS: 6.400694928743178e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 7/71 | LOSS: 6.271539120916714e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 8/71 | LOSS: 6.2697708926862106e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 9/71 | LOSS: 6.020236560289049e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 10/71 | LOSS: 6.157741319673898e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 11/71 | LOSS: 6.0060373622642755e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 12/71 | LOSS: 5.924672327987187e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 13/71 | LOSS: 5.8392318546144196e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 14/71 | LOSS: 5.790861996501917e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 15/71 | LOSS: 5.680336386149065e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 16/71 | LOSS: 5.636082037466729e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 17/71 | LOSS: 5.555901174779541e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 18/71 | LOSS: 5.525546791767211e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 19/71 | LOSS: 5.55338881440548e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 20/71 | LOSS: 5.489148493541593e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 21/71 | LOSS: 5.49607555264877e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 22/71 | LOSS: 5.42352824556039e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 23/71 | LOSS: 5.368099759076965e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 24/71 | LOSS: 5.334574161679484e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 25/71 | LOSS: 5.331467823802645e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 26/71 | LOSS: 5.412370582133816e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 27/71 | LOSS: 5.401541963172869e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 28/71 | LOSS: 5.383324825178816e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 29/71 | LOSS: 5.316638210691356e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 30/71 | LOSS: 5.301959331958525e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 31/71 | LOSS: 5.299030135574867e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 32/71 | LOSS: 5.249315563560231e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 33/71 | LOSS: 5.26515334379784e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 34/71 | LOSS: 5.271761061261973e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 35/71 | LOSS: 5.250488369610038e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 36/71 | LOSS: 5.199935353753185e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 37/71 | LOSS: 5.179256618248473e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 38/71 | LOSS: 5.173339670177367e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 39/71 | LOSS: 5.149748886879024e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 40/71 | LOSS: 5.1275788603273145e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 41/71 | LOSS: 5.133298508380471e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 42/71 | LOSS: 5.106587560624385e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 43/71 | LOSS: 5.086454584877422e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 44/71 | LOSS: 5.070734970205295e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 45/71 | LOSS: 5.033962674647426e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 46/71 | LOSS: 5.007854879488847e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 47/71 | LOSS: 5.0233040932804824e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 48/71 | LOSS: 5.0259804622179885e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 49/71 | LOSS: 4.999847233193577e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 50/71 | LOSS: 4.999991411822004e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 51/71 | LOSS: 4.998053616243571e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 52/71 | LOSS: 4.99553307922467e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 53/71 | LOSS: 4.999402158824119e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 54/71 | LOSS: 4.980220306937223e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 55/71 | LOSS: 4.98268635900396e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 56/71 | LOSS: 4.983809304305173e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 57/71 | LOSS: 4.968613164488363e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 58/71 | LOSS: 4.985480353429073e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 59/71 | LOSS: 4.989676722289005e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 60/71 | LOSS: 4.956168707349952e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 61/71 | LOSS: 4.9340598501833985e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 62/71 | LOSS: 4.917386333891911e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 63/71 | LOSS: 4.897355822208738e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 64/71 | LOSS: 4.891872483242948e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 65/71 | LOSS: 4.901519856282517e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 66/71 | LOSS: 4.907321508382203e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 67/71 | LOSS: 4.8996121690566615e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 68/71 | LOSS: 4.883150637212658e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 69/71 | LOSS: 4.880011049733314e-06\n",
      "TRAIN: EPOCH 451/1000 | BATCH 70/71 | LOSS: 4.8952376000414286e-06\n",
      "VAL: EPOCH 451/1000 | BATCH 0/8 | LOSS: 4.770381110574817e-06\n",
      "VAL: EPOCH 451/1000 | BATCH 1/8 | LOSS: 4.84989095639321e-06\n",
      "VAL: EPOCH 451/1000 | BATCH 2/8 | LOSS: 5.114166924613528e-06\n",
      "VAL: EPOCH 451/1000 | BATCH 3/8 | LOSS: 5.069780399935553e-06\n",
      "VAL: EPOCH 451/1000 | BATCH 4/8 | LOSS: 5.07124605064746e-06\n",
      "VAL: EPOCH 451/1000 | BATCH 5/8 | LOSS: 4.970009437480864e-06\n",
      "VAL: EPOCH 451/1000 | BATCH 6/8 | LOSS: 4.861905576295353e-06\n",
      "VAL: EPOCH 451/1000 | BATCH 7/8 | LOSS: 4.8218277015621425e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 0/71 | LOSS: 4.413128863234306e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 1/71 | LOSS: 4.218487219986855e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 2/71 | LOSS: 4.9238018012450384e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 3/71 | LOSS: 4.788872274730238e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 4/71 | LOSS: 4.718547825177666e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 5/71 | LOSS: 4.8393317229056265e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 6/71 | LOSS: 4.6935439448653985e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 7/71 | LOSS: 5.026467817970115e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 8/71 | LOSS: 5.0234811573722655e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 9/71 | LOSS: 5.284938151817187e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 10/71 | LOSS: 5.183905779466096e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 11/71 | LOSS: 5.291103320814727e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 12/71 | LOSS: 5.361228082060384e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 13/71 | LOSS: 5.503011282468963e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 14/71 | LOSS: 5.532724784037176e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 15/71 | LOSS: 5.378693714419569e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 16/71 | LOSS: 5.5517065554929155e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 17/71 | LOSS: 5.458707240298584e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 18/71 | LOSS: 5.534739078652101e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 19/71 | LOSS: 5.498372888723679e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 20/71 | LOSS: 5.45289534178058e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 21/71 | LOSS: 5.604226337461362e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 22/71 | LOSS: 5.743587458393607e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 23/71 | LOSS: 5.779659867736579e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 24/71 | LOSS: 5.783643773611402e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 25/71 | LOSS: 5.872951760466094e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 26/71 | LOSS: 5.897281661654469e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 27/71 | LOSS: 6.1939854439125545e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 28/71 | LOSS: 6.08391853567245e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 29/71 | LOSS: 6.264876325682659e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 30/71 | LOSS: 6.325887830650073e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 31/71 | LOSS: 6.39336431618176e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 32/71 | LOSS: 6.565366895687696e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 33/71 | LOSS: 6.507059491495431e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 34/71 | LOSS: 6.859227336722792e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 35/71 | LOSS: 6.773334954838599e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 36/71 | LOSS: 7.036985066775201e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 37/71 | LOSS: 6.982008184614212e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 38/71 | LOSS: 7.113669711636761e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 39/71 | LOSS: 7.156308925004851e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 40/71 | LOSS: 7.161361148663251e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 41/71 | LOSS: 7.296698879058351e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 42/71 | LOSS: 7.207850896605144e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 43/71 | LOSS: 7.42499896039755e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 44/71 | LOSS: 7.341408359530356e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 45/71 | LOSS: 7.353312501278276e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 46/71 | LOSS: 7.408918821617609e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 47/71 | LOSS: 7.353091130350246e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 48/71 | LOSS: 7.477715863220807e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 49/71 | LOSS: 7.427377577187144e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 50/71 | LOSS: 7.430536448273593e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 51/71 | LOSS: 7.3662632320729945e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 52/71 | LOSS: 7.445126442998682e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 53/71 | LOSS: 7.411326148738024e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 54/71 | LOSS: 7.37845548428595e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 55/71 | LOSS: 7.3835239098636e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 56/71 | LOSS: 7.333722335221082e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 57/71 | LOSS: 7.308436516754667e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 58/71 | LOSS: 7.269002677368639e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 59/71 | LOSS: 7.2270442918428065e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 60/71 | LOSS: 7.2084046864727525e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 61/71 | LOSS: 7.182255976365395e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 62/71 | LOSS: 7.14272411844124e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 63/71 | LOSS: 7.1130113212802826e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 64/71 | LOSS: 7.07656246539465e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 65/71 | LOSS: 7.032985918571781e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 66/71 | LOSS: 7.0039902461011575e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 67/71 | LOSS: 6.961420420820479e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 68/71 | LOSS: 6.927252356483908e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 69/71 | LOSS: 6.888969470050402e-06\n",
      "TRAIN: EPOCH 452/1000 | BATCH 70/71 | LOSS: 6.8481505980049455e-06\n",
      "VAL: EPOCH 452/1000 | BATCH 0/8 | LOSS: 4.492063453653827e-06\n",
      "VAL: EPOCH 452/1000 | BATCH 1/8 | LOSS: 4.292596713639796e-06\n",
      "VAL: EPOCH 452/1000 | BATCH 2/8 | LOSS: 4.380214704724494e-06\n",
      "VAL: EPOCH 452/1000 | BATCH 3/8 | LOSS: 4.452579105418408e-06\n",
      "VAL: EPOCH 452/1000 | BATCH 4/8 | LOSS: 4.343633736425545e-06\n",
      "VAL: EPOCH 452/1000 | BATCH 5/8 | LOSS: 4.2843892060773214e-06\n",
      "VAL: EPOCH 452/1000 | BATCH 6/8 | LOSS: 4.165693456213505e-06\n",
      "VAL: EPOCH 452/1000 | BATCH 7/8 | LOSS: 4.0539459007504774e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 0/71 | LOSS: 3.181521151418565e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 1/71 | LOSS: 4.241597935106256e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 2/71 | LOSS: 4.263361309616205e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 3/71 | LOSS: 4.12210596323348e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 4/71 | LOSS: 4.385561760500423e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 5/71 | LOSS: 4.57471890058514e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 6/71 | LOSS: 4.667159081301568e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 7/71 | LOSS: 4.6701272538030025e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 8/71 | LOSS: 4.622138905890299e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 9/71 | LOSS: 4.760403066939034e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 10/71 | LOSS: 4.766319369247702e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 11/71 | LOSS: 4.699916549573875e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 12/71 | LOSS: 4.654844168236685e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 13/71 | LOSS: 4.572300960197546e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 14/71 | LOSS: 4.549524237518199e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 15/71 | LOSS: 4.508620889964732e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 16/71 | LOSS: 4.493950146912967e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 17/71 | LOSS: 4.50164990676664e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 18/71 | LOSS: 4.5228560721673265e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 19/71 | LOSS: 4.5005993797531115e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 20/71 | LOSS: 4.527101498838636e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 21/71 | LOSS: 4.539874494598322e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 22/71 | LOSS: 4.549420124100794e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 23/71 | LOSS: 4.5108959056960884e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 24/71 | LOSS: 4.53116292192135e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 25/71 | LOSS: 4.575795488702939e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 26/71 | LOSS: 4.581476484114925e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 27/71 | LOSS: 4.555955846236819e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 28/71 | LOSS: 4.612778913473574e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 29/71 | LOSS: 4.658836739205678e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 30/71 | LOSS: 4.6947711927068965e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 31/71 | LOSS: 4.703550530393841e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 32/71 | LOSS: 4.721570128436504e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 33/71 | LOSS: 4.7072554239361234e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 34/71 | LOSS: 4.6925093166854435e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 35/71 | LOSS: 4.6773791255974275e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 36/71 | LOSS: 4.687188720708946e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 37/71 | LOSS: 4.694291599087974e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 38/71 | LOSS: 4.744906082356838e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 39/71 | LOSS: 4.7373576990139554e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 40/71 | LOSS: 4.7210119968129705e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 41/71 | LOSS: 4.7015568313890115e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 42/71 | LOSS: 4.722084097095851e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 43/71 | LOSS: 4.7340254861245e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 44/71 | LOSS: 4.7228164273999735e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 45/71 | LOSS: 4.7383115365253454e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 46/71 | LOSS: 4.755504302552931e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 47/71 | LOSS: 4.747896165705849e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 48/71 | LOSS: 4.774019971034936e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 49/71 | LOSS: 4.763101542266668e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 50/71 | LOSS: 4.752059464640208e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 51/71 | LOSS: 4.752672452923434e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 52/71 | LOSS: 4.756175764308658e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 53/71 | LOSS: 4.755582301219908e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 54/71 | LOSS: 4.747276191417107e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 55/71 | LOSS: 4.7341205069122746e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 56/71 | LOSS: 4.729317871559097e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 57/71 | LOSS: 4.724391091854316e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 58/71 | LOSS: 4.700545274718641e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 59/71 | LOSS: 4.703475951828295e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 60/71 | LOSS: 4.712164734912937e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 61/71 | LOSS: 4.711896083115848e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 62/71 | LOSS: 4.710432907784938e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 63/71 | LOSS: 4.704562080348751e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 64/71 | LOSS: 4.7003618139849725e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 65/71 | LOSS: 4.732330697868637e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 66/71 | LOSS: 4.718395197713433e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 67/71 | LOSS: 4.71944136560363e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 68/71 | LOSS: 4.704935453565153e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 69/71 | LOSS: 4.683102341524708e-06\n",
      "TRAIN: EPOCH 453/1000 | BATCH 70/71 | LOSS: 4.6919480552441855e-06\n",
      "VAL: EPOCH 453/1000 | BATCH 0/8 | LOSS: 4.916357738693478e-06\n",
      "VAL: EPOCH 453/1000 | BATCH 1/8 | LOSS: 4.846013780479552e-06\n",
      "VAL: EPOCH 453/1000 | BATCH 2/8 | LOSS: 5.067759957455564e-06\n",
      "VAL: EPOCH 453/1000 | BATCH 3/8 | LOSS: 5.062615855422337e-06\n",
      "VAL: EPOCH 453/1000 | BATCH 4/8 | LOSS: 5.009418237023055e-06\n",
      "VAL: EPOCH 453/1000 | BATCH 5/8 | LOSS: 4.832775630347896e-06\n",
      "VAL: EPOCH 453/1000 | BATCH 6/8 | LOSS: 4.7082500844096235e-06\n",
      "VAL: EPOCH 453/1000 | BATCH 7/8 | LOSS: 4.610053508713463e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 0/71 | LOSS: 4.2897418097709306e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 1/71 | LOSS: 4.203635398880579e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 2/71 | LOSS: 4.278146813400478e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 3/71 | LOSS: 4.320178732086788e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 4/71 | LOSS: 4.4528889702633025e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 5/71 | LOSS: 4.247870568481933e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 6/71 | LOSS: 4.512647267672167e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 7/71 | LOSS: 4.6043347765589715e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 8/71 | LOSS: 4.5259010019233556e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 9/71 | LOSS: 4.536790856946027e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 10/71 | LOSS: 4.480719987689306e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 11/71 | LOSS: 4.462581349192381e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 12/71 | LOSS: 4.441090473287309e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 13/71 | LOSS: 4.444193466822201e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 14/71 | LOSS: 4.4924501101680414e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 15/71 | LOSS: 4.450057787153128e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 16/71 | LOSS: 4.52368439094725e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 17/71 | LOSS: 4.544734565570252e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 18/71 | LOSS: 4.532274751430745e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 19/71 | LOSS: 4.4799384909310905e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 20/71 | LOSS: 4.493785787590674e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 21/71 | LOSS: 4.512754013706978e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 22/71 | LOSS: 4.561840826325984e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 23/71 | LOSS: 4.536220169863252e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 24/71 | LOSS: 4.5329930344450984e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 25/71 | LOSS: 4.543698537418095e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 26/71 | LOSS: 4.580339910481362e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 27/71 | LOSS: 4.6024005817863715e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 28/71 | LOSS: 4.64578801225359e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 29/71 | LOSS: 4.658933759552989e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 30/71 | LOSS: 4.663394542325067e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 31/71 | LOSS: 4.676579301587935e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 32/71 | LOSS: 4.698692471334432e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 33/71 | LOSS: 4.681773622691895e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 34/71 | LOSS: 4.719048378579568e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 35/71 | LOSS: 4.693202741792548e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 36/71 | LOSS: 4.719802659933889e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 37/71 | LOSS: 4.684829856447131e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 38/71 | LOSS: 4.669580920772904e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 39/71 | LOSS: 4.695279977795508e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 40/71 | LOSS: 4.705689368010342e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 41/71 | LOSS: 4.740075886339972e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 42/71 | LOSS: 4.770778903463886e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 43/71 | LOSS: 4.780378085136115e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 44/71 | LOSS: 4.769586732638547e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 45/71 | LOSS: 4.762042037683663e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 46/71 | LOSS: 4.803787774978852e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 47/71 | LOSS: 4.7852758247775755e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 48/71 | LOSS: 4.79496000535823e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 49/71 | LOSS: 4.814442850147315e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 50/71 | LOSS: 4.852869154752161e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 51/71 | LOSS: 4.867586500705119e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 52/71 | LOSS: 4.850262154643545e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 53/71 | LOSS: 4.869929057349509e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 54/71 | LOSS: 4.871780290407266e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 55/71 | LOSS: 4.890328215521679e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 56/71 | LOSS: 4.9064161278996226e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 57/71 | LOSS: 4.896662995508513e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 58/71 | LOSS: 4.889929938391275e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 59/71 | LOSS: 4.888625240558516e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 60/71 | LOSS: 4.8757398283177295e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 61/71 | LOSS: 4.8613654459381174e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 62/71 | LOSS: 4.86090852829211e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 63/71 | LOSS: 4.849854494892725e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 64/71 | LOSS: 4.879775885386222e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 65/71 | LOSS: 4.866586457617152e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 66/71 | LOSS: 4.8621980181112e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 67/71 | LOSS: 4.873002363664829e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 68/71 | LOSS: 4.862289178281967e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 69/71 | LOSS: 4.877588237052675e-06\n",
      "TRAIN: EPOCH 454/1000 | BATCH 70/71 | LOSS: 4.893295563050114e-06\n",
      "VAL: EPOCH 454/1000 | BATCH 0/8 | LOSS: 5.103324838273693e-06\n",
      "VAL: EPOCH 454/1000 | BATCH 1/8 | LOSS: 4.8981137297232635e-06\n",
      "VAL: EPOCH 454/1000 | BATCH 2/8 | LOSS: 4.947027264279313e-06\n",
      "VAL: EPOCH 454/1000 | BATCH 3/8 | LOSS: 4.877952733295388e-06\n",
      "VAL: EPOCH 454/1000 | BATCH 4/8 | LOSS: 4.799637918040389e-06\n",
      "VAL: EPOCH 454/1000 | BATCH 5/8 | LOSS: 4.6556458528357325e-06\n",
      "VAL: EPOCH 454/1000 | BATCH 6/8 | LOSS: 4.5548927768582615e-06\n",
      "VAL: EPOCH 454/1000 | BATCH 7/8 | LOSS: 4.431265864468514e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 0/71 | LOSS: 3.082869170611957e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 1/71 | LOSS: 4.135760264034616e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 2/71 | LOSS: 4.714234061490667e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 3/71 | LOSS: 4.5684131464440725e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 4/71 | LOSS: 4.682581038650824e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 5/71 | LOSS: 4.8516754607893136e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 6/71 | LOSS: 4.805346699348385e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 7/71 | LOSS: 4.73726367999916e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 8/71 | LOSS: 4.693222308560507e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 9/71 | LOSS: 4.640068527805852e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 10/71 | LOSS: 4.639217091938587e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 11/71 | LOSS: 4.648335713379008e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 12/71 | LOSS: 4.697874618824034e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 13/71 | LOSS: 4.648520189221017e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 14/71 | LOSS: 4.627446408752197e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 15/71 | LOSS: 4.606148934271914e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 16/71 | LOSS: 4.597555155615898e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 17/71 | LOSS: 4.576889194949116e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 18/71 | LOSS: 4.61960591660739e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 19/71 | LOSS: 4.581247867463389e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 20/71 | LOSS: 4.557001158814057e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 21/71 | LOSS: 4.534096702131634e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 22/71 | LOSS: 4.5321037997263885e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 23/71 | LOSS: 4.466549626158667e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 24/71 | LOSS: 4.444145815796219e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 25/71 | LOSS: 4.443236915764059e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 26/71 | LOSS: 4.431219234969898e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 27/71 | LOSS: 4.396735838391967e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 28/71 | LOSS: 4.41129521214893e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 29/71 | LOSS: 4.407333047614278e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 30/71 | LOSS: 4.420222408483706e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 31/71 | LOSS: 4.447008464580904e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 32/71 | LOSS: 4.446138890753818e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 33/71 | LOSS: 4.438619485239649e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 34/71 | LOSS: 4.446108271492579e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 35/71 | LOSS: 4.471155989449471e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 36/71 | LOSS: 4.4495067155563645e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 37/71 | LOSS: 4.455879212927309e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 38/71 | LOSS: 4.477235885739598e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 39/71 | LOSS: 4.458042855048916e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 40/71 | LOSS: 4.468000469853173e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 41/71 | LOSS: 4.480947008492617e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 42/71 | LOSS: 4.489436647883511e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 43/71 | LOSS: 4.512181305926788e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 44/71 | LOSS: 4.513392029063349e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 45/71 | LOSS: 4.484476023761722e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 46/71 | LOSS: 4.515205004513809e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 47/71 | LOSS: 4.5535312930648315e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 48/71 | LOSS: 4.534220857860827e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 49/71 | LOSS: 4.571712765937263e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 50/71 | LOSS: 4.555672946890791e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 51/71 | LOSS: 4.5271501282793624e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 52/71 | LOSS: 4.544661123873084e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 53/71 | LOSS: 4.540603223783061e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 54/71 | LOSS: 4.53351018140066e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 55/71 | LOSS: 4.514409352753189e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 56/71 | LOSS: 4.505776762910005e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 57/71 | LOSS: 4.495692231667535e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 58/71 | LOSS: 4.487546922233778e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 59/71 | LOSS: 4.4747520708673015e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 60/71 | LOSS: 4.493706865850683e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 61/71 | LOSS: 4.493087515349485e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 62/71 | LOSS: 4.50418491031058e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 63/71 | LOSS: 4.502796045358082e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 64/71 | LOSS: 4.5022809759897286e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 65/71 | LOSS: 4.516938402461661e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 66/71 | LOSS: 4.5253900612236186e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 67/71 | LOSS: 4.5076328058926534e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 68/71 | LOSS: 4.514214638681775e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 69/71 | LOSS: 4.5084606621717935e-06\n",
      "TRAIN: EPOCH 455/1000 | BATCH 70/71 | LOSS: 4.508446674893568e-06\n",
      "VAL: EPOCH 455/1000 | BATCH 0/8 | LOSS: 4.652403276850237e-06\n",
      "VAL: EPOCH 455/1000 | BATCH 1/8 | LOSS: 4.602339231496444e-06\n",
      "VAL: EPOCH 455/1000 | BATCH 2/8 | LOSS: 4.6635639895005925e-06\n",
      "VAL: EPOCH 455/1000 | BATCH 3/8 | LOSS: 4.730971681965457e-06\n",
      "VAL: EPOCH 455/1000 | BATCH 4/8 | LOSS: 4.631312640412943e-06\n",
      "VAL: EPOCH 455/1000 | BATCH 5/8 | LOSS: 4.598409835428659e-06\n",
      "VAL: EPOCH 455/1000 | BATCH 6/8 | LOSS: 4.454234190883913e-06\n",
      "VAL: EPOCH 455/1000 | BATCH 7/8 | LOSS: 4.342197769346967e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 0/71 | LOSS: 3.611307874962222e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 1/71 | LOSS: 4.398364580993075e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 2/71 | LOSS: 4.549892158441556e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 3/71 | LOSS: 4.472558998713794e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 4/71 | LOSS: 4.4986240027355965e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 5/71 | LOSS: 4.513175933122208e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 6/71 | LOSS: 4.527501070177615e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 7/71 | LOSS: 4.477998970742192e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 8/71 | LOSS: 4.51957319253577e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 9/71 | LOSS: 4.498063890423509e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 10/71 | LOSS: 4.518559390734564e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 11/71 | LOSS: 4.584031444210268e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 12/71 | LOSS: 4.584560708910711e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 13/71 | LOSS: 4.54607896114924e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 14/71 | LOSS: 4.580609280916785e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 15/71 | LOSS: 4.547065003634998e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 16/71 | LOSS: 4.5546543440329565e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 17/71 | LOSS: 4.509176329116195e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 18/71 | LOSS: 4.513809160830294e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 19/71 | LOSS: 4.539105771073082e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 20/71 | LOSS: 4.512304734643771e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 21/71 | LOSS: 4.48331120010566e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 22/71 | LOSS: 4.469227208218122e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 23/71 | LOSS: 4.413541186446916e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 24/71 | LOSS: 4.408659351611277e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 25/71 | LOSS: 4.379467047608789e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 26/71 | LOSS: 4.38089195826992e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 27/71 | LOSS: 4.354074917500839e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 28/71 | LOSS: 4.356765296914341e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 29/71 | LOSS: 4.343597296004494e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 30/71 | LOSS: 4.36295592789388e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 31/71 | LOSS: 4.402493928523654e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 32/71 | LOSS: 4.4234700798176965e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 33/71 | LOSS: 4.425800638915355e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 34/71 | LOSS: 4.443347253462499e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 35/71 | LOSS: 4.445437904602538e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 36/71 | LOSS: 4.494693638588541e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 37/71 | LOSS: 4.475352457838629e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 38/71 | LOSS: 4.505223993570932e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 39/71 | LOSS: 4.4860136540592064e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 40/71 | LOSS: 4.4912579962728185e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 41/71 | LOSS: 4.526328038558686e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 42/71 | LOSS: 4.516850906860105e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 43/71 | LOSS: 4.570877846080376e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 44/71 | LOSS: 4.575619479712461e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 45/71 | LOSS: 4.630565528081522e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 46/71 | LOSS: 4.626995149087731e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 47/71 | LOSS: 4.6849202703924675e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 48/71 | LOSS: 4.6630427129216235e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 49/71 | LOSS: 4.704129669335088e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 50/71 | LOSS: 4.7660049609012456e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 51/71 | LOSS: 4.832050911737986e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 52/71 | LOSS: 4.864466140957721e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 53/71 | LOSS: 4.848372192874969e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 54/71 | LOSS: 4.953833831677912e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 55/71 | LOSS: 4.949216799689436e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 56/71 | LOSS: 5.029794305811632e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 57/71 | LOSS: 5.034956192785417e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 58/71 | LOSS: 5.0465898959250264e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 59/71 | LOSS: 5.091512442353027e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 60/71 | LOSS: 5.1113175522616195e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 61/71 | LOSS: 5.166587646770847e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 62/71 | LOSS: 5.164904912759547e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 63/71 | LOSS: 5.178251221593655e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 64/71 | LOSS: 5.1620236473144565e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 65/71 | LOSS: 5.1556337094745794e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 66/71 | LOSS: 5.148958884255175e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 67/71 | LOSS: 5.14613645250125e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 68/71 | LOSS: 5.155963504955213e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 69/71 | LOSS: 5.144243238776523e-06\n",
      "TRAIN: EPOCH 456/1000 | BATCH 70/71 | LOSS: 5.122758351025765e-06\n",
      "VAL: EPOCH 456/1000 | BATCH 0/8 | LOSS: 5.278393018670613e-06\n",
      "VAL: EPOCH 456/1000 | BATCH 1/8 | LOSS: 5.506509296537843e-06\n",
      "VAL: EPOCH 456/1000 | BATCH 2/8 | LOSS: 5.690052603313234e-06\n",
      "VAL: EPOCH 456/1000 | BATCH 3/8 | LOSS: 5.656544317389489e-06\n",
      "VAL: EPOCH 456/1000 | BATCH 4/8 | LOSS: 5.675317152054049e-06\n",
      "VAL: EPOCH 456/1000 | BATCH 5/8 | LOSS: 5.506503005866155e-06\n",
      "VAL: EPOCH 456/1000 | BATCH 6/8 | LOSS: 5.4757664916646064e-06\n",
      "VAL: EPOCH 456/1000 | BATCH 7/8 | LOSS: 5.450494882097701e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 0/71 | LOSS: 5.307757419359405e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 1/71 | LOSS: 5.636165042233188e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 2/71 | LOSS: 4.951039272782509e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 3/71 | LOSS: 5.163561297649721e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 4/71 | LOSS: 5.148083118911018e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 5/71 | LOSS: 5.156784823157068e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 6/71 | LOSS: 4.9954268368830005e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 7/71 | LOSS: 4.896493152273251e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 8/71 | LOSS: 4.89292478454849e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 9/71 | LOSS: 4.752353265757847e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 10/71 | LOSS: 4.700974283299399e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 11/71 | LOSS: 4.741312731463647e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 12/71 | LOSS: 4.780118038420019e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 13/71 | LOSS: 4.742329289391429e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 14/71 | LOSS: 4.677427038283592e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 15/71 | LOSS: 4.658229912024581e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 16/71 | LOSS: 4.630495890095892e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 17/71 | LOSS: 4.6490937645810645e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 18/71 | LOSS: 4.610760829324902e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 19/71 | LOSS: 4.599450210207578e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 20/71 | LOSS: 4.5869153598773885e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 21/71 | LOSS: 4.549592435151598e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 22/71 | LOSS: 4.567798827259179e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 23/71 | LOSS: 4.63018870770308e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 24/71 | LOSS: 4.634792667275178e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 25/71 | LOSS: 4.6156655413981825e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 26/71 | LOSS: 4.669497372931262e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 27/71 | LOSS: 4.697718322534326e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 28/71 | LOSS: 4.714703420867064e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 29/71 | LOSS: 4.703421874789153e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 30/71 | LOSS: 4.740370711935553e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 31/71 | LOSS: 4.741684541897939e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 32/71 | LOSS: 4.726826587515958e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 33/71 | LOSS: 4.698632618355452e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 34/71 | LOSS: 4.7122291562118335e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 35/71 | LOSS: 4.69137083630105e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 36/71 | LOSS: 4.662846279373028e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 37/71 | LOSS: 4.678077666061367e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 38/71 | LOSS: 4.663000184406813e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 39/71 | LOSS: 4.623527547664707e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 40/71 | LOSS: 4.611310143401369e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 41/71 | LOSS: 4.620432719093515e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 42/71 | LOSS: 4.629567736755721e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 43/71 | LOSS: 4.666820297478063e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 44/71 | LOSS: 4.678592035310834e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 45/71 | LOSS: 4.697322241475416e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 46/71 | LOSS: 4.67580345317169e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 47/71 | LOSS: 4.682130632242358e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 48/71 | LOSS: 4.7023904327794966e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 49/71 | LOSS: 4.715970326287788e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 50/71 | LOSS: 4.71593345234093e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 51/71 | LOSS: 4.71931248459721e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 52/71 | LOSS: 4.744855970166786e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 53/71 | LOSS: 4.753982634779446e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 54/71 | LOSS: 4.738927980890201e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 55/71 | LOSS: 4.789379626605036e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 56/71 | LOSS: 4.784567608383664e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 57/71 | LOSS: 4.7956479240132595e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 58/71 | LOSS: 4.815994127396044e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 59/71 | LOSS: 4.843711129372726e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 60/71 | LOSS: 4.858284802273866e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 61/71 | LOSS: 4.87870472405179e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 62/71 | LOSS: 4.92491615601928e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 63/71 | LOSS: 4.911440015575863e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 64/71 | LOSS: 4.992354661226273e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 65/71 | LOSS: 4.974662915353265e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 66/71 | LOSS: 4.998951708194678e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 67/71 | LOSS: 5.010676612554937e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 68/71 | LOSS: 5.027344602991513e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 69/71 | LOSS: 5.03249332593571e-06\n",
      "TRAIN: EPOCH 457/1000 | BATCH 70/71 | LOSS: 5.028278555470588e-06\n",
      "VAL: EPOCH 457/1000 | BATCH 0/8 | LOSS: 5.244140993454494e-06\n",
      "VAL: EPOCH 457/1000 | BATCH 1/8 | LOSS: 5.158671228855383e-06\n",
      "VAL: EPOCH 457/1000 | BATCH 2/8 | LOSS: 5.291465034436745e-06\n",
      "VAL: EPOCH 457/1000 | BATCH 3/8 | LOSS: 5.367429366742726e-06\n",
      "VAL: EPOCH 457/1000 | BATCH 4/8 | LOSS: 5.2629535275627856e-06\n",
      "VAL: EPOCH 457/1000 | BATCH 5/8 | LOSS: 5.3152471840197295e-06\n",
      "VAL: EPOCH 457/1000 | BATCH 6/8 | LOSS: 5.179804377673593e-06\n",
      "VAL: EPOCH 457/1000 | BATCH 7/8 | LOSS: 5.156501686087722e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 0/71 | LOSS: 5.539568519452587e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 1/71 | LOSS: 4.236997256157338e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 2/71 | LOSS: 4.5338036519145435e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 3/71 | LOSS: 4.553221174319333e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 4/71 | LOSS: 4.478422124520876e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 5/71 | LOSS: 4.928093706742705e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 6/71 | LOSS: 4.926621841150336e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 7/71 | LOSS: 5.068055997980991e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 8/71 | LOSS: 5.054851398098334e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 9/71 | LOSS: 5.258314013190102e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 10/71 | LOSS: 5.172125632022719e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 11/71 | LOSS: 5.183998079398104e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 12/71 | LOSS: 5.196022217345531e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 13/71 | LOSS: 5.186927150394435e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 14/71 | LOSS: 5.173365510321067e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 15/71 | LOSS: 5.097455527902639e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 16/71 | LOSS: 5.250289980133803e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 17/71 | LOSS: 5.2358349825630685e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 18/71 | LOSS: 5.32517366848065e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 19/71 | LOSS: 5.25860550624202e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 20/71 | LOSS: 5.273713755048534e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 21/71 | LOSS: 5.249354775184077e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 22/71 | LOSS: 5.224942054590666e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 23/71 | LOSS: 5.278515781507546e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 24/71 | LOSS: 5.258143519313308e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 25/71 | LOSS: 5.244102707225606e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 26/71 | LOSS: 5.176537022769615e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 27/71 | LOSS: 5.1227713129264885e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 28/71 | LOSS: 5.169327574423795e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 29/71 | LOSS: 5.166095873695061e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 30/71 | LOSS: 5.305718258891682e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 31/71 | LOSS: 5.3094276069032276e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 32/71 | LOSS: 5.3606645589311625e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 33/71 | LOSS: 5.325927745883853e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 34/71 | LOSS: 5.478265289379384e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 35/71 | LOSS: 5.4983331539107085e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 36/71 | LOSS: 5.6583224348385655e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 37/71 | LOSS: 5.6618407358511925e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 38/71 | LOSS: 5.663489119820625e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 39/71 | LOSS: 5.7179042983079855e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 40/71 | LOSS: 5.7335026590075494e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 41/71 | LOSS: 5.74448598031484e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 42/71 | LOSS: 5.719867076973165e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 43/71 | LOSS: 5.747492898742414e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 44/71 | LOSS: 5.72668700947866e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 45/71 | LOSS: 5.693987472825582e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 46/71 | LOSS: 5.71920522719317e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 47/71 | LOSS: 5.7107768232829885e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 48/71 | LOSS: 5.761114728241465e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 49/71 | LOSS: 5.749611691499013e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 50/71 | LOSS: 5.753737624735211e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 51/71 | LOSS: 5.7412699841454296e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 52/71 | LOSS: 5.722599214421726e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 53/71 | LOSS: 5.700666524380385e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 54/71 | LOSS: 5.663305835861882e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 55/71 | LOSS: 5.6372375070168346e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 56/71 | LOSS: 5.606515682493888e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 57/71 | LOSS: 5.5793846058804775e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 58/71 | LOSS: 5.55608906679327e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 59/71 | LOSS: 5.554324206968886e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 60/71 | LOSS: 5.527712704354256e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 61/71 | LOSS: 5.497501316283653e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 62/71 | LOSS: 5.502385062884839e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 63/71 | LOSS: 5.467252741198081e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 64/71 | LOSS: 5.4527398928117834e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 65/71 | LOSS: 5.433286732471915e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 66/71 | LOSS: 5.439322024751924e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 67/71 | LOSS: 5.446187762626162e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 68/71 | LOSS: 5.4337884859600365e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 69/71 | LOSS: 5.42893715516714e-06\n",
      "TRAIN: EPOCH 458/1000 | BATCH 70/71 | LOSS: 5.4096757286944765e-06\n",
      "VAL: EPOCH 458/1000 | BATCH 0/8 | LOSS: 6.787488473491976e-06\n",
      "VAL: EPOCH 458/1000 | BATCH 1/8 | LOSS: 6.41844303572725e-06\n",
      "VAL: EPOCH 458/1000 | BATCH 2/8 | LOSS: 6.316057048631289e-06\n",
      "VAL: EPOCH 458/1000 | BATCH 3/8 | LOSS: 6.247935743886046e-06\n",
      "VAL: EPOCH 458/1000 | BATCH 4/8 | LOSS: 6.131435657152906e-06\n",
      "VAL: EPOCH 458/1000 | BATCH 5/8 | LOSS: 5.827124823554186e-06\n",
      "VAL: EPOCH 458/1000 | BATCH 6/8 | LOSS: 5.682883186506972e-06\n",
      "VAL: EPOCH 458/1000 | BATCH 7/8 | LOSS: 5.484341841111018e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 0/71 | LOSS: 4.874980731983669e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 1/71 | LOSS: 4.86030967294937e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 2/71 | LOSS: 4.9192005159663195e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 3/71 | LOSS: 5.204581725593016e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 4/71 | LOSS: 4.8874884669203315e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 5/71 | LOSS: 5.628730074628645e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 6/71 | LOSS: 5.3631369415338014e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 7/71 | LOSS: 5.411886604633764e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 8/71 | LOSS: 5.411292982494666e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 9/71 | LOSS: 5.544657324207947e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 10/71 | LOSS: 5.334844838051305e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 11/71 | LOSS: 5.268089447933259e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 12/71 | LOSS: 5.175930307534425e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 13/71 | LOSS: 5.149767421893817e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 14/71 | LOSS: 5.006192759537953e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 15/71 | LOSS: 5.077333540270956e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 16/71 | LOSS: 5.002013775832465e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 17/71 | LOSS: 5.02299873359233e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 18/71 | LOSS: 4.972400550908979e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 19/71 | LOSS: 4.999840041364223e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 20/71 | LOSS: 4.991856658740462e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 21/71 | LOSS: 4.996801377191415e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 22/71 | LOSS: 5.00656554559891e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 23/71 | LOSS: 4.973967056306113e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 24/71 | LOSS: 4.987240381524316e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 25/71 | LOSS: 4.934364430762518e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 26/71 | LOSS: 4.915797969968278e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 27/71 | LOSS: 4.900147059418357e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 28/71 | LOSS: 4.8633992970155365e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 29/71 | LOSS: 4.894751320231686e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 30/71 | LOSS: 4.892575574441223e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 31/71 | LOSS: 4.862545431194576e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 32/71 | LOSS: 4.852804598461565e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 33/71 | LOSS: 4.84944022820641e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 34/71 | LOSS: 4.812935880441468e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 35/71 | LOSS: 4.779516334312777e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 36/71 | LOSS: 4.7562266998382355e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 37/71 | LOSS: 4.7584178506248894e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 38/71 | LOSS: 4.726852111138541e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 39/71 | LOSS: 4.692411073392577e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 40/71 | LOSS: 4.695775570747551e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 41/71 | LOSS: 4.690412771153552e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 42/71 | LOSS: 4.675642167269469e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 43/71 | LOSS: 4.678561377989933e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 44/71 | LOSS: 4.677786455431487e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 45/71 | LOSS: 4.673809579126891e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 46/71 | LOSS: 4.675128637129401e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 47/71 | LOSS: 4.671696861654103e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 48/71 | LOSS: 4.658166466358626e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 49/71 | LOSS: 4.642995481844991e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 50/71 | LOSS: 4.634908210017088e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 51/71 | LOSS: 4.6195124766130066e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 52/71 | LOSS: 4.612980542590793e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 53/71 | LOSS: 4.588574046027605e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 54/71 | LOSS: 4.571558809567994e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 55/71 | LOSS: 4.562593570004278e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 56/71 | LOSS: 4.558348364936823e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 57/71 | LOSS: 4.556979135862815e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 58/71 | LOSS: 4.559550862430084e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 59/71 | LOSS: 4.542604566874313e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 60/71 | LOSS: 4.535050415083266e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 61/71 | LOSS: 4.513834336384823e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 62/71 | LOSS: 4.521855944836889e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 63/71 | LOSS: 4.52327891053983e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 64/71 | LOSS: 4.516800856016254e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 65/71 | LOSS: 4.510367186109458e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 66/71 | LOSS: 4.515557554726395e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 67/71 | LOSS: 4.525708183845221e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 68/71 | LOSS: 4.528254930187688e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 69/71 | LOSS: 4.529072215778537e-06\n",
      "TRAIN: EPOCH 459/1000 | BATCH 70/71 | LOSS: 4.558599338336753e-06\n",
      "VAL: EPOCH 459/1000 | BATCH 0/8 | LOSS: 4.7816474761930294e-06\n",
      "VAL: EPOCH 459/1000 | BATCH 1/8 | LOSS: 5.0380836000840645e-06\n",
      "VAL: EPOCH 459/1000 | BATCH 2/8 | LOSS: 5.198542263921506e-06\n",
      "VAL: EPOCH 459/1000 | BATCH 3/8 | LOSS: 5.29994213138707e-06\n",
      "VAL: EPOCH 459/1000 | BATCH 4/8 | LOSS: 5.255028190731536e-06\n",
      "VAL: EPOCH 459/1000 | BATCH 5/8 | LOSS: 5.232985131442547e-06\n",
      "VAL: EPOCH 459/1000 | BATCH 6/8 | LOSS: 5.1315024133405785e-06\n",
      "VAL: EPOCH 459/1000 | BATCH 7/8 | LOSS: 5.061041690623824e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 0/71 | LOSS: 4.319278559705708e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 1/71 | LOSS: 4.415790954226395e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 2/71 | LOSS: 4.071011820390898e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 3/71 | LOSS: 4.370973670120293e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 4/71 | LOSS: 4.328912291384768e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 5/71 | LOSS: 4.3972192239986425e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 6/71 | LOSS: 4.6566410674131475e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 7/71 | LOSS: 4.801831835266057e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 8/71 | LOSS: 4.75976205254685e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 9/71 | LOSS: 4.743527006212389e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 10/71 | LOSS: 4.715782862066672e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 11/71 | LOSS: 4.585156015461204e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 12/71 | LOSS: 4.561458861364652e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 13/71 | LOSS: 4.6336939151712746e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 14/71 | LOSS: 4.7001786242617525e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 15/71 | LOSS: 4.765018772445728e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 16/71 | LOSS: 4.808702201711747e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 17/71 | LOSS: 4.772339694783214e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 18/71 | LOSS: 4.791099148573458e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 19/71 | LOSS: 4.780762685641093e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 20/71 | LOSS: 4.7206914413913006e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 21/71 | LOSS: 4.670764093878626e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 22/71 | LOSS: 4.689542927972861e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 23/71 | LOSS: 4.707450822631169e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 24/71 | LOSS: 4.726820952782873e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 25/71 | LOSS: 4.714013777209822e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 26/71 | LOSS: 4.69368441608148e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 27/71 | LOSS: 4.738072480644665e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 28/71 | LOSS: 4.726384367100324e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 29/71 | LOSS: 4.697853925487531e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 30/71 | LOSS: 4.704057177619806e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 31/71 | LOSS: 4.695514590480343e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 32/71 | LOSS: 4.710765083335431e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 33/71 | LOSS: 4.7125224950029995e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 34/71 | LOSS: 4.735069485052788e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 35/71 | LOSS: 4.744479497276188e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 36/71 | LOSS: 4.744494507011385e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 37/71 | LOSS: 4.7393632126170955e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 38/71 | LOSS: 4.7797916289336845e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 39/71 | LOSS: 4.777502738306794e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 40/71 | LOSS: 4.78217163351762e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 41/71 | LOSS: 4.805803612162847e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 42/71 | LOSS: 4.793051217813288e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 43/71 | LOSS: 4.819356397754084e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 44/71 | LOSS: 4.808055635092185e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 45/71 | LOSS: 4.857189740347246e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 46/71 | LOSS: 4.842855937471192e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 47/71 | LOSS: 4.843554885762084e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 48/71 | LOSS: 4.838430571425659e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 49/71 | LOSS: 4.8302627510565795e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 50/71 | LOSS: 4.8370508528594156e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 51/71 | LOSS: 4.838995822025866e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 52/71 | LOSS: 4.829095059732318e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 53/71 | LOSS: 4.85176751344288e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 54/71 | LOSS: 4.8334197179594245e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 55/71 | LOSS: 4.838500810170185e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 56/71 | LOSS: 4.835031831406236e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 57/71 | LOSS: 4.8143443438738325e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 58/71 | LOSS: 4.783281487477124e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 59/71 | LOSS: 4.777140236456034e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 60/71 | LOSS: 4.797532699471883e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 61/71 | LOSS: 4.784501547580857e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 62/71 | LOSS: 4.823772120029649e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 63/71 | LOSS: 4.832340064098162e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 64/71 | LOSS: 4.861018277761804e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 65/71 | LOSS: 4.83950881254824e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 66/71 | LOSS: 4.8267352552224406e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 67/71 | LOSS: 4.836914586924061e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 68/71 | LOSS: 4.810174746323711e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 69/71 | LOSS: 4.800194496315921e-06\n",
      "TRAIN: EPOCH 460/1000 | BATCH 70/71 | LOSS: 4.800558490643826e-06\n",
      "VAL: EPOCH 460/1000 | BATCH 0/8 | LOSS: 4.651601102523273e-06\n",
      "VAL: EPOCH 460/1000 | BATCH 1/8 | LOSS: 5.021084007239551e-06\n",
      "VAL: EPOCH 460/1000 | BATCH 2/8 | LOSS: 5.328775387170026e-06\n",
      "VAL: EPOCH 460/1000 | BATCH 3/8 | LOSS: 5.382541189646872e-06\n",
      "VAL: EPOCH 460/1000 | BATCH 4/8 | LOSS: 5.370348571887007e-06\n",
      "VAL: EPOCH 460/1000 | BATCH 5/8 | LOSS: 5.339974222806632e-06\n",
      "VAL: EPOCH 460/1000 | BATCH 6/8 | LOSS: 5.208882027675697e-06\n",
      "VAL: EPOCH 460/1000 | BATCH 7/8 | LOSS: 5.1560090241764556e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 0/71 | LOSS: 5.080927621747833e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 1/71 | LOSS: 4.38469578512013e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 2/71 | LOSS: 4.333795459388057e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 3/71 | LOSS: 4.674545380112249e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 4/71 | LOSS: 4.464223820832558e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 5/71 | LOSS: 4.73400602155986e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 6/71 | LOSS: 4.5663329534103724e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 7/71 | LOSS: 4.91028853844e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 8/71 | LOSS: 4.951388013473155e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 9/71 | LOSS: 4.999229645363812e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 10/71 | LOSS: 5.08895376779864e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 11/71 | LOSS: 5.012456521550727e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 12/71 | LOSS: 5.074595352338152e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 13/71 | LOSS: 5.0710638756105e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 14/71 | LOSS: 5.111060772833298e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 15/71 | LOSS: 5.096694636108623e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 16/71 | LOSS: 4.9904593411423885e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 17/71 | LOSS: 4.954624639847478e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 18/71 | LOSS: 4.938435423403127e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 19/71 | LOSS: 4.930954662540898e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 20/71 | LOSS: 4.918412254742829e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 21/71 | LOSS: 4.9838429380212226e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 22/71 | LOSS: 4.981123933960233e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 23/71 | LOSS: 4.933243218374628e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 24/71 | LOSS: 4.935376518915291e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 25/71 | LOSS: 4.9819883892828675e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 26/71 | LOSS: 4.978261207118824e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 27/71 | LOSS: 4.946286131663717e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 28/71 | LOSS: 5.006729452361469e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 29/71 | LOSS: 5.045059750348931e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 30/71 | LOSS: 5.000428032989505e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 31/71 | LOSS: 5.019551615248474e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 32/71 | LOSS: 5.031352298884127e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 33/71 | LOSS: 5.039362677320348e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 34/71 | LOSS: 5.0169873182832295e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 35/71 | LOSS: 4.986206546517577e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 36/71 | LOSS: 5.013921195682456e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 37/71 | LOSS: 5.0230285487965665e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 38/71 | LOSS: 5.031370718801881e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 39/71 | LOSS: 5.070657397254763e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 40/71 | LOSS: 5.09721571245059e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 41/71 | LOSS: 5.083985900542064e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 42/71 | LOSS: 5.07551085637567e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 43/71 | LOSS: 5.056059757697023e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 44/71 | LOSS: 5.03030676857937e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 45/71 | LOSS: 5.035484071453064e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 46/71 | LOSS: 5.0382654167828766e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 47/71 | LOSS: 5.03519015164026e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 48/71 | LOSS: 5.048877292953263e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 49/71 | LOSS: 5.050828826824727e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 50/71 | LOSS: 5.031039273828855e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 51/71 | LOSS: 5.0184717766709445e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 52/71 | LOSS: 4.993206016908242e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 53/71 | LOSS: 5.0154474390890555e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 54/71 | LOSS: 4.984183785629946e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 55/71 | LOSS: 4.998032368673323e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 56/71 | LOSS: 4.986684145255263e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 57/71 | LOSS: 4.98519649101644e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 58/71 | LOSS: 4.976860902573562e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 59/71 | LOSS: 4.960527811211554e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 60/71 | LOSS: 4.94467858367048e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 61/71 | LOSS: 4.933667051157118e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 62/71 | LOSS: 4.9286115917932454e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 63/71 | LOSS: 4.9176901200098655e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 64/71 | LOSS: 4.91184378356243e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 65/71 | LOSS: 4.910209471828156e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 66/71 | LOSS: 4.908668727951938e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 67/71 | LOSS: 4.90258506648816e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 68/71 | LOSS: 4.903343225400338e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 69/71 | LOSS: 4.896962796660123e-06\n",
      "TRAIN: EPOCH 461/1000 | BATCH 70/71 | LOSS: 4.878507038291657e-06\n",
      "VAL: EPOCH 461/1000 | BATCH 0/8 | LOSS: 5.23440758115612e-06\n",
      "VAL: EPOCH 461/1000 | BATCH 1/8 | LOSS: 5.237101277089096e-06\n",
      "VAL: EPOCH 461/1000 | BATCH 2/8 | LOSS: 5.173302421705254e-06\n",
      "VAL: EPOCH 461/1000 | BATCH 3/8 | LOSS: 5.225337986303202e-06\n",
      "VAL: EPOCH 461/1000 | BATCH 4/8 | LOSS: 5.1535028433136175e-06\n",
      "VAL: EPOCH 461/1000 | BATCH 5/8 | LOSS: 4.9633923329868894e-06\n",
      "VAL: EPOCH 461/1000 | BATCH 6/8 | LOSS: 4.805999716544258e-06\n",
      "VAL: EPOCH 461/1000 | BATCH 7/8 | LOSS: 4.62243485799263e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 0/71 | LOSS: 4.623884251486743e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 1/71 | LOSS: 4.957674264005618e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 2/71 | LOSS: 4.873628313362133e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 3/71 | LOSS: 4.85698296870396e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 4/71 | LOSS: 5.004779632145074e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 5/71 | LOSS: 4.790161559261226e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 6/71 | LOSS: 4.9101239320797115e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 7/71 | LOSS: 4.857736769281473e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 8/71 | LOSS: 4.914689018025658e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 9/71 | LOSS: 4.860706349063549e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 10/71 | LOSS: 4.9229448367524045e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 11/71 | LOSS: 4.910750552274597e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 12/71 | LOSS: 4.7944599828042556e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 13/71 | LOSS: 4.7971600127181905e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 14/71 | LOSS: 4.751469047429661e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 15/71 | LOSS: 4.709732792207433e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 16/71 | LOSS: 4.757144110686461e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 17/71 | LOSS: 4.755414996301018e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 18/71 | LOSS: 4.695099003009473e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 19/71 | LOSS: 4.677932201957446e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 20/71 | LOSS: 4.668376017694495e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 21/71 | LOSS: 4.681520191628476e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 22/71 | LOSS: 4.6354462226546555e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 23/71 | LOSS: 4.693459705625476e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 24/71 | LOSS: 4.716362454928458e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 25/71 | LOSS: 4.697387007231905e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 26/71 | LOSS: 4.7386013782805224e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 27/71 | LOSS: 4.701039805305689e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 28/71 | LOSS: 4.7977853463290274e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 29/71 | LOSS: 4.786476620211033e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 30/71 | LOSS: 4.876153364039642e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 31/71 | LOSS: 4.843196450110554e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 32/71 | LOSS: 4.875505345063919e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 33/71 | LOSS: 4.864962072721991e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 34/71 | LOSS: 4.847884182319311e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 35/71 | LOSS: 4.841321874159297e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 36/71 | LOSS: 4.822716311684564e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 37/71 | LOSS: 4.793999975941245e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 38/71 | LOSS: 4.762383777909772e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 39/71 | LOSS: 4.747301017005157e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 40/71 | LOSS: 4.7545580933152315e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 41/71 | LOSS: 4.741474407681034e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 42/71 | LOSS: 4.776817451234227e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 43/71 | LOSS: 4.7655823943221716e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 44/71 | LOSS: 4.747570493337763e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 45/71 | LOSS: 4.721591465014448e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 46/71 | LOSS: 4.728358430355079e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 47/71 | LOSS: 4.717236777196376e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 48/71 | LOSS: 4.747007931989489e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 49/71 | LOSS: 4.7569265007041395e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 50/71 | LOSS: 4.727963196095445e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 51/71 | LOSS: 4.700358658737969e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 52/71 | LOSS: 4.707534022806081e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 53/71 | LOSS: 4.694495104289402e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 54/71 | LOSS: 4.685817839344964e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 55/71 | LOSS: 4.687984674741788e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 56/71 | LOSS: 4.706209382308308e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 57/71 | LOSS: 4.70774348129986e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 58/71 | LOSS: 4.731805432329874e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 59/71 | LOSS: 4.725423832496745e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 60/71 | LOSS: 4.731350540556586e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 61/71 | LOSS: 4.730795206987457e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 62/71 | LOSS: 4.7397223184701045e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 63/71 | LOSS: 4.742243113753375e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 64/71 | LOSS: 4.74089748912285e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 65/71 | LOSS: 4.729155748326306e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 66/71 | LOSS: 4.721621640348883e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 67/71 | LOSS: 4.740151118365996e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 68/71 | LOSS: 4.725349510303256e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 69/71 | LOSS: 4.718854760150342e-06\n",
      "TRAIN: EPOCH 462/1000 | BATCH 70/71 | LOSS: 4.690754269791847e-06\n",
      "VAL: EPOCH 462/1000 | BATCH 0/8 | LOSS: 5.014211183151929e-06\n",
      "VAL: EPOCH 462/1000 | BATCH 1/8 | LOSS: 4.93280799673812e-06\n",
      "VAL: EPOCH 462/1000 | BATCH 2/8 | LOSS: 4.850339943610986e-06\n",
      "VAL: EPOCH 462/1000 | BATCH 3/8 | LOSS: 4.9105321977549465e-06\n",
      "VAL: EPOCH 462/1000 | BATCH 4/8 | LOSS: 4.766454549098853e-06\n",
      "VAL: EPOCH 462/1000 | BATCH 5/8 | LOSS: 4.6752003678799765e-06\n",
      "VAL: EPOCH 462/1000 | BATCH 6/8 | LOSS: 4.49630179640995e-06\n",
      "VAL: EPOCH 462/1000 | BATCH 7/8 | LOSS: 4.293306744784786e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 0/71 | LOSS: 4.491348590818234e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 1/71 | LOSS: 4.0569137809143285e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 2/71 | LOSS: 4.425096221893909e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 3/71 | LOSS: 4.226728947287484e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 4/71 | LOSS: 4.40616959167528e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 5/71 | LOSS: 4.2261829473015195e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 6/71 | LOSS: 4.061267288437063e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 7/71 | LOSS: 4.058864561784503e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 8/71 | LOSS: 4.062981765350236e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 9/71 | LOSS: 4.004040079053084e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 10/71 | LOSS: 4.0498342844908954e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 11/71 | LOSS: 4.131212961056008e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 12/71 | LOSS: 4.212512294543558e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 13/71 | LOSS: 4.249459980850848e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 14/71 | LOSS: 4.315964952184004e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 15/71 | LOSS: 4.341364260085356e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 16/71 | LOSS: 4.307329604938893e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 17/71 | LOSS: 4.363051769157917e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 18/71 | LOSS: 4.3601900367656e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 19/71 | LOSS: 4.3518116967788956e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 20/71 | LOSS: 4.336213058715137e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 21/71 | LOSS: 4.348845312630477e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 22/71 | LOSS: 4.356123702603762e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 23/71 | LOSS: 4.354999466234706e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 24/71 | LOSS: 4.338116168582928e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 25/71 | LOSS: 4.330279151732751e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 26/71 | LOSS: 4.348373202581705e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 27/71 | LOSS: 4.3341534186376e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 28/71 | LOSS: 4.315870816346182e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 29/71 | LOSS: 4.300042269278492e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 30/71 | LOSS: 4.296736946226672e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 31/71 | LOSS: 4.3052495399820145e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 32/71 | LOSS: 4.2978986061136695e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 33/71 | LOSS: 4.330129506931425e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 34/71 | LOSS: 4.311627961734692e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 35/71 | LOSS: 4.338621661443338e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 36/71 | LOSS: 4.35241006901354e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 37/71 | LOSS: 4.359680773584267e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 38/71 | LOSS: 4.347209112846684e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 39/71 | LOSS: 4.332292843400865e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 40/71 | LOSS: 4.332566848505649e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 41/71 | LOSS: 4.338979562947186e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 42/71 | LOSS: 4.338092499300581e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 43/71 | LOSS: 4.3629514049247185e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 44/71 | LOSS: 4.354787845133817e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 45/71 | LOSS: 4.366685896892032e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 46/71 | LOSS: 4.340569279172106e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 47/71 | LOSS: 4.361547562818184e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 48/71 | LOSS: 4.365529325788922e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 49/71 | LOSS: 4.379303218229324e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 50/71 | LOSS: 4.390740313657957e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 51/71 | LOSS: 4.385095429335286e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 52/71 | LOSS: 4.405821356450137e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 53/71 | LOSS: 4.410738045879515e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 54/71 | LOSS: 4.41097059096633e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 55/71 | LOSS: 4.401695564735876e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 56/71 | LOSS: 4.370191761968033e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 57/71 | LOSS: 4.373171875691884e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 58/71 | LOSS: 4.364585331162651e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 59/71 | LOSS: 4.373431367336404e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 60/71 | LOSS: 4.3675185239766e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 61/71 | LOSS: 4.374012588971621e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 62/71 | LOSS: 4.354344510497795e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 63/71 | LOSS: 4.353811846158351e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 64/71 | LOSS: 4.34172490293769e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 65/71 | LOSS: 4.3281684018879645e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 66/71 | LOSS: 4.328238372100211e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 67/71 | LOSS: 4.322536496789219e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 68/71 | LOSS: 4.320355935266454e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 69/71 | LOSS: 4.320467002928905e-06\n",
      "TRAIN: EPOCH 463/1000 | BATCH 70/71 | LOSS: 4.317330755108908e-06\n",
      "VAL: EPOCH 463/1000 | BATCH 0/8 | LOSS: 5.244336989562726e-06\n",
      "VAL: EPOCH 463/1000 | BATCH 1/8 | LOSS: 5.002199031878263e-06\n",
      "VAL: EPOCH 463/1000 | BATCH 2/8 | LOSS: 4.83902052413517e-06\n",
      "VAL: EPOCH 463/1000 | BATCH 3/8 | LOSS: 4.8293145482602995e-06\n",
      "VAL: EPOCH 463/1000 | BATCH 4/8 | LOSS: 4.692186303145718e-06\n",
      "VAL: EPOCH 463/1000 | BATCH 5/8 | LOSS: 4.5789379328198265e-06\n",
      "VAL: EPOCH 463/1000 | BATCH 6/8 | LOSS: 4.38616897164528e-06\n",
      "VAL: EPOCH 463/1000 | BATCH 7/8 | LOSS: 4.2366240791125165e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 0/71 | LOSS: 4.637352503777947e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 1/71 | LOSS: 4.999317752663046e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 2/71 | LOSS: 4.593525621506463e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 3/71 | LOSS: 4.657096383198223e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 4/71 | LOSS: 4.541921498457668e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 5/71 | LOSS: 4.433363680315476e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 6/71 | LOSS: 4.465667123960364e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 7/71 | LOSS: 4.407733229072619e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 8/71 | LOSS: 4.261674803274218e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 9/71 | LOSS: 4.218710910208756e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 10/71 | LOSS: 4.290787894040642e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 11/71 | LOSS: 4.3386504178973455e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 12/71 | LOSS: 4.336622133940602e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 13/71 | LOSS: 4.373731176851184e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 14/71 | LOSS: 4.3804638456398e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 15/71 | LOSS: 4.346693629031506e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 16/71 | LOSS: 4.406521143477328e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 17/71 | LOSS: 4.408357906666222e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 18/71 | LOSS: 4.377244677973642e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 19/71 | LOSS: 4.37095063716697e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 20/71 | LOSS: 4.390830543429945e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 21/71 | LOSS: 4.403376351547195e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 22/71 | LOSS: 4.4099499815353696e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 23/71 | LOSS: 4.395076568168103e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 24/71 | LOSS: 4.37516171587049e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 25/71 | LOSS: 4.3407191801984136e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 26/71 | LOSS: 4.349731187647666e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 27/71 | LOSS: 4.347552135186561e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 28/71 | LOSS: 4.3154369011282486e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 29/71 | LOSS: 4.3523797406426945e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 30/71 | LOSS: 4.340306077210698e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 31/71 | LOSS: 4.373093275944484e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 32/71 | LOSS: 4.3671088309510555e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 33/71 | LOSS: 4.435479922904051e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 34/71 | LOSS: 4.439700166715608e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 35/71 | LOSS: 4.407777011137013e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 36/71 | LOSS: 4.446837299444785e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 37/71 | LOSS: 4.49901102982901e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 38/71 | LOSS: 4.4812706316103385e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 39/71 | LOSS: 4.507692403876718e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 40/71 | LOSS: 4.484875972127888e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 41/71 | LOSS: 4.52791755378712e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 42/71 | LOSS: 4.5048395804769825e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 43/71 | LOSS: 4.506996571184242e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 44/71 | LOSS: 4.506478338347127e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 45/71 | LOSS: 4.514078623090071e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 46/71 | LOSS: 4.4917650232859395e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 47/71 | LOSS: 4.49160457568117e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 48/71 | LOSS: 4.499178560633611e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 49/71 | LOSS: 4.501758530750521e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 50/71 | LOSS: 4.525251227403037e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 51/71 | LOSS: 4.524892705586143e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 52/71 | LOSS: 4.520681279314405e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 53/71 | LOSS: 4.508551531519818e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 54/71 | LOSS: 4.498882853195854e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 55/71 | LOSS: 4.507823267171521e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 56/71 | LOSS: 4.524244623258199e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 57/71 | LOSS: 4.526169073744376e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 58/71 | LOSS: 4.551478638350707e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 59/71 | LOSS: 4.541415266127539e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 60/71 | LOSS: 4.574237008172099e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 61/71 | LOSS: 4.570247835633176e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 62/71 | LOSS: 4.593145034064831e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 63/71 | LOSS: 4.6032073086621494e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 64/71 | LOSS: 4.633262086337289e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 65/71 | LOSS: 4.640911563249326e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 66/71 | LOSS: 4.654082903158921e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 67/71 | LOSS: 4.667465759666811e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 68/71 | LOSS: 4.676596580648566e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 69/71 | LOSS: 4.6613229999690835e-06\n",
      "TRAIN: EPOCH 464/1000 | BATCH 70/71 | LOSS: 4.659534768916914e-06\n",
      "VAL: EPOCH 464/1000 | BATCH 0/8 | LOSS: 4.462250217329711e-06\n",
      "VAL: EPOCH 464/1000 | BATCH 1/8 | LOSS: 4.314911848268821e-06\n",
      "VAL: EPOCH 464/1000 | BATCH 2/8 | LOSS: 4.3298531030207714e-06\n",
      "VAL: EPOCH 464/1000 | BATCH 3/8 | LOSS: 4.365175414022815e-06\n",
      "VAL: EPOCH 464/1000 | BATCH 4/8 | LOSS: 4.310257372708292e-06\n",
      "VAL: EPOCH 464/1000 | BATCH 5/8 | LOSS: 4.271963234714349e-06\n",
      "VAL: EPOCH 464/1000 | BATCH 6/8 | LOSS: 4.159162732061564e-06\n",
      "VAL: EPOCH 464/1000 | BATCH 7/8 | LOSS: 4.093746042599378e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 0/71 | LOSS: 3.7019617593614385e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 1/71 | LOSS: 3.783060037676478e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 2/71 | LOSS: 4.294352341579118e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 3/71 | LOSS: 4.2532496991043445e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 4/71 | LOSS: 4.6294939238578085e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 5/71 | LOSS: 4.551405481834081e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 6/71 | LOSS: 4.7687045220560065e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 7/71 | LOSS: 4.742711439575942e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 8/71 | LOSS: 4.842245844985074e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 9/71 | LOSS: 4.737899826068315e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 10/71 | LOSS: 4.656835657938658e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 11/71 | LOSS: 4.582739317508337e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 12/71 | LOSS: 4.554156126127614e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 13/71 | LOSS: 4.5401541553604015e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 14/71 | LOSS: 4.499715669226134e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 15/71 | LOSS: 4.478024806076064e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 16/71 | LOSS: 4.445470521579195e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 17/71 | LOSS: 4.448468593586262e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 18/71 | LOSS: 4.474839347255367e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 19/71 | LOSS: 4.4961014737054935e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 20/71 | LOSS: 4.542920924861738e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 21/71 | LOSS: 4.553739017865155e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 22/71 | LOSS: 4.5785172915202565e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 23/71 | LOSS: 4.566319603327429e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 24/71 | LOSS: 4.571528879750986e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 25/71 | LOSS: 4.572702052358251e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 26/71 | LOSS: 4.5634639415463125e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 27/71 | LOSS: 4.628990544525939e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 28/71 | LOSS: 4.650532336482088e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 29/71 | LOSS: 4.6622820567184435e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 30/71 | LOSS: 4.6650504314569725e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 31/71 | LOSS: 4.65580265540666e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 32/71 | LOSS: 4.63731048787877e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 33/71 | LOSS: 4.634441866608754e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 34/71 | LOSS: 4.642076718092929e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 35/71 | LOSS: 4.646920223826277e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 36/71 | LOSS: 4.647456240196039e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 37/71 | LOSS: 4.657700533004429e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 38/71 | LOSS: 4.677876789243713e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 39/71 | LOSS: 4.699317935319414e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 40/71 | LOSS: 4.6941371825907e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 41/71 | LOSS: 4.700491867229825e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 42/71 | LOSS: 4.702362931248201e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 43/71 | LOSS: 4.673689390570375e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 44/71 | LOSS: 4.651976097294311e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 45/71 | LOSS: 4.647599889326755e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 46/71 | LOSS: 4.626564798448445e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 47/71 | LOSS: 4.603197941340416e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 48/71 | LOSS: 4.589037170800873e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 49/71 | LOSS: 4.582543911055837e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 50/71 | LOSS: 4.612543392010825e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 51/71 | LOSS: 4.618053039324038e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 52/71 | LOSS: 4.654771159176511e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 53/71 | LOSS: 4.659399771164182e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 54/71 | LOSS: 4.679575617542882e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 55/71 | LOSS: 4.68544874771786e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 56/71 | LOSS: 4.698120731342913e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 57/71 | LOSS: 4.707665895913469e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 58/71 | LOSS: 4.70768050344289e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 59/71 | LOSS: 4.712961727667183e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 60/71 | LOSS: 4.693184635351693e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 61/71 | LOSS: 4.693920503438764e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 62/71 | LOSS: 4.691274311664442e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 63/71 | LOSS: 4.702574980086638e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 64/71 | LOSS: 4.7049377092032675e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 65/71 | LOSS: 4.69460795474416e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 66/71 | LOSS: 4.692579197390882e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 67/71 | LOSS: 4.694104568577185e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 68/71 | LOSS: 4.6869086721320246e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 69/71 | LOSS: 4.686192710323667e-06\n",
      "TRAIN: EPOCH 465/1000 | BATCH 70/71 | LOSS: 4.6789852610383835e-06\n",
      "VAL: EPOCH 465/1000 | BATCH 0/8 | LOSS: 4.741544216813054e-06\n",
      "VAL: EPOCH 465/1000 | BATCH 1/8 | LOSS: 4.6323289097927045e-06\n",
      "VAL: EPOCH 465/1000 | BATCH 2/8 | LOSS: 4.491612799029099e-06\n",
      "VAL: EPOCH 465/1000 | BATCH 3/8 | LOSS: 4.494805125432322e-06\n",
      "VAL: EPOCH 465/1000 | BATCH 4/8 | LOSS: 4.388195156934671e-06\n",
      "VAL: EPOCH 465/1000 | BATCH 5/8 | LOSS: 4.247720122900016e-06\n",
      "VAL: EPOCH 465/1000 | BATCH 6/8 | LOSS: 4.073485472742634e-06\n",
      "VAL: EPOCH 465/1000 | BATCH 7/8 | LOSS: 3.924952636680246e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 0/71 | LOSS: 3.7191632600297453e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 1/71 | LOSS: 3.692475274874596e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 2/71 | LOSS: 3.98092652176274e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 3/71 | LOSS: 3.944469426642172e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 4/71 | LOSS: 4.178220660833176e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 5/71 | LOSS: 4.106585758260432e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 6/71 | LOSS: 4.084318691509127e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 7/71 | LOSS: 4.05558242277948e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 8/71 | LOSS: 4.176127403196814e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 9/71 | LOSS: 4.1959223608500905e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 10/71 | LOSS: 4.232468385949191e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 11/71 | LOSS: 4.223297821681626e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 12/71 | LOSS: 4.2203200966748855e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 13/71 | LOSS: 4.335179856492946e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 14/71 | LOSS: 4.347965311050454e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 15/71 | LOSS: 4.339245052165097e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 16/71 | LOSS: 4.36674908729294e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 17/71 | LOSS: 4.506499749368231e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 18/71 | LOSS: 4.500727144611454e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 19/71 | LOSS: 4.554140548407304e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 20/71 | LOSS: 4.555141361249428e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 21/71 | LOSS: 4.632161056344392e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 22/71 | LOSS: 4.64646582852014e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 23/71 | LOSS: 4.718544677947041e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 24/71 | LOSS: 4.715020886578714e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 25/71 | LOSS: 4.797188073378213e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 26/71 | LOSS: 4.7816731694183545e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 27/71 | LOSS: 4.804746554652541e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 28/71 | LOSS: 4.812092709408748e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 29/71 | LOSS: 4.850511739581028e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 30/71 | LOSS: 4.860742951824514e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 31/71 | LOSS: 4.861362761232613e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 32/71 | LOSS: 4.896356675530886e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 33/71 | LOSS: 4.952521274336665e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 34/71 | LOSS: 4.944675850180959e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 35/71 | LOSS: 4.948404106623154e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 36/71 | LOSS: 4.935755029962181e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 37/71 | LOSS: 4.923276665785555e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 38/71 | LOSS: 4.892309369517114e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 39/71 | LOSS: 4.9172920967066606e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 40/71 | LOSS: 4.915443740883791e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 41/71 | LOSS: 4.888996159024828e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 42/71 | LOSS: 4.910364184472074e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 43/71 | LOSS: 4.902268616743558e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 44/71 | LOSS: 4.89963940203274e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 45/71 | LOSS: 4.899455496382396e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 46/71 | LOSS: 4.882264667027444e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 47/71 | LOSS: 4.839226259415834e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 48/71 | LOSS: 4.841922761944279e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 49/71 | LOSS: 4.812849660993379e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 50/71 | LOSS: 4.804864490734854e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 51/71 | LOSS: 4.807107079439167e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 52/71 | LOSS: 4.786103468263117e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 53/71 | LOSS: 4.791078026755713e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 54/71 | LOSS: 4.7663169260142075e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 55/71 | LOSS: 4.7881777699590856e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 56/71 | LOSS: 4.7841263040027115e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 57/71 | LOSS: 4.7731892225093704e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 58/71 | LOSS: 4.769254161855916e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 59/71 | LOSS: 4.765386612840908e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 60/71 | LOSS: 4.752033471392723e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 61/71 | LOSS: 4.733041394853987e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 62/71 | LOSS: 4.742040779265586e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 63/71 | LOSS: 4.731274735547686e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 64/71 | LOSS: 4.726246662847608e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 65/71 | LOSS: 4.7366273437629305e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 66/71 | LOSS: 4.7488563641716394e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 67/71 | LOSS: 4.740454314474739e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 68/71 | LOSS: 4.736215019426933e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 69/71 | LOSS: 4.7295935668704e-06\n",
      "TRAIN: EPOCH 466/1000 | BATCH 70/71 | LOSS: 4.719275751431525e-06\n",
      "VAL: EPOCH 466/1000 | BATCH 0/8 | LOSS: 5.33977572558797e-06\n",
      "VAL: EPOCH 466/1000 | BATCH 1/8 | LOSS: 5.4371437272493495e-06\n",
      "VAL: EPOCH 466/1000 | BATCH 2/8 | LOSS: 5.398369012254989e-06\n",
      "VAL: EPOCH 466/1000 | BATCH 3/8 | LOSS: 5.378623768592661e-06\n",
      "VAL: EPOCH 466/1000 | BATCH 4/8 | LOSS: 5.3196936278254725e-06\n",
      "VAL: EPOCH 466/1000 | BATCH 5/8 | LOSS: 5.063527699652089e-06\n",
      "VAL: EPOCH 466/1000 | BATCH 6/8 | LOSS: 4.904482271480706e-06\n",
      "VAL: EPOCH 466/1000 | BATCH 7/8 | LOSS: 4.744410006196631e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 0/71 | LOSS: 4.450162123248447e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 1/71 | LOSS: 4.939103746437468e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 2/71 | LOSS: 4.5809189638627385e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 3/71 | LOSS: 4.639201051759301e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 4/71 | LOSS: 4.511831775744213e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 5/71 | LOSS: 4.691608637585887e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 6/71 | LOSS: 4.55964904436509e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 7/71 | LOSS: 4.489814273256343e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 8/71 | LOSS: 4.404080906776168e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 9/71 | LOSS: 4.337466452852823e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 10/71 | LOSS: 4.317121568998449e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 11/71 | LOSS: 4.349912880267463e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 12/71 | LOSS: 4.391184050361447e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 13/71 | LOSS: 4.338356624040378e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 14/71 | LOSS: 4.403472621561377e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 15/71 | LOSS: 4.468865270723654e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 16/71 | LOSS: 4.403604273594444e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 17/71 | LOSS: 4.467966277843516e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 18/71 | LOSS: 4.463949907355862e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 19/71 | LOSS: 4.47843675601689e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 20/71 | LOSS: 4.504807325897023e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 21/71 | LOSS: 4.520175655836118e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 22/71 | LOSS: 4.498574812912367e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 23/71 | LOSS: 4.500546320211167e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 24/71 | LOSS: 4.533629735306022e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 25/71 | LOSS: 4.554013221772598e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 26/71 | LOSS: 4.600192663996562e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 27/71 | LOSS: 4.60598866831268e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 28/71 | LOSS: 4.616045434259236e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 29/71 | LOSS: 4.634748340019238e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 30/71 | LOSS: 4.638179579189689e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 31/71 | LOSS: 4.655662230845792e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 32/71 | LOSS: 4.644655932266191e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 33/71 | LOSS: 4.729931701523824e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 34/71 | LOSS: 4.7516292332667425e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 35/71 | LOSS: 4.756191925834072e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 36/71 | LOSS: 4.736989080196804e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 37/71 | LOSS: 4.726799183548505e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 38/71 | LOSS: 4.742129913910713e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 39/71 | LOSS: 4.772522726170792e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 40/71 | LOSS: 4.812883722346359e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 41/71 | LOSS: 4.782392455213301e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 42/71 | LOSS: 4.799660669156109e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 43/71 | LOSS: 4.77969506495025e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 44/71 | LOSS: 4.7710283322683405e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 45/71 | LOSS: 4.767956501500916e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 46/71 | LOSS: 4.768438328299337e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 47/71 | LOSS: 4.756530894193626e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 48/71 | LOSS: 4.750338877449988e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 49/71 | LOSS: 4.745933324556972e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 50/71 | LOSS: 4.7676672363427225e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 51/71 | LOSS: 4.774026780733738e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 52/71 | LOSS: 4.772508474453166e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 53/71 | LOSS: 4.747821254501038e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 54/71 | LOSS: 4.800945721812737e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 55/71 | LOSS: 4.796701684231916e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 56/71 | LOSS: 4.844667425156028e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 57/71 | LOSS: 4.862915124374859e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 58/71 | LOSS: 4.90744141489366e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 59/71 | LOSS: 4.896711334367865e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 60/71 | LOSS: 4.917854848976216e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 61/71 | LOSS: 4.909034575370418e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 62/71 | LOSS: 4.90137257916753e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 63/71 | LOSS: 4.901410605384626e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 64/71 | LOSS: 4.882687356327481e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 65/71 | LOSS: 4.886961767514356e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 66/71 | LOSS: 4.8767993820235205e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 67/71 | LOSS: 4.871179726251285e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 68/71 | LOSS: 4.873425351051728e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 69/71 | LOSS: 4.8880247049964964e-06\n",
      "TRAIN: EPOCH 467/1000 | BATCH 70/71 | LOSS: 4.870292872843672e-06\n",
      "VAL: EPOCH 467/1000 | BATCH 0/8 | LOSS: 4.419023753143847e-06\n",
      "VAL: EPOCH 467/1000 | BATCH 1/8 | LOSS: 4.595306563714985e-06\n",
      "VAL: EPOCH 467/1000 | BATCH 2/8 | LOSS: 4.612887854212507e-06\n",
      "VAL: EPOCH 467/1000 | BATCH 3/8 | LOSS: 4.670165708375862e-06\n",
      "VAL: EPOCH 467/1000 | BATCH 4/8 | LOSS: 4.608679773809854e-06\n",
      "VAL: EPOCH 467/1000 | BATCH 5/8 | LOSS: 4.475949594962003e-06\n",
      "VAL: EPOCH 467/1000 | BATCH 6/8 | LOSS: 4.309127624375313e-06\n",
      "VAL: EPOCH 467/1000 | BATCH 7/8 | LOSS: 4.17039677813591e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 0/71 | LOSS: 4.593273843056522e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 1/71 | LOSS: 4.220234359308961e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 2/71 | LOSS: 4.2182256644688705e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 3/71 | LOSS: 4.362546064839989e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 4/71 | LOSS: 4.150975655647926e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 5/71 | LOSS: 4.355364202031827e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 6/71 | LOSS: 4.363206893945712e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 7/71 | LOSS: 4.324690166868095e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 8/71 | LOSS: 4.37313514743841e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 9/71 | LOSS: 4.3790568724944025e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 10/71 | LOSS: 4.386771954820936e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 11/71 | LOSS: 4.358677908082124e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 12/71 | LOSS: 4.357348433796477e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 13/71 | LOSS: 4.311815457315658e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 14/71 | LOSS: 4.347021983145775e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 15/71 | LOSS: 4.30079873581235e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 16/71 | LOSS: 4.2930896330183124e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 17/71 | LOSS: 4.264610184792319e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 18/71 | LOSS: 4.309933681150524e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 19/71 | LOSS: 4.303151433759922e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 20/71 | LOSS: 4.2687821362397655e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 21/71 | LOSS: 4.322275572121725e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 22/71 | LOSS: 4.321881434684062e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 23/71 | LOSS: 4.341522393739676e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 24/71 | LOSS: 4.342081392678665e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 25/71 | LOSS: 4.3312626390755986e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 26/71 | LOSS: 4.3335633698141595e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 27/71 | LOSS: 4.352928613116092e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 28/71 | LOSS: 4.331741428327875e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 29/71 | LOSS: 4.328596529982557e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 30/71 | LOSS: 4.326084088520034e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 31/71 | LOSS: 4.33097081042888e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 32/71 | LOSS: 4.342070304009019e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 33/71 | LOSS: 4.400080583496567e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 34/71 | LOSS: 4.3956949022166164e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 35/71 | LOSS: 4.421925028610632e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 36/71 | LOSS: 4.430806527395632e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 37/71 | LOSS: 4.4164412190040915e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 38/71 | LOSS: 4.420286131789908e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 39/71 | LOSS: 4.405525669426425e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 40/71 | LOSS: 4.394312948467658e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 41/71 | LOSS: 4.402920099199123e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 42/71 | LOSS: 4.405046289574847e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 43/71 | LOSS: 4.409783297309001e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 44/71 | LOSS: 4.432459364680754e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 45/71 | LOSS: 4.448908177200002e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 46/71 | LOSS: 4.445588719465106e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 47/71 | LOSS: 4.476922498497515e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 48/71 | LOSS: 4.474383962476669e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 49/71 | LOSS: 4.4805390825786165e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 50/71 | LOSS: 4.495598570393472e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 51/71 | LOSS: 4.489940937883847e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 52/71 | LOSS: 4.525476581549364e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 53/71 | LOSS: 4.6419989858905965e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 54/71 | LOSS: 4.6380668787936555e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 55/71 | LOSS: 4.705838096340033e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 56/71 | LOSS: 4.72429047205655e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 57/71 | LOSS: 4.759730449348291e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 58/71 | LOSS: 4.773095196916552e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 59/71 | LOSS: 4.806215838470962e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 60/71 | LOSS: 4.830334054546614e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 61/71 | LOSS: 4.860188167391067e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 62/71 | LOSS: 4.862360814484649e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 63/71 | LOSS: 4.860879847967681e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 64/71 | LOSS: 4.860843854168287e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 65/71 | LOSS: 4.860440991582106e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 66/71 | LOSS: 4.864115745381717e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 67/71 | LOSS: 4.846857156253998e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 68/71 | LOSS: 4.839115072040906e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 69/71 | LOSS: 4.843168725139029e-06\n",
      "TRAIN: EPOCH 468/1000 | BATCH 70/71 | LOSS: 4.841094253183847e-06\n",
      "VAL: EPOCH 468/1000 | BATCH 0/8 | LOSS: 4.321697815612424e-06\n",
      "VAL: EPOCH 468/1000 | BATCH 1/8 | LOSS: 4.184456429356942e-06\n",
      "VAL: EPOCH 468/1000 | BATCH 2/8 | LOSS: 4.323230920514713e-06\n",
      "VAL: EPOCH 468/1000 | BATCH 3/8 | LOSS: 4.346875130067929e-06\n",
      "VAL: EPOCH 468/1000 | BATCH 4/8 | LOSS: 4.273243393981829e-06\n",
      "VAL: EPOCH 468/1000 | BATCH 5/8 | LOSS: 4.251070474007672e-06\n",
      "VAL: EPOCH 468/1000 | BATCH 6/8 | LOSS: 4.136522388112748e-06\n",
      "VAL: EPOCH 468/1000 | BATCH 7/8 | LOSS: 4.065041679268688e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 0/71 | LOSS: 4.76334935228806e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 1/71 | LOSS: 5.125504685565829e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 2/71 | LOSS: 4.999211038618038e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 3/71 | LOSS: 4.695068582805106e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 4/71 | LOSS: 5.052364213042893e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 5/71 | LOSS: 4.8076274197228486e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 6/71 | LOSS: 4.973071489595375e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 7/71 | LOSS: 5.033924082908925e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 8/71 | LOSS: 4.937129081857468e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 9/71 | LOSS: 5.188209934203769e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 10/71 | LOSS: 5.095791369636903e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 11/71 | LOSS: 5.287961774532353e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 12/71 | LOSS: 5.18874877833206e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 13/71 | LOSS: 5.2458173221176755e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 14/71 | LOSS: 5.261320560142243e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 15/71 | LOSS: 5.3074511470185826e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 16/71 | LOSS: 5.295605205901666e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 17/71 | LOSS: 5.238991409189313e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 18/71 | LOSS: 5.239529424503225e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 19/71 | LOSS: 5.179380400477384e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 20/71 | LOSS: 5.1590062477251716e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 21/71 | LOSS: 5.112536014580242e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 22/71 | LOSS: 5.067384889749203e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 23/71 | LOSS: 5.0015348828461965e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 24/71 | LOSS: 5.059149816588615e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 25/71 | LOSS: 5.062726052948654e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 26/71 | LOSS: 5.060818331778002e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 27/71 | LOSS: 5.022349106249229e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 28/71 | LOSS: 5.032259094087781e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 29/71 | LOSS: 4.991525497644034e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 30/71 | LOSS: 5.002058874343939e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 31/71 | LOSS: 4.9938900517076945e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 32/71 | LOSS: 4.97711954322409e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 33/71 | LOSS: 4.9309804273220565e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 34/71 | LOSS: 4.896493984623314e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 35/71 | LOSS: 4.877129387952866e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 36/71 | LOSS: 4.8867752313428256e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 37/71 | LOSS: 4.8864140616818696e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 38/71 | LOSS: 4.8906535877717825e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 39/71 | LOSS: 4.8991987796398465e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 40/71 | LOSS: 4.974904230748951e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 41/71 | LOSS: 4.9496199088545315e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 42/71 | LOSS: 5.075847818874917e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 43/71 | LOSS: 5.063366725979987e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 44/71 | LOSS: 5.0849604930489375e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 45/71 | LOSS: 5.053843912482199e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 46/71 | LOSS: 5.117589154829051e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 47/71 | LOSS: 5.113193116320265e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 48/71 | LOSS: 5.198037365980967e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 49/71 | LOSS: 5.189133366911846e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 50/71 | LOSS: 5.1668920935067125e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 51/71 | LOSS: 5.163870036120408e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 52/71 | LOSS: 5.140239559381679e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 53/71 | LOSS: 5.1167695742319835e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 54/71 | LOSS: 5.123386517880135e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 55/71 | LOSS: 5.106600590514583e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 56/71 | LOSS: 5.093660120748386e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 57/71 | LOSS: 5.078861731974406e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 58/71 | LOSS: 5.072128873260966e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 59/71 | LOSS: 5.067007013318895e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 60/71 | LOSS: 5.062646938522001e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 61/71 | LOSS: 5.060160120709982e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 62/71 | LOSS: 5.0496707610854794e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 63/71 | LOSS: 5.027671807766865e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 64/71 | LOSS: 5.0233184258384145e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 65/71 | LOSS: 5.013857475839331e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 66/71 | LOSS: 5.027965427409561e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 67/71 | LOSS: 5.027469104762156e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 68/71 | LOSS: 5.04367480114498e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 69/71 | LOSS: 5.02481002838197e-06\n",
      "TRAIN: EPOCH 469/1000 | BATCH 70/71 | LOSS: 5.0101032613870496e-06\n",
      "VAL: EPOCH 469/1000 | BATCH 0/8 | LOSS: 6.420297722797841e-06\n",
      "VAL: EPOCH 469/1000 | BATCH 1/8 | LOSS: 7.2102598096535075e-06\n",
      "VAL: EPOCH 469/1000 | BATCH 2/8 | LOSS: 7.380013812507968e-06\n",
      "VAL: EPOCH 469/1000 | BATCH 3/8 | LOSS: 7.433247787957953e-06\n",
      "VAL: EPOCH 469/1000 | BATCH 4/8 | LOSS: 7.448533051501727e-06\n",
      "VAL: EPOCH 469/1000 | BATCH 5/8 | LOSS: 7.391023927993956e-06\n",
      "VAL: EPOCH 469/1000 | BATCH 6/8 | LOSS: 7.225385421146971e-06\n",
      "VAL: EPOCH 469/1000 | BATCH 7/8 | LOSS: 7.2085462647919485e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 0/71 | LOSS: 6.942034815438092e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 1/71 | LOSS: 4.97441294555756e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 2/71 | LOSS: 5.813220847509608e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 3/71 | LOSS: 5.306743616984022e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 4/71 | LOSS: 5.603860199698829e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 5/71 | LOSS: 5.515700612098347e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 6/71 | LOSS: 5.341992976225031e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 7/71 | LOSS: 5.723229577370148e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 8/71 | LOSS: 5.619391155657165e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 9/71 | LOSS: 5.515990937965398e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 10/71 | LOSS: 5.499735607372713e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 11/71 | LOSS: 5.576301399893661e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 12/71 | LOSS: 5.561041392026639e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 13/71 | LOSS: 5.497402087582616e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 14/71 | LOSS: 5.485007462387633e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 15/71 | LOSS: 5.44189573759013e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 16/71 | LOSS: 5.429158872014823e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 17/71 | LOSS: 5.363384199578529e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 18/71 | LOSS: 5.41868514933173e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 19/71 | LOSS: 5.323909294929763e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 20/71 | LOSS: 5.37849196223847e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 21/71 | LOSS: 5.357291870082421e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 22/71 | LOSS: 5.457931424841619e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 23/71 | LOSS: 5.446576608392206e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 24/71 | LOSS: 5.49517617400852e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 25/71 | LOSS: 5.435846595411511e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 26/71 | LOSS: 5.421425225062261e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 27/71 | LOSS: 5.46779399103668e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 28/71 | LOSS: 5.488111567282086e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 29/71 | LOSS: 5.485378763599632e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 30/71 | LOSS: 5.4123600699723844e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 31/71 | LOSS: 5.422141832411853e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 32/71 | LOSS: 5.432239387311821e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 33/71 | LOSS: 5.377012739165152e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 34/71 | LOSS: 5.3300092726463585e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 35/71 | LOSS: 5.316139309899073e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 36/71 | LOSS: 5.3058874122192765e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 37/71 | LOSS: 5.270263751586562e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 38/71 | LOSS: 5.3201918793395935e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 39/71 | LOSS: 5.306293439844012e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 40/71 | LOSS: 5.305808383304904e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 41/71 | LOSS: 5.363805864059638e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 42/71 | LOSS: 5.3237710987961565e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 43/71 | LOSS: 5.3772657534781585e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 44/71 | LOSS: 5.37566709984579e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 45/71 | LOSS: 5.405355795040163e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 46/71 | LOSS: 5.415392658331721e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 47/71 | LOSS: 5.407617498084012e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 48/71 | LOSS: 5.429000919098237e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 49/71 | LOSS: 5.3932271248413595e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 50/71 | LOSS: 5.4123701704834365e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 51/71 | LOSS: 5.391816946834856e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 52/71 | LOSS: 5.386730230006748e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 53/71 | LOSS: 5.37640708578827e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 54/71 | LOSS: 5.393512909987211e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 55/71 | LOSS: 5.369563914265589e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 56/71 | LOSS: 5.347427930788845e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 57/71 | LOSS: 5.321804223967774e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 58/71 | LOSS: 5.301219713239056e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 59/71 | LOSS: 5.29499271806344e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 60/71 | LOSS: 5.278189191216608e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 61/71 | LOSS: 5.269201771566699e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 62/71 | LOSS: 5.2610661665341235e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 63/71 | LOSS: 5.23590593459744e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 64/71 | LOSS: 5.2158376407672995e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 65/71 | LOSS: 5.216972832992421e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 66/71 | LOSS: 5.213663204962521e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 67/71 | LOSS: 5.213786303503345e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 68/71 | LOSS: 5.2174268833935624e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 69/71 | LOSS: 5.2037545856364885e-06\n",
      "TRAIN: EPOCH 470/1000 | BATCH 70/71 | LOSS: 5.194299316352838e-06\n",
      "VAL: EPOCH 470/1000 | BATCH 0/8 | LOSS: 6.216629117261618e-06\n",
      "VAL: EPOCH 470/1000 | BATCH 1/8 | LOSS: 6.45017121314595e-06\n",
      "VAL: EPOCH 470/1000 | BATCH 2/8 | LOSS: 6.586319159396226e-06\n",
      "VAL: EPOCH 470/1000 | BATCH 3/8 | LOSS: 6.465871479122143e-06\n",
      "VAL: EPOCH 470/1000 | BATCH 4/8 | LOSS: 6.413579012587434e-06\n",
      "VAL: EPOCH 470/1000 | BATCH 5/8 | LOSS: 6.3680115545139415e-06\n",
      "VAL: EPOCH 470/1000 | BATCH 6/8 | LOSS: 6.254325139057723e-06\n",
      "VAL: EPOCH 470/1000 | BATCH 7/8 | LOSS: 6.3118563957687e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 0/71 | LOSS: 6.769333595002536e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 1/71 | LOSS: 5.601408247457584e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 2/71 | LOSS: 4.997048790755798e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 3/71 | LOSS: 5.4014969350646425e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 4/71 | LOSS: 5.401331100074458e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 5/71 | LOSS: 5.20192115042543e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 6/71 | LOSS: 5.304539724550393e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 7/71 | LOSS: 5.2772523702060425e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 8/71 | LOSS: 5.24126903655997e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 9/71 | LOSS: 5.212105884311314e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 10/71 | LOSS: 5.059994795043497e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 11/71 | LOSS: 5.022693414957757e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 12/71 | LOSS: 4.94892683141538e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 13/71 | LOSS: 4.91384092323902e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 14/71 | LOSS: 4.808123685506871e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 15/71 | LOSS: 4.917359319733805e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 16/71 | LOSS: 4.944586073500378e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 17/71 | LOSS: 5.008752395951888e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 18/71 | LOSS: 5.080888322003088e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 19/71 | LOSS: 5.021747597311332e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 20/71 | LOSS: 5.025926384405466e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 21/71 | LOSS: 4.988438480392109e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 22/71 | LOSS: 5.005284690370267e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 23/71 | LOSS: 5.006546662874219e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 24/71 | LOSS: 4.9665804544929415e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 25/71 | LOSS: 4.998108859301563e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 26/71 | LOSS: 4.9681299161521665e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 27/71 | LOSS: 4.974952340879619e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 28/71 | LOSS: 4.957419103698942e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 29/71 | LOSS: 4.940595514805561e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 30/71 | LOSS: 4.890435262026468e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 31/71 | LOSS: 4.886569840323318e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 32/71 | LOSS: 4.847133382237658e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 33/71 | LOSS: 4.798177341773921e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 34/71 | LOSS: 4.775967304989795e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 35/71 | LOSS: 4.794281406045937e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 36/71 | LOSS: 4.769499244847228e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 37/71 | LOSS: 4.760278359658275e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 38/71 | LOSS: 4.736508659474426e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 39/71 | LOSS: 4.797168583081657e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 40/71 | LOSS: 4.771157636042051e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 41/71 | LOSS: 4.785160296619108e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 42/71 | LOSS: 4.807088882528681e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 43/71 | LOSS: 4.797121314161566e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 44/71 | LOSS: 4.784995108922077e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 45/71 | LOSS: 4.768003528308485e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 46/71 | LOSS: 4.758326624257879e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 47/71 | LOSS: 4.735537326420551e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 48/71 | LOSS: 4.730631449742109e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 49/71 | LOSS: 4.733446166937938e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 50/71 | LOSS: 4.772069614238503e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 51/71 | LOSS: 4.77056575548956e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 52/71 | LOSS: 4.762170346849708e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 53/71 | LOSS: 4.773239601895006e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 54/71 | LOSS: 4.780494617245329e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 55/71 | LOSS: 4.761366605115265e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 56/71 | LOSS: 4.754753654037415e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 57/71 | LOSS: 4.7590554161082575e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 58/71 | LOSS: 4.746552518955176e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 59/71 | LOSS: 4.730724064453776e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 60/71 | LOSS: 4.729704727890108e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 61/71 | LOSS: 4.746963294300874e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 62/71 | LOSS: 4.738808990897803e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 63/71 | LOSS: 4.7481137102067805e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 64/71 | LOSS: 4.732978445295325e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 65/71 | LOSS: 4.746980518134616e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 66/71 | LOSS: 4.741482469588805e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 67/71 | LOSS: 4.777563808201033e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 68/71 | LOSS: 4.779243170929291e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 69/71 | LOSS: 4.809065639424911e-06\n",
      "TRAIN: EPOCH 471/1000 | BATCH 70/71 | LOSS: 4.830605041180962e-06\n",
      "VAL: EPOCH 471/1000 | BATCH 0/8 | LOSS: 6.7001865318161435e-06\n",
      "VAL: EPOCH 471/1000 | BATCH 1/8 | LOSS: 6.984234460105654e-06\n",
      "VAL: EPOCH 471/1000 | BATCH 2/8 | LOSS: 7.302716464134089e-06\n",
      "VAL: EPOCH 471/1000 | BATCH 3/8 | LOSS: 7.863508699301747e-06\n",
      "VAL: EPOCH 471/1000 | BATCH 4/8 | LOSS: 7.640968033229e-06\n",
      "VAL: EPOCH 471/1000 | BATCH 5/8 | LOSS: 7.947481814577865e-06\n",
      "VAL: EPOCH 471/1000 | BATCH 6/8 | LOSS: 7.860337258794711e-06\n",
      "VAL: EPOCH 471/1000 | BATCH 7/8 | LOSS: 7.7511367067018e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 0/71 | LOSS: 8.171255103661679e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 1/71 | LOSS: 7.5536718213697895e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 2/71 | LOSS: 6.692160847402799e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 3/71 | LOSS: 7.000779987720307e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 4/71 | LOSS: 6.7434553784551096e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 5/71 | LOSS: 6.8386703636254724e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 6/71 | LOSS: 6.534685066331544e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 7/71 | LOSS: 6.493612261238013e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 8/71 | LOSS: 6.299473625404062e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 9/71 | LOSS: 6.115524547567474e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 10/71 | LOSS: 6.054563114461913e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 11/71 | LOSS: 5.985487784225067e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 12/71 | LOSS: 5.938886540836673e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 13/71 | LOSS: 5.910566187594668e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 14/71 | LOSS: 5.8002193024246175e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 15/71 | LOSS: 5.696667727761451e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 16/71 | LOSS: 5.55386899952068e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 17/71 | LOSS: 5.533428482825305e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 18/71 | LOSS: 5.51157315959771e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 19/71 | LOSS: 5.458216480747069e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 20/71 | LOSS: 5.4223598042534876e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 21/71 | LOSS: 5.355417329155898e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 22/71 | LOSS: 5.359831398575055e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 23/71 | LOSS: 5.327106038066631e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 24/71 | LOSS: 5.249917976470897e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 25/71 | LOSS: 5.216876248736158e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 26/71 | LOSS: 5.183551753294456e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 27/71 | LOSS: 5.1882575137694535e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 28/71 | LOSS: 5.200771299534044e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 29/71 | LOSS: 5.196322247987458e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 30/71 | LOSS: 5.179764068616937e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 31/71 | LOSS: 5.201017415856768e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 32/71 | LOSS: 5.170729750578616e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 33/71 | LOSS: 5.176337227192776e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 34/71 | LOSS: 5.125148150649953e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 35/71 | LOSS: 5.132196785477087e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 36/71 | LOSS: 5.146340385722028e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 37/71 | LOSS: 5.119872496020674e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 38/71 | LOSS: 5.124497886591155e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 39/71 | LOSS: 5.111610312269476e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 40/71 | LOSS: 5.084543218112142e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 41/71 | LOSS: 5.075723059971573e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 42/71 | LOSS: 5.054556822869927e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 43/71 | LOSS: 5.029831279237194e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 44/71 | LOSS: 4.992365181048323e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 45/71 | LOSS: 4.987459572910903e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 46/71 | LOSS: 4.997244965860328e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 47/71 | LOSS: 5.001431522562901e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 48/71 | LOSS: 5.005862058911886e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 49/71 | LOSS: 4.987407082808204e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 50/71 | LOSS: 4.960148036094752e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 51/71 | LOSS: 4.936929511901466e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 52/71 | LOSS: 4.913398221483561e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 53/71 | LOSS: 4.918481379347752e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 54/71 | LOSS: 4.906386042917306e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 55/71 | LOSS: 4.8901549973834335e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 56/71 | LOSS: 4.900374729562859e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 57/71 | LOSS: 4.8950478897275955e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 58/71 | LOSS: 4.905872995134244e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 59/71 | LOSS: 4.923936387513095e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 60/71 | LOSS: 4.933652701124704e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 61/71 | LOSS: 4.942297235380289e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 62/71 | LOSS: 4.958252222624853e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 63/71 | LOSS: 4.958294827162035e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 64/71 | LOSS: 4.962492427624126e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 65/71 | LOSS: 4.972857482243853e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 66/71 | LOSS: 4.979259339092802e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 67/71 | LOSS: 4.958310057645576e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 68/71 | LOSS: 4.955393577525367e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 69/71 | LOSS: 4.9625515495271036e-06\n",
      "TRAIN: EPOCH 472/1000 | BATCH 70/71 | LOSS: 4.957283440681993e-06\n",
      "VAL: EPOCH 472/1000 | BATCH 0/8 | LOSS: 4.652844836527947e-06\n",
      "VAL: EPOCH 472/1000 | BATCH 1/8 | LOSS: 4.58232034361572e-06\n",
      "VAL: EPOCH 472/1000 | BATCH 2/8 | LOSS: 4.602364242600743e-06\n",
      "VAL: EPOCH 472/1000 | BATCH 3/8 | LOSS: 4.62529999367689e-06\n",
      "VAL: EPOCH 472/1000 | BATCH 4/8 | LOSS: 4.533665014605504e-06\n",
      "VAL: EPOCH 472/1000 | BATCH 5/8 | LOSS: 4.450613232620526e-06\n",
      "VAL: EPOCH 472/1000 | BATCH 6/8 | LOSS: 4.257105989121815e-06\n",
      "VAL: EPOCH 472/1000 | BATCH 7/8 | LOSS: 4.169701981027174e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 0/71 | LOSS: 4.149882443016395e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 1/71 | LOSS: 3.6962533158657607e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 2/71 | LOSS: 4.1840082000514185e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 3/71 | LOSS: 4.387766921354341e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 4/71 | LOSS: 4.931164221488871e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 5/71 | LOSS: 4.764111205683245e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 6/71 | LOSS: 4.719199750979897e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 7/71 | LOSS: 4.721800678453292e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 8/71 | LOSS: 4.740340045827907e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 9/71 | LOSS: 4.9191359721589835e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 10/71 | LOSS: 4.976508038949263e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 11/71 | LOSS: 5.3180650259794975e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 12/71 | LOSS: 5.261362240651999e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 13/71 | LOSS: 5.274512692007452e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 14/71 | LOSS: 5.214205126928088e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 15/71 | LOSS: 5.191736050846885e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 16/71 | LOSS: 5.153934981696212e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 17/71 | LOSS: 5.102503008755674e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 18/71 | LOSS: 5.080418687659978e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 19/71 | LOSS: 5.070932775197434e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 20/71 | LOSS: 5.066057643229474e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 21/71 | LOSS: 5.034511063554832e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 22/71 | LOSS: 5.004816300598854e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 23/71 | LOSS: 4.966098401837371e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 24/71 | LOSS: 4.961276099493261e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 25/71 | LOSS: 4.9423327674538505e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 26/71 | LOSS: 4.94164618834879e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 27/71 | LOSS: 4.913317801375732e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 28/71 | LOSS: 4.936672212352278e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 29/71 | LOSS: 4.902482002459389e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 30/71 | LOSS: 4.916189643535613e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 31/71 | LOSS: 4.894910858865842e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 32/71 | LOSS: 4.895501440456208e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 33/71 | LOSS: 4.894953208884758e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 34/71 | LOSS: 4.861592099457214e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 35/71 | LOSS: 4.834503607121911e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 36/71 | LOSS: 4.832567564999162e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 37/71 | LOSS: 4.798628998813863e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 38/71 | LOSS: 4.757528172260022e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 39/71 | LOSS: 4.754236243798005e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 40/71 | LOSS: 4.74751720680726e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 41/71 | LOSS: 4.7467670821701865e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 42/71 | LOSS: 4.728109261545902e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 43/71 | LOSS: 4.708155772319018e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 44/71 | LOSS: 4.70728714390134e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 45/71 | LOSS: 4.691730831164813e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 46/71 | LOSS: 4.679089636525493e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 47/71 | LOSS: 4.6631566921936e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 48/71 | LOSS: 4.6485983083195085e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 49/71 | LOSS: 4.655511106648191e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 50/71 | LOSS: 4.659085401298773e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 51/71 | LOSS: 4.652799602284246e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 52/71 | LOSS: 4.650217525128062e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 53/71 | LOSS: 4.6448801008914924e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 54/71 | LOSS: 4.638763291289004e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 55/71 | LOSS: 4.641798824422949e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 56/71 | LOSS: 4.64045204177653e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 57/71 | LOSS: 4.630550890773332e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 58/71 | LOSS: 4.634720668899856e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 59/71 | LOSS: 4.615210745366009e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 60/71 | LOSS: 4.609717414078644e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 61/71 | LOSS: 4.598595065515603e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 62/71 | LOSS: 4.584733496524259e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 63/71 | LOSS: 4.588680521777633e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 64/71 | LOSS: 4.586612364549252e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 65/71 | LOSS: 4.5902559639381995e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 66/71 | LOSS: 4.583288463395751e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 67/71 | LOSS: 4.610491490599516e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 68/71 | LOSS: 4.609954941401341e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 69/71 | LOSS: 4.628046076504688e-06\n",
      "TRAIN: EPOCH 473/1000 | BATCH 70/71 | LOSS: 4.614603994923527e-06\n",
      "VAL: EPOCH 473/1000 | BATCH 0/8 | LOSS: 5.723840331484098e-06\n",
      "VAL: EPOCH 473/1000 | BATCH 1/8 | LOSS: 5.662147486873437e-06\n",
      "VAL: EPOCH 473/1000 | BATCH 2/8 | LOSS: 5.59523990280771e-06\n",
      "VAL: EPOCH 473/1000 | BATCH 3/8 | LOSS: 5.57302496417833e-06\n",
      "VAL: EPOCH 473/1000 | BATCH 4/8 | LOSS: 5.468037852551788e-06\n",
      "VAL: EPOCH 473/1000 | BATCH 5/8 | LOSS: 5.2470683537346e-06\n",
      "VAL: EPOCH 473/1000 | BATCH 6/8 | LOSS: 5.094044809084153e-06\n",
      "VAL: EPOCH 473/1000 | BATCH 7/8 | LOSS: 4.891908304216486e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 0/71 | LOSS: 5.053007043898106e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 1/71 | LOSS: 4.430995431903284e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 2/71 | LOSS: 4.505478955252329e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 3/71 | LOSS: 4.399670501697983e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 4/71 | LOSS: 4.2705225496320056e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 5/71 | LOSS: 4.349829320441738e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 6/71 | LOSS: 4.303032190884031e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 7/71 | LOSS: 4.1611792767071165e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 8/71 | LOSS: 4.304786772182625e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 9/71 | LOSS: 4.300650289223995e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 10/71 | LOSS: 4.372254476369232e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 11/71 | LOSS: 4.32508975715488e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 12/71 | LOSS: 4.3614951524572325e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 13/71 | LOSS: 4.341198307754114e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 14/71 | LOSS: 4.404449600770022e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 15/71 | LOSS: 4.506285861793913e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 16/71 | LOSS: 4.488478024915607e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 17/71 | LOSS: 4.550967623294531e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 18/71 | LOSS: 4.5963255448250015e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 19/71 | LOSS: 4.5988975557520465e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 20/71 | LOSS: 4.6477551634216935e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 21/71 | LOSS: 4.618935246402213e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 22/71 | LOSS: 4.604391417318687e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 23/71 | LOSS: 4.571483287918454e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 24/71 | LOSS: 4.588592864820384e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 25/71 | LOSS: 4.531640342975152e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 26/71 | LOSS: 4.503917757069252e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 27/71 | LOSS: 4.539211610270806e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 28/71 | LOSS: 4.537051060244016e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 29/71 | LOSS: 4.4994930703978754e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 30/71 | LOSS: 4.476430633310942e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 31/71 | LOSS: 4.456085505921692e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 32/71 | LOSS: 4.446695170785957e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 33/71 | LOSS: 4.469082488653572e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 34/71 | LOSS: 4.4861355880649565e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 35/71 | LOSS: 4.51095643762124e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 36/71 | LOSS: 4.520723155724841e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 37/71 | LOSS: 4.502830291274126e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 38/71 | LOSS: 4.554486552548881e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 39/71 | LOSS: 4.558031446322275e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 40/71 | LOSS: 4.5785493678455925e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 41/71 | LOSS: 4.570249584070379e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 42/71 | LOSS: 4.564497832562179e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 43/71 | LOSS: 4.569907353222582e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 44/71 | LOSS: 4.587422578576176e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 45/71 | LOSS: 4.609284745959525e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 46/71 | LOSS: 4.605617872817056e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 47/71 | LOSS: 4.6115986265249376e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 48/71 | LOSS: 4.622012947576729e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 49/71 | LOSS: 4.626317586371443e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 50/71 | LOSS: 4.651440799623778e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 51/71 | LOSS: 4.649726660178343e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 52/71 | LOSS: 4.645388584459346e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 53/71 | LOSS: 4.640349128816591e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 54/71 | LOSS: 4.653109590435633e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 55/71 | LOSS: 4.669044601866647e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 56/71 | LOSS: 4.654563845338487e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 57/71 | LOSS: 4.660236699833306e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 58/71 | LOSS: 4.666864859215004e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 59/71 | LOSS: 4.646515329416919e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 60/71 | LOSS: 4.6342985920315094e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 61/71 | LOSS: 4.636751327780075e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 62/71 | LOSS: 4.6464464474642195e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 63/71 | LOSS: 4.64917906839446e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 64/71 | LOSS: 4.654223091795127e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 65/71 | LOSS: 4.661077437876057e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 66/71 | LOSS: 4.682593586962799e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 67/71 | LOSS: 4.6756330622134985e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 68/71 | LOSS: 4.675224120190883e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 69/71 | LOSS: 4.6791389260241495e-06\n",
      "TRAIN: EPOCH 474/1000 | BATCH 70/71 | LOSS: 4.661773150666038e-06\n",
      "VAL: EPOCH 474/1000 | BATCH 0/8 | LOSS: 6.937580110388808e-06\n",
      "VAL: EPOCH 474/1000 | BATCH 1/8 | LOSS: 6.5380036176065914e-06\n",
      "VAL: EPOCH 474/1000 | BATCH 2/8 | LOSS: 6.47591059532715e-06\n",
      "VAL: EPOCH 474/1000 | BATCH 3/8 | LOSS: 6.347145813379029e-06\n",
      "VAL: EPOCH 474/1000 | BATCH 4/8 | LOSS: 6.320496231637662e-06\n",
      "VAL: EPOCH 474/1000 | BATCH 5/8 | LOSS: 6.041564195887379e-06\n",
      "VAL: EPOCH 474/1000 | BATCH 6/8 | LOSS: 5.894870485332961e-06\n",
      "VAL: EPOCH 474/1000 | BATCH 7/8 | LOSS: 5.761951172189583e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 0/71 | LOSS: 6.270076482906006e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 1/71 | LOSS: 5.350059836928267e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 2/71 | LOSS: 5.701879217667738e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 3/71 | LOSS: 5.399816586759698e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 4/71 | LOSS: 5.131620946485782e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 5/71 | LOSS: 4.932405848497486e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 6/71 | LOSS: 4.991484079904954e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 7/71 | LOSS: 4.999592931653751e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 8/71 | LOSS: 4.9292328488566755e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 9/71 | LOSS: 4.8885524392972e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 10/71 | LOSS: 4.872269404255679e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 11/71 | LOSS: 4.912484958670878e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 12/71 | LOSS: 4.862571061564197e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 13/71 | LOSS: 4.843012123144165e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 14/71 | LOSS: 4.737539196260817e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 15/71 | LOSS: 4.686297160105823e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 16/71 | LOSS: 4.704889039096409e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 17/71 | LOSS: 4.616297979333063e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 18/71 | LOSS: 4.66635267577275e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 19/71 | LOSS: 4.653023688661051e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 20/71 | LOSS: 4.77493395443335e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 21/71 | LOSS: 4.816141799065835e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 22/71 | LOSS: 4.900304564058467e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 23/71 | LOSS: 4.887818704446545e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 24/71 | LOSS: 4.859713353653205e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 25/71 | LOSS: 4.909662266072701e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 26/71 | LOSS: 4.897965835779259e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 27/71 | LOSS: 4.881695742499557e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 28/71 | LOSS: 4.8930148515300344e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 29/71 | LOSS: 4.9172786930284925e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 30/71 | LOSS: 4.925856486689524e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 31/71 | LOSS: 4.941053433071829e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 32/71 | LOSS: 4.958349274437007e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 33/71 | LOSS: 4.970349931788994e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 34/71 | LOSS: 4.961769874561079e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 35/71 | LOSS: 4.96965323135454e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 36/71 | LOSS: 4.926355592721626e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 37/71 | LOSS: 4.980081341368771e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 38/71 | LOSS: 4.967860546364533e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 39/71 | LOSS: 5.016763350340625e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 40/71 | LOSS: 5.000828035163733e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 41/71 | LOSS: 5.040632480640419e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 42/71 | LOSS: 5.0355790314592185e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 43/71 | LOSS: 5.0432462744125095e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 44/71 | LOSS: 5.078657250123797e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 45/71 | LOSS: 5.097260604405006e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 46/71 | LOSS: 5.115799970727939e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 47/71 | LOSS: 5.121331649130904e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 48/71 | LOSS: 5.102203180731336e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 49/71 | LOSS: 5.110736246933811e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 50/71 | LOSS: 5.11360220865326e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 51/71 | LOSS: 5.1233220994198255e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 52/71 | LOSS: 5.104540566705664e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 53/71 | LOSS: 5.1001984249726065e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 54/71 | LOSS: 5.0782796196554875e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 55/71 | LOSS: 5.074469559726172e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 56/71 | LOSS: 5.046034291074632e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 57/71 | LOSS: 5.035424167011837e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 58/71 | LOSS: 5.035145862829098e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 59/71 | LOSS: 5.005649912466955e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 60/71 | LOSS: 5.006172494952315e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 61/71 | LOSS: 4.9879660999126385e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 62/71 | LOSS: 4.960882961313473e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 63/71 | LOSS: 4.972713881556956e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 64/71 | LOSS: 5.000346824929763e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 65/71 | LOSS: 5.074650868085552e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 66/71 | LOSS: 5.0651067917286455e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 67/71 | LOSS: 5.112040198315192e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 68/71 | LOSS: 5.117866109777832e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 69/71 | LOSS: 5.162034325881645e-06\n",
      "TRAIN: EPOCH 475/1000 | BATCH 70/71 | LOSS: 5.179205998783672e-06\n",
      "VAL: EPOCH 475/1000 | BATCH 0/8 | LOSS: 1.0727746484917589e-05\n",
      "VAL: EPOCH 475/1000 | BATCH 1/8 | LOSS: 1.0923466106760316e-05\n",
      "VAL: EPOCH 475/1000 | BATCH 2/8 | LOSS: 1.10046124367121e-05\n",
      "VAL: EPOCH 475/1000 | BATCH 3/8 | LOSS: 1.097166386898607e-05\n",
      "VAL: EPOCH 475/1000 | BATCH 4/8 | LOSS: 1.0968744572892319e-05\n",
      "VAL: EPOCH 475/1000 | BATCH 5/8 | LOSS: 1.0618376715380387e-05\n",
      "VAL: EPOCH 475/1000 | BATCH 6/8 | LOSS: 1.0700207505059162e-05\n",
      "VAL: EPOCH 475/1000 | BATCH 7/8 | LOSS: 1.053420601238031e-05\n",
      "TRAIN: EPOCH 476/1000 | BATCH 0/71 | LOSS: 9.501906788500492e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 1/71 | LOSS: 8.418399829679402e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 2/71 | LOSS: 7.912727899868818e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 3/71 | LOSS: 8.379617838727427e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 4/71 | LOSS: 8.23706686787773e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 5/71 | LOSS: 7.765500564952768e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 6/71 | LOSS: 7.5636289693647996e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 7/71 | LOSS: 7.837745442884625e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 8/71 | LOSS: 7.435702476262425e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 9/71 | LOSS: 7.553047362307553e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 10/71 | LOSS: 7.724484021309763e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 11/71 | LOSS: 7.674814317700415e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 12/71 | LOSS: 7.687482978396405e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 13/71 | LOSS: 7.582145112142566e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 14/71 | LOSS: 7.782680404488928e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 15/71 | LOSS: 7.64830491561952e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 16/71 | LOSS: 7.5476532669306515e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 17/71 | LOSS: 7.347940077630079e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 18/71 | LOSS: 7.301299870289911e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 19/71 | LOSS: 7.207725957414368e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 20/71 | LOSS: 7.063344604657254e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 21/71 | LOSS: 6.9798265733631775e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 22/71 | LOSS: 6.881224358632766e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 23/71 | LOSS: 6.790000365223629e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 24/71 | LOSS: 6.689197616651654e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 25/71 | LOSS: 6.632537616286964e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 26/71 | LOSS: 6.6468825631697355e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 27/71 | LOSS: 6.560135846354699e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 28/71 | LOSS: 6.544591307264737e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 29/71 | LOSS: 6.47821389065939e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 30/71 | LOSS: 6.450009887855906e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 31/71 | LOSS: 6.369358985125473e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 32/71 | LOSS: 6.391602407044654e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 33/71 | LOSS: 6.315498226553352e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 34/71 | LOSS: 6.279097384062229e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 35/71 | LOSS: 6.238746158891849e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 36/71 | LOSS: 6.198630229404001e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 37/71 | LOSS: 6.140675838507473e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 38/71 | LOSS: 6.113998050494e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 39/71 | LOSS: 6.061745784791128e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 40/71 | LOSS: 6.0389865233801395e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 41/71 | LOSS: 5.985300255079415e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 42/71 | LOSS: 6.021922408128672e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 43/71 | LOSS: 5.972358239473992e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 44/71 | LOSS: 6.002251939207781e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 45/71 | LOSS: 5.974366539031388e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 46/71 | LOSS: 5.973692613553524e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 47/71 | LOSS: 5.960813600343802e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 48/71 | LOSS: 5.944482187000436e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 49/71 | LOSS: 5.913999693802907e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 50/71 | LOSS: 5.879850380320111e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 51/71 | LOSS: 5.85929791878488e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 52/71 | LOSS: 5.847220306333049e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 53/71 | LOSS: 5.803522888662108e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 54/71 | LOSS: 5.787361219104654e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 55/71 | LOSS: 5.781010315786261e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 56/71 | LOSS: 5.746105192596779e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 57/71 | LOSS: 5.729899228377878e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 58/71 | LOSS: 5.722070616653675e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 59/71 | LOSS: 5.698604074192796e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 60/71 | LOSS: 5.667979398002317e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 61/71 | LOSS: 5.666961787057751e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 62/71 | LOSS: 5.650842373893644e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 63/71 | LOSS: 5.633879311517376e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 64/71 | LOSS: 5.613553296895519e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 65/71 | LOSS: 5.584275823600889e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 66/71 | LOSS: 5.589958744221532e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 67/71 | LOSS: 5.570056778063136e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 68/71 | LOSS: 5.5484020098104505e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 69/71 | LOSS: 5.543983196990406e-06\n",
      "TRAIN: EPOCH 476/1000 | BATCH 70/71 | LOSS: 5.499265209667575e-06\n",
      "VAL: EPOCH 476/1000 | BATCH 0/8 | LOSS: 5.909828360017855e-06\n",
      "VAL: EPOCH 476/1000 | BATCH 1/8 | LOSS: 6.176828264869982e-06\n",
      "VAL: EPOCH 476/1000 | BATCH 2/8 | LOSS: 6.14649919346751e-06\n",
      "VAL: EPOCH 476/1000 | BATCH 3/8 | LOSS: 6.0856243635498686e-06\n",
      "VAL: EPOCH 476/1000 | BATCH 4/8 | LOSS: 6.080506773287198e-06\n",
      "VAL: EPOCH 476/1000 | BATCH 5/8 | LOSS: 5.805227146993275e-06\n",
      "VAL: EPOCH 476/1000 | BATCH 6/8 | LOSS: 5.6723717014912315e-06\n",
      "VAL: EPOCH 476/1000 | BATCH 7/8 | LOSS: 5.488559793320746e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 0/71 | LOSS: 6.2576191339758225e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 1/71 | LOSS: 5.371100542106433e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 2/71 | LOSS: 5.3903171040777425e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 3/71 | LOSS: 4.958258557508088e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 4/71 | LOSS: 5.013366853745538e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 5/71 | LOSS: 4.863141498390178e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 6/71 | LOSS: 4.990678949720209e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 7/71 | LOSS: 4.9628599469997425e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 8/71 | LOSS: 5.056809868619894e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 9/71 | LOSS: 5.063839512331469e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 10/71 | LOSS: 5.121184937128088e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 11/71 | LOSS: 4.967351685536414e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 12/71 | LOSS: 4.94267744845554e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 13/71 | LOSS: 4.85127463889512e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 14/71 | LOSS: 4.831308236437811e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 15/71 | LOSS: 4.82549806690713e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 16/71 | LOSS: 4.705884775295785e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 17/71 | LOSS: 4.685613905369084e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 18/71 | LOSS: 4.680936901997101e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 19/71 | LOSS: 4.696833184425486e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 20/71 | LOSS: 4.703170859665122e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 21/71 | LOSS: 4.696465153260347e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 22/71 | LOSS: 4.668534727344472e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 23/71 | LOSS: 4.644708875882013e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 24/71 | LOSS: 4.605198846547864e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 25/71 | LOSS: 4.5804341040750914e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 26/71 | LOSS: 4.567468968835547e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 27/71 | LOSS: 4.51173641684883e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 28/71 | LOSS: 4.5424900816152365e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 29/71 | LOSS: 4.524334576672117e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 30/71 | LOSS: 4.502258295155292e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 31/71 | LOSS: 4.505497535944869e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 32/71 | LOSS: 4.496472814880138e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 33/71 | LOSS: 4.495256575864914e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 34/71 | LOSS: 4.488741478780867e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 35/71 | LOSS: 4.4954974466943695e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 36/71 | LOSS: 4.466015175116686e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 37/71 | LOSS: 4.485875604730513e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 38/71 | LOSS: 4.491927116899975e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 39/71 | LOSS: 4.473741012134269e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 40/71 | LOSS: 4.499069457712566e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 41/71 | LOSS: 4.519562145062428e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 42/71 | LOSS: 4.487029664867509e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 43/71 | LOSS: 4.465309664489971e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 44/71 | LOSS: 4.444339836279849e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 45/71 | LOSS: 4.431803513002904e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 46/71 | LOSS: 4.419724678456909e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 47/71 | LOSS: 4.432770107124877e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 48/71 | LOSS: 4.454840531512413e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 49/71 | LOSS: 4.433312933542766e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 50/71 | LOSS: 4.406517429707296e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 51/71 | LOSS: 4.4262126485031895e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 52/71 | LOSS: 4.432687885711411e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 53/71 | LOSS: 4.433591420315173e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 54/71 | LOSS: 4.415537167997999e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 55/71 | LOSS: 4.440217086961898e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 56/71 | LOSS: 4.43338264407781e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 57/71 | LOSS: 4.434553245808839e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 58/71 | LOSS: 4.4430677061720445e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 59/71 | LOSS: 4.451619266395331e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 60/71 | LOSS: 4.4519493511587396e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 61/71 | LOSS: 4.447815887297404e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 62/71 | LOSS: 4.4525008924831616e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 63/71 | LOSS: 4.445576113454308e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 64/71 | LOSS: 4.433949614114961e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 65/71 | LOSS: 4.451531529439583e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 66/71 | LOSS: 4.4428910355032825e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 67/71 | LOSS: 4.439588749124489e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 68/71 | LOSS: 4.429975311670232e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 69/71 | LOSS: 4.448209038595711e-06\n",
      "TRAIN: EPOCH 477/1000 | BATCH 70/71 | LOSS: 4.4276995512019044e-06\n",
      "VAL: EPOCH 477/1000 | BATCH 0/8 | LOSS: 4.796721896127565e-06\n",
      "VAL: EPOCH 477/1000 | BATCH 1/8 | LOSS: 5.0083660880773095e-06\n",
      "VAL: EPOCH 477/1000 | BATCH 2/8 | LOSS: 5.341953207486465e-06\n",
      "VAL: EPOCH 477/1000 | BATCH 3/8 | LOSS: 5.38312110620609e-06\n",
      "VAL: EPOCH 477/1000 | BATCH 4/8 | LOSS: 5.351836171030299e-06\n",
      "VAL: EPOCH 477/1000 | BATCH 5/8 | LOSS: 5.409375717135845e-06\n",
      "VAL: EPOCH 477/1000 | BATCH 6/8 | LOSS: 5.277344761062912e-06\n",
      "VAL: EPOCH 477/1000 | BATCH 7/8 | LOSS: 5.279953029457829e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 0/71 | LOSS: 4.61315630673198e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 1/71 | LOSS: 4.4532994252222124e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 2/71 | LOSS: 4.7127089904582436e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 3/71 | LOSS: 4.328834847910912e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 4/71 | LOSS: 4.197128509986214e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 5/71 | LOSS: 4.2539610755435815e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 6/71 | LOSS: 4.222843797054208e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 7/71 | LOSS: 4.2406506963743595e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 8/71 | LOSS: 4.185864276627803e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 9/71 | LOSS: 4.068990756422864e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 10/71 | LOSS: 4.148325842834311e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 11/71 | LOSS: 4.1684892645813916e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 12/71 | LOSS: 4.121880745235383e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 13/71 | LOSS: 4.125693424482181e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 14/71 | LOSS: 4.190879705371723e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 15/71 | LOSS: 4.19314176269836e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 16/71 | LOSS: 4.244276918624939e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 17/71 | LOSS: 4.207515391701438e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 18/71 | LOSS: 4.2687881887104595e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 19/71 | LOSS: 4.252598944276542e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 20/71 | LOSS: 4.46189403105347e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 21/71 | LOSS: 4.508777351475146e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 22/71 | LOSS: 4.5734275120238355e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 23/71 | LOSS: 4.650965659417731e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 24/71 | LOSS: 4.70893198325939e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 25/71 | LOSS: 4.7707974405196955e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 26/71 | LOSS: 4.858703210562386e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 27/71 | LOSS: 4.897234808660349e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 28/71 | LOSS: 4.854112634787051e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 29/71 | LOSS: 4.944898872357347e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 30/71 | LOSS: 4.9419546529304256e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 31/71 | LOSS: 4.975088607750422e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 32/71 | LOSS: 4.96260488971805e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 33/71 | LOSS: 4.964274192870482e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 34/71 | LOSS: 4.958537484916243e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 35/71 | LOSS: 4.957637953692837e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 36/71 | LOSS: 4.9883662012615715e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 37/71 | LOSS: 5.0095355606428805e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 38/71 | LOSS: 5.0023502353724325e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 39/71 | LOSS: 4.9908724520264515e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 40/71 | LOSS: 4.991717133168827e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 41/71 | LOSS: 5.010414914371116e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 42/71 | LOSS: 4.989278374713924e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 43/71 | LOSS: 5.026204415902281e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 44/71 | LOSS: 5.018102178332305e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 45/71 | LOSS: 5.010239414846381e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 46/71 | LOSS: 4.999187825539612e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 47/71 | LOSS: 4.9969370508051725e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 48/71 | LOSS: 5.0029130826650155e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 49/71 | LOSS: 4.999651141588402e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 50/71 | LOSS: 4.99071174375536e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 51/71 | LOSS: 4.978839224524843e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 52/71 | LOSS: 4.990940590018137e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 53/71 | LOSS: 4.985667807752044e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 54/71 | LOSS: 4.976762966180104e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 55/71 | LOSS: 4.9698999638232114e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 56/71 | LOSS: 4.968525449281483e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 57/71 | LOSS: 4.967780141826019e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 58/71 | LOSS: 4.961643032972569e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 59/71 | LOSS: 4.973379164615229e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 60/71 | LOSS: 4.960723654370608e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 61/71 | LOSS: 4.977936570075174e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 62/71 | LOSS: 4.991500252308758e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 63/71 | LOSS: 4.975263816930919e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 64/71 | LOSS: 4.975212172771885e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 65/71 | LOSS: 4.983199904596734e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 66/71 | LOSS: 4.974531222289253e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 67/71 | LOSS: 4.983667258115661e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 68/71 | LOSS: 5.004505395107486e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 69/71 | LOSS: 4.987721139774034e-06\n",
      "TRAIN: EPOCH 478/1000 | BATCH 70/71 | LOSS: 5.036774231945894e-06\n",
      "VAL: EPOCH 478/1000 | BATCH 0/8 | LOSS: 5.500488441612106e-06\n",
      "VAL: EPOCH 478/1000 | BATCH 1/8 | LOSS: 5.74772889194719e-06\n",
      "VAL: EPOCH 478/1000 | BATCH 2/8 | LOSS: 5.95655440823369e-06\n",
      "VAL: EPOCH 478/1000 | BATCH 3/8 | LOSS: 5.990950626255653e-06\n",
      "VAL: EPOCH 478/1000 | BATCH 4/8 | LOSS: 5.9046446949651e-06\n",
      "VAL: EPOCH 478/1000 | BATCH 5/8 | LOSS: 5.844951601829962e-06\n",
      "VAL: EPOCH 478/1000 | BATCH 6/8 | LOSS: 5.712416233499036e-06\n",
      "VAL: EPOCH 478/1000 | BATCH 7/8 | LOSS: 5.669309246059129e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 0/71 | LOSS: 5.434825652628206e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 1/71 | LOSS: 6.261620910663623e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 2/71 | LOSS: 5.769355842251874e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 3/71 | LOSS: 5.961744022897619e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 4/71 | LOSS: 6.474325982708251e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 5/71 | LOSS: 6.632339818679611e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 6/71 | LOSS: 6.5558026887759165e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 7/71 | LOSS: 6.4619511590535694e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 8/71 | LOSS: 6.387753223356816e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 9/71 | LOSS: 6.251539571167086e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 10/71 | LOSS: 6.322137571921551e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 11/71 | LOSS: 6.144563978220201e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 12/71 | LOSS: 6.022582615864499e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 13/71 | LOSS: 5.9656975801252495e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 14/71 | LOSS: 5.856550766717798e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 15/71 | LOSS: 5.876716670627502e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 16/71 | LOSS: 5.763386766107835e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 17/71 | LOSS: 5.785430782149585e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 18/71 | LOSS: 5.688813416782068e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 19/71 | LOSS: 5.74372108985699e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 20/71 | LOSS: 5.6630008960686e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 21/71 | LOSS: 5.703646820620634e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 22/71 | LOSS: 5.67320911732563e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 23/71 | LOSS: 5.63815683563007e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 24/71 | LOSS: 5.583041020145174e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 25/71 | LOSS: 5.516434647198856e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 26/71 | LOSS: 5.449700573215658e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 27/71 | LOSS: 5.395441901003194e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 28/71 | LOSS: 5.343476961319664e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 29/71 | LOSS: 5.303154921421083e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 30/71 | LOSS: 5.266198528378511e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 31/71 | LOSS: 5.227240819749568e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 32/71 | LOSS: 5.18703861664769e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 33/71 | LOSS: 5.187561836304239e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 34/71 | LOSS: 5.161089393368456e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 35/71 | LOSS: 5.133868676744492e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 36/71 | LOSS: 5.132199807908242e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 37/71 | LOSS: 5.093535851062227e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 38/71 | LOSS: 5.075530715168077e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 39/71 | LOSS: 5.067143717951694e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 40/71 | LOSS: 5.04277217836849e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 41/71 | LOSS: 5.066892180409577e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 42/71 | LOSS: 5.0587489589139124e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 43/71 | LOSS: 5.019728777452573e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 44/71 | LOSS: 5.033037854850085e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 45/71 | LOSS: 5.040679594537283e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 46/71 | LOSS: 5.083679375371851e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 47/71 | LOSS: 5.0715153084487e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 48/71 | LOSS: 5.074110207232832e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 49/71 | LOSS: 5.0828604889829876e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 50/71 | LOSS: 5.073795408879603e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 51/71 | LOSS: 5.073546154003102e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 52/71 | LOSS: 5.064247832122865e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 53/71 | LOSS: 5.0603876355092294e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 54/71 | LOSS: 5.030290466575852e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 55/71 | LOSS: 5.018626483531209e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 56/71 | LOSS: 4.993388289987802e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 57/71 | LOSS: 4.984392854441529e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 58/71 | LOSS: 4.962941725892252e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 59/71 | LOSS: 4.961906468755236e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 60/71 | LOSS: 4.967414519878879e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 61/71 | LOSS: 4.9719328891965675e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 62/71 | LOSS: 4.976509819879004e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 63/71 | LOSS: 4.979165296958854e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 64/71 | LOSS: 4.9922148089255925e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 65/71 | LOSS: 4.989763579722036e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 66/71 | LOSS: 4.9820645024249356e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 67/71 | LOSS: 4.977803454868776e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 68/71 | LOSS: 4.979988057459485e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 69/71 | LOSS: 4.987154417384382e-06\n",
      "TRAIN: EPOCH 479/1000 | BATCH 70/71 | LOSS: 4.980094916032279e-06\n",
      "VAL: EPOCH 479/1000 | BATCH 0/8 | LOSS: 6.176326678541955e-06\n",
      "VAL: EPOCH 479/1000 | BATCH 1/8 | LOSS: 6.044246902092709e-06\n",
      "VAL: EPOCH 479/1000 | BATCH 2/8 | LOSS: 6.224028008242992e-06\n",
      "VAL: EPOCH 479/1000 | BATCH 3/8 | LOSS: 6.1560378981084796e-06\n",
      "VAL: EPOCH 479/1000 | BATCH 4/8 | LOSS: 6.193116678332444e-06\n",
      "VAL: EPOCH 479/1000 | BATCH 5/8 | LOSS: 6.090823565803778e-06\n",
      "VAL: EPOCH 479/1000 | BATCH 6/8 | LOSS: 6.078921744899292e-06\n",
      "VAL: EPOCH 479/1000 | BATCH 7/8 | LOSS: 6.01999045102275e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 0/71 | LOSS: 5.220120328885969e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 1/71 | LOSS: 6.000631856295513e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 2/71 | LOSS: 5.069607141194865e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 3/71 | LOSS: 5.142071358932299e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 4/71 | LOSS: 5.3925447900837755e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 5/71 | LOSS: 5.206939097964399e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 6/71 | LOSS: 5.089234036859125e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 7/71 | LOSS: 5.27879734590897e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 8/71 | LOSS: 5.166923175339535e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 9/71 | LOSS: 5.015843862565817e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 10/71 | LOSS: 4.965277019602416e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 11/71 | LOSS: 4.887419284690016e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 12/71 | LOSS: 4.872312064645274e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 13/71 | LOSS: 4.769606415980629e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 14/71 | LOSS: 4.769216563242177e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 15/71 | LOSS: 4.818137199436023e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 16/71 | LOSS: 4.793273734214702e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 17/71 | LOSS: 4.920398825763388e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 18/71 | LOSS: 4.960142584758087e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 19/71 | LOSS: 4.970344275534444e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 20/71 | LOSS: 4.9635405914505965e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 21/71 | LOSS: 4.985831806565826e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 22/71 | LOSS: 5.051304252328523e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 23/71 | LOSS: 5.0510207264172396e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 24/71 | LOSS: 5.018745814595604e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 25/71 | LOSS: 5.1411063850537175e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 26/71 | LOSS: 5.117198337824515e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 27/71 | LOSS: 5.125146019833794e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 28/71 | LOSS: 5.158185182342e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 29/71 | LOSS: 5.093655545351794e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 30/71 | LOSS: 5.104525341941494e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 31/71 | LOSS: 5.110040490308165e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 32/71 | LOSS: 5.092069345251233e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 33/71 | LOSS: 5.0772416537203e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 34/71 | LOSS: 5.102106713041264e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 35/71 | LOSS: 5.090652848593891e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 36/71 | LOSS: 5.060668320004117e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 37/71 | LOSS: 5.076099916167502e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 38/71 | LOSS: 5.073676955082472e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 39/71 | LOSS: 5.045933824021631e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 40/71 | LOSS: 5.080280727004098e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 41/71 | LOSS: 5.056271436929126e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 42/71 | LOSS: 5.044477432441686e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 43/71 | LOSS: 5.034937978300811e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 44/71 | LOSS: 5.010207425786778e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 45/71 | LOSS: 4.991205316954343e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 46/71 | LOSS: 4.979138309941033e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 47/71 | LOSS: 5.00077607057392e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 48/71 | LOSS: 4.98848429724232e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 49/71 | LOSS: 4.9767480049922595e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 50/71 | LOSS: 4.99207246820351e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 51/71 | LOSS: 4.9781098928528645e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 52/71 | LOSS: 4.9894334414654265e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 53/71 | LOSS: 5.009018903163249e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 54/71 | LOSS: 4.997147483333141e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 55/71 | LOSS: 4.970546407483718e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 56/71 | LOSS: 4.967116231120216e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 57/71 | LOSS: 4.948427729310612e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 58/71 | LOSS: 4.940700310433354e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 59/71 | LOSS: 4.945948785461951e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 60/71 | LOSS: 4.9277061675713384e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 61/71 | LOSS: 4.926289508852454e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 62/71 | LOSS: 4.914805455002121e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 63/71 | LOSS: 4.909657725704619e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 64/71 | LOSS: 4.891510787694222e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 65/71 | LOSS: 4.899317814582019e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 66/71 | LOSS: 4.899178188068943e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 67/71 | LOSS: 4.915887767337528e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 68/71 | LOSS: 4.9133108688323954e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 69/71 | LOSS: 4.924728373875301e-06\n",
      "TRAIN: EPOCH 480/1000 | BATCH 70/71 | LOSS: 4.91553620557964e-06\n",
      "VAL: EPOCH 480/1000 | BATCH 0/8 | LOSS: 4.5303618207981344e-06\n",
      "VAL: EPOCH 480/1000 | BATCH 1/8 | LOSS: 4.678865707319346e-06\n",
      "VAL: EPOCH 480/1000 | BATCH 2/8 | LOSS: 4.873210703711568e-06\n",
      "VAL: EPOCH 480/1000 | BATCH 3/8 | LOSS: 4.949803610543313e-06\n",
      "VAL: EPOCH 480/1000 | BATCH 4/8 | LOSS: 4.851970152230933e-06\n",
      "VAL: EPOCH 480/1000 | BATCH 5/8 | LOSS: 4.772899198239126e-06\n",
      "VAL: EPOCH 480/1000 | BATCH 6/8 | LOSS: 4.565796481464142e-06\n",
      "VAL: EPOCH 480/1000 | BATCH 7/8 | LOSS: 4.464702186623981e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 0/71 | LOSS: 5.391377271735109e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 1/71 | LOSS: 5.597066547124996e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 2/71 | LOSS: 5.0573306301278835e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 3/71 | LOSS: 5.139743393556273e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 4/71 | LOSS: 5.02105012856191e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 5/71 | LOSS: 4.9218986835815786e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 6/71 | LOSS: 5.023299828670653e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 7/71 | LOSS: 4.918318722957338e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 8/71 | LOSS: 4.742811698734941e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 9/71 | LOSS: 4.676706157624721e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 10/71 | LOSS: 4.559201897791354e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 11/71 | LOSS: 4.648324685755749e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 12/71 | LOSS: 4.580912743152182e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 13/71 | LOSS: 4.786823069480306e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 14/71 | LOSS: 4.796466040109711e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 15/71 | LOSS: 4.840255726890064e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 16/71 | LOSS: 4.8842257055024245e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 17/71 | LOSS: 4.886430626053577e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 18/71 | LOSS: 4.959111493008095e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 19/71 | LOSS: 4.995778920147132e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 20/71 | LOSS: 4.968238402268201e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 21/71 | LOSS: 4.933910107948495e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 22/71 | LOSS: 5.036590773275036e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 23/71 | LOSS: 5.024631216580626e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 24/71 | LOSS: 4.9931751982512655e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 25/71 | LOSS: 5.071291813972213e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 26/71 | LOSS: 5.063403890475186e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 27/71 | LOSS: 5.027281588354526e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 28/71 | LOSS: 5.023811252550363e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 29/71 | LOSS: 5.011003781874024e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 30/71 | LOSS: 5.011958357441003e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 31/71 | LOSS: 4.959224817469021e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 32/71 | LOSS: 4.978892382661632e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 33/71 | LOSS: 4.950551362312313e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 34/71 | LOSS: 4.950545131318255e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 35/71 | LOSS: 4.92043911511549e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 36/71 | LOSS: 4.915457320503019e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 37/71 | LOSS: 4.92018620278893e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 38/71 | LOSS: 4.890640814035298e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 39/71 | LOSS: 4.889498524107694e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 40/71 | LOSS: 4.8756143225945405e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 41/71 | LOSS: 4.850111021629086e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 42/71 | LOSS: 4.854002401221213e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 43/71 | LOSS: 4.845581233067523e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 44/71 | LOSS: 4.825119908168239e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 45/71 | LOSS: 4.812620338033531e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 46/71 | LOSS: 4.797880456920966e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 47/71 | LOSS: 4.7827416835843906e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 48/71 | LOSS: 4.77322658539657e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 49/71 | LOSS: 4.7572520816174806e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 50/71 | LOSS: 4.7394800472544625e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 51/71 | LOSS: 4.745359293477892e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 52/71 | LOSS: 4.737666208625943e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 53/71 | LOSS: 4.780401796880243e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 54/71 | LOSS: 4.804803806605791e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 55/71 | LOSS: 4.824021792566262e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 56/71 | LOSS: 4.827346529374042e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 57/71 | LOSS: 4.853519424708291e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 58/71 | LOSS: 4.850651016490933e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 59/71 | LOSS: 4.879915096959545e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 60/71 | LOSS: 4.878417108616826e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 61/71 | LOSS: 4.88561021660233e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 62/71 | LOSS: 4.877435876840045e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 63/71 | LOSS: 4.912800349643476e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 64/71 | LOSS: 4.901173199938897e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 65/71 | LOSS: 4.9044646829949645e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 66/71 | LOSS: 4.8971317944472216e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 67/71 | LOSS: 4.905353019988551e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 68/71 | LOSS: 4.891922608162597e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 69/71 | LOSS: 4.903153467239463e-06\n",
      "TRAIN: EPOCH 481/1000 | BATCH 70/71 | LOSS: 4.888172068757753e-06\n",
      "VAL: EPOCH 481/1000 | BATCH 0/8 | LOSS: 5.782322659797501e-06\n",
      "VAL: EPOCH 481/1000 | BATCH 1/8 | LOSS: 5.444103635454667e-06\n",
      "VAL: EPOCH 481/1000 | BATCH 2/8 | LOSS: 5.515781443439967e-06\n",
      "VAL: EPOCH 481/1000 | BATCH 3/8 | LOSS: 5.405055730989261e-06\n",
      "VAL: EPOCH 481/1000 | BATCH 4/8 | LOSS: 5.4011232350603676e-06\n",
      "VAL: EPOCH 481/1000 | BATCH 5/8 | LOSS: 5.216989999704917e-06\n",
      "VAL: EPOCH 481/1000 | BATCH 6/8 | LOSS: 5.101542813333383e-06\n",
      "VAL: EPOCH 481/1000 | BATCH 7/8 | LOSS: 4.998441283987631e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 0/71 | LOSS: 4.142228590353625e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 1/71 | LOSS: 4.044608203912503e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 2/71 | LOSS: 3.8420729045659146e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 3/71 | LOSS: 3.867373720822798e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 4/71 | LOSS: 3.96329514842364e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 5/71 | LOSS: 4.159421905569616e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 6/71 | LOSS: 4.242586393437315e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 7/71 | LOSS: 4.279589916222903e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 8/71 | LOSS: 4.21024009887737e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 9/71 | LOSS: 4.275010223864229e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 10/71 | LOSS: 4.336889891088834e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 11/71 | LOSS: 4.460552493886401e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 12/71 | LOSS: 4.45309220035247e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 13/71 | LOSS: 4.6597231175837805e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 14/71 | LOSS: 4.6756046989078944e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 15/71 | LOSS: 4.795835309323593e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 16/71 | LOSS: 4.803600484806606e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 17/71 | LOSS: 4.86316111568562e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 18/71 | LOSS: 4.832759115838838e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 19/71 | LOSS: 4.826159283766174e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 20/71 | LOSS: 4.791487775773498e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 21/71 | LOSS: 4.782584338417192e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 22/71 | LOSS: 4.85300856582952e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 23/71 | LOSS: 4.840324019520874e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 24/71 | LOSS: 4.916699526802404e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 25/71 | LOSS: 4.933805712425965e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 26/71 | LOSS: 4.9284356262230885e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 27/71 | LOSS: 4.925845230120883e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 28/71 | LOSS: 4.920484555221829e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 29/71 | LOSS: 4.934028144513528e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 30/71 | LOSS: 4.91189077013499e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 31/71 | LOSS: 4.950916320467513e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 32/71 | LOSS: 4.9311194853737215e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 33/71 | LOSS: 4.933517228731516e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 34/71 | LOSS: 4.9595418074334575e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 35/71 | LOSS: 4.927542988752975e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 36/71 | LOSS: 4.953104071564913e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 37/71 | LOSS: 4.936193536913809e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 38/71 | LOSS: 4.999309876205725e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 39/71 | LOSS: 5.006449856637118e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 40/71 | LOSS: 4.9951509747809655e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 41/71 | LOSS: 5.034910844057906e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 42/71 | LOSS: 5.0184923915113296e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 43/71 | LOSS: 5.029862331246642e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 44/71 | LOSS: 5.023110442279075e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 45/71 | LOSS: 5.007982088539828e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 46/71 | LOSS: 5.0120453714895274e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 47/71 | LOSS: 5.017764555266997e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 48/71 | LOSS: 4.993322061970818e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 49/71 | LOSS: 4.962128036822832e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 50/71 | LOSS: 4.960909287618444e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 51/71 | LOSS: 4.95178472175002e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 52/71 | LOSS: 4.927019037220919e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 53/71 | LOSS: 4.9118259666000895e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 54/71 | LOSS: 4.885641890888854e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 55/71 | LOSS: 4.863218070535627e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 56/71 | LOSS: 4.851421639965306e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 57/71 | LOSS: 4.848915315553063e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 58/71 | LOSS: 4.831764268385734e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 59/71 | LOSS: 4.8185079132660276e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 60/71 | LOSS: 4.847607874953461e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 61/71 | LOSS: 4.834258968531362e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 62/71 | LOSS: 4.833787822159745e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 63/71 | LOSS: 4.882644372372624e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 64/71 | LOSS: 4.921306673657874e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 65/71 | LOSS: 4.912234292973925e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 66/71 | LOSS: 4.932628778770953e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 67/71 | LOSS: 4.998847139310837e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 68/71 | LOSS: 4.988463061912942e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 69/71 | LOSS: 4.981714921216605e-06\n",
      "TRAIN: EPOCH 482/1000 | BATCH 70/71 | LOSS: 5.039601742090537e-06\n",
      "VAL: EPOCH 482/1000 | BATCH 0/8 | LOSS: 4.059536877321079e-06\n",
      "VAL: EPOCH 482/1000 | BATCH 1/8 | LOSS: 4.197734597255476e-06\n",
      "VAL: EPOCH 482/1000 | BATCH 2/8 | LOSS: 4.394652326785338e-06\n",
      "VAL: EPOCH 482/1000 | BATCH 3/8 | LOSS: 4.408538870848133e-06\n",
      "VAL: EPOCH 482/1000 | BATCH 4/8 | LOSS: 4.403854291012976e-06\n",
      "VAL: EPOCH 482/1000 | BATCH 5/8 | LOSS: 4.329246394263464e-06\n",
      "VAL: EPOCH 482/1000 | BATCH 6/8 | LOSS: 4.225740277823726e-06\n",
      "VAL: EPOCH 482/1000 | BATCH 7/8 | LOSS: 4.1796513414738e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 0/71 | LOSS: 3.55272868546308e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 1/71 | LOSS: 4.569021712086396e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 2/71 | LOSS: 4.789276772498852e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 3/71 | LOSS: 4.876387379226799e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 4/71 | LOSS: 4.876444199908292e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 5/71 | LOSS: 4.688455343663615e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 6/71 | LOSS: 4.62406894387511e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 7/71 | LOSS: 4.935399942951335e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 8/71 | LOSS: 4.767889423520602e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 9/71 | LOSS: 4.7278967258534975e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 10/71 | LOSS: 4.7192691412915755e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 11/71 | LOSS: 4.662854754163466e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 12/71 | LOSS: 4.594746892158023e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 13/71 | LOSS: 4.7250174150446715e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 14/71 | LOSS: 4.776913025731726e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 15/71 | LOSS: 4.730371117034338e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 16/71 | LOSS: 4.795460333351751e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 17/71 | LOSS: 4.794168224483049e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 18/71 | LOSS: 4.783287570382144e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 19/71 | LOSS: 4.740842348383012e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 20/71 | LOSS: 4.740485987245588e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 21/71 | LOSS: 4.679013892944733e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 22/71 | LOSS: 4.656402384368804e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 23/71 | LOSS: 4.6470151649676455e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 24/71 | LOSS: 4.6703502630407456e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 25/71 | LOSS: 4.643499621054686e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 26/71 | LOSS: 4.660356066947484e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 27/71 | LOSS: 4.6349826854046635e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 28/71 | LOSS: 4.64293703646098e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 29/71 | LOSS: 4.664822639218376e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 30/71 | LOSS: 4.642350353195422e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 31/71 | LOSS: 4.654307929286006e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 32/71 | LOSS: 4.6870559069220414e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 33/71 | LOSS: 4.705109257188537e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 34/71 | LOSS: 4.737739761497193e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 35/71 | LOSS: 4.788388966719342e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 36/71 | LOSS: 4.755373225093904e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 37/71 | LOSS: 4.825932370821599e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 38/71 | LOSS: 4.8172706288031086e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 39/71 | LOSS: 4.874079132832776e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 40/71 | LOSS: 4.855117617253421e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 41/71 | LOSS: 4.8930846733561515e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 42/71 | LOSS: 4.8828548357093054e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 43/71 | LOSS: 4.887013589764551e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 44/71 | LOSS: 4.886558432796543e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 45/71 | LOSS: 4.859428723529677e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 46/71 | LOSS: 4.851180286076418e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 47/71 | LOSS: 4.84825389908868e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 48/71 | LOSS: 4.858559169424923e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 49/71 | LOSS: 4.839287607865117e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 50/71 | LOSS: 4.840023852831983e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 51/71 | LOSS: 4.834490513216084e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 52/71 | LOSS: 4.817979005273851e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 53/71 | LOSS: 4.826890277500853e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 54/71 | LOSS: 4.816458315624252e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 55/71 | LOSS: 4.84311351962268e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 56/71 | LOSS: 4.833278983808173e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 57/71 | LOSS: 4.844080943508618e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 58/71 | LOSS: 4.82321634389321e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 59/71 | LOSS: 4.828063489033715e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 60/71 | LOSS: 4.8373768945044715e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 61/71 | LOSS: 4.858704455141231e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 62/71 | LOSS: 4.865108120077547e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 63/71 | LOSS: 4.85537506378364e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 64/71 | LOSS: 4.856883169150723e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 65/71 | LOSS: 4.8380427312043715e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 66/71 | LOSS: 4.837042270218493e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 67/71 | LOSS: 4.842650024356237e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 68/71 | LOSS: 4.852833557300113e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 69/71 | LOSS: 4.848375897381629e-06\n",
      "TRAIN: EPOCH 483/1000 | BATCH 70/71 | LOSS: 4.835954817114636e-06\n",
      "VAL: EPOCH 483/1000 | BATCH 0/8 | LOSS: 4.671871920436388e-06\n",
      "VAL: EPOCH 483/1000 | BATCH 1/8 | LOSS: 5.315641828929074e-06\n",
      "VAL: EPOCH 483/1000 | BATCH 2/8 | LOSS: 5.5940164808513755e-06\n",
      "VAL: EPOCH 483/1000 | BATCH 3/8 | LOSS: 5.756416499025363e-06\n",
      "VAL: EPOCH 483/1000 | BATCH 4/8 | LOSS: 5.706029514840338e-06\n",
      "VAL: EPOCH 483/1000 | BATCH 5/8 | LOSS: 5.586973505463296e-06\n",
      "VAL: EPOCH 483/1000 | BATCH 6/8 | LOSS: 5.409658894807633e-06\n",
      "VAL: EPOCH 483/1000 | BATCH 7/8 | LOSS: 5.2528158676068415e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 0/71 | LOSS: 4.907062702841358e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 1/71 | LOSS: 5.079730044599273e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 2/71 | LOSS: 4.842125539047022e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 3/71 | LOSS: 4.662797664423124e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 4/71 | LOSS: 4.411733834785992e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 5/71 | LOSS: 4.520533783155163e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 6/71 | LOSS: 4.412808266351931e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 7/71 | LOSS: 4.2432961890881415e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 8/71 | LOSS: 4.185242889636558e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 9/71 | LOSS: 4.400119723868556e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 10/71 | LOSS: 4.321101187088061e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 11/71 | LOSS: 4.367812986553569e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 12/71 | LOSS: 4.591591767586158e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 13/71 | LOSS: 4.552994401560032e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 14/71 | LOSS: 4.713487699821902e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 15/71 | LOSS: 4.8088362518683425e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 16/71 | LOSS: 4.8132125598118255e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 17/71 | LOSS: 4.770747864313307e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 18/71 | LOSS: 4.792095224777433e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 19/71 | LOSS: 4.809784240933368e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 20/71 | LOSS: 4.7613360991471426e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 21/71 | LOSS: 4.727573448028786e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 22/71 | LOSS: 4.691555382716773e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 23/71 | LOSS: 4.680697050692591e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 24/71 | LOSS: 4.7124962657107975e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 25/71 | LOSS: 4.685142007824302e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 26/71 | LOSS: 4.6892949925407905e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 27/71 | LOSS: 4.76903354800535e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 28/71 | LOSS: 4.784124986803488e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 29/71 | LOSS: 4.836250885394596e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 30/71 | LOSS: 4.804476365309036e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 31/71 | LOSS: 4.812669871512298e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 32/71 | LOSS: 4.832909174486606e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 33/71 | LOSS: 4.82054542771948e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 34/71 | LOSS: 4.834272671619796e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 35/71 | LOSS: 4.829703786728916e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 36/71 | LOSS: 4.810074544267893e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 37/71 | LOSS: 4.808099112396073e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 38/71 | LOSS: 4.83164645769508e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 39/71 | LOSS: 4.8361622702941535e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 40/71 | LOSS: 4.8489115969703825e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 41/71 | LOSS: 4.8429585604026215e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 42/71 | LOSS: 4.871361799687716e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 43/71 | LOSS: 4.866146303522163e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 44/71 | LOSS: 4.887054809741029e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 45/71 | LOSS: 4.864205780192962e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 46/71 | LOSS: 4.881666804879948e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 47/71 | LOSS: 4.8709526746885485e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 48/71 | LOSS: 4.881813515103018e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 49/71 | LOSS: 4.849114411626942e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 50/71 | LOSS: 4.833256075382415e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 51/71 | LOSS: 4.8245324600989425e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 52/71 | LOSS: 4.848022167864673e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 53/71 | LOSS: 4.842723237069785e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 54/71 | LOSS: 4.848588973386954e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 55/71 | LOSS: 4.878627635207522e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 56/71 | LOSS: 4.8727036444592825e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 57/71 | LOSS: 4.917770434860421e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 58/71 | LOSS: 4.913798674952901e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 59/71 | LOSS: 4.934445102359556e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 60/71 | LOSS: 4.922917722695752e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 61/71 | LOSS: 4.948410679067057e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 62/71 | LOSS: 4.962094256673784e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 63/71 | LOSS: 4.977234510761264e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 64/71 | LOSS: 4.9776595705435075e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 65/71 | LOSS: 4.9606644483212605e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 66/71 | LOSS: 4.991315135931242e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 67/71 | LOSS: 5.003938025872819e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 68/71 | LOSS: 5.040181125535299e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 69/71 | LOSS: 5.055362693643214e-06\n",
      "TRAIN: EPOCH 484/1000 | BATCH 70/71 | LOSS: 5.047496274592434e-06\n",
      "VAL: EPOCH 484/1000 | BATCH 0/8 | LOSS: 8.665794666740112e-06\n",
      "VAL: EPOCH 484/1000 | BATCH 1/8 | LOSS: 8.211424756154884e-06\n",
      "VAL: EPOCH 484/1000 | BATCH 2/8 | LOSS: 8.30037546014258e-06\n",
      "VAL: EPOCH 484/1000 | BATCH 3/8 | LOSS: 8.253279020209447e-06\n",
      "VAL: EPOCH 484/1000 | BATCH 4/8 | LOSS: 8.001649894140428e-06\n",
      "VAL: EPOCH 484/1000 | BATCH 5/8 | LOSS: 7.850400379538769e-06\n",
      "VAL: EPOCH 484/1000 | BATCH 6/8 | LOSS: 7.678721691003634e-06\n",
      "VAL: EPOCH 484/1000 | BATCH 7/8 | LOSS: 7.512682827837125e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 0/71 | LOSS: 7.724578608758748e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 1/71 | LOSS: 6.641726031375583e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 2/71 | LOSS: 6.143804663831058e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 3/71 | LOSS: 5.757340659329202e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 4/71 | LOSS: 5.806574881717097e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 5/71 | LOSS: 5.5848511995767085e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 6/71 | LOSS: 5.572750004440812e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 7/71 | LOSS: 5.626521442536614e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 8/71 | LOSS: 5.636595233227126e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 9/71 | LOSS: 5.9167546169192065e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 10/71 | LOSS: 5.836437842596999e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 11/71 | LOSS: 6.145938793148768e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 12/71 | LOSS: 6.023749637489135e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 13/71 | LOSS: 6.174080876267648e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 14/71 | LOSS: 6.004800555577579e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 15/71 | LOSS: 5.962160486205903e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 16/71 | LOSS: 5.944201615527752e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 17/71 | LOSS: 5.877936650399028e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 18/71 | LOSS: 5.801313818665221e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 19/71 | LOSS: 5.726462040911428e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 20/71 | LOSS: 5.746977855042565e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 21/71 | LOSS: 5.776666326214168e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 22/71 | LOSS: 5.711780827171068e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 23/71 | LOSS: 5.7382266049899044e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 24/71 | LOSS: 5.674907442880794e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 25/71 | LOSS: 5.703406574097104e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 26/71 | LOSS: 5.644183686483841e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 27/71 | LOSS: 5.636771545271456e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 28/71 | LOSS: 5.682737223808012e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 29/71 | LOSS: 5.642641796536433e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 30/71 | LOSS: 5.601672059598176e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 31/71 | LOSS: 5.634292236322835e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 32/71 | LOSS: 5.597892361424094e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 33/71 | LOSS: 5.572316414305961e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 34/71 | LOSS: 5.542743084723562e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 35/71 | LOSS: 5.5233865370812255e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 36/71 | LOSS: 5.475378749316725e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 37/71 | LOSS: 5.448070988964709e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 38/71 | LOSS: 5.4304466979934096e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 39/71 | LOSS: 5.372897004463084e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 40/71 | LOSS: 5.350122065221246e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 41/71 | LOSS: 5.337845339324433e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 42/71 | LOSS: 5.326309519659844e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 43/71 | LOSS: 5.3462260222080325e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 44/71 | LOSS: 5.331814579929212e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 45/71 | LOSS: 5.327342018577126e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 46/71 | LOSS: 5.303681310830685e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 47/71 | LOSS: 5.302819152082823e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 48/71 | LOSS: 5.285826917464206e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 49/71 | LOSS: 5.274152094898454e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 50/71 | LOSS: 5.247487287502451e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 51/71 | LOSS: 5.239743486002296e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 52/71 | LOSS: 5.232514274881258e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 53/71 | LOSS: 5.2223151290794094e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 54/71 | LOSS: 5.217760665552553e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 55/71 | LOSS: 5.205192463465989e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 56/71 | LOSS: 5.2051517095127275e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 57/71 | LOSS: 5.220618618295703e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 58/71 | LOSS: 5.193883722868276e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 59/71 | LOSS: 5.164316390467623e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 60/71 | LOSS: 5.1540192566417466e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 61/71 | LOSS: 5.135882236484782e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 62/71 | LOSS: 5.115882689977528e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 63/71 | LOSS: 5.111798838441928e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 64/71 | LOSS: 5.108534518372975e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 65/71 | LOSS: 5.087634445827104e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 66/71 | LOSS: 5.079764585036119e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 67/71 | LOSS: 5.059873993051203e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 68/71 | LOSS: 5.055907267624338e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 69/71 | LOSS: 5.0609572651540344e-06\n",
      "TRAIN: EPOCH 485/1000 | BATCH 70/71 | LOSS: 5.047495592471407e-06\n",
      "VAL: EPOCH 485/1000 | BATCH 0/8 | LOSS: 4.187980266578961e-06\n",
      "VAL: EPOCH 485/1000 | BATCH 1/8 | LOSS: 4.231177399560693e-06\n",
      "VAL: EPOCH 485/1000 | BATCH 2/8 | LOSS: 4.301222512973861e-06\n",
      "VAL: EPOCH 485/1000 | BATCH 3/8 | LOSS: 4.25117616487114e-06\n",
      "VAL: EPOCH 485/1000 | BATCH 4/8 | LOSS: 4.225622342346469e-06\n",
      "VAL: EPOCH 485/1000 | BATCH 5/8 | LOSS: 4.120498829252028e-06\n",
      "VAL: EPOCH 485/1000 | BATCH 6/8 | LOSS: 3.975622121288325e-06\n",
      "VAL: EPOCH 485/1000 | BATCH 7/8 | LOSS: 3.896653055335264e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 0/71 | LOSS: 3.0544824767275713e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 1/71 | LOSS: 3.5250777727924287e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 2/71 | LOSS: 3.647511372643445e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 3/71 | LOSS: 3.819466314780584e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 4/71 | LOSS: 3.6293496123107617e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 5/71 | LOSS: 3.7898873870290117e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 6/71 | LOSS: 4.072042494434365e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 7/71 | LOSS: 4.094884445748903e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 8/71 | LOSS: 4.037553329302075e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 9/71 | LOSS: 4.02764721911808e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 10/71 | LOSS: 4.1782472676872695e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 11/71 | LOSS: 4.188602815702325e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 12/71 | LOSS: 4.282197026511242e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 13/71 | LOSS: 4.399374152593996e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 14/71 | LOSS: 4.3655660192598585e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 15/71 | LOSS: 4.3467222496929026e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 16/71 | LOSS: 4.36971525713863e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 17/71 | LOSS: 4.3501373612444795e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 18/71 | LOSS: 4.374138131281238e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 19/71 | LOSS: 4.419242895892239e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 20/71 | LOSS: 4.427560400342502e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 21/71 | LOSS: 4.4041393762613135e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 22/71 | LOSS: 4.414971301783858e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 23/71 | LOSS: 4.389531123176009e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 24/71 | LOSS: 4.421685689521837e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 25/71 | LOSS: 4.394014808895008e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 26/71 | LOSS: 4.409901782343729e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 27/71 | LOSS: 4.4711450511515845e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 28/71 | LOSS: 4.445587059120138e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 29/71 | LOSS: 4.4511860399628254e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 30/71 | LOSS: 4.499975331430417e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 31/71 | LOSS: 4.496636982764812e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 32/71 | LOSS: 4.490284723171058e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 33/71 | LOSS: 4.493067208118994e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 34/71 | LOSS: 4.483383606514378e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 35/71 | LOSS: 4.480044386430866e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 36/71 | LOSS: 4.5272710812659585e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 37/71 | LOSS: 4.5742043898587185e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 38/71 | LOSS: 4.573613488481407e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 39/71 | LOSS: 4.547038793134562e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 40/71 | LOSS: 4.662618089130818e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 41/71 | LOSS: 4.647691401346708e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 42/71 | LOSS: 4.810284799320872e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 43/71 | LOSS: 4.78752715273665e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 44/71 | LOSS: 4.947584521788587e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 45/71 | LOSS: 4.937930626787736e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 46/71 | LOSS: 5.040261356121033e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 47/71 | LOSS: 5.02720287916721e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 48/71 | LOSS: 5.078844006508304e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 49/71 | LOSS: 5.11605981955654e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 50/71 | LOSS: 5.1343197043809366e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 51/71 | LOSS: 5.182048286000141e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 52/71 | LOSS: 5.15849971188285e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 53/71 | LOSS: 5.245047439664766e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 54/71 | LOSS: 5.233887184824033e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 55/71 | LOSS: 5.276434965903068e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 56/71 | LOSS: 5.280952959047251e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 57/71 | LOSS: 5.306099089058739e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 58/71 | LOSS: 5.2809494695408165e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 59/71 | LOSS: 5.2900100020754815e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 60/71 | LOSS: 5.267476920335702e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 61/71 | LOSS: 5.291135516193707e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 62/71 | LOSS: 5.289509859028926e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 63/71 | LOSS: 5.307221208283863e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 64/71 | LOSS: 5.346402555845159e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 65/71 | LOSS: 5.336152340036775e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 66/71 | LOSS: 5.3867871198480005e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 67/71 | LOSS: 5.374355489894858e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 68/71 | LOSS: 5.419218380837634e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 69/71 | LOSS: 5.384608523075128e-06\n",
      "TRAIN: EPOCH 486/1000 | BATCH 70/71 | LOSS: 5.403907024769914e-06\n",
      "VAL: EPOCH 486/1000 | BATCH 0/8 | LOSS: 8.485455509799067e-06\n",
      "VAL: EPOCH 486/1000 | BATCH 1/8 | LOSS: 8.239661838160828e-06\n",
      "VAL: EPOCH 486/1000 | BATCH 2/8 | LOSS: 8.199795956898015e-06\n",
      "VAL: EPOCH 486/1000 | BATCH 3/8 | LOSS: 8.199327339752926e-06\n",
      "VAL: EPOCH 486/1000 | BATCH 4/8 | LOSS: 8.038137639232446e-06\n",
      "VAL: EPOCH 486/1000 | BATCH 5/8 | LOSS: 7.804841213025307e-06\n",
      "VAL: EPOCH 486/1000 | BATCH 6/8 | LOSS: 7.68493886685714e-06\n",
      "VAL: EPOCH 486/1000 | BATCH 7/8 | LOSS: 7.4840799584308115e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 0/71 | LOSS: 7.997166903805919e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 1/71 | LOSS: 6.264790499699302e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 2/71 | LOSS: 6.404931203481586e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 3/71 | LOSS: 6.097154937378946e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 4/71 | LOSS: 6.080235471017658e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 5/71 | LOSS: 5.8046375670528505e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 6/71 | LOSS: 5.541208598255512e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 7/71 | LOSS: 5.619286071123497e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 8/71 | LOSS: 5.457137326124615e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 9/71 | LOSS: 5.348575223251828e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 10/71 | LOSS: 5.3604619345357856e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 11/71 | LOSS: 5.237293066784332e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 12/71 | LOSS: 5.177498468793391e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 13/71 | LOSS: 5.117130707990977e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 14/71 | LOSS: 5.094366194195269e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 15/71 | LOSS: 5.020677320999312e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 16/71 | LOSS: 5.018282723433324e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 17/71 | LOSS: 5.00139450802332e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 18/71 | LOSS: 4.969714154867688e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 19/71 | LOSS: 4.948052765030297e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 20/71 | LOSS: 4.9038435030351615e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 21/71 | LOSS: 4.847111457903256e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 22/71 | LOSS: 4.842850399733794e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 23/71 | LOSS: 4.855097567239379e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 24/71 | LOSS: 4.881765416939743e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 25/71 | LOSS: 4.857610082791115e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 26/71 | LOSS: 4.873829682226103e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 27/71 | LOSS: 4.923179825969523e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 28/71 | LOSS: 4.857275441814824e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 29/71 | LOSS: 4.821777921885465e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 30/71 | LOSS: 4.7846330025711196e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 31/71 | LOSS: 4.785933462869707e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 32/71 | LOSS: 4.741258132498567e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 33/71 | LOSS: 4.724408099451359e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 34/71 | LOSS: 4.694915631781831e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 35/71 | LOSS: 4.673357314762446e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 36/71 | LOSS: 4.646033747465825e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 37/71 | LOSS: 4.649818391187996e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 38/71 | LOSS: 4.649277582594107e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 39/71 | LOSS: 4.627998077921802e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 40/71 | LOSS: 4.642418699825989e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 41/71 | LOSS: 4.629807541451078e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 42/71 | LOSS: 4.643087037080942e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 43/71 | LOSS: 4.653334485738014e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 44/71 | LOSS: 4.643070072537133e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 45/71 | LOSS: 4.624914639783168e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 46/71 | LOSS: 4.62716844653032e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 47/71 | LOSS: 4.620158279067255e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 48/71 | LOSS: 4.610828900509585e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 49/71 | LOSS: 4.621697362381383e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 50/71 | LOSS: 4.63904770381929e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 51/71 | LOSS: 4.649238235288067e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 52/71 | LOSS: 4.649243752317248e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 53/71 | LOSS: 4.663577530866152e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 54/71 | LOSS: 4.665113506648182e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 55/71 | LOSS: 4.660048676084573e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 56/71 | LOSS: 4.685297961004197e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 57/71 | LOSS: 4.670270582523542e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 58/71 | LOSS: 4.677620975182775e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 59/71 | LOSS: 4.681031153571288e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 60/71 | LOSS: 4.692292857159417e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 61/71 | LOSS: 4.7081593507872665e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 62/71 | LOSS: 4.7261050678484714e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 63/71 | LOSS: 4.718739873510458e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 64/71 | LOSS: 4.7049960742767255e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 65/71 | LOSS: 4.710846992979483e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 66/71 | LOSS: 4.694502273033227e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 67/71 | LOSS: 4.716366240298906e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 68/71 | LOSS: 4.70719690555057e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 69/71 | LOSS: 4.733638037838058e-06\n",
      "TRAIN: EPOCH 487/1000 | BATCH 70/71 | LOSS: 4.71274001041897e-06\n",
      "VAL: EPOCH 487/1000 | BATCH 0/8 | LOSS: 5.916529062233167e-06\n",
      "VAL: EPOCH 487/1000 | BATCH 1/8 | LOSS: 5.872075689694611e-06\n",
      "VAL: EPOCH 487/1000 | BATCH 2/8 | LOSS: 5.9760989946274394e-06\n",
      "VAL: EPOCH 487/1000 | BATCH 3/8 | LOSS: 5.842503469466465e-06\n",
      "VAL: EPOCH 487/1000 | BATCH 4/8 | LOSS: 5.911254356760765e-06\n",
      "VAL: EPOCH 487/1000 | BATCH 5/8 | LOSS: 5.731786510902263e-06\n",
      "VAL: EPOCH 487/1000 | BATCH 6/8 | LOSS: 5.6810749161481255e-06\n",
      "VAL: EPOCH 487/1000 | BATCH 7/8 | LOSS: 5.584603229635832e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 0/71 | LOSS: 3.912256488547428e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 1/71 | LOSS: 4.111916496185586e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 2/71 | LOSS: 3.851038854918443e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 3/71 | LOSS: 3.715232651302358e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 4/71 | LOSS: 3.94830021832604e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 5/71 | LOSS: 4.012397160598387e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 6/71 | LOSS: 4.153094583411335e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 7/71 | LOSS: 4.305830657358456e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 8/71 | LOSS: 4.25181411224508e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 9/71 | LOSS: 4.362082381703658e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 10/71 | LOSS: 4.338790941718881e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 11/71 | LOSS: 4.3245939499077695e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 12/71 | LOSS: 4.356706330537026e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 13/71 | LOSS: 4.395072957257591e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 14/71 | LOSS: 4.460121544980211e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 15/71 | LOSS: 4.48950305553808e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 16/71 | LOSS: 4.467686481160961e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 17/71 | LOSS: 4.4953925074272165e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 18/71 | LOSS: 4.669322678589531e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 19/71 | LOSS: 4.644593718694523e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 20/71 | LOSS: 4.747699178898031e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 21/71 | LOSS: 4.765808191884637e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 22/71 | LOSS: 4.844664604404344e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 23/71 | LOSS: 4.7513216732871415e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 24/71 | LOSS: 4.741332168123335e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 25/71 | LOSS: 4.730669922117579e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 26/71 | LOSS: 4.717587663199757e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 27/71 | LOSS: 4.734247933616384e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 28/71 | LOSS: 4.719173744135709e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 29/71 | LOSS: 4.760997709733298e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 30/71 | LOSS: 4.760511369421428e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 31/71 | LOSS: 4.764843858140466e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 32/71 | LOSS: 4.819556214429075e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 33/71 | LOSS: 4.841561850726601e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 34/71 | LOSS: 4.8247056903554555e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 35/71 | LOSS: 4.806912247406419e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 36/71 | LOSS: 4.7970270254547755e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 37/71 | LOSS: 4.7712018517574365e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 38/71 | LOSS: 4.7303375047191275e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 39/71 | LOSS: 4.778122877269197e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 40/71 | LOSS: 4.792770296927945e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 41/71 | LOSS: 4.782945888152989e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 42/71 | LOSS: 4.777577889272348e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 43/71 | LOSS: 4.74751816041548e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 44/71 | LOSS: 4.73354613051116e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 45/71 | LOSS: 4.712287635732577e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 46/71 | LOSS: 4.687948666422868e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 47/71 | LOSS: 4.6649388375878216e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 48/71 | LOSS: 4.645406334577497e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 49/71 | LOSS: 4.6447449449260604e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 50/71 | LOSS: 4.63167939236811e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 51/71 | LOSS: 4.643861455651159e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 52/71 | LOSS: 4.67321646678067e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 53/71 | LOSS: 4.676406707336288e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 54/71 | LOSS: 4.678887245807511e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 55/71 | LOSS: 4.691291520332795e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 56/71 | LOSS: 4.692079436720457e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 57/71 | LOSS: 4.704432806898569e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 58/71 | LOSS: 4.714549415932536e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 59/71 | LOSS: 4.706553716005146e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 60/71 | LOSS: 4.712487631624128e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 61/71 | LOSS: 4.7142830776318724e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 62/71 | LOSS: 4.699285494690042e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 63/71 | LOSS: 4.686231992678813e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 64/71 | LOSS: 4.682936983394589e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 65/71 | LOSS: 4.6724552131501804e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 66/71 | LOSS: 4.653742772502279e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 67/71 | LOSS: 4.662001354999495e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 68/71 | LOSS: 4.663104246519388e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 69/71 | LOSS: 4.645939095228512e-06\n",
      "TRAIN: EPOCH 488/1000 | BATCH 70/71 | LOSS: 4.6371279674697776e-06\n",
      "VAL: EPOCH 488/1000 | BATCH 0/8 | LOSS: 4.4678226913674735e-06\n",
      "VAL: EPOCH 488/1000 | BATCH 1/8 | LOSS: 4.702320666183368e-06\n",
      "VAL: EPOCH 488/1000 | BATCH 2/8 | LOSS: 4.65023534464611e-06\n",
      "VAL: EPOCH 488/1000 | BATCH 3/8 | LOSS: 4.6929114887461765e-06\n",
      "VAL: EPOCH 488/1000 | BATCH 4/8 | LOSS: 4.620994150172919e-06\n",
      "VAL: EPOCH 488/1000 | BATCH 5/8 | LOSS: 4.527492516596491e-06\n",
      "VAL: EPOCH 488/1000 | BATCH 6/8 | LOSS: 4.3570316847763024e-06\n",
      "VAL: EPOCH 488/1000 | BATCH 7/8 | LOSS: 4.162299262588931e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 0/71 | LOSS: 4.235287178744329e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 1/71 | LOSS: 4.035867277707439e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 2/71 | LOSS: 4.189781824000723e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 3/71 | LOSS: 4.18652132339048e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 4/71 | LOSS: 4.216634170006728e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 5/71 | LOSS: 4.286352198808648e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 6/71 | LOSS: 4.343444028823537e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 7/71 | LOSS: 4.341319083778217e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 8/71 | LOSS: 4.4986280853562575e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 9/71 | LOSS: 4.421626067596663e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 10/71 | LOSS: 4.46162653133797e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 11/71 | LOSS: 4.5045946042894984e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 12/71 | LOSS: 4.588045560292532e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 13/71 | LOSS: 4.56075695508064e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 14/71 | LOSS: 4.853785640079877e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 15/71 | LOSS: 4.877196275288043e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 16/71 | LOSS: 4.869815831946599e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 17/71 | LOSS: 4.862348747438874e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 18/71 | LOSS: 4.863418792554416e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 19/71 | LOSS: 4.821733898552338e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 20/71 | LOSS: 4.880469305135193e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 21/71 | LOSS: 4.886573451172311e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 22/71 | LOSS: 4.869589375907115e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 23/71 | LOSS: 4.861396926495824e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 24/71 | LOSS: 4.8266629983118035e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 25/71 | LOSS: 4.775431998146255e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 26/71 | LOSS: 4.7616368832408355e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 27/71 | LOSS: 4.774650010728822e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 28/71 | LOSS: 4.770485929839197e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 29/71 | LOSS: 4.791244092909134e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 30/71 | LOSS: 4.795410852180325e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 31/71 | LOSS: 4.762636159227895e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 32/71 | LOSS: 4.790686165451365e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 33/71 | LOSS: 4.741281954653352e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 34/71 | LOSS: 4.711341898655519e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 35/71 | LOSS: 4.694370419302787e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 36/71 | LOSS: 4.682219438305961e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 37/71 | LOSS: 4.656949224562068e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 38/71 | LOSS: 4.661244352414192e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 39/71 | LOSS: 4.662364614205217e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 40/71 | LOSS: 4.674884338805342e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 41/71 | LOSS: 4.653936923118419e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 42/71 | LOSS: 4.692299372373084e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 43/71 | LOSS: 4.675144685444883e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 44/71 | LOSS: 4.678901334247914e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 45/71 | LOSS: 4.6545516961091575e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 46/71 | LOSS: 4.643310008527515e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 47/71 | LOSS: 4.626723793421661e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 48/71 | LOSS: 4.609739961575097e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 49/71 | LOSS: 4.603859902090335e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 50/71 | LOSS: 4.603190683494676e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 51/71 | LOSS: 4.599686047430833e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 52/71 | LOSS: 4.5994476003438906e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 53/71 | LOSS: 4.5929507576955884e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 54/71 | LOSS: 4.579486320918394e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 55/71 | LOSS: 4.588299885556678e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 56/71 | LOSS: 4.578416931971126e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 57/71 | LOSS: 4.56886349529041e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 58/71 | LOSS: 4.567568297161261e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 59/71 | LOSS: 4.561976822969882e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 60/71 | LOSS: 4.537779954053341e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 61/71 | LOSS: 4.520690873916113e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 62/71 | LOSS: 4.523514188270109e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 63/71 | LOSS: 4.534405444900358e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 64/71 | LOSS: 4.538921512693588e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 65/71 | LOSS: 4.54156693677126e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 66/71 | LOSS: 4.542279001141841e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 67/71 | LOSS: 4.5487951350718074e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 68/71 | LOSS: 4.5477099741918314e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 69/71 | LOSS: 4.5444518783759645e-06\n",
      "TRAIN: EPOCH 489/1000 | BATCH 70/71 | LOSS: 4.500678803939901e-06\n",
      "VAL: EPOCH 489/1000 | BATCH 0/8 | LOSS: 5.720427907363046e-06\n",
      "VAL: EPOCH 489/1000 | BATCH 1/8 | LOSS: 5.49924925508094e-06\n",
      "VAL: EPOCH 489/1000 | BATCH 2/8 | LOSS: 5.474687895912211e-06\n",
      "VAL: EPOCH 489/1000 | BATCH 3/8 | LOSS: 5.4608669870503945e-06\n",
      "VAL: EPOCH 489/1000 | BATCH 4/8 | LOSS: 5.373171279643429e-06\n",
      "VAL: EPOCH 489/1000 | BATCH 5/8 | LOSS: 5.287241341041711e-06\n",
      "VAL: EPOCH 489/1000 | BATCH 6/8 | LOSS: 5.143177856682866e-06\n",
      "VAL: EPOCH 489/1000 | BATCH 7/8 | LOSS: 4.96500919666687e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 0/71 | LOSS: 5.7223123803851195e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 1/71 | LOSS: 5.666207925969502e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 2/71 | LOSS: 5.41201037170443e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 3/71 | LOSS: 4.96915021130917e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 4/71 | LOSS: 4.977266553396476e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 5/71 | LOSS: 5.0239425869828365e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 6/71 | LOSS: 5.038117868545149e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 7/71 | LOSS: 4.997310298904267e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 8/71 | LOSS: 4.937145680135775e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 9/71 | LOSS: 4.7979320925151114e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 10/71 | LOSS: 4.9171111103615575e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 11/71 | LOSS: 4.864938375703787e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 12/71 | LOSS: 4.872465401953937e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 13/71 | LOSS: 4.8921687364002405e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 14/71 | LOSS: 4.8387167377465325e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 15/71 | LOSS: 4.888307486794474e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 16/71 | LOSS: 4.868220872487975e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 17/71 | LOSS: 4.811592172397165e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 18/71 | LOSS: 4.76946466302539e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 19/71 | LOSS: 4.7205114924508965e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 20/71 | LOSS: 4.728529845687306e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 21/71 | LOSS: 4.710007488029078e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 22/71 | LOSS: 4.752529963772521e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 23/71 | LOSS: 4.759898502015858e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 24/71 | LOSS: 4.707979314844124e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 25/71 | LOSS: 4.678492936923599e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 26/71 | LOSS: 4.682902541659402e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 27/71 | LOSS: 4.700304300188561e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 28/71 | LOSS: 4.702163080545343e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 29/71 | LOSS: 4.673622667420811e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 30/71 | LOSS: 4.689077095756675e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 31/71 | LOSS: 4.6550385732757604e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 32/71 | LOSS: 4.691493655407227e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 33/71 | LOSS: 4.701437486640798e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 34/71 | LOSS: 4.695035782528326e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 35/71 | LOSS: 4.6728681266156654e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 36/71 | LOSS: 4.673137267230699e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 37/71 | LOSS: 4.6517995458902744e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 38/71 | LOSS: 4.62515370203712e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 39/71 | LOSS: 4.6201393672617995e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 40/71 | LOSS: 4.610676364236719e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 41/71 | LOSS: 4.594922957256217e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 42/71 | LOSS: 4.599779379341906e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 43/71 | LOSS: 4.6159969429027115e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 44/71 | LOSS: 4.599156793321729e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 45/71 | LOSS: 4.602548410334946e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 46/71 | LOSS: 4.5905134250441484e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 47/71 | LOSS: 4.582780808467153e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 48/71 | LOSS: 4.5704581800831024e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 49/71 | LOSS: 4.575532252601988e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 50/71 | LOSS: 4.574004989598168e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 51/71 | LOSS: 4.587538871174519e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 52/71 | LOSS: 4.561570243445118e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 53/71 | LOSS: 4.548648925501092e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 54/71 | LOSS: 4.542267441303755e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 55/71 | LOSS: 4.543484509765351e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 56/71 | LOSS: 4.547039675902884e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 57/71 | LOSS: 4.5484581427256766e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 58/71 | LOSS: 4.5484928019289065e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 59/71 | LOSS: 4.535009698732513e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 60/71 | LOSS: 4.535841895392609e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 61/71 | LOSS: 4.5434750217605e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 62/71 | LOSS: 4.525516144909373e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 63/71 | LOSS: 4.5397807504343746e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 64/71 | LOSS: 4.542734058514515e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 65/71 | LOSS: 4.531891518540347e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 66/71 | LOSS: 4.538031491202714e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 67/71 | LOSS: 4.528888054852252e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 68/71 | LOSS: 4.5210121328821424e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 69/71 | LOSS: 4.526811273665641e-06\n",
      "TRAIN: EPOCH 490/1000 | BATCH 70/71 | LOSS: 4.522935018363748e-06\n",
      "VAL: EPOCH 490/1000 | BATCH 0/8 | LOSS: 4.630821422324516e-06\n",
      "VAL: EPOCH 490/1000 | BATCH 1/8 | LOSS: 4.885497446593945e-06\n",
      "VAL: EPOCH 490/1000 | BATCH 2/8 | LOSS: 5.228946520219324e-06\n",
      "VAL: EPOCH 490/1000 | BATCH 3/8 | LOSS: 5.162294314686733e-06\n",
      "VAL: EPOCH 490/1000 | BATCH 4/8 | LOSS: 5.168609368411125e-06\n",
      "VAL: EPOCH 490/1000 | BATCH 5/8 | LOSS: 5.118693403953027e-06\n",
      "VAL: EPOCH 490/1000 | BATCH 6/8 | LOSS: 4.990457195422745e-06\n",
      "VAL: EPOCH 490/1000 | BATCH 7/8 | LOSS: 4.99147211030504e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 0/71 | LOSS: 4.315771548135672e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 1/71 | LOSS: 4.0201141473517055e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 2/71 | LOSS: 4.020424057671335e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 3/71 | LOSS: 4.090310085302917e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 4/71 | LOSS: 4.272127534932224e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 5/71 | LOSS: 4.239158973481001e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 6/71 | LOSS: 4.14258725608566e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 7/71 | LOSS: 4.2442942458365e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 8/71 | LOSS: 4.318202880110928e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 9/71 | LOSS: 4.4507131406135155e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 10/71 | LOSS: 4.3870700623797765e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 11/71 | LOSS: 4.470461855513956e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 12/71 | LOSS: 4.407735087415159e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 13/71 | LOSS: 4.508703812332117e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 14/71 | LOSS: 4.545677575151785e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 15/71 | LOSS: 4.582340650927108e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 16/71 | LOSS: 4.566708933934398e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 17/71 | LOSS: 4.5629069392008305e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 18/71 | LOSS: 4.510830721253449e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 19/71 | LOSS: 4.535579012099333e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 20/71 | LOSS: 4.536753955281234e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 21/71 | LOSS: 4.515277096702035e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 22/71 | LOSS: 4.483200150982315e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 23/71 | LOSS: 4.4379694278025754e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 24/71 | LOSS: 4.452042112461641e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 25/71 | LOSS: 4.434132375326254e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 26/71 | LOSS: 4.433583474868404e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 27/71 | LOSS: 4.464053607275023e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 28/71 | LOSS: 4.428066459099525e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 29/71 | LOSS: 4.460657320729903e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 30/71 | LOSS: 4.455273916215857e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 31/71 | LOSS: 4.422473729448484e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 32/71 | LOSS: 4.494729299756617e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 33/71 | LOSS: 4.531036244556804e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 34/71 | LOSS: 4.581182390341253e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 35/71 | LOSS: 4.572021572989292e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 36/71 | LOSS: 4.692595476208717e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 37/71 | LOSS: 4.712376715743678e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 38/71 | LOSS: 4.760343163303705e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 39/71 | LOSS: 4.810314129599646e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 40/71 | LOSS: 4.839567997428608e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 41/71 | LOSS: 4.8884858783605435e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 42/71 | LOSS: 4.940865136272488e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 43/71 | LOSS: 5.009412311871777e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 44/71 | LOSS: 4.995728224659817e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 45/71 | LOSS: 5.022792399121618e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 46/71 | LOSS: 5.017706134241691e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 47/71 | LOSS: 5.000462328060469e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 48/71 | LOSS: 5.019749091750745e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 49/71 | LOSS: 5.009814672121138e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 50/71 | LOSS: 5.097547143889767e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 51/71 | LOSS: 5.099854858004759e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 52/71 | LOSS: 5.120609960973731e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 53/71 | LOSS: 5.118612050494078e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 54/71 | LOSS: 5.158253841066405e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 55/71 | LOSS: 5.138971114807386e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 56/71 | LOSS: 5.134208115059505e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 57/71 | LOSS: 5.103730155371585e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 58/71 | LOSS: 5.117372951244144e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 59/71 | LOSS: 5.091708927314661e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 60/71 | LOSS: 5.08319782861914e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 61/71 | LOSS: 5.063935557905831e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 62/71 | LOSS: 5.040159186094007e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 63/71 | LOSS: 5.030040203024555e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 64/71 | LOSS: 5.01725850461718e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 65/71 | LOSS: 5.0037949573710314e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 66/71 | LOSS: 4.993945445513323e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 67/71 | LOSS: 4.984508687838945e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 68/71 | LOSS: 4.965777047866562e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 69/71 | LOSS: 4.9572255290643494e-06\n",
      "TRAIN: EPOCH 491/1000 | BATCH 70/71 | LOSS: 4.933839014663302e-06\n",
      "VAL: EPOCH 491/1000 | BATCH 0/8 | LOSS: 6.783804565202445e-06\n",
      "VAL: EPOCH 491/1000 | BATCH 1/8 | LOSS: 6.616796554226312e-06\n",
      "VAL: EPOCH 491/1000 | BATCH 2/8 | LOSS: 6.644485286718312e-06\n",
      "VAL: EPOCH 491/1000 | BATCH 3/8 | LOSS: 6.647589998465264e-06\n",
      "VAL: EPOCH 491/1000 | BATCH 4/8 | LOSS: 6.566537649632664e-06\n",
      "VAL: EPOCH 491/1000 | BATCH 5/8 | LOSS: 6.3000095451570814e-06\n",
      "VAL: EPOCH 491/1000 | BATCH 6/8 | LOSS: 6.094909362478315e-06\n",
      "VAL: EPOCH 491/1000 | BATCH 7/8 | LOSS: 5.854940013705345e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 0/71 | LOSS: 8.459435775876045e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 1/71 | LOSS: 6.495893785540829e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 2/71 | LOSS: 6.494087604854333e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 3/71 | LOSS: 6.425453761949029e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 4/71 | LOSS: 5.99946579313837e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 5/71 | LOSS: 5.635303220212033e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 6/71 | LOSS: 5.625226094707614e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 7/71 | LOSS: 5.421056528120971e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 8/71 | LOSS: 5.449994457901792e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 9/71 | LOSS: 5.270824431136134e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 10/71 | LOSS: 5.1175445192215685e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 11/71 | LOSS: 5.043246630975773e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 12/71 | LOSS: 5.049321543992846e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 13/71 | LOSS: 5.018604546031773e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 14/71 | LOSS: 4.897369293151617e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 15/71 | LOSS: 4.840414206341848e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 16/71 | LOSS: 4.8014215361249895e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 17/71 | LOSS: 4.736591348672745e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 18/71 | LOSS: 4.710963163521037e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 19/71 | LOSS: 4.623957784133381e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 20/71 | LOSS: 4.62384737364205e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 21/71 | LOSS: 4.651022560541275e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 22/71 | LOSS: 4.671374881581869e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 23/71 | LOSS: 4.663353990963515e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 24/71 | LOSS: 4.645876342692645e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 25/71 | LOSS: 4.675847584132078e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 26/71 | LOSS: 4.652373768799913e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 27/71 | LOSS: 4.629486949982363e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 28/71 | LOSS: 4.611872774493632e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 29/71 | LOSS: 4.601517048286041e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 30/71 | LOSS: 4.586397637702307e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 31/71 | LOSS: 4.626059606493982e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 32/71 | LOSS: 4.620233787797836e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 33/71 | LOSS: 4.606780962871305e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 34/71 | LOSS: 4.5822885242939395e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 35/71 | LOSS: 4.6206059525704604e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 36/71 | LOSS: 4.664109297123473e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 37/71 | LOSS: 4.651343093236822e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 38/71 | LOSS: 4.713241581227451e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 39/71 | LOSS: 4.706823119704495e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 40/71 | LOSS: 4.699107249485323e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 41/71 | LOSS: 4.701301209241224e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 42/71 | LOSS: 4.713483989224526e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 43/71 | LOSS: 4.733862273166199e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 44/71 | LOSS: 4.7259948890617425e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 45/71 | LOSS: 4.737323716535289e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 46/71 | LOSS: 4.7057636986713254e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 47/71 | LOSS: 4.69404331700692e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 48/71 | LOSS: 4.705789372249511e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 49/71 | LOSS: 4.6957459471741454e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 50/71 | LOSS: 4.695212229468298e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 51/71 | LOSS: 4.684028147914012e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 52/71 | LOSS: 4.659172551534007e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 53/71 | LOSS: 4.670286645907456e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 54/71 | LOSS: 4.655481155747442e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 55/71 | LOSS: 4.653491073115999e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 56/71 | LOSS: 4.679211063998273e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 57/71 | LOSS: 4.687132057620931e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 58/71 | LOSS: 4.664939720989146e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 59/71 | LOSS: 4.650360021211479e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 60/71 | LOSS: 4.661243595170968e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 61/71 | LOSS: 4.646390556034373e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 62/71 | LOSS: 4.66998434358319e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 63/71 | LOSS: 4.660449583582249e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 64/71 | LOSS: 4.670216891939233e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 65/71 | LOSS: 4.680447862924528e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 66/71 | LOSS: 4.678106839193462e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 67/71 | LOSS: 4.70005022634723e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 68/71 | LOSS: 4.710451499388158e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 69/71 | LOSS: 4.721201886630817e-06\n",
      "TRAIN: EPOCH 492/1000 | BATCH 70/71 | LOSS: 4.711430648685692e-06\n",
      "VAL: EPOCH 492/1000 | BATCH 0/8 | LOSS: 6.41730639472371e-06\n",
      "VAL: EPOCH 492/1000 | BATCH 1/8 | LOSS: 6.345508609229e-06\n",
      "VAL: EPOCH 492/1000 | BATCH 2/8 | LOSS: 6.382231428384936e-06\n",
      "VAL: EPOCH 492/1000 | BATCH 3/8 | LOSS: 6.234381544345524e-06\n",
      "VAL: EPOCH 492/1000 | BATCH 4/8 | LOSS: 6.262020542635583e-06\n",
      "VAL: EPOCH 492/1000 | BATCH 5/8 | LOSS: 6.168314484966686e-06\n",
      "VAL: EPOCH 492/1000 | BATCH 6/8 | LOSS: 6.1653729192455235e-06\n",
      "VAL: EPOCH 492/1000 | BATCH 7/8 | LOSS: 6.123346793174278e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 0/71 | LOSS: 5.864816557732411e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 1/71 | LOSS: 4.728372687168303e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 2/71 | LOSS: 4.585543896003704e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 3/71 | LOSS: 4.661163416130876e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 4/71 | LOSS: 4.781290317623643e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 5/71 | LOSS: 4.649531244164488e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 6/71 | LOSS: 4.7312599105810345e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 7/71 | LOSS: 4.861285447077535e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 8/71 | LOSS: 4.748042960677089e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 9/71 | LOSS: 4.740441909234505e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 10/71 | LOSS: 4.8555512793510305e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 11/71 | LOSS: 4.8530725583380745e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 12/71 | LOSS: 4.811133679210728e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 13/71 | LOSS: 4.801488492895649e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 14/71 | LOSS: 4.790324904509665e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 15/71 | LOSS: 4.781783104590431e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 16/71 | LOSS: 4.809340279119651e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 17/71 | LOSS: 4.7516706318775605e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 18/71 | LOSS: 4.774321544238391e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 19/71 | LOSS: 4.790203502125223e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 20/71 | LOSS: 4.798366760604993e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 21/71 | LOSS: 4.835978477951192e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 22/71 | LOSS: 4.851262929150835e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 23/71 | LOSS: 4.808875947522513e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 24/71 | LOSS: 4.9076702453021426e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 25/71 | LOSS: 4.895213053784503e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 26/71 | LOSS: 4.915253864762943e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 27/71 | LOSS: 4.908802744856595e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 28/71 | LOSS: 4.929969424571529e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 29/71 | LOSS: 4.868961589939621e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 30/71 | LOSS: 4.813562364072441e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 31/71 | LOSS: 4.779240796892736e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 32/71 | LOSS: 4.732879240305699e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 33/71 | LOSS: 4.7147107660053094e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 34/71 | LOSS: 4.735241106703013e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 35/71 | LOSS: 4.725072598274791e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 36/71 | LOSS: 4.68956433930568e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 37/71 | LOSS: 4.66286342328347e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 38/71 | LOSS: 4.658524047931095e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 39/71 | LOSS: 4.63705305833173e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 40/71 | LOSS: 4.635703506698142e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 41/71 | LOSS: 4.6207317641996525e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 42/71 | LOSS: 4.604131152857092e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 43/71 | LOSS: 4.622943797792025e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 44/71 | LOSS: 4.611058905841977e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 45/71 | LOSS: 4.619457340397829e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 46/71 | LOSS: 4.614936299131319e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 47/71 | LOSS: 4.643603872978019e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 48/71 | LOSS: 4.631811096408697e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 49/71 | LOSS: 4.640748688871099e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 50/71 | LOSS: 4.6671456108443265e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 51/71 | LOSS: 4.6537379734428005e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 52/71 | LOSS: 4.661545762126411e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 53/71 | LOSS: 4.666500221564665e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 54/71 | LOSS: 4.669388226830052e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 55/71 | LOSS: 4.670035337994705e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 56/71 | LOSS: 4.6961879753214085e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 57/71 | LOSS: 4.683971496888262e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 58/71 | LOSS: 4.668499132556894e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 59/71 | LOSS: 4.666852362333884e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 60/71 | LOSS: 4.661449010513263e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 61/71 | LOSS: 4.666647427139621e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 62/71 | LOSS: 4.684761889288821e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 63/71 | LOSS: 4.679827259934655e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 64/71 | LOSS: 4.685032976899842e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 65/71 | LOSS: 4.685029900677116e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 66/71 | LOSS: 4.672679894488894e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 67/71 | LOSS: 4.689435182653517e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 68/71 | LOSS: 4.686823924361227e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 69/71 | LOSS: 4.685182333170295e-06\n",
      "TRAIN: EPOCH 493/1000 | BATCH 70/71 | LOSS: 4.683695803104341e-06\n",
      "VAL: EPOCH 493/1000 | BATCH 0/8 | LOSS: 6.3407014749827795e-06\n",
      "VAL: EPOCH 493/1000 | BATCH 1/8 | LOSS: 6.431592737499159e-06\n",
      "VAL: EPOCH 493/1000 | BATCH 2/8 | LOSS: 6.442967787734233e-06\n",
      "VAL: EPOCH 493/1000 | BATCH 3/8 | LOSS: 6.296100536928861e-06\n",
      "VAL: EPOCH 493/1000 | BATCH 4/8 | LOSS: 6.360572933772346e-06\n",
      "VAL: EPOCH 493/1000 | BATCH 5/8 | LOSS: 6.116510045709826e-06\n",
      "VAL: EPOCH 493/1000 | BATCH 6/8 | LOSS: 6.114151217064188e-06\n",
      "VAL: EPOCH 493/1000 | BATCH 7/8 | LOSS: 5.988558768876828e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 0/71 | LOSS: 4.826034910365706e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 1/71 | LOSS: 4.726419092548895e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 2/71 | LOSS: 4.852884709786546e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 3/71 | LOSS: 4.783218969350855e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 4/71 | LOSS: 5.078664707980352e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 5/71 | LOSS: 4.8876837202745565e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 6/71 | LOSS: 4.809123246169682e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 7/71 | LOSS: 4.978564163593546e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 8/71 | LOSS: 5.025311212294683e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 9/71 | LOSS: 5.169479391042842e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 10/71 | LOSS: 5.229794706710064e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 11/71 | LOSS: 5.428478061730857e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 12/71 | LOSS: 5.398038760736549e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 13/71 | LOSS: 5.68439093383079e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 14/71 | LOSS: 5.6189624956459735e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 15/71 | LOSS: 5.774337239472516e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 16/71 | LOSS: 5.811498527584584e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 17/71 | LOSS: 5.701802137991763e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 18/71 | LOSS: 6.041723161998609e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 19/71 | LOSS: 5.945185898781347e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 20/71 | LOSS: 6.154376478662016e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 21/71 | LOSS: 6.175797373295857e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 22/71 | LOSS: 6.208210992475e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 23/71 | LOSS: 6.278957926042494e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 24/71 | LOSS: 6.305599526967853e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 25/71 | LOSS: 6.468745666027714e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 26/71 | LOSS: 6.481023791383012e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 27/71 | LOSS: 6.573658554641172e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 28/71 | LOSS: 6.471970632917184e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 29/71 | LOSS: 6.4413123709528005e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 30/71 | LOSS: 6.434439323892283e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 31/71 | LOSS: 6.354909956485244e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 32/71 | LOSS: 6.309676474770778e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 33/71 | LOSS: 6.311672316716046e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 34/71 | LOSS: 6.222299387965385e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 35/71 | LOSS: 6.213544363870622e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 36/71 | LOSS: 6.145145494588792e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 37/71 | LOSS: 6.111694472709037e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 38/71 | LOSS: 6.057311397037269e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 39/71 | LOSS: 6.025795113373533e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 40/71 | LOSS: 5.981446847505228e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 41/71 | LOSS: 5.943170830981953e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 42/71 | LOSS: 5.950193538008661e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 43/71 | LOSS: 5.921706241968588e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 44/71 | LOSS: 5.893734755267764e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 45/71 | LOSS: 5.869271406125968e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 46/71 | LOSS: 5.843828395224224e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 47/71 | LOSS: 5.795017164208123e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 48/71 | LOSS: 5.773852187618958e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 49/71 | LOSS: 5.744034638155426e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 50/71 | LOSS: 5.724873388174399e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 51/71 | LOSS: 5.709442475038556e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 52/71 | LOSS: 5.671718550077475e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 53/71 | LOSS: 5.635499009999707e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 54/71 | LOSS: 5.612255976798637e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 55/71 | LOSS: 5.589065515161045e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 56/71 | LOSS: 5.571768505225855e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 57/71 | LOSS: 5.547333585284299e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 58/71 | LOSS: 5.532678892835728e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 59/71 | LOSS: 5.509082692848703e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 60/71 | LOSS: 5.501711536796437e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 61/71 | LOSS: 5.488774622937208e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 62/71 | LOSS: 5.471712899042546e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 63/71 | LOSS: 5.456442938367445e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 64/71 | LOSS: 5.464567672019117e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 65/71 | LOSS: 5.470457212499186e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 66/71 | LOSS: 5.439817981036256e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 67/71 | LOSS: 5.428776332524542e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 68/71 | LOSS: 5.4031682444421705e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 69/71 | LOSS: 5.389583962564107e-06\n",
      "TRAIN: EPOCH 494/1000 | BATCH 70/71 | LOSS: 5.3896297673341445e-06\n",
      "VAL: EPOCH 494/1000 | BATCH 0/8 | LOSS: 5.335844889486907e-06\n",
      "VAL: EPOCH 494/1000 | BATCH 1/8 | LOSS: 5.401524731496465e-06\n",
      "VAL: EPOCH 494/1000 | BATCH 2/8 | LOSS: 5.5163962618583655e-06\n",
      "VAL: EPOCH 494/1000 | BATCH 3/8 | LOSS: 5.417127340479055e-06\n",
      "VAL: EPOCH 494/1000 | BATCH 4/8 | LOSS: 5.424760638561566e-06\n",
      "VAL: EPOCH 494/1000 | BATCH 5/8 | LOSS: 5.41660665476229e-06\n",
      "VAL: EPOCH 494/1000 | BATCH 6/8 | LOSS: 5.310461606963404e-06\n",
      "VAL: EPOCH 494/1000 | BATCH 7/8 | LOSS: 5.319861770658463e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 0/71 | LOSS: 6.155110440886347e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 1/71 | LOSS: 5.736284265367431e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 2/71 | LOSS: 5.45936897348535e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 3/71 | LOSS: 5.075648232377716e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 4/71 | LOSS: 5.255770975054474e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 5/71 | LOSS: 5.12773408445355e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 6/71 | LOSS: 5.192079242988257e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 7/71 | LOSS: 5.252158700841392e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 8/71 | LOSS: 5.5083086307503336e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 9/71 | LOSS: 5.294907987263287e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 10/71 | LOSS: 5.268740220675351e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 11/71 | LOSS: 5.267035514104161e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 12/71 | LOSS: 5.194834207381623e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 13/71 | LOSS: 5.194800646027683e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 14/71 | LOSS: 5.1625423414710285e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 15/71 | LOSS: 5.172479518478212e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 16/71 | LOSS: 5.156062316553503e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 17/71 | LOSS: 5.213772485351203e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 18/71 | LOSS: 5.247264509403306e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 19/71 | LOSS: 5.207694812270347e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 20/71 | LOSS: 5.271177845618998e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 21/71 | LOSS: 5.205356025974404e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 22/71 | LOSS: 5.215200891815703e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 23/71 | LOSS: 5.291312826708842e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 24/71 | LOSS: 5.261427340883529e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 25/71 | LOSS: 5.24078431086449e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 26/71 | LOSS: 5.1953452721473125e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 27/71 | LOSS: 5.196756140841379e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 28/71 | LOSS: 5.195214114415836e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 29/71 | LOSS: 5.194651779068711e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 30/71 | LOSS: 5.201509632188977e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 31/71 | LOSS: 5.231180381315426e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 32/71 | LOSS: 5.222392329332334e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 33/71 | LOSS: 5.189645823039053e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 34/71 | LOSS: 5.2290676908991635e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 35/71 | LOSS: 5.180681941712666e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 36/71 | LOSS: 5.192008677260821e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 37/71 | LOSS: 5.15380641948133e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 38/71 | LOSS: 5.1744364158460085e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 39/71 | LOSS: 5.1430726045964544e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 40/71 | LOSS: 5.140315390856747e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 41/71 | LOSS: 5.153285877938851e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 42/71 | LOSS: 5.157164412932701e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 43/71 | LOSS: 5.150391132247023e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 44/71 | LOSS: 5.119969728184515e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 45/71 | LOSS: 5.110774850436428e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 46/71 | LOSS: 5.104651892636073e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 47/71 | LOSS: 5.078348204771525e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 48/71 | LOSS: 5.067009443355717e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 49/71 | LOSS: 5.05081847677502e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 50/71 | LOSS: 5.040427104890399e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 51/71 | LOSS: 5.065200005342026e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 52/71 | LOSS: 5.054277998392793e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 53/71 | LOSS: 5.061705657546419e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 54/71 | LOSS: 5.049668053206088e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 55/71 | LOSS: 5.059635388501452e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 56/71 | LOSS: 5.084983972106752e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 57/71 | LOSS: 5.085803066102581e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 58/71 | LOSS: 5.080082096124682e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 59/71 | LOSS: 5.068893350805107e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 60/71 | LOSS: 5.065089826381214e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 61/71 | LOSS: 5.054396909497061e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 62/71 | LOSS: 5.052370011432527e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 63/71 | LOSS: 5.059557810938031e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 64/71 | LOSS: 5.056507538588798e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 65/71 | LOSS: 5.062682727398307e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 66/71 | LOSS: 5.06567712974758e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 67/71 | LOSS: 5.062923007164286e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 68/71 | LOSS: 5.0533683077156786e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 69/71 | LOSS: 5.0694180669909525e-06\n",
      "TRAIN: EPOCH 495/1000 | BATCH 70/71 | LOSS: 5.08794497863433e-06\n",
      "VAL: EPOCH 495/1000 | BATCH 0/8 | LOSS: 6.340828349493677e-06\n",
      "VAL: EPOCH 495/1000 | BATCH 1/8 | LOSS: 6.607563364013913e-06\n",
      "VAL: EPOCH 495/1000 | BATCH 2/8 | LOSS: 6.911830799557113e-06\n",
      "VAL: EPOCH 495/1000 | BATCH 3/8 | LOSS: 6.896802233313792e-06\n",
      "VAL: EPOCH 495/1000 | BATCH 4/8 | LOSS: 6.841720278316643e-06\n",
      "VAL: EPOCH 495/1000 | BATCH 5/8 | LOSS: 6.792135688253135e-06\n",
      "VAL: EPOCH 495/1000 | BATCH 6/8 | LOSS: 6.701804263034969e-06\n",
      "VAL: EPOCH 495/1000 | BATCH 7/8 | LOSS: 6.715105428156676e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 0/71 | LOSS: 7.677756002522074e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 1/71 | LOSS: 6.418168368327315e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 2/71 | LOSS: 6.2884707100844634e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 3/71 | LOSS: 6.098126505094115e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 4/71 | LOSS: 5.810122183902422e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 5/71 | LOSS: 5.542585995499394e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 6/71 | LOSS: 5.769058394175122e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 7/71 | LOSS: 5.754252185852238e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 8/71 | LOSS: 5.439666337123425e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 9/71 | LOSS: 5.436986975837499e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 10/71 | LOSS: 5.3466076017436785e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 11/71 | LOSS: 5.358566947203751e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 12/71 | LOSS: 5.216920813976545e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 13/71 | LOSS: 5.29124588410923e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 14/71 | LOSS: 5.200959776630043e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 15/71 | LOSS: 5.127809643568071e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 16/71 | LOSS: 5.154926023297937e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 17/71 | LOSS: 5.108032900756774e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 18/71 | LOSS: 5.086256123831845e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 19/71 | LOSS: 5.059429952325445e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 20/71 | LOSS: 4.978393841762833e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 21/71 | LOSS: 4.980829534030239e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 22/71 | LOSS: 4.9257929652452255e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 23/71 | LOSS: 4.919410126073369e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 24/71 | LOSS: 4.939297732562409e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 25/71 | LOSS: 4.985419632948134e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 26/71 | LOSS: 4.9099585423930506e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 27/71 | LOSS: 4.891519592677339e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 28/71 | LOSS: 4.866426236511525e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 29/71 | LOSS: 4.886188063816614e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 30/71 | LOSS: 4.863243768869001e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 31/71 | LOSS: 4.877793720936552e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 32/71 | LOSS: 4.9514142602040945e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 33/71 | LOSS: 4.935774012772206e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 34/71 | LOSS: 4.923024939021811e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 35/71 | LOSS: 4.906057881726156e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 36/71 | LOSS: 4.905159726518291e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 37/71 | LOSS: 4.905307336182312e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 38/71 | LOSS: 4.879954204648712e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 39/71 | LOSS: 4.887870301217845e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 40/71 | LOSS: 4.884766279402112e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 41/71 | LOSS: 4.869231074302488e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 42/71 | LOSS: 4.8856170239961886e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 43/71 | LOSS: 4.90122702827152e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 44/71 | LOSS: 4.94495392457692e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 45/71 | LOSS: 4.9559577558719194e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 46/71 | LOSS: 4.967108979860526e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 47/71 | LOSS: 4.993872697885611e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 48/71 | LOSS: 5.0038099094048704e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 49/71 | LOSS: 4.983297626495186e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 50/71 | LOSS: 4.985082025051074e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 51/71 | LOSS: 5.038291144151467e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 52/71 | LOSS: 5.02325762235554e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 53/71 | LOSS: 5.057429061876233e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 54/71 | LOSS: 5.099886539028375e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 55/71 | LOSS: 5.072444387061685e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 56/71 | LOSS: 5.096512543451881e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 57/71 | LOSS: 5.181222302875038e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 58/71 | LOSS: 5.203247333079393e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 59/71 | LOSS: 5.2434218522042405e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 60/71 | LOSS: 5.293504455784998e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 61/71 | LOSS: 5.279992383440268e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 62/71 | LOSS: 5.304315216505082e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 63/71 | LOSS: 5.292627040631714e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 64/71 | LOSS: 5.290928888476068e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 65/71 | LOSS: 5.293323741887324e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 66/71 | LOSS: 5.288284000926069e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 67/71 | LOSS: 5.275749334915043e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 68/71 | LOSS: 5.298333838294164e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 69/71 | LOSS: 5.288165445043497e-06\n",
      "TRAIN: EPOCH 496/1000 | BATCH 70/71 | LOSS: 5.261788389912575e-06\n",
      "VAL: EPOCH 496/1000 | BATCH 0/8 | LOSS: 5.350636911316542e-06\n",
      "VAL: EPOCH 496/1000 | BATCH 1/8 | LOSS: 6.035392516423599e-06\n",
      "VAL: EPOCH 496/1000 | BATCH 2/8 | LOSS: 6.223855052667204e-06\n",
      "VAL: EPOCH 496/1000 | BATCH 3/8 | LOSS: 6.517835231534264e-06\n",
      "VAL: EPOCH 496/1000 | BATCH 4/8 | LOSS: 6.48380537313642e-06\n",
      "VAL: EPOCH 496/1000 | BATCH 5/8 | LOSS: 6.550573137549994e-06\n",
      "VAL: EPOCH 496/1000 | BATCH 6/8 | LOSS: 6.479112471424742e-06\n",
      "VAL: EPOCH 496/1000 | BATCH 7/8 | LOSS: 6.3267040673054e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 0/71 | LOSS: 5.678652996721212e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 1/71 | LOSS: 5.480359732246143e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 2/71 | LOSS: 6.013352958689211e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 3/71 | LOSS: 5.594732670033409e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 4/71 | LOSS: 5.73464494664222e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 5/71 | LOSS: 5.6311449346443015e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 6/71 | LOSS: 5.8644293078811775e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 7/71 | LOSS: 5.659865053075919e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 8/71 | LOSS: 5.6019182718753454e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 9/71 | LOSS: 5.60009534638084e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 10/71 | LOSS: 5.611166629519588e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 11/71 | LOSS: 5.528606152438442e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 12/71 | LOSS: 5.464654735148472e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 13/71 | LOSS: 5.402315147356214e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 14/71 | LOSS: 5.4211376057840726e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 15/71 | LOSS: 5.31887117460883e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 16/71 | LOSS: 5.373920604303267e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 17/71 | LOSS: 5.486700059817748e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 18/71 | LOSS: 5.507066661218567e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 19/71 | LOSS: 5.4509151482307064e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 20/71 | LOSS: 5.537328986195332e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 21/71 | LOSS: 5.550139276651449e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 22/71 | LOSS: 5.478122098477046e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 23/71 | LOSS: 5.417558279911343e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 24/71 | LOSS: 5.426262096079881e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 25/71 | LOSS: 5.4893660997270044e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 26/71 | LOSS: 5.444127905489209e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 27/71 | LOSS: 5.493475417292107e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 28/71 | LOSS: 5.480429653571578e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 29/71 | LOSS: 5.471681068532537e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 30/71 | LOSS: 5.447529085886931e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 31/71 | LOSS: 5.456638938028391e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 32/71 | LOSS: 5.39881733113196e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 33/71 | LOSS: 5.383352218172256e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 34/71 | LOSS: 5.3549005899965e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 35/71 | LOSS: 5.337617058861118e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 36/71 | LOSS: 5.2994566885899985e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 37/71 | LOSS: 5.295336504068952e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 38/71 | LOSS: 5.275835362156683e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 39/71 | LOSS: 5.281480065377764e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 40/71 | LOSS: 5.260052952641073e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 41/71 | LOSS: 5.276102883943254e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 42/71 | LOSS: 5.262361783871013e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 43/71 | LOSS: 5.259778447452845e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 44/71 | LOSS: 5.256432789085213e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 45/71 | LOSS: 5.247071271696768e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 46/71 | LOSS: 5.238369925443313e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 47/71 | LOSS: 5.2195028530377385e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 48/71 | LOSS: 5.18276541400582e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 49/71 | LOSS: 5.185654727029032e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 50/71 | LOSS: 5.193053258314167e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 51/71 | LOSS: 5.201080101030059e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 52/71 | LOSS: 5.194327668333536e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 53/71 | LOSS: 5.175742250718974e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 54/71 | LOSS: 5.222284818790451e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 55/71 | LOSS: 5.21522183914515e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 56/71 | LOSS: 5.197906559037061e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 57/71 | LOSS: 5.210668969711746e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 58/71 | LOSS: 5.243903206835473e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 59/71 | LOSS: 5.2277913027865e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 60/71 | LOSS: 5.218656951910816e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 61/71 | LOSS: 5.2052120530964704e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 62/71 | LOSS: 5.220367206170016e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 63/71 | LOSS: 5.1944109529245e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 64/71 | LOSS: 5.2048253914668185e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 65/71 | LOSS: 5.193674276211603e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 66/71 | LOSS: 5.189262069574968e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 67/71 | LOSS: 5.161023844212811e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 68/71 | LOSS: 5.142851945941178e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 69/71 | LOSS: 5.139593986314139e-06\n",
      "TRAIN: EPOCH 497/1000 | BATCH 70/71 | LOSS: 5.142880690726369e-06\n",
      "VAL: EPOCH 497/1000 | BATCH 0/8 | LOSS: 4.479712515603751e-06\n",
      "VAL: EPOCH 497/1000 | BATCH 1/8 | LOSS: 4.470979092729976e-06\n",
      "VAL: EPOCH 497/1000 | BATCH 2/8 | LOSS: 4.755534670645527e-06\n",
      "VAL: EPOCH 497/1000 | BATCH 3/8 | LOSS: 4.69914107270597e-06\n",
      "VAL: EPOCH 497/1000 | BATCH 4/8 | LOSS: 4.7016777898534204e-06\n",
      "VAL: EPOCH 497/1000 | BATCH 5/8 | LOSS: 4.668659433567275e-06\n",
      "VAL: EPOCH 497/1000 | BATCH 6/8 | LOSS: 4.566945660501785e-06\n",
      "VAL: EPOCH 497/1000 | BATCH 7/8 | LOSS: 4.566314828480245e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 0/71 | LOSS: 4.480281859287061e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 1/71 | LOSS: 4.9204252263734816e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 2/71 | LOSS: 5.091379686443058e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 3/71 | LOSS: 5.4447829143100535e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 4/71 | LOSS: 5.419412809715141e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 5/71 | LOSS: 5.276550761360947e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 6/71 | LOSS: 5.4947988376495364e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 7/71 | LOSS: 5.243687269285147e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 8/71 | LOSS: 5.325520659324765e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 9/71 | LOSS: 5.2286470236140305e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 10/71 | LOSS: 5.2045563114172015e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 11/71 | LOSS: 5.0901167014671955e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 12/71 | LOSS: 5.143603350286587e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 13/71 | LOSS: 5.003024885549426e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 14/71 | LOSS: 4.885989574177074e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 15/71 | LOSS: 4.875684808780534e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 16/71 | LOSS: 4.826718168260413e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 17/71 | LOSS: 4.854348478349695e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 18/71 | LOSS: 4.818883565492701e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 19/71 | LOSS: 4.80190504958955e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 20/71 | LOSS: 4.7365990992277925e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 21/71 | LOSS: 4.721011691799504e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 22/71 | LOSS: 4.7617795446611755e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 23/71 | LOSS: 4.70005971919818e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 24/71 | LOSS: 4.66673178380006e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 25/71 | LOSS: 4.685193901493151e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 26/71 | LOSS: 4.6723746352115456e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 27/71 | LOSS: 4.7047280012131e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 28/71 | LOSS: 4.677109339762206e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 29/71 | LOSS: 4.73513009637827e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 30/71 | LOSS: 4.7443660268101905e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 31/71 | LOSS: 4.743131725604144e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 32/71 | LOSS: 4.736831711356486e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 33/71 | LOSS: 4.696467472593427e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 34/71 | LOSS: 4.725658339209206e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 35/71 | LOSS: 4.707815725800578e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 36/71 | LOSS: 4.72934375711948e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 37/71 | LOSS: 4.706808382300216e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 38/71 | LOSS: 4.682176676140397e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 39/71 | LOSS: 4.687513552426026e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 40/71 | LOSS: 4.671122435799433e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 41/71 | LOSS: 4.690608621092993e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 42/71 | LOSS: 4.678097892856174e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 43/71 | LOSS: 4.68629971547588e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 44/71 | LOSS: 4.702402187225138e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 45/71 | LOSS: 4.699068753048461e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 46/71 | LOSS: 4.680387891834896e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 47/71 | LOSS: 4.677294209424569e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 48/71 | LOSS: 4.68029575781007e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 49/71 | LOSS: 4.658721841224178e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 50/71 | LOSS: 4.655140824694422e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 51/71 | LOSS: 4.643652499243427e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 52/71 | LOSS: 4.668216236513097e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 53/71 | LOSS: 4.672077563083332e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 54/71 | LOSS: 4.669729560191627e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 55/71 | LOSS: 4.670786491292997e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 56/71 | LOSS: 4.673301753810895e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 57/71 | LOSS: 4.6782701725407666e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 58/71 | LOSS: 4.667024818522573e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 59/71 | LOSS: 4.664734717607644e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 60/71 | LOSS: 4.6958318911433186e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 61/71 | LOSS: 4.669562852746646e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 62/71 | LOSS: 4.6660206068722e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 63/71 | LOSS: 4.67621420341402e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 64/71 | LOSS: 4.680524181402199e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 65/71 | LOSS: 4.669044432320692e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 66/71 | LOSS: 4.66712100517983e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 67/71 | LOSS: 4.680859453454853e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 68/71 | LOSS: 4.685624772073293e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 69/71 | LOSS: 4.667011775641835e-06\n",
      "TRAIN: EPOCH 498/1000 | BATCH 70/71 | LOSS: 4.663550473042947e-06\n",
      "VAL: EPOCH 498/1000 | BATCH 0/8 | LOSS: 4.655068551073782e-06\n",
      "VAL: EPOCH 498/1000 | BATCH 1/8 | LOSS: 4.553799954010174e-06\n",
      "VAL: EPOCH 498/1000 | BATCH 2/8 | LOSS: 4.700671221750478e-06\n",
      "VAL: EPOCH 498/1000 | BATCH 3/8 | LOSS: 4.700553745351499e-06\n",
      "VAL: EPOCH 498/1000 | BATCH 4/8 | LOSS: 4.580010681820568e-06\n",
      "VAL: EPOCH 498/1000 | BATCH 5/8 | LOSS: 4.504956981084736e-06\n",
      "VAL: EPOCH 498/1000 | BATCH 6/8 | LOSS: 4.340333686871288e-06\n",
      "VAL: EPOCH 498/1000 | BATCH 7/8 | LOSS: 4.176519155407732e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 0/71 | LOSS: 3.55141537511372e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 1/71 | LOSS: 5.293640242598485e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 2/71 | LOSS: 4.733133512975958e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 3/71 | LOSS: 5.335495416147751e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 4/71 | LOSS: 5.169151245354442e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 5/71 | LOSS: 5.1203213994692005e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 6/71 | LOSS: 5.235988477839523e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 7/71 | LOSS: 5.500580130046728e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 8/71 | LOSS: 5.816664421824195e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 9/71 | LOSS: 5.736106049880618e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 10/71 | LOSS: 5.901076953953386e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 11/71 | LOSS: 5.968801057557964e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 12/71 | LOSS: 6.029058078218861e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 13/71 | LOSS: 5.937027708569076e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 14/71 | LOSS: 5.996791696816217e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 15/71 | LOSS: 6.1465161138585245e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 16/71 | LOSS: 6.0452337707187435e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 17/71 | LOSS: 6.025294422013556e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 18/71 | LOSS: 6.2774167753033045e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 19/71 | LOSS: 6.292004536589957e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 20/71 | LOSS: 6.255585330585572e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 21/71 | LOSS: 6.181916288286712e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 22/71 | LOSS: 6.269912081852849e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 23/71 | LOSS: 6.184628773553413e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 24/71 | LOSS: 6.129326120571932e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 25/71 | LOSS: 6.1799999877201535e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 26/71 | LOSS: 6.173969773549794e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 27/71 | LOSS: 6.116616467417251e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 28/71 | LOSS: 6.103103871658999e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 29/71 | LOSS: 6.132311409601243e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 30/71 | LOSS: 6.144110967955296e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 31/71 | LOSS: 6.119415417060736e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 32/71 | LOSS: 6.208031277999522e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 33/71 | LOSS: 6.243110473245478e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 34/71 | LOSS: 6.240584791937311e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 35/71 | LOSS: 6.231586490280784e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 36/71 | LOSS: 6.2635727477459726e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 37/71 | LOSS: 6.269459654025636e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 38/71 | LOSS: 6.2213054358532345e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 39/71 | LOSS: 6.202065753768693e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 40/71 | LOSS: 6.163871237332365e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 41/71 | LOSS: 6.1325531532929745e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 42/71 | LOSS: 6.0903571360287e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 43/71 | LOSS: 6.048925921607985e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 44/71 | LOSS: 6.00956310437242e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 45/71 | LOSS: 5.956005190415928e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 46/71 | LOSS: 5.932764882713178e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 47/71 | LOSS: 5.8755077484799285e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 48/71 | LOSS: 5.866159616911021e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 49/71 | LOSS: 5.8305807078795625e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 50/71 | LOSS: 5.784337302019612e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 51/71 | LOSS: 5.768721905496932e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 52/71 | LOSS: 5.763121497745127e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 53/71 | LOSS: 5.7557767705576325e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 54/71 | LOSS: 5.7105271645758135e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 55/71 | LOSS: 5.6918770334440135e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 56/71 | LOSS: 5.668479931492791e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 57/71 | LOSS: 5.6679774263165166e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 58/71 | LOSS: 5.638731281732809e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 59/71 | LOSS: 5.630851516495265e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 60/71 | LOSS: 5.620132916922811e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 61/71 | LOSS: 5.590371023161776e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 62/71 | LOSS: 5.577628216419798e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 63/71 | LOSS: 5.540434244721837e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 64/71 | LOSS: 5.514135542538357e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 65/71 | LOSS: 5.483487033223245e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 66/71 | LOSS: 5.4498960443156855e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 67/71 | LOSS: 5.439190541731959e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 68/71 | LOSS: 5.420125723727667e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 69/71 | LOSS: 5.413518917001576e-06\n",
      "TRAIN: EPOCH 499/1000 | BATCH 70/71 | LOSS: 5.376382153974166e-06\n",
      "VAL: EPOCH 499/1000 | BATCH 0/8 | LOSS: 5.406432137533557e-06\n",
      "VAL: EPOCH 499/1000 | BATCH 1/8 | LOSS: 5.1570336836448405e-06\n",
      "VAL: EPOCH 499/1000 | BATCH 2/8 | LOSS: 5.198921220047244e-06\n",
      "VAL: EPOCH 499/1000 | BATCH 3/8 | LOSS: 5.110831580168451e-06\n",
      "VAL: EPOCH 499/1000 | BATCH 4/8 | LOSS: 5.0857670430559665e-06\n",
      "VAL: EPOCH 499/1000 | BATCH 5/8 | LOSS: 4.894104222330498e-06\n",
      "VAL: EPOCH 499/1000 | BATCH 6/8 | LOSS: 4.789347323302147e-06\n",
      "VAL: EPOCH 499/1000 | BATCH 7/8 | LOSS: 4.667602638619428e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 0/71 | LOSS: 4.227217232255498e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 1/71 | LOSS: 5.139383347341209e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 2/71 | LOSS: 5.328994878558054e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 3/71 | LOSS: 5.214262159825012e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 4/71 | LOSS: 4.879613106822944e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 5/71 | LOSS: 4.859635699479743e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 6/71 | LOSS: 4.617768484292485e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 7/71 | LOSS: 4.5439417135639815e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 8/71 | LOSS: 4.3507952796062455e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 9/71 | LOSS: 4.4333422010822686e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 10/71 | LOSS: 4.353217098634394e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 11/71 | LOSS: 4.369722319097491e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 12/71 | LOSS: 4.301750987696533e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 13/71 | LOSS: 4.2233957791333e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 14/71 | LOSS: 4.3265810594069386e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 15/71 | LOSS: 4.345091767277154e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 16/71 | LOSS: 4.385167264498544e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 17/71 | LOSS: 4.402258822564262e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 18/71 | LOSS: 4.359994124426617e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 19/71 | LOSS: 4.385573993204161e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 20/71 | LOSS: 4.445767271612394e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 21/71 | LOSS: 4.44656519572494e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 22/71 | LOSS: 4.499803886257881e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 23/71 | LOSS: 4.517379200782064e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 24/71 | LOSS: 4.512479172262829e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 25/71 | LOSS: 4.563522942100042e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 26/71 | LOSS: 4.557840973710107e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 27/71 | LOSS: 4.539118630678526e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 28/71 | LOSS: 4.5381041453420665e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 29/71 | LOSS: 4.535392963589402e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 30/71 | LOSS: 4.557330876489305e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 31/71 | LOSS: 4.538405903531384e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 32/71 | LOSS: 4.561016725567628e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 33/71 | LOSS: 4.568282707516439e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 34/71 | LOSS: 4.577078353967019e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 35/71 | LOSS: 4.563069018735809e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 36/71 | LOSS: 4.56750036096924e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 37/71 | LOSS: 4.552582798791455e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 38/71 | LOSS: 4.52374760220081e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 39/71 | LOSS: 4.51225105280173e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 40/71 | LOSS: 4.496661293204652e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 41/71 | LOSS: 4.489546935950611e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 42/71 | LOSS: 4.471950316740752e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 43/71 | LOSS: 4.451032122109279e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 44/71 | LOSS: 4.439011662624479e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 45/71 | LOSS: 4.430112416348888e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 46/71 | LOSS: 4.438550144805823e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 47/71 | LOSS: 4.411486699495981e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 48/71 | LOSS: 4.3996996124888586e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 49/71 | LOSS: 4.383159257486114e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 50/71 | LOSS: 4.396056336939664e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 51/71 | LOSS: 4.395401622157526e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 52/71 | LOSS: 4.391527222908752e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 53/71 | LOSS: 4.397385290994188e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 54/71 | LOSS: 4.40141343874116e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 55/71 | LOSS: 4.433169903807409e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 56/71 | LOSS: 4.427615318778216e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 57/71 | LOSS: 4.429208902052026e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 58/71 | LOSS: 4.4075416985448345e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 59/71 | LOSS: 4.414972962270743e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 60/71 | LOSS: 4.404440885276121e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 61/71 | LOSS: 4.369476678514348e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 62/71 | LOSS: 4.368913774698509e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 63/71 | LOSS: 4.365156943464399e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 64/71 | LOSS: 4.352905219862945e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 65/71 | LOSS: 4.351991378825915e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 66/71 | LOSS: 4.34777383300421e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 67/71 | LOSS: 4.350665372331766e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 68/71 | LOSS: 4.339787292865103e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 69/71 | LOSS: 4.3368494743845075e-06\n",
      "TRAIN: EPOCH 500/1000 | BATCH 70/71 | LOSS: 4.353829818286049e-06\n",
      "VAL: EPOCH 500/1000 | BATCH 0/8 | LOSS: 4.208209247735795e-06\n",
      "VAL: EPOCH 500/1000 | BATCH 1/8 | LOSS: 4.2730357563414145e-06\n",
      "VAL: EPOCH 500/1000 | BATCH 2/8 | LOSS: 4.5512227491902495e-06\n",
      "VAL: EPOCH 500/1000 | BATCH 3/8 | LOSS: 4.47864772468165e-06\n",
      "VAL: EPOCH 500/1000 | BATCH 4/8 | LOSS: 4.495762004808057e-06\n",
      "VAL: EPOCH 500/1000 | BATCH 5/8 | LOSS: 4.454517844957688e-06\n",
      "VAL: EPOCH 500/1000 | BATCH 6/8 | LOSS: 4.373991941974964e-06\n",
      "VAL: EPOCH 500/1000 | BATCH 7/8 | LOSS: 4.3082831666652055e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 0/71 | LOSS: 4.573280421027448e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 1/71 | LOSS: 4.303579771658406e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 2/71 | LOSS: 4.342980294798811e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 3/71 | LOSS: 4.288549007469555e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 4/71 | LOSS: 4.38469578512013e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 5/71 | LOSS: 4.3794739212899e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 6/71 | LOSS: 4.322790508532697e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 7/71 | LOSS: 4.281912595160975e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 8/71 | LOSS: 4.309775199039399e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 9/71 | LOSS: 4.2625162222975636e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 10/71 | LOSS: 4.234227369372754e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 11/71 | LOSS: 4.325476235559715e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 12/71 | LOSS: 4.294687012308994e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 13/71 | LOSS: 4.2631110903000395e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 14/71 | LOSS: 4.216216757413349e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 15/71 | LOSS: 4.235897520743492e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 16/71 | LOSS: 4.2427460919177116e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 17/71 | LOSS: 4.2873508113593234e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 18/71 | LOSS: 4.2800373217974805e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 19/71 | LOSS: 4.424745577580325e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 20/71 | LOSS: 4.477995292160943e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 21/71 | LOSS: 4.654005589476252e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 22/71 | LOSS: 4.583859277851803e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 23/71 | LOSS: 4.9000463775428216e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 24/71 | LOSS: 4.893539362456067e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 25/71 | LOSS: 5.10593777282618e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 26/71 | LOSS: 5.123581213747794e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 27/71 | LOSS: 5.1582636209397085e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 28/71 | LOSS: 5.217708968334121e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 29/71 | LOSS: 5.177863226890622e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 30/71 | LOSS: 5.302692465369135e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 31/71 | LOSS: 5.287574033729925e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 32/71 | LOSS: 5.399783076703016e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 33/71 | LOSS: 5.442474536507421e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 34/71 | LOSS: 5.4964514999612995e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 35/71 | LOSS: 5.545492797409679e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 36/71 | LOSS: 5.557109470582627e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 37/71 | LOSS: 5.6230149874337305e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 38/71 | LOSS: 5.561960745417776e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 39/71 | LOSS: 5.565012884289899e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 40/71 | LOSS: 5.528060328066292e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 41/71 | LOSS: 5.508999969433284e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 42/71 | LOSS: 5.456281838252285e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 43/71 | LOSS: 5.434516693143947e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 44/71 | LOSS: 5.408503279290421e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 45/71 | LOSS: 5.395127019532399e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 46/71 | LOSS: 5.356593796835675e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 47/71 | LOSS: 5.340353884738154e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 48/71 | LOSS: 5.3298329206956285e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 49/71 | LOSS: 5.292319269756263e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 50/71 | LOSS: 5.291517290257331e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 51/71 | LOSS: 5.296524517024619e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 52/71 | LOSS: 5.300407676842505e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 53/71 | LOSS: 5.281536910270343e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 54/71 | LOSS: 5.310648368156928e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 55/71 | LOSS: 5.302033185574016e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 56/71 | LOSS: 5.291983426471697e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 57/71 | LOSS: 5.278926731330887e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 58/71 | LOSS: 5.25179441834645e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 59/71 | LOSS: 5.231922557413782e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 60/71 | LOSS: 5.206102514421218e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 61/71 | LOSS: 5.202722857780651e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 62/71 | LOSS: 5.188210465464166e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 63/71 | LOSS: 5.202440394924679e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 64/71 | LOSS: 5.19346190930474e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 65/71 | LOSS: 5.175858311965633e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 66/71 | LOSS: 5.172134780959416e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 67/71 | LOSS: 5.1590413965427346e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 68/71 | LOSS: 5.14442414263929e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 69/71 | LOSS: 5.135056189179262e-06\n",
      "TRAIN: EPOCH 501/1000 | BATCH 70/71 | LOSS: 5.129923859475268e-06\n",
      "VAL: EPOCH 501/1000 | BATCH 0/8 | LOSS: 5.6185467656177934e-06\n",
      "VAL: EPOCH 501/1000 | BATCH 1/8 | LOSS: 5.514544909601682e-06\n",
      "VAL: EPOCH 501/1000 | BATCH 2/8 | LOSS: 5.627994899744711e-06\n",
      "VAL: EPOCH 501/1000 | BATCH 3/8 | LOSS: 5.504343562279246e-06\n",
      "VAL: EPOCH 501/1000 | BATCH 4/8 | LOSS: 5.542151575355092e-06\n",
      "VAL: EPOCH 501/1000 | BATCH 5/8 | LOSS: 5.459237930457069e-06\n",
      "VAL: EPOCH 501/1000 | BATCH 6/8 | LOSS: 5.385154898769022e-06\n",
      "VAL: EPOCH 501/1000 | BATCH 7/8 | LOSS: 5.337604250144068e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 0/71 | LOSS: 3.9562714846397284e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 1/71 | LOSS: 4.160385515206144e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 2/71 | LOSS: 4.04712272938923e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 3/71 | LOSS: 4.2755845015562954e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 4/71 | LOSS: 4.593855464918306e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 5/71 | LOSS: 4.603261913871393e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 6/71 | LOSS: 4.434116947647583e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 7/71 | LOSS: 4.5992068180567e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 8/71 | LOSS: 4.69069982500514e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 9/71 | LOSS: 4.815773263544543e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 10/71 | LOSS: 4.777253045838072e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 11/71 | LOSS: 4.959977862502758e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 12/71 | LOSS: 4.901886313746218e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 13/71 | LOSS: 4.935462519435012e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 14/71 | LOSS: 4.931054233262936e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 15/71 | LOSS: 4.978637917929518e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 16/71 | LOSS: 5.0392673609381105e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 17/71 | LOSS: 5.091086147028061e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 18/71 | LOSS: 5.101191331241832e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 19/71 | LOSS: 5.126521023157693e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 20/71 | LOSS: 5.078149981619347e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 21/71 | LOSS: 5.055345375446698e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 22/71 | LOSS: 4.976821509958625e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 23/71 | LOSS: 4.906663311506539e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 24/71 | LOSS: 4.8948514358926334e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 25/71 | LOSS: 4.854347978904955e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 26/71 | LOSS: 4.801677682230042e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 27/71 | LOSS: 4.817234867005027e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 28/71 | LOSS: 4.834111913278948e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 29/71 | LOSS: 4.838054928768543e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 30/71 | LOSS: 4.81677833733791e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 31/71 | LOSS: 4.852165474744652e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 32/71 | LOSS: 4.7848597356884666e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 33/71 | LOSS: 4.805813211853948e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 34/71 | LOSS: 4.827291123936967e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 35/71 | LOSS: 4.821495173877742e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 36/71 | LOSS: 4.85423683548524e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 37/71 | LOSS: 4.824421353077923e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 38/71 | LOSS: 4.801653240531615e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 39/71 | LOSS: 4.824381136359079e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 40/71 | LOSS: 4.8001375533913675e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 41/71 | LOSS: 4.823259830897898e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 42/71 | LOSS: 4.78507044410731e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 43/71 | LOSS: 4.7587502132790345e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 44/71 | LOSS: 4.726354654849274e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 45/71 | LOSS: 4.7439294259833256e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 46/71 | LOSS: 4.74556575082193e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 47/71 | LOSS: 4.707680261617497e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 48/71 | LOSS: 4.6883552543903886e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 49/71 | LOSS: 4.671781221077254e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 50/71 | LOSS: 4.6929234102600616e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 51/71 | LOSS: 4.723642243439267e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 52/71 | LOSS: 4.720178071264219e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 53/71 | LOSS: 4.728388405426274e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 54/71 | LOSS: 4.710325931062636e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 55/71 | LOSS: 4.702077412892842e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 56/71 | LOSS: 4.713245293156401e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 57/71 | LOSS: 4.714701723404969e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 58/71 | LOSS: 4.726516655124615e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 59/71 | LOSS: 4.730528905838582e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 60/71 | LOSS: 4.750192948583882e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 61/71 | LOSS: 4.7468781208555165e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 62/71 | LOSS: 4.7505844750481805e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 63/71 | LOSS: 4.790594768877554e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 64/71 | LOSS: 4.796930019997839e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 65/71 | LOSS: 4.814293288980094e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 66/71 | LOSS: 4.8029767457144285e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 67/71 | LOSS: 4.818431431003239e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 68/71 | LOSS: 4.804844972265225e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 69/71 | LOSS: 4.816713099639206e-06\n",
      "TRAIN: EPOCH 502/1000 | BATCH 70/71 | LOSS: 4.824377192946972e-06\n",
      "VAL: EPOCH 502/1000 | BATCH 0/8 | LOSS: 7.146298230509274e-06\n",
      "VAL: EPOCH 502/1000 | BATCH 1/8 | LOSS: 7.108107183739776e-06\n",
      "VAL: EPOCH 502/1000 | BATCH 2/8 | LOSS: 7.080616645301537e-06\n",
      "VAL: EPOCH 502/1000 | BATCH 3/8 | LOSS: 7.001230301284522e-06\n",
      "VAL: EPOCH 502/1000 | BATCH 4/8 | LOSS: 6.9880014962109275e-06\n",
      "VAL: EPOCH 502/1000 | BATCH 5/8 | LOSS: 6.718925988025148e-06\n",
      "VAL: EPOCH 502/1000 | BATCH 6/8 | LOSS: 6.635385748917802e-06\n",
      "VAL: EPOCH 502/1000 | BATCH 7/8 | LOSS: 6.460791439621971e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 0/71 | LOSS: 6.446924999181647e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 1/71 | LOSS: 6.1700402511633e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 2/71 | LOSS: 5.816276219168988e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 3/71 | LOSS: 6.040205335011706e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 4/71 | LOSS: 5.713625614589546e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 5/71 | LOSS: 5.993182109402066e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 6/71 | LOSS: 5.736936080731019e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 7/71 | LOSS: 5.713322082101513e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 8/71 | LOSS: 5.68379725033689e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 9/71 | LOSS: 5.814553196614724e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 10/71 | LOSS: 5.668383519977479e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 11/71 | LOSS: 5.531927627089317e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 12/71 | LOSS: 5.370171300875461e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 13/71 | LOSS: 5.2894304352386305e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 14/71 | LOSS: 5.283654400045634e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 15/71 | LOSS: 5.246008768722277e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 16/71 | LOSS: 5.2184913302506385e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 17/71 | LOSS: 5.2425879933556035e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 18/71 | LOSS: 5.176734523621762e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 19/71 | LOSS: 5.120441153394495e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 20/71 | LOSS: 5.0491678285609286e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 21/71 | LOSS: 5.000919517525207e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 22/71 | LOSS: 4.958146667416694e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 23/71 | LOSS: 4.926672488636541e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 24/71 | LOSS: 4.885656780970749e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 25/71 | LOSS: 4.911476865415282e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 26/71 | LOSS: 4.893188563275746e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 27/71 | LOSS: 4.905549206211747e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 28/71 | LOSS: 4.980125234804541e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 29/71 | LOSS: 4.986257696752242e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 30/71 | LOSS: 4.985763245067867e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 31/71 | LOSS: 4.999785815584801e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 32/71 | LOSS: 5.074599178470001e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 33/71 | LOSS: 5.02547312167093e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 34/71 | LOSS: 5.060460534878075e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 35/71 | LOSS: 5.080369608852682e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 36/71 | LOSS: 5.124447103003763e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 37/71 | LOSS: 5.145231709416533e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 38/71 | LOSS: 5.204566022234175e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 39/71 | LOSS: 5.244776036761323e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 40/71 | LOSS: 5.2197830061742e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 41/71 | LOSS: 5.279302978947055e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 42/71 | LOSS: 5.304975666360914e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 43/71 | LOSS: 5.324588641997252e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 44/71 | LOSS: 5.32232223930704e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 45/71 | LOSS: 5.371579855700079e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 46/71 | LOSS: 5.361642445464856e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 47/71 | LOSS: 5.343330087725917e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 48/71 | LOSS: 5.3940590003883105e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 49/71 | LOSS: 5.377816160034854e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 50/71 | LOSS: 5.343043254719769e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 51/71 | LOSS: 5.3250473771326005e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 52/71 | LOSS: 5.368842661448311e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 53/71 | LOSS: 5.343352170341488e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 54/71 | LOSS: 5.36358375029522e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 55/71 | LOSS: 5.356699578637095e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 56/71 | LOSS: 5.361498655506773e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 57/71 | LOSS: 5.382386501063091e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 58/71 | LOSS: 5.395106577683907e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 59/71 | LOSS: 5.400197414928698e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 60/71 | LOSS: 5.382914319166868e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 61/71 | LOSS: 5.406853189572669e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 62/71 | LOSS: 5.391355602662929e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 63/71 | LOSS: 5.37807343903296e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 64/71 | LOSS: 5.393739817149113e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 65/71 | LOSS: 5.390228855531225e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 66/71 | LOSS: 5.393026172054876e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 67/71 | LOSS: 5.392554652751321e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 68/71 | LOSS: 5.426941717577511e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 69/71 | LOSS: 5.4084954431995614e-06\n",
      "TRAIN: EPOCH 503/1000 | BATCH 70/71 | LOSS: 5.394555219155195e-06\n",
      "VAL: EPOCH 503/1000 | BATCH 0/8 | LOSS: 9.416633474756964e-06\n",
      "VAL: EPOCH 503/1000 | BATCH 1/8 | LOSS: 9.958179362001829e-06\n",
      "VAL: EPOCH 503/1000 | BATCH 2/8 | LOSS: 9.702748684503604e-06\n",
      "VAL: EPOCH 503/1000 | BATCH 3/8 | LOSS: 9.501216709395521e-06\n",
      "VAL: EPOCH 503/1000 | BATCH 4/8 | LOSS: 9.462691377848386e-06\n",
      "VAL: EPOCH 503/1000 | BATCH 5/8 | LOSS: 9.053312396645197e-06\n",
      "VAL: EPOCH 503/1000 | BATCH 6/8 | LOSS: 8.84555395584487e-06\n",
      "VAL: EPOCH 503/1000 | BATCH 7/8 | LOSS: 8.599255068020284e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 0/71 | LOSS: 6.9745592554681934e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 1/71 | LOSS: 5.404347575677093e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 2/71 | LOSS: 5.563134133505325e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 3/71 | LOSS: 5.7347770052729174e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 4/71 | LOSS: 5.884975325898267e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 5/71 | LOSS: 5.547674769938264e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 6/71 | LOSS: 5.392546557102053e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 7/71 | LOSS: 5.577433682901756e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 8/71 | LOSS: 5.4202546885385854e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 9/71 | LOSS: 5.468579911394045e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 10/71 | LOSS: 5.365352907657242e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 11/71 | LOSS: 5.27850193066115e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 12/71 | LOSS: 5.228624908900551e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 13/71 | LOSS: 5.2788837885080805e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 14/71 | LOSS: 5.177346307997747e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 15/71 | LOSS: 5.0650181009359585e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 16/71 | LOSS: 5.01190808844352e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 17/71 | LOSS: 4.981594871525077e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 18/71 | LOSS: 4.955140806678652e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 19/71 | LOSS: 4.953142376962205e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 20/71 | LOSS: 4.926618971911098e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 21/71 | LOSS: 4.9090949584173025e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 22/71 | LOSS: 4.861657811014321e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 23/71 | LOSS: 4.869495863128274e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 24/71 | LOSS: 4.8292181872966465e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 25/71 | LOSS: 4.7785177037859776e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 26/71 | LOSS: 4.75710584800658e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 27/71 | LOSS: 4.745066503021787e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 28/71 | LOSS: 4.716012450318948e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 29/71 | LOSS: 4.71614014259103e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 30/71 | LOSS: 4.755437621204164e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 31/71 | LOSS: 4.75962784918238e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 32/71 | LOSS: 4.768273694218916e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 33/71 | LOSS: 4.806812612781976e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 34/71 | LOSS: 4.757560554935481e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 35/71 | LOSS: 4.734117377817408e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 36/71 | LOSS: 4.7137669888478425e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 37/71 | LOSS: 4.738302915316844e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 38/71 | LOSS: 4.759488821293463e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 39/71 | LOSS: 4.737430907653107e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 40/71 | LOSS: 4.715258638697844e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 41/71 | LOSS: 4.6980821014672295e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 42/71 | LOSS: 4.682774842453804e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 43/71 | LOSS: 4.6590495695785315e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 44/71 | LOSS: 4.648333778176392e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 45/71 | LOSS: 4.641083740392313e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 46/71 | LOSS: 4.665008346103392e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 47/71 | LOSS: 4.655275105847068e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 48/71 | LOSS: 4.656022996159107e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 49/71 | LOSS: 4.645731964956213e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 50/71 | LOSS: 4.632321112213129e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 51/71 | LOSS: 4.654373281727413e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 52/71 | LOSS: 4.650623614512404e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 53/71 | LOSS: 4.630612731186072e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 54/71 | LOSS: 4.614921292820575e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 55/71 | LOSS: 4.6032661081036556e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 56/71 | LOSS: 4.602579988306643e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 57/71 | LOSS: 4.602453620056371e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 58/71 | LOSS: 4.586667354472457e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 59/71 | LOSS: 4.5852094482749335e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 60/71 | LOSS: 4.57335237920457e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 61/71 | LOSS: 4.577265610438893e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 62/71 | LOSS: 4.577139056963792e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 63/71 | LOSS: 4.578772870189596e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 64/71 | LOSS: 4.570944765174108e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 65/71 | LOSS: 4.5668293578824945e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 66/71 | LOSS: 4.564019453896639e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 67/71 | LOSS: 4.5499080523073e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 68/71 | LOSS: 4.535650908149799e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 69/71 | LOSS: 4.536993687257304e-06\n",
      "TRAIN: EPOCH 504/1000 | BATCH 70/71 | LOSS: 4.542464434770165e-06\n",
      "VAL: EPOCH 504/1000 | BATCH 0/8 | LOSS: 4.5552587835118175e-06\n",
      "VAL: EPOCH 504/1000 | BATCH 1/8 | LOSS: 5.240389555183356e-06\n",
      "VAL: EPOCH 504/1000 | BATCH 2/8 | LOSS: 5.4351990002032835e-06\n",
      "VAL: EPOCH 504/1000 | BATCH 3/8 | LOSS: 5.6775280654619564e-06\n",
      "VAL: EPOCH 504/1000 | BATCH 4/8 | LOSS: 5.620650244964054e-06\n",
      "VAL: EPOCH 504/1000 | BATCH 5/8 | LOSS: 5.653207684493585e-06\n",
      "VAL: EPOCH 504/1000 | BATCH 6/8 | LOSS: 5.516831281836078e-06\n",
      "VAL: EPOCH 504/1000 | BATCH 7/8 | LOSS: 5.370333155951812e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 0/71 | LOSS: 6.406683041859651e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 1/71 | LOSS: 5.128056500325329e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 2/71 | LOSS: 4.660413120897526e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 3/71 | LOSS: 5.08725980807867e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 4/71 | LOSS: 5.10652694174496e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 5/71 | LOSS: 5.299700016318336e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 6/71 | LOSS: 5.203981312921055e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 7/71 | LOSS: 5.40730766829256e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 8/71 | LOSS: 5.314353403365304e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 9/71 | LOSS: 5.220530351834896e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 10/71 | LOSS: 5.345399027977361e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 11/71 | LOSS: 5.2315526962350605e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 12/71 | LOSS: 5.36574111720256e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 13/71 | LOSS: 5.269527832751919e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 14/71 | LOSS: 5.214290210157439e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 15/71 | LOSS: 5.13131205082118e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 16/71 | LOSS: 5.150205291048384e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 17/71 | LOSS: 5.075518515695876e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 18/71 | LOSS: 5.0714602007486205e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 19/71 | LOSS: 5.109235860345507e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 20/71 | LOSS: 5.164638066292225e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 21/71 | LOSS: 5.0811504376146e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 22/71 | LOSS: 5.176211575614972e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 23/71 | LOSS: 5.178249116018681e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 24/71 | LOSS: 5.154397613296169e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 25/71 | LOSS: 5.217072288570097e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 26/71 | LOSS: 5.246318887994103e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 27/71 | LOSS: 5.191706277076134e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 28/71 | LOSS: 5.172514592827768e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 29/71 | LOSS: 5.178169681130385e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 30/71 | LOSS: 5.13682997030452e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 31/71 | LOSS: 5.1509637657431995e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 32/71 | LOSS: 5.178036458759876e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 33/71 | LOSS: 5.130321135445283e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 34/71 | LOSS: 5.11306685828978e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 35/71 | LOSS: 5.113354297413429e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 36/71 | LOSS: 5.084171475942098e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 37/71 | LOSS: 5.049915438049586e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 38/71 | LOSS: 5.070124124801786e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 39/71 | LOSS: 5.045403071335386e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 40/71 | LOSS: 5.040208429263879e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 41/71 | LOSS: 5.0430427661142725e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 42/71 | LOSS: 5.0426250825609695e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 43/71 | LOSS: 5.0206130595142895e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 44/71 | LOSS: 5.050172558185295e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 45/71 | LOSS: 5.012472293539917e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 46/71 | LOSS: 5.066326260001664e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 47/71 | LOSS: 5.060625933121325e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 48/71 | LOSS: 5.054862000104884e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 49/71 | LOSS: 5.0297753386985275e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 50/71 | LOSS: 5.009769879774574e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 51/71 | LOSS: 4.992145870976226e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 52/71 | LOSS: 4.9816020828933136e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 53/71 | LOSS: 4.980054856989429e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 54/71 | LOSS: 4.979909549133894e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 55/71 | LOSS: 4.953302111841497e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 56/71 | LOSS: 4.972488664191731e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 57/71 | LOSS: 4.960276422433049e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 58/71 | LOSS: 4.991506827182247e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 59/71 | LOSS: 4.959131698948719e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 60/71 | LOSS: 4.954499247821227e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 61/71 | LOSS: 4.939609080763823e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 62/71 | LOSS: 4.930941717219188e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 63/71 | LOSS: 4.950041279982997e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 64/71 | LOSS: 4.935445094116193e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 65/71 | LOSS: 4.952531670906782e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 66/71 | LOSS: 4.92638159880359e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 67/71 | LOSS: 4.9675927317259586e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 68/71 | LOSS: 4.946253323831352e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 69/71 | LOSS: 4.966596796813454e-06\n",
      "TRAIN: EPOCH 505/1000 | BATCH 70/71 | LOSS: 4.936413646851499e-06\n",
      "VAL: EPOCH 505/1000 | BATCH 0/8 | LOSS: 6.5456974880362395e-06\n",
      "VAL: EPOCH 505/1000 | BATCH 1/8 | LOSS: 6.537689614560804e-06\n",
      "VAL: EPOCH 505/1000 | BATCH 2/8 | LOSS: 6.570169261976844e-06\n",
      "VAL: EPOCH 505/1000 | BATCH 3/8 | LOSS: 6.458825055233319e-06\n",
      "VAL: EPOCH 505/1000 | BATCH 4/8 | LOSS: 6.505343480966985e-06\n",
      "VAL: EPOCH 505/1000 | BATCH 5/8 | LOSS: 6.247367612862338e-06\n",
      "VAL: EPOCH 505/1000 | BATCH 6/8 | LOSS: 6.183861972073958e-06\n",
      "VAL: EPOCH 505/1000 | BATCH 7/8 | LOSS: 6.022594050136831e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 0/71 | LOSS: 4.623265795089537e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 1/71 | LOSS: 5.238895255388343e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 2/71 | LOSS: 5.064247034169966e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 3/71 | LOSS: 5.023627068112546e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 4/71 | LOSS: 4.9868252062879035e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 5/71 | LOSS: 5.048987153107494e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 6/71 | LOSS: 5.024627561007426e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 7/71 | LOSS: 5.1297146228534984e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 8/71 | LOSS: 5.05562547914451e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 9/71 | LOSS: 5.3945789659337605e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 10/71 | LOSS: 5.435226078340995e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 11/71 | LOSS: 5.463835009322793e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 12/71 | LOSS: 5.423950894440238e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 13/71 | LOSS: 5.4366692958345605e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 14/71 | LOSS: 5.331714176766885e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 15/71 | LOSS: 5.306572859353764e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 16/71 | LOSS: 5.322925463748534e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 17/71 | LOSS: 5.242751613978827e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 18/71 | LOSS: 5.292627663852488e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 19/71 | LOSS: 5.255785913504951e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 20/71 | LOSS: 5.244701653628865e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 21/71 | LOSS: 5.211309991698892e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 22/71 | LOSS: 5.19223741597987e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 23/71 | LOSS: 5.142765322337557e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 24/71 | LOSS: 5.11839281898574e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 25/71 | LOSS: 5.100597316133924e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 26/71 | LOSS: 5.094032295110756e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 27/71 | LOSS: 5.016442855776404e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 28/71 | LOSS: 5.035161377926325e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 29/71 | LOSS: 5.012276809187218e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 30/71 | LOSS: 4.976375118032241e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 31/71 | LOSS: 4.962103062666756e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 32/71 | LOSS: 4.925913813305229e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 33/71 | LOSS: 4.893788253171164e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 34/71 | LOSS: 4.894396442978177e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 35/71 | LOSS: 4.867316849615438e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 36/71 | LOSS: 4.84231802417796e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 37/71 | LOSS: 4.807077319032703e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 38/71 | LOSS: 4.820775908835155e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 39/71 | LOSS: 4.796610039647931e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 40/71 | LOSS: 4.785882438449246e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 41/71 | LOSS: 4.750981864682606e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 42/71 | LOSS: 4.765177304444626e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 43/71 | LOSS: 4.754056574669945e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 44/71 | LOSS: 4.763875348443334e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 45/71 | LOSS: 4.7878728043107675e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 46/71 | LOSS: 4.775160219806105e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 47/71 | LOSS: 4.76415127555659e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 48/71 | LOSS: 4.813868084669111e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 49/71 | LOSS: 4.811977064491657e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 50/71 | LOSS: 4.836059249510963e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 51/71 | LOSS: 4.8407772466746864e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 52/71 | LOSS: 4.829585470429694e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 53/71 | LOSS: 4.821363968740452e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 54/71 | LOSS: 4.84890114090707e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 55/71 | LOSS: 4.829089086959877e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 56/71 | LOSS: 4.842041590292642e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 57/71 | LOSS: 4.831847886948551e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 58/71 | LOSS: 4.815422521537353e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 59/71 | LOSS: 4.814940579459896e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 60/71 | LOSS: 4.819718629941698e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 61/71 | LOSS: 4.843658845651765e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 62/71 | LOSS: 4.845283084496115e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 63/71 | LOSS: 4.842261770221512e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 64/71 | LOSS: 4.8596245960652595e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 65/71 | LOSS: 4.836649374986089e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 66/71 | LOSS: 4.817680689090743e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 67/71 | LOSS: 4.834599953001623e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 68/71 | LOSS: 4.8272093085804535e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 69/71 | LOSS: 4.8101406719069925e-06\n",
      "TRAIN: EPOCH 506/1000 | BATCH 70/71 | LOSS: 4.808291475750542e-06\n",
      "VAL: EPOCH 506/1000 | BATCH 0/8 | LOSS: 3.949731308239279e-06\n",
      "VAL: EPOCH 506/1000 | BATCH 1/8 | LOSS: 4.2664673856052104e-06\n",
      "VAL: EPOCH 506/1000 | BATCH 2/8 | LOSS: 4.665240946148212e-06\n",
      "VAL: EPOCH 506/1000 | BATCH 3/8 | LOSS: 4.702659452959779e-06\n",
      "VAL: EPOCH 506/1000 | BATCH 4/8 | LOSS: 4.7090772568481045e-06\n",
      "VAL: EPOCH 506/1000 | BATCH 5/8 | LOSS: 4.737953986477805e-06\n",
      "VAL: EPOCH 506/1000 | BATCH 6/8 | LOSS: 4.660899906282014e-06\n",
      "VAL: EPOCH 506/1000 | BATCH 7/8 | LOSS: 4.666238737627282e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 0/71 | LOSS: 4.541403086477658e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 1/71 | LOSS: 4.1813921143329935e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 2/71 | LOSS: 4.145106837919836e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 3/71 | LOSS: 3.901348009094363e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 4/71 | LOSS: 3.921384359273361e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 5/71 | LOSS: 3.931176782619635e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 6/71 | LOSS: 4.226937367742981e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 7/71 | LOSS: 4.108700551341826e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 8/71 | LOSS: 4.206723057601873e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 9/71 | LOSS: 4.209019175505091e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 10/71 | LOSS: 4.386499995234772e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 11/71 | LOSS: 4.354093391611968e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 12/71 | LOSS: 4.3634138714528836e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 13/71 | LOSS: 4.383629094003741e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 14/71 | LOSS: 4.557736231921202e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 15/71 | LOSS: 4.670030577358375e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 16/71 | LOSS: 4.760892205227078e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 17/71 | LOSS: 4.7934182450464705e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 18/71 | LOSS: 4.814714657186414e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 19/71 | LOSS: 4.912340216378652e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 20/71 | LOSS: 4.940455634520428e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 21/71 | LOSS: 5.1327427647927175e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 22/71 | LOSS: 5.108341047557389e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 23/71 | LOSS: 5.141549375290803e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 24/71 | LOSS: 5.111943510200945e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 25/71 | LOSS: 5.1765763373623486e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 26/71 | LOSS: 5.1315705015305736e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 27/71 | LOSS: 5.243530357087625e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 28/71 | LOSS: 5.178003424714853e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 29/71 | LOSS: 5.181930320456255e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 30/71 | LOSS: 5.140946592702324e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 31/71 | LOSS: 5.096788335379188e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 32/71 | LOSS: 5.082093049636707e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 33/71 | LOSS: 5.054071801382901e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 34/71 | LOSS: 5.045306969415313e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 35/71 | LOSS: 5.03450543950142e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 36/71 | LOSS: 5.027194373650599e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 37/71 | LOSS: 5.013218463701426e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 38/71 | LOSS: 4.987298401222991e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 39/71 | LOSS: 4.994074907926916e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 40/71 | LOSS: 4.976025543439347e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 41/71 | LOSS: 4.938357868749812e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 42/71 | LOSS: 4.940731144436773e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 43/71 | LOSS: 4.9577912322588835e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 44/71 | LOSS: 4.967314064035438e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 45/71 | LOSS: 4.971958822392006e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 46/71 | LOSS: 4.945902291899287e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 47/71 | LOSS: 4.969901998682265e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 48/71 | LOSS: 4.985594215417668e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 49/71 | LOSS: 4.996042084712826e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 50/71 | LOSS: 4.963607009402967e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 51/71 | LOSS: 4.9850196520450345e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 52/71 | LOSS: 4.983913241922669e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 53/71 | LOSS: 4.98616180653631e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 54/71 | LOSS: 5.000487987093617e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 55/71 | LOSS: 4.9850369277594704e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 56/71 | LOSS: 5.002990032983035e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 57/71 | LOSS: 4.984681862063961e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 58/71 | LOSS: 5.0158780749817675e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 59/71 | LOSS: 4.991316363126922e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 60/71 | LOSS: 4.9988942225995665e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 61/71 | LOSS: 4.985510233176859e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 62/71 | LOSS: 4.97353403622424e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 63/71 | LOSS: 4.953964847231873e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 64/71 | LOSS: 4.959120129709705e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 65/71 | LOSS: 4.951730151140399e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 66/71 | LOSS: 4.928746936606493e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 67/71 | LOSS: 4.935335268515594e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 68/71 | LOSS: 4.934634159945889e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 69/71 | LOSS: 4.920864882608709e-06\n",
      "TRAIN: EPOCH 507/1000 | BATCH 70/71 | LOSS: 4.9511231608589635e-06\n",
      "VAL: EPOCH 507/1000 | BATCH 0/8 | LOSS: 4.261798039806308e-06\n",
      "VAL: EPOCH 507/1000 | BATCH 1/8 | LOSS: 4.612144039128907e-06\n",
      "VAL: EPOCH 507/1000 | BATCH 2/8 | LOSS: 4.685137658573997e-06\n",
      "VAL: EPOCH 507/1000 | BATCH 3/8 | LOSS: 4.656773512579093e-06\n",
      "VAL: EPOCH 507/1000 | BATCH 4/8 | LOSS: 4.661592356569599e-06\n",
      "VAL: EPOCH 507/1000 | BATCH 5/8 | LOSS: 4.5580480521797044e-06\n",
      "VAL: EPOCH 507/1000 | BATCH 6/8 | LOSS: 4.409725956325669e-06\n",
      "VAL: EPOCH 507/1000 | BATCH 7/8 | LOSS: 4.303472508127015e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 0/71 | LOSS: 3.884273155563278e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 1/71 | LOSS: 3.812617364928883e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 2/71 | LOSS: 3.997595664865609e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 3/71 | LOSS: 4.474466720694181e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 4/71 | LOSS: 4.412435328049469e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 5/71 | LOSS: 4.7669763792631175e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 6/71 | LOSS: 4.973301884092507e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 7/71 | LOSS: 4.864048577246649e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 8/71 | LOSS: 4.765020270901408e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 9/71 | LOSS: 5.0303962552789015e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 10/71 | LOSS: 4.966289473238496e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 11/71 | LOSS: 5.175106499185252e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 12/71 | LOSS: 5.033545221522218e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 13/71 | LOSS: 5.016638364655332e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 14/71 | LOSS: 5.035513277107384e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 15/71 | LOSS: 4.994002267721953e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 16/71 | LOSS: 4.988557173098397e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 17/71 | LOSS: 4.918826612588924e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 18/71 | LOSS: 4.870359685140245e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 19/71 | LOSS: 4.7704810754112256e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 20/71 | LOSS: 4.809850138155439e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 21/71 | LOSS: 4.78124757757376e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 22/71 | LOSS: 4.761705035296314e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 23/71 | LOSS: 4.71857734396508e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 24/71 | LOSS: 4.734396488856874e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 25/71 | LOSS: 4.750963652301065e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 26/71 | LOSS: 4.764019153029679e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 27/71 | LOSS: 4.759617021526148e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 28/71 | LOSS: 4.759082732311197e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 29/71 | LOSS: 4.751310090493158e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 30/71 | LOSS: 4.738985419481328e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 31/71 | LOSS: 4.790372990726155e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 32/71 | LOSS: 4.794732507379552e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 33/71 | LOSS: 4.859601584511813e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 34/71 | LOSS: 4.87299772820344e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 35/71 | LOSS: 4.930905820149039e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 36/71 | LOSS: 4.901123942102926e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 37/71 | LOSS: 4.8760653941683e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 38/71 | LOSS: 4.89204681122292e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 39/71 | LOSS: 4.87102983584009e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 40/71 | LOSS: 4.859000305592284e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 41/71 | LOSS: 4.837360864127861e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 42/71 | LOSS: 4.8272538606397575e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 43/71 | LOSS: 4.810721288157021e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 44/71 | LOSS: 4.814285375687177e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 45/71 | LOSS: 4.8188099289409765e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 46/71 | LOSS: 4.81636043622665e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 47/71 | LOSS: 4.853745475467501e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 48/71 | LOSS: 4.852518630435284e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 49/71 | LOSS: 4.848123367082735e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 50/71 | LOSS: 4.865408081790449e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 51/71 | LOSS: 4.87294861036739e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 52/71 | LOSS: 4.854609968324954e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 53/71 | LOSS: 4.884664370500239e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 54/71 | LOSS: 4.8587972039306555e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 55/71 | LOSS: 4.845201686975997e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 56/71 | LOSS: 4.862617854821126e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 57/71 | LOSS: 4.8573731615322e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 58/71 | LOSS: 4.848282696141307e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 59/71 | LOSS: 4.837213657538086e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 60/71 | LOSS: 4.835739326929305e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 61/71 | LOSS: 4.821001484302659e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 62/71 | LOSS: 4.8122463607479215e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 63/71 | LOSS: 4.8063342283910515e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 64/71 | LOSS: 4.7897421189610704e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 65/71 | LOSS: 4.817669088714121e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 66/71 | LOSS: 4.81565425005427e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 67/71 | LOSS: 4.819993605184102e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 68/71 | LOSS: 4.8291981329384726e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 69/71 | LOSS: 4.812782841716918e-06\n",
      "TRAIN: EPOCH 508/1000 | BATCH 70/71 | LOSS: 4.806549873457801e-06\n",
      "VAL: EPOCH 508/1000 | BATCH 0/8 | LOSS: 4.4045264075975865e-06\n",
      "VAL: EPOCH 508/1000 | BATCH 1/8 | LOSS: 4.797327164851595e-06\n",
      "VAL: EPOCH 508/1000 | BATCH 2/8 | LOSS: 4.863063622906338e-06\n",
      "VAL: EPOCH 508/1000 | BATCH 3/8 | LOSS: 4.923228289044346e-06\n",
      "VAL: EPOCH 508/1000 | BATCH 4/8 | LOSS: 4.901434385828906e-06\n",
      "VAL: EPOCH 508/1000 | BATCH 5/8 | LOSS: 4.825703778503036e-06\n",
      "VAL: EPOCH 508/1000 | BATCH 6/8 | LOSS: 4.701124778096398e-06\n",
      "VAL: EPOCH 508/1000 | BATCH 7/8 | LOSS: 4.575438026677148e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 0/71 | LOSS: 5.480216714204289e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 1/71 | LOSS: 5.1686251936189365e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 2/71 | LOSS: 4.841726270872944e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 3/71 | LOSS: 4.861618094764708e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 4/71 | LOSS: 4.774479475599947e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 5/71 | LOSS: 4.5287000981867704e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 6/71 | LOSS: 4.476592429065411e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 7/71 | LOSS: 4.545986826087756e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 8/71 | LOSS: 4.5970373826599425e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 9/71 | LOSS: 4.565763811115176e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 10/71 | LOSS: 4.631849647425539e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 11/71 | LOSS: 4.5560943438734585e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 12/71 | LOSS: 4.558529641284482e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 13/71 | LOSS: 4.5688847356879184e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 14/71 | LOSS: 4.60181366482478e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 15/71 | LOSS: 4.6088908618457936e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 16/71 | LOSS: 4.574126358099474e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 17/71 | LOSS: 4.60958136096047e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 18/71 | LOSS: 4.680244249979487e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 19/71 | LOSS: 4.7667729404565765e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 20/71 | LOSS: 4.713740573996412e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 21/71 | LOSS: 4.795734701639793e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 22/71 | LOSS: 4.762732299676155e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 23/71 | LOSS: 4.777698165980837e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 24/71 | LOSS: 4.759100029332331e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 25/71 | LOSS: 4.774983626003422e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 26/71 | LOSS: 4.796945951831596e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 27/71 | LOSS: 4.775686388061981e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 28/71 | LOSS: 4.771110415559038e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 29/71 | LOSS: 4.730952726580047e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 30/71 | LOSS: 4.707703209855789e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 31/71 | LOSS: 4.698676889347553e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 32/71 | LOSS: 4.689087800767139e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 33/71 | LOSS: 4.689572412697838e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 34/71 | LOSS: 4.695668095726952e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 35/71 | LOSS: 4.684725834320287e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 36/71 | LOSS: 4.656150468764355e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 37/71 | LOSS: 4.655961878310527e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 38/71 | LOSS: 4.649267455720408e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 39/71 | LOSS: 4.652309576158586e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 40/71 | LOSS: 4.6335232205243165e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 41/71 | LOSS: 4.673285708295658e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 42/71 | LOSS: 4.64568283013948e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 43/71 | LOSS: 4.639365050190298e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 44/71 | LOSS: 4.649044704921027e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 45/71 | LOSS: 4.664465391017628e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 46/71 | LOSS: 4.684926043023068e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 47/71 | LOSS: 4.670546815077614e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 48/71 | LOSS: 4.686851762202497e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 49/71 | LOSS: 4.678974719354301e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 50/71 | LOSS: 4.677945240723156e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 51/71 | LOSS: 4.669794031122785e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 52/71 | LOSS: 4.671888986332254e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 53/71 | LOSS: 4.673274319160285e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 54/71 | LOSS: 4.649210677598603e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 55/71 | LOSS: 4.6738706000074415e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 56/71 | LOSS: 4.663641783209689e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 57/71 | LOSS: 4.704146829618398e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 58/71 | LOSS: 4.717251058382779e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 59/71 | LOSS: 4.721232638379055e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 60/71 | LOSS: 4.708268131596059e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 61/71 | LOSS: 4.696533910793087e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 62/71 | LOSS: 4.695308761588494e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 63/71 | LOSS: 4.675422513145122e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 64/71 | LOSS: 4.6784118904515004e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 65/71 | LOSS: 4.669032481422509e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 66/71 | LOSS: 4.6827677110809075e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 67/71 | LOSS: 4.6752126650062564e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 68/71 | LOSS: 4.670680110011481e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 69/71 | LOSS: 4.6781260380157615e-06\n",
      "TRAIN: EPOCH 509/1000 | BATCH 70/71 | LOSS: 4.650215650669875e-06\n",
      "VAL: EPOCH 509/1000 | BATCH 0/8 | LOSS: 4.157903731538681e-06\n",
      "VAL: EPOCH 509/1000 | BATCH 1/8 | LOSS: 4.696285714089754e-06\n",
      "VAL: EPOCH 509/1000 | BATCH 2/8 | LOSS: 4.81785461185306e-06\n",
      "VAL: EPOCH 509/1000 | BATCH 3/8 | LOSS: 4.957269879923842e-06\n",
      "VAL: EPOCH 509/1000 | BATCH 4/8 | LOSS: 4.905254536424764e-06\n",
      "VAL: EPOCH 509/1000 | BATCH 5/8 | LOSS: 4.908983783025178e-06\n",
      "VAL: EPOCH 509/1000 | BATCH 6/8 | LOSS: 4.7850357987044845e-06\n",
      "VAL: EPOCH 509/1000 | BATCH 7/8 | LOSS: 4.640461355620573e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 0/71 | LOSS: 5.058829628978856e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 1/71 | LOSS: 4.497527015701053e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 2/71 | LOSS: 4.22799287965366e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 3/71 | LOSS: 4.115566866857989e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 4/71 | LOSS: 4.172099943389185e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 5/71 | LOSS: 4.231863082774605e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 6/71 | LOSS: 4.231391033369748e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 7/71 | LOSS: 4.222594384373224e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 8/71 | LOSS: 4.20742708229227e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 9/71 | LOSS: 4.294001973903505e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 10/71 | LOSS: 4.254493267746346e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 11/71 | LOSS: 4.292051547357308e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 12/71 | LOSS: 4.301969616230613e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 13/71 | LOSS: 4.400561205589579e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 14/71 | LOSS: 4.527681115481149e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 15/71 | LOSS: 4.562576862099377e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 16/71 | LOSS: 4.543371687759645e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 17/71 | LOSS: 4.55853119597628e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 18/71 | LOSS: 4.564789785370904e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 19/71 | LOSS: 4.517741297149769e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 20/71 | LOSS: 4.545570921739757e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 21/71 | LOSS: 4.532424007017901e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 22/71 | LOSS: 4.566080317248555e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 23/71 | LOSS: 4.564918574108863e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 24/71 | LOSS: 4.5528625651058974e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 25/71 | LOSS: 4.582807264341682e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 26/71 | LOSS: 4.624894876946604e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 27/71 | LOSS: 4.629821489743335e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 28/71 | LOSS: 4.62945574546556e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 29/71 | LOSS: 4.648035708972505e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 30/71 | LOSS: 4.6411346301606355e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 31/71 | LOSS: 4.643986251551269e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 32/71 | LOSS: 4.667913296516142e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 33/71 | LOSS: 4.704716977411247e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 34/71 | LOSS: 4.684691800451089e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 35/71 | LOSS: 4.694325645636531e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 36/71 | LOSS: 4.729717129275495e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 37/71 | LOSS: 4.682294334806532e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 38/71 | LOSS: 4.739044027538069e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 39/71 | LOSS: 4.7336124339381055e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 40/71 | LOSS: 4.746617955012281e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 41/71 | LOSS: 4.7749906246151245e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 42/71 | LOSS: 4.816836820303484e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 43/71 | LOSS: 4.8286348043223946e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 44/71 | LOSS: 4.8278336281429e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 45/71 | LOSS: 4.846369027132902e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 46/71 | LOSS: 4.874937250396544e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 47/71 | LOSS: 4.84266823264079e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 48/71 | LOSS: 4.924812067341064e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 49/71 | LOSS: 4.93023370836454e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 50/71 | LOSS: 4.99527990826869e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 51/71 | LOSS: 5.0216263983971794e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 52/71 | LOSS: 5.036839368431346e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 53/71 | LOSS: 5.110000798864618e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 54/71 | LOSS: 5.11809911668851e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 55/71 | LOSS: 5.214229564184539e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 56/71 | LOSS: 5.208591566068271e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 57/71 | LOSS: 5.278777601641446e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 58/71 | LOSS: 5.288038179152586e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 59/71 | LOSS: 5.349835729854628e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 60/71 | LOSS: 5.339074236797417e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 61/71 | LOSS: 5.349089853494192e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 62/71 | LOSS: 5.35963472285648e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 63/71 | LOSS: 5.379888236234365e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 64/71 | LOSS: 5.380565471568843e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 65/71 | LOSS: 5.3836068728383965e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 66/71 | LOSS: 5.3957242491731505e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 67/71 | LOSS: 5.387882331032254e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 68/71 | LOSS: 5.380556835323501e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 69/71 | LOSS: 5.348252277924205e-06\n",
      "TRAIN: EPOCH 510/1000 | BATCH 70/71 | LOSS: 5.359881217219468e-06\n",
      "VAL: EPOCH 510/1000 | BATCH 0/8 | LOSS: 5.077611604065169e-06\n",
      "VAL: EPOCH 510/1000 | BATCH 1/8 | LOSS: 5.2295561090431875e-06\n",
      "VAL: EPOCH 510/1000 | BATCH 2/8 | LOSS: 5.363064701668918e-06\n",
      "VAL: EPOCH 510/1000 | BATCH 3/8 | LOSS: 5.312380608302192e-06\n",
      "VAL: EPOCH 510/1000 | BATCH 4/8 | LOSS: 5.316529404808535e-06\n",
      "VAL: EPOCH 510/1000 | BATCH 5/8 | LOSS: 5.153474300338227e-06\n",
      "VAL: EPOCH 510/1000 | BATCH 6/8 | LOSS: 5.072007817424102e-06\n",
      "VAL: EPOCH 510/1000 | BATCH 7/8 | LOSS: 4.897249482382904e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 0/71 | LOSS: 5.052896995039191e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 1/71 | LOSS: 5.27706788489013e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 2/71 | LOSS: 5.262301177329694e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 3/71 | LOSS: 5.635491220346012e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 4/71 | LOSS: 5.499813232745509e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 5/71 | LOSS: 5.254028868269718e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 6/71 | LOSS: 5.609719794717551e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 7/71 | LOSS: 5.487198677656124e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 8/71 | LOSS: 5.521295962454234e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 9/71 | LOSS: 5.4406631988968e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 10/71 | LOSS: 5.415628203825856e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 11/71 | LOSS: 5.372549935600546e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 12/71 | LOSS: 5.276658752196594e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 13/71 | LOSS: 5.400321924753371e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 14/71 | LOSS: 5.363032141758595e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 15/71 | LOSS: 5.420471495654056e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 16/71 | LOSS: 5.3252411977583136e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 17/71 | LOSS: 5.34662702496765e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 18/71 | LOSS: 5.332608572261004e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 19/71 | LOSS: 5.306545040184573e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 20/71 | LOSS: 5.29734643350821e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 21/71 | LOSS: 5.288138874510134e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 22/71 | LOSS: 5.258552332436564e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 23/71 | LOSS: 5.2262988674556254e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 24/71 | LOSS: 5.194936966290697e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 25/71 | LOSS: 5.2166590893858274e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 26/71 | LOSS: 5.203008051332155e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 27/71 | LOSS: 5.196523228992841e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 28/71 | LOSS: 5.159870240810239e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 29/71 | LOSS: 5.145071888061162e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 30/71 | LOSS: 5.1047298315567e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 31/71 | LOSS: 5.069563144388667e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 32/71 | LOSS: 5.0330714161232626e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 33/71 | LOSS: 5.026189362123499e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 34/71 | LOSS: 5.047958321873531e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 35/71 | LOSS: 5.025852146900434e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 36/71 | LOSS: 5.065809742715115e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 37/71 | LOSS: 5.081781062855091e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 38/71 | LOSS: 5.090104539890606e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 39/71 | LOSS: 5.0979122988792366e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 40/71 | LOSS: 5.1144941763351844e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 41/71 | LOSS: 5.089496501871811e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 42/71 | LOSS: 5.050184564572911e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 43/71 | LOSS: 5.035781255587195e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 44/71 | LOSS: 5.023486068643655e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 45/71 | LOSS: 5.006811840718475e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 46/71 | LOSS: 4.9939717524815e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 47/71 | LOSS: 4.978589847344968e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 48/71 | LOSS: 4.967038217282792e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 49/71 | LOSS: 4.942256350659591e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 50/71 | LOSS: 4.957109365254748e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 51/71 | LOSS: 4.981129550812441e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 52/71 | LOSS: 4.989090446130986e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 53/71 | LOSS: 4.984184236626014e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 54/71 | LOSS: 4.971666174407636e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 55/71 | LOSS: 4.9585753012187136e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 56/71 | LOSS: 4.939109015922122e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 57/71 | LOSS: 4.926415601466001e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 58/71 | LOSS: 4.9211361429269795e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 59/71 | LOSS: 4.902571841588118e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 60/71 | LOSS: 4.886206248616485e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 61/71 | LOSS: 4.864807817372615e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 62/71 | LOSS: 4.877026838816135e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 63/71 | LOSS: 4.8625958619652465e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 64/71 | LOSS: 4.847614844458384e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 65/71 | LOSS: 4.828571907940689e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 66/71 | LOSS: 4.8216191608749365e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 67/71 | LOSS: 4.8045619511118705e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 68/71 | LOSS: 4.778195195773361e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 69/71 | LOSS: 4.770353517155204e-06\n",
      "TRAIN: EPOCH 511/1000 | BATCH 70/71 | LOSS: 4.78264287651232e-06\n",
      "VAL: EPOCH 511/1000 | BATCH 0/8 | LOSS: 3.817107426584698e-06\n",
      "VAL: EPOCH 511/1000 | BATCH 1/8 | LOSS: 4.221432845952222e-06\n",
      "VAL: EPOCH 511/1000 | BATCH 2/8 | LOSS: 4.5091631667067604e-06\n",
      "VAL: EPOCH 511/1000 | BATCH 3/8 | LOSS: 4.535717266662687e-06\n",
      "VAL: EPOCH 511/1000 | BATCH 4/8 | LOSS: 4.541238649835577e-06\n",
      "VAL: EPOCH 511/1000 | BATCH 5/8 | LOSS: 4.5441414234422455e-06\n",
      "VAL: EPOCH 511/1000 | BATCH 6/8 | LOSS: 4.4323887128224925e-06\n",
      "VAL: EPOCH 511/1000 | BATCH 7/8 | LOSS: 4.383878831504262e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 0/71 | LOSS: 4.774572516907938e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 1/71 | LOSS: 4.132857952754421e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 2/71 | LOSS: 4.844782703609478e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 3/71 | LOSS: 4.906482047317695e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 4/71 | LOSS: 4.915776980851661e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 5/71 | LOSS: 5.205627038170253e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 6/71 | LOSS: 5.25179266982637e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 7/71 | LOSS: 5.230589607663205e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 8/71 | LOSS: 5.045597441090245e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 9/71 | LOSS: 4.867482175541227e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 10/71 | LOSS: 4.881562621118395e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 11/71 | LOSS: 4.780444934719223e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 12/71 | LOSS: 4.763139084109123e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 13/71 | LOSS: 4.740952525545643e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 14/71 | LOSS: 4.778297625307459e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 15/71 | LOSS: 4.740783708712115e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 16/71 | LOSS: 4.713268508032129e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 17/71 | LOSS: 4.62211493059941e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 18/71 | LOSS: 4.57869698852607e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 19/71 | LOSS: 4.515213890954328e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 20/71 | LOSS: 4.492733296501683e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 21/71 | LOSS: 4.4394912286158865e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 22/71 | LOSS: 4.41724013517555e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 23/71 | LOSS: 4.4511341267631605e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 24/71 | LOSS: 4.409797520565917e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 25/71 | LOSS: 4.500882027969055e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 26/71 | LOSS: 4.5258528744953865e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 27/71 | LOSS: 4.502190230531207e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 28/71 | LOSS: 4.476152761181483e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 29/71 | LOSS: 4.496214849799193e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 30/71 | LOSS: 4.473274782046041e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 31/71 | LOSS: 4.470823192548323e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 32/71 | LOSS: 4.488814235600964e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 33/71 | LOSS: 4.4811867329532535e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 34/71 | LOSS: 4.460975852842759e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 35/71 | LOSS: 4.482393421767483e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 36/71 | LOSS: 4.523446447107031e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 37/71 | LOSS: 4.555210933320172e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 38/71 | LOSS: 4.5928961870418e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 39/71 | LOSS: 4.598181175197169e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 40/71 | LOSS: 4.605598710773714e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 41/71 | LOSS: 4.585545579651634e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 42/71 | LOSS: 4.59680425296409e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 43/71 | LOSS: 4.601809238961736e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 44/71 | LOSS: 4.6045757042318455e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 45/71 | LOSS: 4.59336882099508e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 46/71 | LOSS: 4.563720756081277e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 47/71 | LOSS: 4.587649579927226e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 48/71 | LOSS: 4.570779983438041e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 49/71 | LOSS: 4.561527007354016e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 50/71 | LOSS: 4.555335827521333e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 51/71 | LOSS: 4.544749174328899e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 52/71 | LOSS: 4.532961581372667e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 53/71 | LOSS: 4.504692107595339e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 54/71 | LOSS: 4.491941759128663e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 55/71 | LOSS: 4.503573982057267e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 56/71 | LOSS: 4.491778291163715e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 57/71 | LOSS: 4.495123773957512e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 58/71 | LOSS: 4.478748250822734e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 59/71 | LOSS: 4.467615876061852e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 60/71 | LOSS: 4.483481644292326e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 61/71 | LOSS: 4.490212254202033e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 62/71 | LOSS: 4.497963724740354e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 63/71 | LOSS: 4.497300036376828e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 64/71 | LOSS: 4.5234828541232186e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 65/71 | LOSS: 4.513897641283455e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 66/71 | LOSS: 4.524952753773002e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 67/71 | LOSS: 4.5337189897723234e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 68/71 | LOSS: 4.541640612882e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 69/71 | LOSS: 4.5324474058361375e-06\n",
      "TRAIN: EPOCH 512/1000 | BATCH 70/71 | LOSS: 4.53060606175736e-06\n",
      "VAL: EPOCH 512/1000 | BATCH 0/8 | LOSS: 3.99316149923834e-06\n",
      "VAL: EPOCH 512/1000 | BATCH 1/8 | LOSS: 4.190432946415967e-06\n",
      "VAL: EPOCH 512/1000 | BATCH 2/8 | LOSS: 4.38851763343943e-06\n",
      "VAL: EPOCH 512/1000 | BATCH 3/8 | LOSS: 4.3835776750711375e-06\n",
      "VAL: EPOCH 512/1000 | BATCH 4/8 | LOSS: 4.346053265180671e-06\n",
      "VAL: EPOCH 512/1000 | BATCH 5/8 | LOSS: 4.229386831866577e-06\n",
      "VAL: EPOCH 512/1000 | BATCH 6/8 | LOSS: 4.070519643002106e-06\n",
      "VAL: EPOCH 512/1000 | BATCH 7/8 | LOSS: 3.932763149805396e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 0/71 | LOSS: 3.7515781059482833e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 1/71 | LOSS: 5.077617856841243e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 2/71 | LOSS: 4.80249301896644e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 3/71 | LOSS: 5.0118014200961625e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 4/71 | LOSS: 4.779732034876361e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 5/71 | LOSS: 4.739243233113181e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 6/71 | LOSS: 4.7585803193734525e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 7/71 | LOSS: 4.7526440027922945e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 8/71 | LOSS: 4.875711687822735e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 9/71 | LOSS: 4.796179086952179e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 10/71 | LOSS: 4.775387651534402e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 11/71 | LOSS: 4.61043007741561e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 12/71 | LOSS: 4.5011927340965485e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 13/71 | LOSS: 4.40394485069971e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 14/71 | LOSS: 4.399097633722704e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 15/71 | LOSS: 4.350371284544963e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 16/71 | LOSS: 4.319865424536949e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 17/71 | LOSS: 4.266910941118516e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 18/71 | LOSS: 4.2603035964068685e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 19/71 | LOSS: 4.254066175235493e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 20/71 | LOSS: 4.252861951196489e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 21/71 | LOSS: 4.252472081134329e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 22/71 | LOSS: 4.302094270236052e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 23/71 | LOSS: 4.318071186541299e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 24/71 | LOSS: 4.3233144060650374e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 25/71 | LOSS: 4.323067112356792e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 26/71 | LOSS: 4.325409800340249e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 27/71 | LOSS: 4.295503742923756e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 28/71 | LOSS: 4.29561732587523e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 29/71 | LOSS: 4.321213896218978e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 30/71 | LOSS: 4.33098354381312e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 31/71 | LOSS: 4.326124404485654e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 32/71 | LOSS: 4.31474072546266e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 33/71 | LOSS: 4.323206125638321e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 34/71 | LOSS: 4.302824728646166e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 35/71 | LOSS: 4.287516699403366e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 36/71 | LOSS: 4.293232063286564e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 37/71 | LOSS: 4.2810188760038155e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 38/71 | LOSS: 4.2854093209872444e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 39/71 | LOSS: 4.32331822253218e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 40/71 | LOSS: 4.315006236072627e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 41/71 | LOSS: 4.318795872461037e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 42/71 | LOSS: 4.341641633165933e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 43/71 | LOSS: 4.375094957512457e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 44/71 | LOSS: 4.386082486639174e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 45/71 | LOSS: 4.39834451773767e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 46/71 | LOSS: 4.373599843328269e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 47/71 | LOSS: 4.376483123754345e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 48/71 | LOSS: 4.389315173674342e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 49/71 | LOSS: 4.392185464894283e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 50/71 | LOSS: 4.3956007781768376e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 51/71 | LOSS: 4.3997028412307445e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 52/71 | LOSS: 4.388698479882771e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 53/71 | LOSS: 4.4192158838995965e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 54/71 | LOSS: 4.416737357876793e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 55/71 | LOSS: 4.445536766135254e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 56/71 | LOSS: 4.444920370205134e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 57/71 | LOSS: 4.457701755921429e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 58/71 | LOSS: 4.484995122889443e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 59/71 | LOSS: 4.48621426585305e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 60/71 | LOSS: 4.513155001076797e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 61/71 | LOSS: 4.527310070003413e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 62/71 | LOSS: 4.532400150007258e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 63/71 | LOSS: 4.516683688393641e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 64/71 | LOSS: 4.554957474913233e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 65/71 | LOSS: 4.536091775221531e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 66/71 | LOSS: 4.523691162450713e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 67/71 | LOSS: 4.50630792623569e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 68/71 | LOSS: 4.522448241492595e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 69/71 | LOSS: 4.5244384312224745e-06\n",
      "TRAIN: EPOCH 513/1000 | BATCH 70/71 | LOSS: 4.496832164951344e-06\n",
      "VAL: EPOCH 513/1000 | BATCH 0/8 | LOSS: 6.26265546088689e-06\n",
      "VAL: EPOCH 513/1000 | BATCH 1/8 | LOSS: 6.421259058697615e-06\n",
      "VAL: EPOCH 513/1000 | BATCH 2/8 | LOSS: 6.362685932496485e-06\n",
      "VAL: EPOCH 513/1000 | BATCH 3/8 | LOSS: 6.287003998295404e-06\n",
      "VAL: EPOCH 513/1000 | BATCH 4/8 | LOSS: 6.209011189639568e-06\n",
      "VAL: EPOCH 513/1000 | BATCH 5/8 | LOSS: 5.937348684407577e-06\n",
      "VAL: EPOCH 513/1000 | BATCH 6/8 | LOSS: 5.678122436490542e-06\n",
      "VAL: EPOCH 513/1000 | BATCH 7/8 | LOSS: 5.423396487458376e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 0/71 | LOSS: 4.60168075733236e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 1/71 | LOSS: 3.830261903203791e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 2/71 | LOSS: 3.7966583477100357e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 3/71 | LOSS: 4.009843223684584e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 4/71 | LOSS: 4.1122902985080145e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 5/71 | LOSS: 4.225728995758497e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 6/71 | LOSS: 4.21722292490553e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 7/71 | LOSS: 4.222112295337865e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 8/71 | LOSS: 4.185271286082247e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 9/71 | LOSS: 4.182747670711251e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 10/71 | LOSS: 4.2902029235847294e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 11/71 | LOSS: 4.329829569845363e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 12/71 | LOSS: 4.363893857281744e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 13/71 | LOSS: 4.346114565123571e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 14/71 | LOSS: 4.334675001397651e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 15/71 | LOSS: 4.352644651817172e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 16/71 | LOSS: 4.357663649844814e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 17/71 | LOSS: 4.342655655717762e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 18/71 | LOSS: 4.318450680186384e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 19/71 | LOSS: 4.316639387980104e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 20/71 | LOSS: 4.314208689763854e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 21/71 | LOSS: 4.27293806834231e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 22/71 | LOSS: 4.307101315372046e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 23/71 | LOSS: 4.265471488906769e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 24/71 | LOSS: 4.307374765630811e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 25/71 | LOSS: 4.327245313526453e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 26/71 | LOSS: 4.357495274436789e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 27/71 | LOSS: 4.3995144583927214e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 28/71 | LOSS: 4.403352296980427e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 29/71 | LOSS: 4.398081136969267e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 30/71 | LOSS: 4.425551123272124e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 31/71 | LOSS: 4.4335377111792695e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 32/71 | LOSS: 4.5058446272433326e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 33/71 | LOSS: 4.5005095698262966e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 34/71 | LOSS: 4.497498555013278e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 35/71 | LOSS: 4.487431371873956e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 36/71 | LOSS: 4.4948625465032225e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 37/71 | LOSS: 4.501093299998167e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 38/71 | LOSS: 4.4991611036657095e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 39/71 | LOSS: 4.515035516305943e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 40/71 | LOSS: 4.516747051128754e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 41/71 | LOSS: 4.541165264440462e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 42/71 | LOSS: 4.547334387901829e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 43/71 | LOSS: 4.546640070657304e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 44/71 | LOSS: 4.552990726046523e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 45/71 | LOSS: 4.550242149138206e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 46/71 | LOSS: 4.55079699601443e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 47/71 | LOSS: 4.567130758914573e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 48/71 | LOSS: 4.537844340436039e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 49/71 | LOSS: 4.510425246735394e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 50/71 | LOSS: 4.478768593852531e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 51/71 | LOSS: 4.469366165092371e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 52/71 | LOSS: 4.465465049436733e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 53/71 | LOSS: 4.453429752444478e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 54/71 | LOSS: 4.434432007242735e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 55/71 | LOSS: 4.441122452395316e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 56/71 | LOSS: 4.434651752186862e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 57/71 | LOSS: 4.425580229884684e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 58/71 | LOSS: 4.427945395175395e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 59/71 | LOSS: 4.416945485748632e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 60/71 | LOSS: 4.411802294390461e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 61/71 | LOSS: 4.399712254105966e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 62/71 | LOSS: 4.395412180735822e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 63/71 | LOSS: 4.408125555244169e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 64/71 | LOSS: 4.424466386318762e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 65/71 | LOSS: 4.4167991084329765e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 66/71 | LOSS: 4.4298333352290045e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 67/71 | LOSS: 4.45123116739577e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 68/71 | LOSS: 4.459525529376191e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 69/71 | LOSS: 4.466315730106934e-06\n",
      "TRAIN: EPOCH 514/1000 | BATCH 70/71 | LOSS: 4.500180335193184e-06\n",
      "VAL: EPOCH 514/1000 | BATCH 0/8 | LOSS: 5.0556031965243164e-06\n",
      "VAL: EPOCH 514/1000 | BATCH 1/8 | LOSS: 5.727473990191356e-06\n",
      "VAL: EPOCH 514/1000 | BATCH 2/8 | LOSS: 6.262575728518034e-06\n",
      "VAL: EPOCH 514/1000 | BATCH 3/8 | LOSS: 6.250017236197891e-06\n",
      "VAL: EPOCH 514/1000 | BATCH 4/8 | LOSS: 6.278316413954599e-06\n",
      "VAL: EPOCH 514/1000 | BATCH 5/8 | LOSS: 6.32328821363141e-06\n",
      "VAL: EPOCH 514/1000 | BATCH 6/8 | LOSS: 6.226303867151728e-06\n",
      "VAL: EPOCH 514/1000 | BATCH 7/8 | LOSS: 6.274852864862623e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 0/71 | LOSS: 6.1335408645391e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 1/71 | LOSS: 5.486554300659918e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 2/71 | LOSS: 5.6684889386815485e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 3/71 | LOSS: 5.589960323959531e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 4/71 | LOSS: 5.678471370629268e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 5/71 | LOSS: 5.463987614954628e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 6/71 | LOSS: 5.849404975119146e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 7/71 | LOSS: 5.640735651013529e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 8/71 | LOSS: 5.695535492122872e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 9/71 | LOSS: 5.581565437751123e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 10/71 | LOSS: 5.603047777020203e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 11/71 | LOSS: 5.545536775268071e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 12/71 | LOSS: 5.439623202199493e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 13/71 | LOSS: 5.432259415881292e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 14/71 | LOSS: 5.356224119168474e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 15/71 | LOSS: 5.328917808355982e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 16/71 | LOSS: 5.236707546132761e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 17/71 | LOSS: 5.1730757175189665e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 18/71 | LOSS: 5.157973347243234e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 19/71 | LOSS: 5.100894543375034e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 20/71 | LOSS: 5.158942940397537e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 21/71 | LOSS: 5.115844932003132e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 22/71 | LOSS: 5.086333766999103e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 23/71 | LOSS: 5.044221997726102e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 24/71 | LOSS: 5.044935778641957e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 25/71 | LOSS: 4.949606678704955e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 26/71 | LOSS: 4.911991052520332e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 27/71 | LOSS: 4.86409988655266e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 28/71 | LOSS: 4.835946466018554e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 29/71 | LOSS: 4.829869302132768e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 30/71 | LOSS: 4.788700160192562e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 31/71 | LOSS: 4.733295746461863e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 32/71 | LOSS: 4.7119689856062354e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 33/71 | LOSS: 4.72312584567108e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 34/71 | LOSS: 4.681528577878323e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 35/71 | LOSS: 4.672039741156671e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 36/71 | LOSS: 4.660959637083185e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 37/71 | LOSS: 4.628389900206362e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 38/71 | LOSS: 4.6268306878352096e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 39/71 | LOSS: 4.602193064329185e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 40/71 | LOSS: 4.601212323195352e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 41/71 | LOSS: 4.597233209140194e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 42/71 | LOSS: 4.645758720069479e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 43/71 | LOSS: 4.642180732100149e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 44/71 | LOSS: 4.651546876428054e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 45/71 | LOSS: 4.669873058683942e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 46/71 | LOSS: 4.657788743296554e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 47/71 | LOSS: 4.671134793928407e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 48/71 | LOSS: 4.696332232887638e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 49/71 | LOSS: 4.7066346814972345e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 50/71 | LOSS: 4.700191204713465e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 51/71 | LOSS: 4.725244313983869e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 52/71 | LOSS: 4.751581160097936e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 53/71 | LOSS: 4.75400183522315e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 54/71 | LOSS: 4.802305449464422e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 55/71 | LOSS: 4.808014499043825e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 56/71 | LOSS: 4.846925979731359e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 57/71 | LOSS: 4.858487351345408e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 58/71 | LOSS: 4.922721426876289e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 59/71 | LOSS: 4.9256075650797966e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 60/71 | LOSS: 4.942441644177096e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 61/71 | LOSS: 4.965787286803994e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 62/71 | LOSS: 4.981983916906526e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 63/71 | LOSS: 4.981950318949657e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 64/71 | LOSS: 4.979789691088417e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 65/71 | LOSS: 4.999014732454794e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 66/71 | LOSS: 4.9938001673092625e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 67/71 | LOSS: 4.987361267815097e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 68/71 | LOSS: 4.967777992230064e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 69/71 | LOSS: 4.990915652212737e-06\n",
      "TRAIN: EPOCH 515/1000 | BATCH 70/71 | LOSS: 4.974509683449123e-06\n",
      "VAL: EPOCH 515/1000 | BATCH 0/8 | LOSS: 4.424710823514033e-06\n",
      "VAL: EPOCH 515/1000 | BATCH 1/8 | LOSS: 4.492530479183188e-06\n",
      "VAL: EPOCH 515/1000 | BATCH 2/8 | LOSS: 4.740111611075311e-06\n",
      "VAL: EPOCH 515/1000 | BATCH 3/8 | LOSS: 4.818878437617968e-06\n",
      "VAL: EPOCH 515/1000 | BATCH 4/8 | LOSS: 4.799755970452679e-06\n",
      "VAL: EPOCH 515/1000 | BATCH 5/8 | LOSS: 4.91524868569589e-06\n",
      "VAL: EPOCH 515/1000 | BATCH 6/8 | LOSS: 4.849852757615736e-06\n",
      "VAL: EPOCH 515/1000 | BATCH 7/8 | LOSS: 4.7580336399732914e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 0/71 | LOSS: 3.975177605752833e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 1/71 | LOSS: 4.856017085330677e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 2/71 | LOSS: 4.530908123949e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 3/71 | LOSS: 4.872107297160255e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 4/71 | LOSS: 4.947772231389535e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 5/71 | LOSS: 4.971885346094496e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 6/71 | LOSS: 4.8951176592839015e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 7/71 | LOSS: 4.8095207034748455e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 8/71 | LOSS: 4.683636487041238e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 9/71 | LOSS: 4.765062249134644e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 10/71 | LOSS: 4.855706513560356e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 11/71 | LOSS: 4.773789858821449e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 12/71 | LOSS: 4.88101348030166e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 13/71 | LOSS: 4.824448101966741e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 14/71 | LOSS: 4.811363154052136e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 15/71 | LOSS: 4.853457710396469e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 16/71 | LOSS: 4.876200310879058e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 17/71 | LOSS: 4.876533340494562e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 18/71 | LOSS: 4.8592606904812305e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 19/71 | LOSS: 4.874330943493987e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 20/71 | LOSS: 4.832647391594946e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 21/71 | LOSS: 4.8410397126148874e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 22/71 | LOSS: 4.815615205228349e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 23/71 | LOSS: 4.818280748016453e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 24/71 | LOSS: 4.831753230973846e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 25/71 | LOSS: 4.8221834626579385e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 26/71 | LOSS: 4.803344634435942e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 27/71 | LOSS: 4.817843351442466e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 28/71 | LOSS: 4.7878851358385375e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 29/71 | LOSS: 4.774364106197026e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 30/71 | LOSS: 4.773590482669064e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 31/71 | LOSS: 4.730351435000557e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 32/71 | LOSS: 4.716474904853385e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 33/71 | LOSS: 4.719561408873072e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 34/71 | LOSS: 4.71525386923791e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 35/71 | LOSS: 4.7350788968793094e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 36/71 | LOSS: 4.699991622318693e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 37/71 | LOSS: 4.675066604345614e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 38/71 | LOSS: 4.658239784196037e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 39/71 | LOSS: 4.670910948334495e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 40/71 | LOSS: 4.667814609114785e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 41/71 | LOSS: 4.679064908313671e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 42/71 | LOSS: 4.670012680604529e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 43/71 | LOSS: 4.670238871023519e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 44/71 | LOSS: 4.646253758740689e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 45/71 | LOSS: 4.626271002843683e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 46/71 | LOSS: 4.6107834737569565e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 47/71 | LOSS: 4.59888430744589e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 48/71 | LOSS: 4.594375500995998e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 49/71 | LOSS: 4.587802104651928e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 50/71 | LOSS: 4.587519594911011e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 51/71 | LOSS: 4.580707276055853e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 52/71 | LOSS: 4.578439349540342e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 53/71 | LOSS: 4.594014373853699e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 54/71 | LOSS: 4.5927195969852615e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 55/71 | LOSS: 4.606835724579079e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 56/71 | LOSS: 4.611909070374902e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 57/71 | LOSS: 4.606228936483666e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 58/71 | LOSS: 4.601162915713434e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 59/71 | LOSS: 4.6237566114844714e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 60/71 | LOSS: 4.629679536816691e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 61/71 | LOSS: 4.64775329448692e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 62/71 | LOSS: 4.663480835699288e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 63/71 | LOSS: 4.658967405646308e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 64/71 | LOSS: 4.655582667440355e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 65/71 | LOSS: 4.6656079410406e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 66/71 | LOSS: 4.6691184998867836e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 67/71 | LOSS: 4.651433029908624e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 68/71 | LOSS: 4.6556110547775775e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 69/71 | LOSS: 4.6429862939995865e-06\n",
      "TRAIN: EPOCH 516/1000 | BATCH 70/71 | LOSS: 4.6326805864148e-06\n",
      "VAL: EPOCH 516/1000 | BATCH 0/8 | LOSS: 4.097257715329761e-06\n",
      "VAL: EPOCH 516/1000 | BATCH 1/8 | LOSS: 4.488988224693458e-06\n",
      "VAL: EPOCH 516/1000 | BATCH 2/8 | LOSS: 4.603534307534574e-06\n",
      "VAL: EPOCH 516/1000 | BATCH 3/8 | LOSS: 4.573117735162668e-06\n",
      "VAL: EPOCH 516/1000 | BATCH 4/8 | LOSS: 4.57376891063177e-06\n",
      "VAL: EPOCH 516/1000 | BATCH 5/8 | LOSS: 4.4447250502344104e-06\n",
      "VAL: EPOCH 516/1000 | BATCH 6/8 | LOSS: 4.349771350981817e-06\n",
      "VAL: EPOCH 516/1000 | BATCH 7/8 | LOSS: 4.242966070933107e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 0/71 | LOSS: 4.3832665141962934e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 1/71 | LOSS: 4.345883326095645e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 2/71 | LOSS: 4.399062618176686e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 3/71 | LOSS: 4.159836009876017e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 4/71 | LOSS: 4.239754935042583e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 5/71 | LOSS: 4.65523313171919e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 6/71 | LOSS: 4.524381081085137e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 7/71 | LOSS: 4.621365377488473e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 8/71 | LOSS: 4.585337617552594e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 9/71 | LOSS: 4.692494735536457e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 10/71 | LOSS: 4.689229688830727e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 11/71 | LOSS: 4.815145549249185e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 12/71 | LOSS: 4.8119774979008634e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 13/71 | LOSS: 4.874317011984074e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 14/71 | LOSS: 4.915065513462954e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 15/71 | LOSS: 4.8856469589964036e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 16/71 | LOSS: 4.94407056387846e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 17/71 | LOSS: 4.919456450301772e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 18/71 | LOSS: 4.9685259838090705e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 19/71 | LOSS: 4.948876301114069e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 20/71 | LOSS: 4.935593822318922e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 21/71 | LOSS: 4.880365467711685e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 22/71 | LOSS: 4.883225054309941e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 23/71 | LOSS: 4.878437901349268e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 24/71 | LOSS: 4.850996983805089e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 25/71 | LOSS: 4.829320766055809e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 26/71 | LOSS: 4.798552818438448e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 27/71 | LOSS: 4.752934671614639e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 28/71 | LOSS: 4.734808553935024e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 29/71 | LOSS: 4.736292483660994e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 30/71 | LOSS: 4.733578092078284e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 31/71 | LOSS: 4.7564208571770905e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 32/71 | LOSS: 4.77549331828248e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 33/71 | LOSS: 4.774872362598698e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 34/71 | LOSS: 4.775835746581184e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 35/71 | LOSS: 4.7489005762852484e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 36/71 | LOSS: 4.767392855117922e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 37/71 | LOSS: 4.738418056149385e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 38/71 | LOSS: 4.730346832869915e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 39/71 | LOSS: 4.723167035081133e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 40/71 | LOSS: 4.715776978583762e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 41/71 | LOSS: 4.6932897086143e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 42/71 | LOSS: 4.699617029261791e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 43/71 | LOSS: 4.6926318501308826e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 44/71 | LOSS: 4.712349467202633e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 45/71 | LOSS: 4.689841721696923e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 46/71 | LOSS: 4.677718253971877e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 47/71 | LOSS: 4.687327868661366e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 48/71 | LOSS: 4.658819191216917e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 49/71 | LOSS: 4.655168900171702e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 50/71 | LOSS: 4.644553227455403e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 51/71 | LOSS: 4.63619651414914e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 52/71 | LOSS: 4.6352654579713315e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 53/71 | LOSS: 4.629005148773204e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 54/71 | LOSS: 4.618816873599744e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 55/71 | LOSS: 4.616434941746255e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 56/71 | LOSS: 4.610987596670543e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 57/71 | LOSS: 4.61852885266502e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 58/71 | LOSS: 4.583923316466222e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 59/71 | LOSS: 4.569302264674964e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 60/71 | LOSS: 4.570946467523396e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 61/71 | LOSS: 4.563738974455543e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 62/71 | LOSS: 4.570619791723272e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 63/71 | LOSS: 4.555113495285923e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 64/71 | LOSS: 4.591088442724028e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 65/71 | LOSS: 4.579377456469626e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 66/71 | LOSS: 4.600552586846639e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 67/71 | LOSS: 4.621318001842583e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 68/71 | LOSS: 4.618308063678948e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 69/71 | LOSS: 4.618236740887889e-06\n",
      "TRAIN: EPOCH 517/1000 | BATCH 70/71 | LOSS: 4.628799695663628e-06\n",
      "VAL: EPOCH 517/1000 | BATCH 0/8 | LOSS: 5.013664122088812e-06\n",
      "VAL: EPOCH 517/1000 | BATCH 1/8 | LOSS: 5.439244205263094e-06\n",
      "VAL: EPOCH 517/1000 | BATCH 2/8 | LOSS: 5.636440164380474e-06\n",
      "VAL: EPOCH 517/1000 | BATCH 3/8 | LOSS: 5.609536287920491e-06\n",
      "VAL: EPOCH 517/1000 | BATCH 4/8 | LOSS: 5.607816365227336e-06\n",
      "VAL: EPOCH 517/1000 | BATCH 5/8 | LOSS: 5.6212046880925e-06\n",
      "VAL: EPOCH 517/1000 | BATCH 6/8 | LOSS: 5.486617023312387e-06\n",
      "VAL: EPOCH 517/1000 | BATCH 7/8 | LOSS: 5.4481977826981165e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 0/71 | LOSS: 5.467856226459844e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 1/71 | LOSS: 4.788972319147433e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 2/71 | LOSS: 5.038597161425666e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 3/71 | LOSS: 5.024049869462033e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 4/71 | LOSS: 4.952816743752919e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 5/71 | LOSS: 4.987045410113448e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 6/71 | LOSS: 5.01871162279193e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 7/71 | LOSS: 5.015672002173233e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 8/71 | LOSS: 5.065648717087849e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 9/71 | LOSS: 5.130033605382777e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 10/71 | LOSS: 5.05875839900744e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 11/71 | LOSS: 5.057257567386841e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 12/71 | LOSS: 4.983860260556237e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 13/71 | LOSS: 4.998491996437744e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 14/71 | LOSS: 4.965196876582923e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 15/71 | LOSS: 4.961941215242405e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 16/71 | LOSS: 4.959870720460244e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 17/71 | LOSS: 4.882511322850607e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 18/71 | LOSS: 4.886846622148848e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 19/71 | LOSS: 4.9095320946435095e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 20/71 | LOSS: 4.8376899983503834e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 21/71 | LOSS: 4.879587612032827e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 22/71 | LOSS: 4.851109165231363e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 23/71 | LOSS: 4.838039937264209e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 24/71 | LOSS: 4.803449628525413e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 25/71 | LOSS: 4.803014812401111e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 26/71 | LOSS: 4.7551121935140585e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 27/71 | LOSS: 4.73223976119438e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 28/71 | LOSS: 4.726862165437618e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 29/71 | LOSS: 4.720633023680421e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 30/71 | LOSS: 4.710359917226015e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 31/71 | LOSS: 4.73375084197869e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 32/71 | LOSS: 4.696020024504943e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 33/71 | LOSS: 4.671116076839526e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 34/71 | LOSS: 4.672750531296645e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 35/71 | LOSS: 4.69115138912457e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 36/71 | LOSS: 4.681139855804488e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 37/71 | LOSS: 4.6568076246139275e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 38/71 | LOSS: 4.614317195097474e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 39/71 | LOSS: 4.594753465880785e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 40/71 | LOSS: 4.604352342561825e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 41/71 | LOSS: 4.6070129731864045e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 42/71 | LOSS: 4.604492756317445e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 43/71 | LOSS: 4.585809981769919e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 44/71 | LOSS: 4.579471139246986e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 45/71 | LOSS: 4.557860393106239e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 46/71 | LOSS: 4.527416780199038e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 47/71 | LOSS: 4.499745505389304e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 48/71 | LOSS: 4.5098565713450794e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 49/71 | LOSS: 4.498994626374042e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 50/71 | LOSS: 4.526082233603708e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 51/71 | LOSS: 4.521534619400952e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 52/71 | LOSS: 4.521402950490054e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 53/71 | LOSS: 4.529654676035635e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 54/71 | LOSS: 4.525341050380534e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 55/71 | LOSS: 4.551978248303255e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 56/71 | LOSS: 4.563079125562166e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 57/71 | LOSS: 4.5597577205814465e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 58/71 | LOSS: 4.556903438966956e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 59/71 | LOSS: 4.576948515477852e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 60/71 | LOSS: 4.58685428279974e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 61/71 | LOSS: 4.628708149040552e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 62/71 | LOSS: 4.62875289572583e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 63/71 | LOSS: 4.642284157085896e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 64/71 | LOSS: 4.629748557887909e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 65/71 | LOSS: 4.631531203702969e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 66/71 | LOSS: 4.632082691251295e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 67/71 | LOSS: 4.652844585748158e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 68/71 | LOSS: 4.6373747633312465e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 69/71 | LOSS: 4.63343615138001e-06\n",
      "TRAIN: EPOCH 518/1000 | BATCH 70/71 | LOSS: 4.629864733174081e-06\n",
      "VAL: EPOCH 518/1000 | BATCH 0/8 | LOSS: 4.349384653323796e-06\n",
      "VAL: EPOCH 518/1000 | BATCH 1/8 | LOSS: 4.730304681288544e-06\n",
      "VAL: EPOCH 518/1000 | BATCH 2/8 | LOSS: 4.840601074344401e-06\n",
      "VAL: EPOCH 518/1000 | BATCH 3/8 | LOSS: 4.935427682539739e-06\n",
      "VAL: EPOCH 518/1000 | BATCH 4/8 | LOSS: 4.8674818572180815e-06\n",
      "VAL: EPOCH 518/1000 | BATCH 5/8 | LOSS: 4.823989456781419e-06\n",
      "VAL: EPOCH 518/1000 | BATCH 6/8 | LOSS: 4.680568703666463e-06\n",
      "VAL: EPOCH 518/1000 | BATCH 7/8 | LOSS: 4.474822276279156e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 0/71 | LOSS: 4.912471922580153e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 1/71 | LOSS: 4.888517651124857e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 2/71 | LOSS: 4.786687895830255e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 3/71 | LOSS: 4.722269977719407e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 4/71 | LOSS: 4.542208171187667e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 5/71 | LOSS: 4.424230534520272e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 6/71 | LOSS: 4.483252593802588e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 7/71 | LOSS: 4.471761258173501e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 8/71 | LOSS: 4.507745567631598e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 9/71 | LOSS: 4.466658174351323e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 10/71 | LOSS: 4.44493591864805e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 11/71 | LOSS: 4.477848127256341e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 12/71 | LOSS: 4.534902822902391e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 13/71 | LOSS: 4.476340937539394e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 14/71 | LOSS: 4.48037091397661e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 15/71 | LOSS: 4.5156807715329705e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 16/71 | LOSS: 4.647608327158035e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 17/71 | LOSS: 4.719286342454628e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 18/71 | LOSS: 4.715563986074382e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 19/71 | LOSS: 4.7938584998519215e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 20/71 | LOSS: 4.7631163808892735e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 21/71 | LOSS: 4.786190205860211e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 22/71 | LOSS: 4.748738092530402e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 23/71 | LOSS: 4.824083333687668e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 24/71 | LOSS: 4.808831590707996e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 25/71 | LOSS: 4.800323696284837e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 26/71 | LOSS: 4.802073068209144e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 27/71 | LOSS: 4.828770775994988e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 28/71 | LOSS: 4.8205758330682174e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 29/71 | LOSS: 4.823926481852444e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 30/71 | LOSS: 4.811592244521073e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 31/71 | LOSS: 4.807842692855502e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 32/71 | LOSS: 4.784927796208649e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 33/71 | LOSS: 4.8149710343803985e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 34/71 | LOSS: 4.82458240185224e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 35/71 | LOSS: 4.811444006867936e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 36/71 | LOSS: 4.810670171119037e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 37/71 | LOSS: 4.7751060133362585e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 38/71 | LOSS: 4.768113011521517e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 39/71 | LOSS: 4.752440401034619e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 40/71 | LOSS: 4.736927560704161e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 41/71 | LOSS: 4.71054584392862e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 42/71 | LOSS: 4.720851351056237e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 43/71 | LOSS: 4.718654942983201e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 44/71 | LOSS: 4.703772497628557e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 45/71 | LOSS: 4.68910802144661e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 46/71 | LOSS: 4.690285879134651e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 47/71 | LOSS: 4.706376131480283e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 48/71 | LOSS: 4.754883617834468e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 49/71 | LOSS: 4.7635226565034826e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 50/71 | LOSS: 4.801778098172851e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 51/71 | LOSS: 4.803426900952428e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 52/71 | LOSS: 4.79068076633409e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 53/71 | LOSS: 4.838355220339575e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 54/71 | LOSS: 4.824166973610938e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 55/71 | LOSS: 4.840880884720329e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 56/71 | LOSS: 4.823868860975174e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 57/71 | LOSS: 4.846577251649658e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 58/71 | LOSS: 4.832200428902101e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 59/71 | LOSS: 4.830220882467984e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 60/71 | LOSS: 4.840382696217647e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 61/71 | LOSS: 4.824512577596902e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 62/71 | LOSS: 4.8161882804133855e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 63/71 | LOSS: 4.804196308327846e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 64/71 | LOSS: 4.804725134678078e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 65/71 | LOSS: 4.797191319415685e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 66/71 | LOSS: 4.791960114445503e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 67/71 | LOSS: 4.796112895800273e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 68/71 | LOSS: 4.789256071608574e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 69/71 | LOSS: 4.760878963289932e-06\n",
      "TRAIN: EPOCH 519/1000 | BATCH 70/71 | LOSS: 4.743222762941437e-06\n",
      "VAL: EPOCH 519/1000 | BATCH 0/8 | LOSS: 5.116165993968025e-06\n",
      "VAL: EPOCH 519/1000 | BATCH 1/8 | LOSS: 5.067223355581518e-06\n",
      "VAL: EPOCH 519/1000 | BATCH 2/8 | LOSS: 5.0844986011118936e-06\n",
      "VAL: EPOCH 519/1000 | BATCH 3/8 | LOSS: 5.040445103077218e-06\n",
      "VAL: EPOCH 519/1000 | BATCH 4/8 | LOSS: 4.9664902689983135e-06\n",
      "VAL: EPOCH 519/1000 | BATCH 5/8 | LOSS: 4.791205962343763e-06\n",
      "VAL: EPOCH 519/1000 | BATCH 6/8 | LOSS: 4.589209830945558e-06\n",
      "VAL: EPOCH 519/1000 | BATCH 7/8 | LOSS: 4.396884207835683e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 0/71 | LOSS: 4.778801667271182e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 1/71 | LOSS: 5.1038032324868254e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 2/71 | LOSS: 5.367704640472463e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 3/71 | LOSS: 5.340655661711935e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 4/71 | LOSS: 5.563836202782113e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 5/71 | LOSS: 5.418604966204536e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 6/71 | LOSS: 5.176077463277449e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 7/71 | LOSS: 5.077539924513985e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 8/71 | LOSS: 5.102968215295631e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 9/71 | LOSS: 5.031827686252654e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 10/71 | LOSS: 5.0779230233191894e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 11/71 | LOSS: 5.082250671269624e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 12/71 | LOSS: 5.059980524562819e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 13/71 | LOSS: 4.934384881900249e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 14/71 | LOSS: 4.8295101654124055e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 15/71 | LOSS: 4.787648364867891e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 16/71 | LOSS: 4.800770284418832e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 17/71 | LOSS: 4.788526830957886e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 18/71 | LOSS: 4.807863505400172e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 19/71 | LOSS: 4.839583436933026e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 20/71 | LOSS: 4.831642150921176e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 21/71 | LOSS: 4.8036101092789485e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 22/71 | LOSS: 4.798056174284126e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 23/71 | LOSS: 4.757201348108235e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 24/71 | LOSS: 4.693572991527617e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 25/71 | LOSS: 4.7205663017498755e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 26/71 | LOSS: 4.680662877271297e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 27/71 | LOSS: 4.657158147633059e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 28/71 | LOSS: 4.664378217245845e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 29/71 | LOSS: 4.6545705420915816e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 30/71 | LOSS: 4.6155795746838725e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 31/71 | LOSS: 4.618556140201235e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 32/71 | LOSS: 4.641091980815735e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 33/71 | LOSS: 4.6310058023129176e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 34/71 | LOSS: 4.6117718544077695e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 35/71 | LOSS: 4.582198187108588e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 36/71 | LOSS: 4.560714461223216e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 37/71 | LOSS: 4.514415147576456e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 38/71 | LOSS: 4.50509777203059e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 39/71 | LOSS: 4.500995169109956e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 40/71 | LOSS: 4.494365140733241e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 41/71 | LOSS: 4.4914800561119114e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 42/71 | LOSS: 4.500870714806762e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 43/71 | LOSS: 4.489338741884487e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 44/71 | LOSS: 4.484279361349763e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 45/71 | LOSS: 4.462177279808953e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 46/71 | LOSS: 4.44258948652745e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 47/71 | LOSS: 4.433695930098717e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 48/71 | LOSS: 4.45579531997839e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 49/71 | LOSS: 4.446047087185434e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 50/71 | LOSS: 4.4381854868512234e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 51/71 | LOSS: 4.426709796671975e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 52/71 | LOSS: 4.420819224387227e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 53/71 | LOSS: 4.420941304825074e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 54/71 | LOSS: 4.412350600349865e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 55/71 | LOSS: 4.440536713445908e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 56/71 | LOSS: 4.421888892817119e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 57/71 | LOSS: 4.425473795483162e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 58/71 | LOSS: 4.410078830360208e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 59/71 | LOSS: 4.41303776597124e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 60/71 | LOSS: 4.42727880789655e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 61/71 | LOSS: 4.464575622918229e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 62/71 | LOSS: 4.4638381914699005e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 63/71 | LOSS: 4.4760855608672045e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 64/71 | LOSS: 4.509666055477842e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 65/71 | LOSS: 4.528301515578774e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 66/71 | LOSS: 4.534948494207373e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 67/71 | LOSS: 4.5572586854552145e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 68/71 | LOSS: 4.573083538491408e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 69/71 | LOSS: 4.574612704085926e-06\n",
      "TRAIN: EPOCH 520/1000 | BATCH 70/71 | LOSS: 4.621788208860893e-06\n",
      "VAL: EPOCH 520/1000 | BATCH 0/8 | LOSS: 5.439726010081358e-06\n",
      "VAL: EPOCH 520/1000 | BATCH 1/8 | LOSS: 5.349931143427966e-06\n",
      "VAL: EPOCH 520/1000 | BATCH 2/8 | LOSS: 5.343650324599973e-06\n",
      "VAL: EPOCH 520/1000 | BATCH 3/8 | LOSS: 5.357582381293469e-06\n",
      "VAL: EPOCH 520/1000 | BATCH 4/8 | LOSS: 5.312375742505538e-06\n",
      "VAL: EPOCH 520/1000 | BATCH 5/8 | LOSS: 5.203346897057297e-06\n",
      "VAL: EPOCH 520/1000 | BATCH 6/8 | LOSS: 5.038148693919149e-06\n",
      "VAL: EPOCH 520/1000 | BATCH 7/8 | LOSS: 4.883159107293977e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 0/71 | LOSS: 4.374086529423948e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 1/71 | LOSS: 6.337483682727907e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 2/71 | LOSS: 5.605517192937744e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 3/71 | LOSS: 5.958615702184034e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 4/71 | LOSS: 5.710377990908455e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 5/71 | LOSS: 5.471999808529897e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 6/71 | LOSS: 5.461426618629568e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 7/71 | LOSS: 5.498205212006724e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 8/71 | LOSS: 5.621276007635363e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 9/71 | LOSS: 5.5564800732099686e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 10/71 | LOSS: 5.4716576662692455e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 11/71 | LOSS: 5.3461804251734675e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 12/71 | LOSS: 5.312992890629595e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 13/71 | LOSS: 5.225188910376996e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 14/71 | LOSS: 5.24617553310236e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 15/71 | LOSS: 5.191432791207262e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 16/71 | LOSS: 5.138729380525868e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 17/71 | LOSS: 5.07288930546363e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 18/71 | LOSS: 5.039255029259948e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 19/71 | LOSS: 5.041458075538685e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 20/71 | LOSS: 5.003526677596494e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 21/71 | LOSS: 5.0185368846459e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 22/71 | LOSS: 4.97617393984515e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 23/71 | LOSS: 4.945481634877069e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 24/71 | LOSS: 4.916585185128497e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 25/71 | LOSS: 4.865932466530537e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 26/71 | LOSS: 4.837269073134478e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 27/71 | LOSS: 4.796571455959306e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 28/71 | LOSS: 4.769199833766646e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 29/71 | LOSS: 4.754989079932178e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 30/71 | LOSS: 4.7221734172536905e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 31/71 | LOSS: 4.712391898920032e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 32/71 | LOSS: 4.7263702761097885e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 33/71 | LOSS: 4.713218900446116e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 34/71 | LOSS: 4.739661952563827e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 35/71 | LOSS: 4.73046730399397e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 36/71 | LOSS: 4.755735689458733e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 37/71 | LOSS: 4.799790184207316e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 38/71 | LOSS: 4.773453080512506e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 39/71 | LOSS: 4.743770602999575e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 40/71 | LOSS: 4.778999593282806e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 41/71 | LOSS: 4.783687191298841e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 42/71 | LOSS: 4.756641892315603e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 43/71 | LOSS: 4.755126486667326e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 44/71 | LOSS: 4.744769093021103e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 45/71 | LOSS: 4.7301640753848124e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 46/71 | LOSS: 4.710608431815536e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 47/71 | LOSS: 4.730537796149292e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 48/71 | LOSS: 4.727480291894979e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 49/71 | LOSS: 4.730658311018488e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 50/71 | LOSS: 4.743257490832441e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 51/71 | LOSS: 4.741178127705308e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 52/71 | LOSS: 4.716884790949099e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 53/71 | LOSS: 4.715825184575806e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 54/71 | LOSS: 4.697960503108334e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 55/71 | LOSS: 4.696563654046518e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 56/71 | LOSS: 4.689910024592935e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 57/71 | LOSS: 4.675807202084829e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 58/71 | LOSS: 4.656077539100622e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 59/71 | LOSS: 4.678236492357731e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 60/71 | LOSS: 4.658506057120442e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 61/71 | LOSS: 4.68024487902092e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 62/71 | LOSS: 4.6971129356120765e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 63/71 | LOSS: 4.697773622552859e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 64/71 | LOSS: 4.698013705050331e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 65/71 | LOSS: 4.6951556428074115e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 66/71 | LOSS: 4.716902411480498e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 67/71 | LOSS: 4.712842306108702e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 68/71 | LOSS: 4.718393994693727e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 69/71 | LOSS: 4.729877845680416e-06\n",
      "TRAIN: EPOCH 521/1000 | BATCH 70/71 | LOSS: 4.753209335011517e-06\n",
      "VAL: EPOCH 521/1000 | BATCH 0/8 | LOSS: 5.9792496358568314e-06\n",
      "VAL: EPOCH 521/1000 | BATCH 1/8 | LOSS: 6.024193680786993e-06\n",
      "VAL: EPOCH 521/1000 | BATCH 2/8 | LOSS: 6.122815345103542e-06\n",
      "VAL: EPOCH 521/1000 | BATCH 3/8 | LOSS: 5.941476729276474e-06\n",
      "VAL: EPOCH 521/1000 | BATCH 4/8 | LOSS: 6.051061609468888e-06\n",
      "VAL: EPOCH 521/1000 | BATCH 5/8 | LOSS: 5.799302243758575e-06\n",
      "VAL: EPOCH 521/1000 | BATCH 6/8 | LOSS: 5.713083152971064e-06\n",
      "VAL: EPOCH 521/1000 | BATCH 7/8 | LOSS: 5.610469656858186e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 0/71 | LOSS: 5.249209152680123e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 1/71 | LOSS: 4.805903017768287e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 2/71 | LOSS: 5.06033514587519e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 3/71 | LOSS: 4.734531103167683e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 4/71 | LOSS: 4.992852154828142e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 5/71 | LOSS: 4.870459861194831e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 6/71 | LOSS: 4.774294991096083e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 7/71 | LOSS: 4.695575967161858e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 8/71 | LOSS: 4.657399383884493e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 9/71 | LOSS: 4.585476062857197e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 10/71 | LOSS: 4.525333100570027e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 11/71 | LOSS: 4.534367008091067e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 12/71 | LOSS: 4.489004543127241e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 13/71 | LOSS: 4.4349466341892755e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 14/71 | LOSS: 4.352621454017936e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 15/71 | LOSS: 4.33987935366531e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 16/71 | LOSS: 4.282244965318515e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 17/71 | LOSS: 4.268949156640802e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 18/71 | LOSS: 4.2521271792027525e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 19/71 | LOSS: 4.258026604020415e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 20/71 | LOSS: 4.259804979613234e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 21/71 | LOSS: 4.301377667946905e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 22/71 | LOSS: 4.27347198756647e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 23/71 | LOSS: 4.286177528456392e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 24/71 | LOSS: 4.3104080668854295e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 25/71 | LOSS: 4.324485827895002e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 26/71 | LOSS: 4.398082140781938e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 27/71 | LOSS: 4.38651908650302e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 28/71 | LOSS: 4.3662745266092e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 29/71 | LOSS: 4.370898773231602e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 30/71 | LOSS: 4.3573086625861655e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 31/71 | LOSS: 4.3763841048871654e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 32/71 | LOSS: 4.364961782674513e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 33/71 | LOSS: 4.3495198135989245e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 34/71 | LOSS: 4.349143526789183e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 35/71 | LOSS: 4.36284281906612e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 36/71 | LOSS: 4.340003774691345e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 37/71 | LOSS: 4.300319927240612e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 38/71 | LOSS: 4.300892751183337e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 39/71 | LOSS: 4.276651185364244e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 40/71 | LOSS: 4.263691541227007e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 41/71 | LOSS: 4.289299243152657e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 42/71 | LOSS: 4.304755060899012e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 43/71 | LOSS: 4.348471365615709e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 44/71 | LOSS: 4.3428121696504406e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 45/71 | LOSS: 4.360133704287127e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 46/71 | LOSS: 4.361204311011379e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 47/71 | LOSS: 4.383676791045825e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 48/71 | LOSS: 4.381994493050282e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 49/71 | LOSS: 4.385895990708377e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 50/71 | LOSS: 4.407425163377661e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 51/71 | LOSS: 4.424184068669162e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 52/71 | LOSS: 4.423099387665543e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 53/71 | LOSS: 4.426952247942811e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 54/71 | LOSS: 4.430694860440616e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 55/71 | LOSS: 4.41346502709296e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 56/71 | LOSS: 4.405384914749894e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 57/71 | LOSS: 4.409635533287154e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 58/71 | LOSS: 4.400106556235094e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 59/71 | LOSS: 4.410428573464742e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 60/71 | LOSS: 4.406365786083176e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 61/71 | LOSS: 4.410097627412817e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 62/71 | LOSS: 4.4070511721079205e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 63/71 | LOSS: 4.413920727586174e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 64/71 | LOSS: 4.406835516699805e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 65/71 | LOSS: 4.417013852534265e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 66/71 | LOSS: 4.407760733651664e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 67/71 | LOSS: 4.416581385371769e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 68/71 | LOSS: 4.402303612432266e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 69/71 | LOSS: 4.3975406015306365e-06\n",
      "TRAIN: EPOCH 522/1000 | BATCH 70/71 | LOSS: 4.428276788914932e-06\n",
      "VAL: EPOCH 522/1000 | BATCH 0/8 | LOSS: 4.059720140503487e-06\n",
      "VAL: EPOCH 522/1000 | BATCH 1/8 | LOSS: 4.45382397629146e-06\n",
      "VAL: EPOCH 522/1000 | BATCH 2/8 | LOSS: 4.717761385109043e-06\n",
      "VAL: EPOCH 522/1000 | BATCH 3/8 | LOSS: 4.72259864636726e-06\n",
      "VAL: EPOCH 522/1000 | BATCH 4/8 | LOSS: 4.751496544486145e-06\n",
      "VAL: EPOCH 522/1000 | BATCH 5/8 | LOSS: 4.708932086335456e-06\n",
      "VAL: EPOCH 522/1000 | BATCH 6/8 | LOSS: 4.6014520194148645e-06\n",
      "VAL: EPOCH 522/1000 | BATCH 7/8 | LOSS: 4.567401049371256e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 0/71 | LOSS: 4.277228072169237e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 1/71 | LOSS: 4.792231720784912e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 2/71 | LOSS: 4.834386042299836e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 3/71 | LOSS: 4.763224069392891e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 4/71 | LOSS: 4.75327969979844e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 5/71 | LOSS: 4.862526263120041e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 6/71 | LOSS: 4.757292312465974e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 7/71 | LOSS: 4.8177794269577134e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 8/71 | LOSS: 4.716768469532124e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 9/71 | LOSS: 4.713176895165816e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 10/71 | LOSS: 4.735230463252678e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 11/71 | LOSS: 4.7376753779341625e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 12/71 | LOSS: 4.776971204242168e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 13/71 | LOSS: 4.880267364829446e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 14/71 | LOSS: 4.909901357071552e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 15/71 | LOSS: 4.862108738734605e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 16/71 | LOSS: 4.833280426657225e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 17/71 | LOSS: 4.825658809042781e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 18/71 | LOSS: 4.768334627825473e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 19/71 | LOSS: 4.8099833975356885e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 20/71 | LOSS: 4.795845208198963e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 21/71 | LOSS: 4.803033293269849e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 22/71 | LOSS: 4.760896899824729e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 23/71 | LOSS: 4.748416112458169e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 24/71 | LOSS: 4.679419998865342e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 25/71 | LOSS: 4.647698950485881e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 26/71 | LOSS: 4.6415588494036695e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 27/71 | LOSS: 4.6175057052875805e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 28/71 | LOSS: 4.6073082714805234e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 29/71 | LOSS: 4.592667528413585e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 30/71 | LOSS: 4.602590516080982e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 31/71 | LOSS: 4.5607377359146994e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 32/71 | LOSS: 4.594855950430925e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 33/71 | LOSS: 4.5633774815916484e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 34/71 | LOSS: 4.550544573638555e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 35/71 | LOSS: 4.511051258759835e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 36/71 | LOSS: 4.552431391857681e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 37/71 | LOSS: 4.525258735779253e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 38/71 | LOSS: 4.596641439582947e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 39/71 | LOSS: 4.576095062702734e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 40/71 | LOSS: 4.617161316562532e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 41/71 | LOSS: 4.604024628038349e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 42/71 | LOSS: 4.59954512100308e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 43/71 | LOSS: 4.58253890656124e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 44/71 | LOSS: 4.609783774059098e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 45/71 | LOSS: 4.586363176080656e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 46/71 | LOSS: 4.59067801939648e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 47/71 | LOSS: 4.5760031165779464e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 48/71 | LOSS: 4.575094797993577e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 49/71 | LOSS: 4.570733181026299e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 50/71 | LOSS: 4.563766928114608e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 51/71 | LOSS: 4.547693160580261e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 52/71 | LOSS: 4.5704923993397176e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 53/71 | LOSS: 4.611738749763053e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 54/71 | LOSS: 4.604675533218225e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 55/71 | LOSS: 4.618083794736906e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 56/71 | LOSS: 4.621919655172653e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 57/71 | LOSS: 4.600304880019896e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 58/71 | LOSS: 4.6034340627202405e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 59/71 | LOSS: 4.617884208831432e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 60/71 | LOSS: 4.617728411146539e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 61/71 | LOSS: 4.627174055185019e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 62/71 | LOSS: 4.6077500789420265e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 63/71 | LOSS: 4.632219241074154e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 64/71 | LOSS: 4.623834666535214e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 65/71 | LOSS: 4.635079631732493e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 66/71 | LOSS: 4.6449966540326865e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 67/71 | LOSS: 4.627039606261592e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 68/71 | LOSS: 4.614137651515193e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 69/71 | LOSS: 4.604290156099263e-06\n",
      "TRAIN: EPOCH 523/1000 | BATCH 70/71 | LOSS: 4.58954793148351e-06\n",
      "VAL: EPOCH 523/1000 | BATCH 0/8 | LOSS: 4.928795533487573e-06\n",
      "VAL: EPOCH 523/1000 | BATCH 1/8 | LOSS: 5.2031530231033685e-06\n",
      "VAL: EPOCH 523/1000 | BATCH 2/8 | LOSS: 5.170995791559108e-06\n",
      "VAL: EPOCH 523/1000 | BATCH 3/8 | LOSS: 5.221858145887381e-06\n",
      "VAL: EPOCH 523/1000 | BATCH 4/8 | LOSS: 5.150840206624707e-06\n",
      "VAL: EPOCH 523/1000 | BATCH 5/8 | LOSS: 5.074221386773085e-06\n",
      "VAL: EPOCH 523/1000 | BATCH 6/8 | LOSS: 4.891981396732652e-06\n",
      "VAL: EPOCH 523/1000 | BATCH 7/8 | LOSS: 4.659125067973946e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 0/71 | LOSS: 4.535927928372985e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 1/71 | LOSS: 4.418681555762305e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 2/71 | LOSS: 4.390315098135034e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 3/71 | LOSS: 4.4479671714725555e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 4/71 | LOSS: 4.511222232395084e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 5/71 | LOSS: 4.4101505712509e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 6/71 | LOSS: 4.536009847860052e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 7/71 | LOSS: 4.4683237661047315e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 8/71 | LOSS: 4.447234409275956e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 9/71 | LOSS: 4.469139867069316e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 10/71 | LOSS: 4.48538904410353e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 11/71 | LOSS: 4.621417057630121e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 12/71 | LOSS: 4.6165718441233576e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 13/71 | LOSS: 4.653965561374207e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 14/71 | LOSS: 4.809015081264079e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 15/71 | LOSS: 4.823772201234533e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 16/71 | LOSS: 4.782784291070557e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 17/71 | LOSS: 4.88892361419049e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 18/71 | LOSS: 4.9096817603736115e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 19/71 | LOSS: 4.883829569735099e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 20/71 | LOSS: 4.998694185771802e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 21/71 | LOSS: 5.0291432878359705e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 22/71 | LOSS: 5.0011513532966925e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 23/71 | LOSS: 5.007602093580014e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 24/71 | LOSS: 5.069742983323522e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 25/71 | LOSS: 5.0946266758098946e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 26/71 | LOSS: 5.122190874249942e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 27/71 | LOSS: 5.185659103647465e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 28/71 | LOSS: 5.189113584618036e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 29/71 | LOSS: 5.195877747610211e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 30/71 | LOSS: 5.261132437519864e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 31/71 | LOSS: 5.264163007723255e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 32/71 | LOSS: 5.270554483462492e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 33/71 | LOSS: 5.322771371271194e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 34/71 | LOSS: 5.325379957607116e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 35/71 | LOSS: 5.413491319611947e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 36/71 | LOSS: 5.4731694945076015e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 37/71 | LOSS: 5.4775276854151775e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 38/71 | LOSS: 5.471074741646403e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 39/71 | LOSS: 5.577784793331375e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 40/71 | LOSS: 5.539613960913479e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 41/71 | LOSS: 5.58817136412212e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 42/71 | LOSS: 5.577382400884363e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 43/71 | LOSS: 5.6292812938739765e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 44/71 | LOSS: 5.59696156617267e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 45/71 | LOSS: 5.591822400674573e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 46/71 | LOSS: 5.576611977417958e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 47/71 | LOSS: 5.537370990775041e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 48/71 | LOSS: 5.544965145473929e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 49/71 | LOSS: 5.569743825617479e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 50/71 | LOSS: 5.567501081232502e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 51/71 | LOSS: 5.561337117726977e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 52/71 | LOSS: 5.5519289642963385e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 53/71 | LOSS: 5.574116896780281e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 54/71 | LOSS: 5.54880011391519e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 55/71 | LOSS: 5.547729017507663e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 56/71 | LOSS: 5.542142955898287e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 57/71 | LOSS: 5.53190367444868e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 58/71 | LOSS: 5.498085101862671e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 59/71 | LOSS: 5.479588799062185e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 60/71 | LOSS: 5.4665695672871956e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 61/71 | LOSS: 5.440289757438398e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 62/71 | LOSS: 5.414034737282597e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 63/71 | LOSS: 5.400142701006416e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 64/71 | LOSS: 5.393278510942205e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 65/71 | LOSS: 5.360723989586845e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 66/71 | LOSS: 5.339406956029133e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 67/71 | LOSS: 5.334102541730638e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 68/71 | LOSS: 5.31347135607127e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 69/71 | LOSS: 5.305755533819528e-06\n",
      "TRAIN: EPOCH 524/1000 | BATCH 70/71 | LOSS: 5.306340135191445e-06\n",
      "VAL: EPOCH 524/1000 | BATCH 0/8 | LOSS: 4.264754807081772e-06\n",
      "VAL: EPOCH 524/1000 | BATCH 1/8 | LOSS: 4.375243634058279e-06\n",
      "VAL: EPOCH 524/1000 | BATCH 2/8 | LOSS: 4.5669062274100725e-06\n",
      "VAL: EPOCH 524/1000 | BATCH 3/8 | LOSS: 4.584365001392143e-06\n",
      "VAL: EPOCH 524/1000 | BATCH 4/8 | LOSS: 4.544594503386179e-06\n",
      "VAL: EPOCH 524/1000 | BATCH 5/8 | LOSS: 4.434553450967845e-06\n",
      "VAL: EPOCH 524/1000 | BATCH 6/8 | LOSS: 4.2534562193655546e-06\n",
      "VAL: EPOCH 524/1000 | BATCH 7/8 | LOSS: 4.079116564525975e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 0/71 | LOSS: 4.0316672311746515e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 1/71 | LOSS: 4.110268264412298e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 2/71 | LOSS: 4.349046776042087e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 3/71 | LOSS: 4.3162541487618e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 4/71 | LOSS: 4.5333527850743845e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 5/71 | LOSS: 4.507290441324585e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 6/71 | LOSS: 4.418845881965743e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 7/71 | LOSS: 4.68139484155472e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 8/71 | LOSS: 4.603904067658328e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 9/71 | LOSS: 4.5983597374288365e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 10/71 | LOSS: 4.603773670003813e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 11/71 | LOSS: 4.590987335480652e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 12/71 | LOSS: 4.544637809326209e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 13/71 | LOSS: 4.561061067371546e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 14/71 | LOSS: 4.486572667398529e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 15/71 | LOSS: 4.561853970130869e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 16/71 | LOSS: 4.6286746269555916e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 17/71 | LOSS: 4.590576269139193e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 18/71 | LOSS: 4.632788431990775e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 19/71 | LOSS: 4.71713470915347e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 20/71 | LOSS: 4.774117347291654e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 21/71 | LOSS: 4.804769404968696e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 22/71 | LOSS: 4.948511529975171e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 23/71 | LOSS: 4.943560185211027e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 24/71 | LOSS: 4.973814293407486e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 25/71 | LOSS: 5.084376959110579e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 26/71 | LOSS: 5.087182006280744e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 27/71 | LOSS: 5.11051081275582e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 28/71 | LOSS: 5.1493756206488385e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 29/71 | LOSS: 5.2253954815265995e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 30/71 | LOSS: 5.187228607023679e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 31/71 | LOSS: 5.268703894500959e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 32/71 | LOSS: 5.231521403071714e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 33/71 | LOSS: 5.231832594344108e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 34/71 | LOSS: 5.219942957929951e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 35/71 | LOSS: 5.249672900238996e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 36/71 | LOSS: 5.199404860242204e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 37/71 | LOSS: 5.196978409721372e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 38/71 | LOSS: 5.174848055480069e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 39/71 | LOSS: 5.183538900155327e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 40/71 | LOSS: 5.157597997378794e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 41/71 | LOSS: 5.116190653184492e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 42/71 | LOSS: 5.0968072987733505e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 43/71 | LOSS: 5.059381330531985e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 44/71 | LOSS: 5.0973394031138414e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 45/71 | LOSS: 5.0761448263706965e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 46/71 | LOSS: 5.038647588391657e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 47/71 | LOSS: 5.030226091662371e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 48/71 | LOSS: 5.000960063440215e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 49/71 | LOSS: 4.978370711796743e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 50/71 | LOSS: 4.971196974521245e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 51/71 | LOSS: 4.936805221579842e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 52/71 | LOSS: 4.927985349601137e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 53/71 | LOSS: 4.918651126428341e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 54/71 | LOSS: 4.912928567467712e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 55/71 | LOSS: 4.893422292135645e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 56/71 | LOSS: 4.880692625974597e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 57/71 | LOSS: 4.9112466254443684e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 58/71 | LOSS: 4.879650447746518e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 59/71 | LOSS: 4.874532351095695e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 60/71 | LOSS: 4.8689511101253975e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 61/71 | LOSS: 4.86035679064263e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 62/71 | LOSS: 4.859225555264857e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 63/71 | LOSS: 4.844889346600212e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 64/71 | LOSS: 4.83931165105717e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 65/71 | LOSS: 4.824138510560839e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 66/71 | LOSS: 4.812812662816784e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 67/71 | LOSS: 4.79828593945379e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 68/71 | LOSS: 4.813209097451561e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 69/71 | LOSS: 4.793543114699007e-06\n",
      "TRAIN: EPOCH 525/1000 | BATCH 70/71 | LOSS: 4.794053720487823e-06\n",
      "VAL: EPOCH 525/1000 | BATCH 0/8 | LOSS: 5.030454303778242e-06\n",
      "VAL: EPOCH 525/1000 | BATCH 1/8 | LOSS: 5.10959466737404e-06\n",
      "VAL: EPOCH 525/1000 | BATCH 2/8 | LOSS: 5.13342077586761e-06\n",
      "VAL: EPOCH 525/1000 | BATCH 3/8 | LOSS: 5.21828451383044e-06\n",
      "VAL: EPOCH 525/1000 | BATCH 4/8 | LOSS: 5.17883563588839e-06\n",
      "VAL: EPOCH 525/1000 | BATCH 5/8 | LOSS: 5.200283567319275e-06\n",
      "VAL: EPOCH 525/1000 | BATCH 6/8 | LOSS: 5.072973116122219e-06\n",
      "VAL: EPOCH 525/1000 | BATCH 7/8 | LOSS: 4.923128585687664e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 0/71 | LOSS: 4.43574344899389e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 1/71 | LOSS: 4.563819629765931e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 2/71 | LOSS: 4.5215019781608135e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 3/71 | LOSS: 4.73385011900973e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 4/71 | LOSS: 4.878929848928237e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 5/71 | LOSS: 4.949674272817599e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 6/71 | LOSS: 5.191236856002693e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 7/71 | LOSS: 4.993556672161503e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 8/71 | LOSS: 4.911494316830714e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 9/71 | LOSS: 4.971784983354155e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 10/71 | LOSS: 4.971870277420914e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 11/71 | LOSS: 4.903993499283388e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 12/71 | LOSS: 4.921983872918645e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 13/71 | LOSS: 4.8816382007186935e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 14/71 | LOSS: 4.834139357020225e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 15/71 | LOSS: 4.773474188368709e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 16/71 | LOSS: 4.717412754389978e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 17/71 | LOSS: 4.64564921761242e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 18/71 | LOSS: 4.642731542653085e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 19/71 | LOSS: 4.639620465241024e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 20/71 | LOSS: 4.5895493972888545e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 21/71 | LOSS: 4.56264586484229e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 22/71 | LOSS: 4.523037788480313e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 23/71 | LOSS: 4.4763923009819946e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 24/71 | LOSS: 4.464238527361886e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 25/71 | LOSS: 4.434587000245161e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 26/71 | LOSS: 4.4163687444206856e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 27/71 | LOSS: 4.402300476158416e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 28/71 | LOSS: 4.382809250054034e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 29/71 | LOSS: 4.383596334870769e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 30/71 | LOSS: 4.365945282951279e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 31/71 | LOSS: 4.3759446484159525e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 32/71 | LOSS: 4.350507321181464e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 33/71 | LOSS: 4.350301711544007e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 34/71 | LOSS: 4.344562713803108e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 35/71 | LOSS: 4.346722126532161e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 36/71 | LOSS: 4.3336107766564866e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 37/71 | LOSS: 4.336167654927474e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 38/71 | LOSS: 4.3830234167566796e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 39/71 | LOSS: 4.389604538346248e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 40/71 | LOSS: 4.403369180957869e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 41/71 | LOSS: 4.435761590164995e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 42/71 | LOSS: 4.4284499624556576e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 43/71 | LOSS: 4.426153535322638e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 44/71 | LOSS: 4.42453392174179e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 45/71 | LOSS: 4.426543974343234e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 46/71 | LOSS: 4.434767972386598e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 47/71 | LOSS: 4.4388787661849465e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 48/71 | LOSS: 4.442796854182605e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 49/71 | LOSS: 4.437217708073149e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 50/71 | LOSS: 4.427901210553554e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 51/71 | LOSS: 4.457473564273711e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 52/71 | LOSS: 4.450940045813387e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 53/71 | LOSS: 4.4785952055732466e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 54/71 | LOSS: 4.455048817610739e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 55/71 | LOSS: 4.518364493476058e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 56/71 | LOSS: 4.503755466781481e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 57/71 | LOSS: 4.530524522343857e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 58/71 | LOSS: 4.51450400606071e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 59/71 | LOSS: 4.510088399456436e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 60/71 | LOSS: 4.509134710207916e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 61/71 | LOSS: 4.515838594993364e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 62/71 | LOSS: 4.519876729174275e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 63/71 | LOSS: 4.505543827804104e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 64/71 | LOSS: 4.506963551438485e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 65/71 | LOSS: 4.509803802372489e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 66/71 | LOSS: 4.51237904721282e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 67/71 | LOSS: 4.504995866441545e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 68/71 | LOSS: 4.507239137255701e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 69/71 | LOSS: 4.5008825834104624e-06\n",
      "TRAIN: EPOCH 526/1000 | BATCH 70/71 | LOSS: 4.489643073688518e-06\n",
      "VAL: EPOCH 526/1000 | BATCH 0/8 | LOSS: 4.61119634564966e-06\n",
      "VAL: EPOCH 526/1000 | BATCH 1/8 | LOSS: 5.112371582072228e-06\n",
      "VAL: EPOCH 526/1000 | BATCH 2/8 | LOSS: 5.326774650408576e-06\n",
      "VAL: EPOCH 526/1000 | BATCH 3/8 | LOSS: 5.4154445479071e-06\n",
      "VAL: EPOCH 526/1000 | BATCH 4/8 | LOSS: 5.409665482147829e-06\n",
      "VAL: EPOCH 526/1000 | BATCH 5/8 | LOSS: 5.4604079953908995e-06\n",
      "VAL: EPOCH 526/1000 | BATCH 6/8 | LOSS: 5.376886487543483e-06\n",
      "VAL: EPOCH 526/1000 | BATCH 7/8 | LOSS: 5.290178876293794e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 0/71 | LOSS: 5.391713784774765e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 1/71 | LOSS: 4.5506865262723295e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 2/71 | LOSS: 5.124052677274449e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 3/71 | LOSS: 4.846862907470495e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 4/71 | LOSS: 5.22391719641746e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 5/71 | LOSS: 5.000878748736189e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 6/71 | LOSS: 5.296698483497104e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 7/71 | LOSS: 5.100166390548111e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 8/71 | LOSS: 5.181119023392158e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 9/71 | LOSS: 5.158448084330303e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 10/71 | LOSS: 5.040921636960278e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 11/71 | LOSS: 5.097424567187166e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 12/71 | LOSS: 5.047765276126133e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 13/71 | LOSS: 5.053727428665817e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 14/71 | LOSS: 4.945565660818829e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 15/71 | LOSS: 5.116822777040397e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 16/71 | LOSS: 5.0748699724114406e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 17/71 | LOSS: 5.109192658083985e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 18/71 | LOSS: 5.092211710545694e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 19/71 | LOSS: 5.19769749871557e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 20/71 | LOSS: 5.204766055266234e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 21/71 | LOSS: 5.1512407914990694e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 22/71 | LOSS: 5.180584959974943e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 23/71 | LOSS: 5.214512962462929e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 24/71 | LOSS: 5.243377390797832e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 25/71 | LOSS: 5.245741118205143e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 26/71 | LOSS: 5.313933637875447e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 27/71 | LOSS: 5.275528069757586e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 28/71 | LOSS: 5.33510784595725e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 29/71 | LOSS: 5.2849951089228854e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 30/71 | LOSS: 5.286596712223048e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 31/71 | LOSS: 5.294001802269577e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 32/71 | LOSS: 5.275046167739537e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 33/71 | LOSS: 5.230570376197113e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 34/71 | LOSS: 5.216837839075846e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 35/71 | LOSS: 5.201586980597818e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 36/71 | LOSS: 5.183101971065421e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 37/71 | LOSS: 5.151777971054559e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 38/71 | LOSS: 5.1286875030853434e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 39/71 | LOSS: 5.164515783917523e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 40/71 | LOSS: 5.137055429103628e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 41/71 | LOSS: 5.111225266860163e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 42/71 | LOSS: 5.1487498525560555e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 43/71 | LOSS: 5.138822380310805e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 44/71 | LOSS: 5.157903418269901e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 45/71 | LOSS: 5.162686554048497e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 46/71 | LOSS: 5.154315740970135e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 47/71 | LOSS: 5.1557665917319655e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 48/71 | LOSS: 5.150395778326463e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 49/71 | LOSS: 5.224809215178539e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 50/71 | LOSS: 5.187372969029478e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 51/71 | LOSS: 5.284710154613164e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 52/71 | LOSS: 5.261271244651069e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 53/71 | LOSS: 5.297660247075934e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 54/71 | LOSS: 5.2696663632642325e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 55/71 | LOSS: 5.273073786124119e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 56/71 | LOSS: 5.289762248937398e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 57/71 | LOSS: 5.2948582088933546e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 58/71 | LOSS: 5.294437326067394e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 59/71 | LOSS: 5.289158036703157e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 60/71 | LOSS: 5.301134616856274e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 61/71 | LOSS: 5.2978005390785605e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 62/71 | LOSS: 5.30316222986065e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 63/71 | LOSS: 5.300805749897108e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 64/71 | LOSS: 5.276373146373841e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 65/71 | LOSS: 5.308801488271539e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 66/71 | LOSS: 5.284876694409533e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 67/71 | LOSS: 5.290401729245573e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 68/71 | LOSS: 5.287848517727452e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 69/71 | LOSS: 5.258855276224495e-06\n",
      "TRAIN: EPOCH 527/1000 | BATCH 70/71 | LOSS: 5.28915092230895e-06\n",
      "VAL: EPOCH 527/1000 | BATCH 0/8 | LOSS: 6.132011549198069e-06\n",
      "VAL: EPOCH 527/1000 | BATCH 1/8 | LOSS: 6.218060661922209e-06\n",
      "VAL: EPOCH 527/1000 | BATCH 2/8 | LOSS: 6.660374159158285e-06\n",
      "VAL: EPOCH 527/1000 | BATCH 3/8 | LOSS: 6.673260145362292e-06\n",
      "VAL: EPOCH 527/1000 | BATCH 4/8 | LOSS: 6.698390370729612e-06\n",
      "VAL: EPOCH 527/1000 | BATCH 5/8 | LOSS: 6.840895366622135e-06\n",
      "VAL: EPOCH 527/1000 | BATCH 6/8 | LOSS: 6.752437457180349e-06\n",
      "VAL: EPOCH 527/1000 | BATCH 7/8 | LOSS: 6.8284558665254735e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 0/71 | LOSS: 6.478167051682249e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 1/71 | LOSS: 6.368678896251367e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 2/71 | LOSS: 6.240860178271153e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 3/71 | LOSS: 6.1427765558619285e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 4/71 | LOSS: 5.664869604515843e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 5/71 | LOSS: 5.685008166741075e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 6/71 | LOSS: 5.525743030635308e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 7/71 | LOSS: 5.57666652412081e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 8/71 | LOSS: 5.584720838669455e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 9/71 | LOSS: 5.625984476864687e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 10/71 | LOSS: 5.649979398599085e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 11/71 | LOSS: 5.456508953708787e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 12/71 | LOSS: 5.367023172416689e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 13/71 | LOSS: 5.34734772372758e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 14/71 | LOSS: 5.309375046635978e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 15/71 | LOSS: 5.334767536169238e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 16/71 | LOSS: 5.327891224570682e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 17/71 | LOSS: 5.302497558861634e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 18/71 | LOSS: 5.26334650586298e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 19/71 | LOSS: 5.258295459498186e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 20/71 | LOSS: 5.218612993000231e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 21/71 | LOSS: 5.2179087891669376e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 22/71 | LOSS: 5.2058399175977055e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 23/71 | LOSS: 5.201071796060812e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 24/71 | LOSS: 5.210625931795221e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 25/71 | LOSS: 5.254405060930787e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 26/71 | LOSS: 5.24304464811899e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 27/71 | LOSS: 5.2600951383802955e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 28/71 | LOSS: 5.240697771961127e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 29/71 | LOSS: 5.2352268842999665e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 30/71 | LOSS: 5.21375360192671e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 31/71 | LOSS: 5.161523347396724e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 32/71 | LOSS: 5.14092252243162e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 33/71 | LOSS: 5.144308180901776e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 34/71 | LOSS: 5.1273503491497e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 35/71 | LOSS: 5.084602018238608e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 36/71 | LOSS: 5.0457790559842065e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 37/71 | LOSS: 5.011741486190371e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 38/71 | LOSS: 5.035635473550661e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 39/71 | LOSS: 5.010467918964423e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 40/71 | LOSS: 5.027922503447881e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 41/71 | LOSS: 4.997891892344342e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 42/71 | LOSS: 5.03410996485442e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 43/71 | LOSS: 5.009964711048682e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 44/71 | LOSS: 5.022311395603336e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 45/71 | LOSS: 5.026969901060195e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 46/71 | LOSS: 5.027315627886949e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 47/71 | LOSS: 5.0177751897232765e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 48/71 | LOSS: 5.0132578656544234e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 49/71 | LOSS: 4.999121874789125e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 50/71 | LOSS: 5.00648542690803e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 51/71 | LOSS: 4.9988514617255605e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 52/71 | LOSS: 4.988341129748458e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 53/71 | LOSS: 4.96292202738173e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 54/71 | LOSS: 4.949701422612469e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 55/71 | LOSS: 4.94646102668282e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 56/71 | LOSS: 4.949734602632816e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 57/71 | LOSS: 4.948590995370288e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 58/71 | LOSS: 4.934746939243731e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 59/71 | LOSS: 4.920230757458436e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 60/71 | LOSS: 4.91171908834076e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 61/71 | LOSS: 4.891278319607203e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 62/71 | LOSS: 4.882748322011645e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 63/71 | LOSS: 4.8887866483937614e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 64/71 | LOSS: 4.871399365723706e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 65/71 | LOSS: 4.8616093064276464e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 66/71 | LOSS: 4.8548454016320225e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 67/71 | LOSS: 4.8378505907214065e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 68/71 | LOSS: 4.819059633018037e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 69/71 | LOSS: 4.8142592796856274e-06\n",
      "TRAIN: EPOCH 528/1000 | BATCH 70/71 | LOSS: 4.7990239521131515e-06\n",
      "VAL: EPOCH 528/1000 | BATCH 0/8 | LOSS: 3.6967794585507363e-06\n",
      "VAL: EPOCH 528/1000 | BATCH 1/8 | LOSS: 4.071882813150296e-06\n",
      "VAL: EPOCH 528/1000 | BATCH 2/8 | LOSS: 4.2944226758360555e-06\n",
      "VAL: EPOCH 528/1000 | BATCH 3/8 | LOSS: 4.337858626968227e-06\n",
      "VAL: EPOCH 528/1000 | BATCH 4/8 | LOSS: 4.3436698433652055e-06\n",
      "VAL: EPOCH 528/1000 | BATCH 5/8 | LOSS: 4.275489042508222e-06\n",
      "VAL: EPOCH 528/1000 | BATCH 6/8 | LOSS: 4.137900694851331e-06\n",
      "VAL: EPOCH 528/1000 | BATCH 7/8 | LOSS: 4.009172073438094e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 0/71 | LOSS: 3.7945328585919924e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 1/71 | LOSS: 3.872252364089945e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 2/71 | LOSS: 4.067830862671447e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 3/71 | LOSS: 3.910769692083704e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 4/71 | LOSS: 4.2340047002653595e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 5/71 | LOSS: 4.217414925733465e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 6/71 | LOSS: 4.321982487454079e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 7/71 | LOSS: 4.3115596213283425e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 8/71 | LOSS: 4.330003270701531e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 9/71 | LOSS: 4.210236966173397e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 10/71 | LOSS: 4.237717224565461e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 11/71 | LOSS: 4.187490541577669e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 12/71 | LOSS: 4.26626260937505e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 13/71 | LOSS: 4.230115337122697e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 14/71 | LOSS: 4.2378397968908155e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 15/71 | LOSS: 4.216827676373214e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 16/71 | LOSS: 4.228438576640602e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 17/71 | LOSS: 4.213433157929103e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 18/71 | LOSS: 4.2448447952904785e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 19/71 | LOSS: 4.248658433425589e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 20/71 | LOSS: 4.278610525770568e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 21/71 | LOSS: 4.299909795174079e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 22/71 | LOSS: 4.28060012263418e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 23/71 | LOSS: 4.276819178509565e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 24/71 | LOSS: 4.247051856509643e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 25/71 | LOSS: 4.226702808059949e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 26/71 | LOSS: 4.244334377290215e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 27/71 | LOSS: 4.237814307219066e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 28/71 | LOSS: 4.258291138002873e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 29/71 | LOSS: 4.2719200488742596e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 30/71 | LOSS: 4.288724261230879e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 31/71 | LOSS: 4.3127177491442126e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 32/71 | LOSS: 4.348644049032098e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 33/71 | LOSS: 4.345757401204216e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 34/71 | LOSS: 4.347651390292282e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 35/71 | LOSS: 4.336325319956005e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 36/71 | LOSS: 4.342496619780856e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 37/71 | LOSS: 4.323149542678469e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 38/71 | LOSS: 4.328065543655285e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 39/71 | LOSS: 4.331373207833167e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 40/71 | LOSS: 4.308779746613099e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 41/71 | LOSS: 4.340224888567588e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 42/71 | LOSS: 4.329913614382412e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 43/71 | LOSS: 4.335634121782112e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 44/71 | LOSS: 4.3296606792056185e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 45/71 | LOSS: 4.338756273524347e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 46/71 | LOSS: 4.3596775935355335e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 47/71 | LOSS: 4.367514042276828e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 48/71 | LOSS: 4.366844608256895e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 49/71 | LOSS: 4.37245133525721e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 50/71 | LOSS: 4.387727509917265e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 51/71 | LOSS: 4.374326627280075e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 52/71 | LOSS: 4.358428797340306e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 53/71 | LOSS: 4.384966671380055e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 54/71 | LOSS: 4.391084249536189e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 55/71 | LOSS: 4.3781732667217255e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 56/71 | LOSS: 4.3940097328774295e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 57/71 | LOSS: 4.394518010360179e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 58/71 | LOSS: 4.4139715524672e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 59/71 | LOSS: 4.4125188007152856e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 60/71 | LOSS: 4.434396185418142e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 61/71 | LOSS: 4.442653001264731e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 62/71 | LOSS: 4.4323459990534634e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 63/71 | LOSS: 4.451834524132892e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 64/71 | LOSS: 4.4451251911735965e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 65/71 | LOSS: 4.434747397267942e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 66/71 | LOSS: 4.4346468121826794e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 67/71 | LOSS: 4.429826841554613e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 68/71 | LOSS: 4.43285139753889e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 69/71 | LOSS: 4.430519594019903e-06\n",
      "TRAIN: EPOCH 529/1000 | BATCH 70/71 | LOSS: 4.4269515526561724e-06\n",
      "VAL: EPOCH 529/1000 | BATCH 0/8 | LOSS: 5.761398824688513e-06\n",
      "VAL: EPOCH 529/1000 | BATCH 1/8 | LOSS: 5.675050033460138e-06\n",
      "VAL: EPOCH 529/1000 | BATCH 2/8 | LOSS: 5.836024229211034e-06\n",
      "VAL: EPOCH 529/1000 | BATCH 3/8 | LOSS: 5.772471581622085e-06\n",
      "VAL: EPOCH 529/1000 | BATCH 4/8 | LOSS: 5.665718254022067e-06\n",
      "VAL: EPOCH 529/1000 | BATCH 5/8 | LOSS: 5.5609445250108065e-06\n",
      "VAL: EPOCH 529/1000 | BATCH 6/8 | LOSS: 5.335056812327821e-06\n",
      "VAL: EPOCH 529/1000 | BATCH 7/8 | LOSS: 5.093330571526167e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 0/71 | LOSS: 5.289321961754467e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 1/71 | LOSS: 5.287239218887407e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 2/71 | LOSS: 5.645048380150304e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 3/71 | LOSS: 5.586557563219685e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 4/71 | LOSS: 5.715862698707497e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 5/71 | LOSS: 5.961810302324011e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 6/71 | LOSS: 6.168912932480453e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 7/71 | LOSS: 6.435071270516346e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 8/71 | LOSS: 6.467856615523084e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 9/71 | LOSS: 6.414764629880665e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 10/71 | LOSS: 6.243115670010659e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 11/71 | LOSS: 6.317777433650917e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 12/71 | LOSS: 6.173878073940824e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 13/71 | LOSS: 6.37499355044773e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 14/71 | LOSS: 6.355541730348098e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 15/71 | LOSS: 6.476206209526936e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 16/71 | LOSS: 6.324528706446633e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 17/71 | LOSS: 6.271192811861208e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 18/71 | LOSS: 6.2439113495010564e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 19/71 | LOSS: 6.160158727652743e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 20/71 | LOSS: 6.207511736041245e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 21/71 | LOSS: 6.1345394897216465e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 22/71 | LOSS: 6.070829563807068e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 23/71 | LOSS: 6.042180984877632e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 24/71 | LOSS: 5.99858773057349e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 25/71 | LOSS: 6.066783848487271e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 26/71 | LOSS: 6.031263385213808e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 27/71 | LOSS: 6.085735744168882e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 28/71 | LOSS: 6.074977199173661e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 29/71 | LOSS: 6.1295478493169264e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 30/71 | LOSS: 6.063402994890742e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 31/71 | LOSS: 6.099009993931759e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 32/71 | LOSS: 6.057037932226773e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 33/71 | LOSS: 6.092136305489796e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 34/71 | LOSS: 6.024736857008455e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 35/71 | LOSS: 6.065623135024604e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 36/71 | LOSS: 6.024924889946281e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 37/71 | LOSS: 6.029542155853338e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 38/71 | LOSS: 6.01180071941762e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 39/71 | LOSS: 5.946912426679773e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 40/71 | LOSS: 5.894706014149095e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 41/71 | LOSS: 5.823600813095081e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 42/71 | LOSS: 5.805956831419851e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 43/71 | LOSS: 5.778008233970798e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 44/71 | LOSS: 5.7247152300179656e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 45/71 | LOSS: 5.697104869716935e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 46/71 | LOSS: 5.664841047349755e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 47/71 | LOSS: 5.646922607146128e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 48/71 | LOSS: 5.591603031480561e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 49/71 | LOSS: 5.559861738220206e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 50/71 | LOSS: 5.53004963198784e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 51/71 | LOSS: 5.503670072707441e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 52/71 | LOSS: 5.469192390008621e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 53/71 | LOSS: 5.440776025925179e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 54/71 | LOSS: 5.419321211196472e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 55/71 | LOSS: 5.409700045382644e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 56/71 | LOSS: 5.399214463371512e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 57/71 | LOSS: 5.384131464654337e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 58/71 | LOSS: 5.386500819348207e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 59/71 | LOSS: 5.353523367072436e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 60/71 | LOSS: 5.326569199034387e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 61/71 | LOSS: 5.331280084236266e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 62/71 | LOSS: 5.301447402864199e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 63/71 | LOSS: 5.282538509732149e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 64/71 | LOSS: 5.270273056969297e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 65/71 | LOSS: 5.265967943022119e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 66/71 | LOSS: 5.258738682058e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 67/71 | LOSS: 5.246952596013109e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 68/71 | LOSS: 5.2539539140433184e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 69/71 | LOSS: 5.2633707517608335e-06\n",
      "TRAIN: EPOCH 530/1000 | BATCH 70/71 | LOSS: 5.267160067375353e-06\n",
      "VAL: EPOCH 530/1000 | BATCH 0/8 | LOSS: 3.698539330798667e-06\n",
      "VAL: EPOCH 530/1000 | BATCH 1/8 | LOSS: 4.293164920454728e-06\n",
      "VAL: EPOCH 530/1000 | BATCH 2/8 | LOSS: 4.674956926464802e-06\n",
      "VAL: EPOCH 530/1000 | BATCH 3/8 | LOSS: 4.816063892576494e-06\n",
      "VAL: EPOCH 530/1000 | BATCH 4/8 | LOSS: 4.805432854482205e-06\n",
      "VAL: EPOCH 530/1000 | BATCH 5/8 | LOSS: 4.806792806751521e-06\n",
      "VAL: EPOCH 530/1000 | BATCH 6/8 | LOSS: 4.68667401167165e-06\n",
      "VAL: EPOCH 530/1000 | BATCH 7/8 | LOSS: 4.5935414050291e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 0/71 | LOSS: 5.468870767799672e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 1/71 | LOSS: 6.424961611628532e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 2/71 | LOSS: 6.1218082313037785e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 3/71 | LOSS: 5.686307304131333e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 4/71 | LOSS: 6.184025369293522e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 5/71 | LOSS: 6.040343729788826e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 6/71 | LOSS: 6.172207577037625e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 7/71 | LOSS: 5.960288433470851e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 8/71 | LOSS: 6.000842656956391e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 9/71 | LOSS: 5.927091024204856e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 10/71 | LOSS: 5.728617907152511e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 11/71 | LOSS: 5.897539267607499e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 12/71 | LOSS: 5.761815548164752e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 13/71 | LOSS: 5.639425093509739e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 14/71 | LOSS: 5.557300301006762e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 15/71 | LOSS: 5.471656095323851e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 16/71 | LOSS: 5.422778887056184e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 17/71 | LOSS: 5.291933828428026e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 18/71 | LOSS: 5.24306470221123e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 19/71 | LOSS: 5.19216948760004e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 20/71 | LOSS: 5.0929488679685164e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 21/71 | LOSS: 5.155961740787131e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 22/71 | LOSS: 5.1663472257369785e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 23/71 | LOSS: 5.13045087776239e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 24/71 | LOSS: 5.078829108242644e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 25/71 | LOSS: 5.0400580256302e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 26/71 | LOSS: 5.012853374932159e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 27/71 | LOSS: 4.986073739538759e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 28/71 | LOSS: 4.9760178940075625e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 29/71 | LOSS: 5.011552563397951e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 30/71 | LOSS: 5.001876161259133e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 31/71 | LOSS: 5.0011476417921585e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 32/71 | LOSS: 4.982115528533044e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 33/71 | LOSS: 5.027394496103036e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 34/71 | LOSS: 4.981405144072986e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 35/71 | LOSS: 4.96747206726569e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 36/71 | LOSS: 4.969069403012179e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 37/71 | LOSS: 4.9420204779955e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 38/71 | LOSS: 4.911652730208096e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 39/71 | LOSS: 4.878163605326336e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 40/71 | LOSS: 4.832510074532077e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 41/71 | LOSS: 4.876850149620233e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 42/71 | LOSS: 4.849122623729568e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 43/71 | LOSS: 4.840387232847703e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 44/71 | LOSS: 4.840778779503631e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 45/71 | LOSS: 4.856982365669429e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 46/71 | LOSS: 4.886347548389049e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 47/71 | LOSS: 4.901372098705299e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 48/71 | LOSS: 4.89960944016312e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 49/71 | LOSS: 4.928278722218238e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 50/71 | LOSS: 4.938217729182261e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 51/71 | LOSS: 4.991681486479558e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 52/71 | LOSS: 4.98147959283329e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 53/71 | LOSS: 4.993117120951259e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 54/71 | LOSS: 4.9957211641330185e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 55/71 | LOSS: 4.988035422164623e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 56/71 | LOSS: 5.002914804208558e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 57/71 | LOSS: 4.985385736478653e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 58/71 | LOSS: 4.992347932506996e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 59/71 | LOSS: 4.983705343875044e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 60/71 | LOSS: 4.977243578218431e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 61/71 | LOSS: 4.982597022629512e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 62/71 | LOSS: 4.9699595885093525e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 63/71 | LOSS: 4.953639752613981e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 64/71 | LOSS: 4.943530711898347e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 65/71 | LOSS: 4.9341379398163445e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 66/71 | LOSS: 4.928379792599755e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 67/71 | LOSS: 4.9135521280999565e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 68/71 | LOSS: 4.94812235521512e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 69/71 | LOSS: 4.946847593600978e-06\n",
      "TRAIN: EPOCH 531/1000 | BATCH 70/71 | LOSS: 4.9486181882790515e-06\n",
      "VAL: EPOCH 531/1000 | BATCH 0/8 | LOSS: 6.605217549804365e-06\n",
      "VAL: EPOCH 531/1000 | BATCH 1/8 | LOSS: 7.0543769652431365e-06\n",
      "VAL: EPOCH 531/1000 | BATCH 2/8 | LOSS: 7.248740985232871e-06\n",
      "VAL: EPOCH 531/1000 | BATCH 3/8 | LOSS: 7.126488299036282e-06\n",
      "VAL: EPOCH 531/1000 | BATCH 4/8 | LOSS: 7.141750666050939e-06\n",
      "VAL: EPOCH 531/1000 | BATCH 5/8 | LOSS: 7.180805520571691e-06\n",
      "VAL: EPOCH 531/1000 | BATCH 6/8 | LOSS: 7.044358881103108e-06\n",
      "VAL: EPOCH 531/1000 | BATCH 7/8 | LOSS: 7.061207497827127e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 0/71 | LOSS: 6.68288657834637e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 1/71 | LOSS: 5.160629029887787e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 2/71 | LOSS: 5.589687210279711e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 3/71 | LOSS: 5.55727814344209e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 4/71 | LOSS: 5.437141180664185e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 5/71 | LOSS: 5.849841537989657e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 6/71 | LOSS: 5.6744846850571255e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 7/71 | LOSS: 5.721047870110851e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 8/71 | LOSS: 5.678546712159813e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 9/71 | LOSS: 5.840740709572856e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 10/71 | LOSS: 5.8890403910151114e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 11/71 | LOSS: 6.010484810303751e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 12/71 | LOSS: 5.913072615070492e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 13/71 | LOSS: 5.8454105880757974e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 14/71 | LOSS: 5.85645173790302e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 15/71 | LOSS: 5.840914738541869e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 16/71 | LOSS: 5.7777215127263094e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 17/71 | LOSS: 5.730425060493063e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 18/71 | LOSS: 5.703752166434577e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 19/71 | LOSS: 5.722320668155589e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 20/71 | LOSS: 5.714177762326601e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 21/71 | LOSS: 5.593216521695219e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 22/71 | LOSS: 5.573402384707942e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 23/71 | LOSS: 5.539882247755183e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 24/71 | LOSS: 5.467495284392498e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 25/71 | LOSS: 5.4191857368725605e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 26/71 | LOSS: 5.406403016860605e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 27/71 | LOSS: 5.368185790237996e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 28/71 | LOSS: 5.437210504545686e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 29/71 | LOSS: 5.391535341914277e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 30/71 | LOSS: 5.4646169730638635e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 31/71 | LOSS: 5.442799022148392e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 32/71 | LOSS: 5.548481539969516e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 33/71 | LOSS: 5.543591294768061e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 34/71 | LOSS: 5.519739897863474e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 35/71 | LOSS: 5.516234700230092e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 36/71 | LOSS: 5.538138744619524e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 37/71 | LOSS: 5.563392804967742e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 38/71 | LOSS: 5.5207030923972925e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 39/71 | LOSS: 5.525603978639993e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 40/71 | LOSS: 5.528500856243914e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 41/71 | LOSS: 5.540746509983105e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 42/71 | LOSS: 5.5278348533415e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 43/71 | LOSS: 5.5430740634289e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 44/71 | LOSS: 5.5002810919783466e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 45/71 | LOSS: 5.472591999972318e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 46/71 | LOSS: 5.485915612843336e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 47/71 | LOSS: 5.4777283224893836e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 48/71 | LOSS: 5.442826050614026e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 49/71 | LOSS: 5.4017842285247755e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 50/71 | LOSS: 5.39346902038304e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 51/71 | LOSS: 5.374517671113538e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 52/71 | LOSS: 5.374822271647336e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 53/71 | LOSS: 5.3451129436628735e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 54/71 | LOSS: 5.33722675093238e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 55/71 | LOSS: 5.318443291863721e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 56/71 | LOSS: 5.302771744920806e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 57/71 | LOSS: 5.284223624553776e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 58/71 | LOSS: 5.278558962915775e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 59/71 | LOSS: 5.279266990025159e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 60/71 | LOSS: 5.259234229492531e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 61/71 | LOSS: 5.239250471746463e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 62/71 | LOSS: 5.222270042323212e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 63/71 | LOSS: 5.198117822402537e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 64/71 | LOSS: 5.179424935984291e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 65/71 | LOSS: 5.163616354807208e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 66/71 | LOSS: 5.173269966092706e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 67/71 | LOSS: 5.166612070565059e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 68/71 | LOSS: 5.1537195014279335e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 69/71 | LOSS: 5.1546737105517036e-06\n",
      "TRAIN: EPOCH 532/1000 | BATCH 70/71 | LOSS: 5.131575194418858e-06\n",
      "VAL: EPOCH 532/1000 | BATCH 0/8 | LOSS: 5.952304491074756e-06\n",
      "VAL: EPOCH 532/1000 | BATCH 1/8 | LOSS: 5.91044681641506e-06\n",
      "VAL: EPOCH 532/1000 | BATCH 2/8 | LOSS: 5.861017749945556e-06\n",
      "VAL: EPOCH 532/1000 | BATCH 3/8 | LOSS: 5.7215144124711514e-06\n",
      "VAL: EPOCH 532/1000 | BATCH 4/8 | LOSS: 5.66423696000129e-06\n",
      "VAL: EPOCH 532/1000 | BATCH 5/8 | LOSS: 5.474062769887193e-06\n",
      "VAL: EPOCH 532/1000 | BATCH 6/8 | LOSS: 5.261765831424522e-06\n",
      "VAL: EPOCH 532/1000 | BATCH 7/8 | LOSS: 5.061032112507746e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 0/71 | LOSS: 5.83224118599901e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 1/71 | LOSS: 5.358548605727265e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 2/71 | LOSS: 5.488263013830874e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 3/71 | LOSS: 5.0777766773535404e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 4/71 | LOSS: 5.179629533813568e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 5/71 | LOSS: 4.95242458479576e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 6/71 | LOSS: 5.000128437261862e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 7/71 | LOSS: 4.999365728508565e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 8/71 | LOSS: 5.021126627171826e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 9/71 | LOSS: 4.973635213900707e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 10/71 | LOSS: 4.978948047871447e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 11/71 | LOSS: 4.868624046139303e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 12/71 | LOSS: 4.921928533664099e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 13/71 | LOSS: 4.863769195903192e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 14/71 | LOSS: 4.878675129778761e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 15/71 | LOSS: 4.831692848483726e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 16/71 | LOSS: 4.828730278163943e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 17/71 | LOSS: 4.799654182837306e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 18/71 | LOSS: 4.774911686629486e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 19/71 | LOSS: 4.756640555569902e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 20/71 | LOSS: 4.71162212263499e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 21/71 | LOSS: 4.726504440358522e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 22/71 | LOSS: 4.76405334073048e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 23/71 | LOSS: 4.756436605172591e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 24/71 | LOSS: 4.7953648936527315e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 25/71 | LOSS: 4.755512883583344e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 26/71 | LOSS: 4.729997069969411e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 27/71 | LOSS: 4.786709886112865e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 28/71 | LOSS: 4.735302966231569e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 29/71 | LOSS: 4.8041753871075345e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 30/71 | LOSS: 4.801935222443013e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 31/71 | LOSS: 4.850739600215093e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 32/71 | LOSS: 4.86569235197914e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 33/71 | LOSS: 4.841917336093234e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 34/71 | LOSS: 4.894221365248086e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 35/71 | LOSS: 4.85010458659316e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 36/71 | LOSS: 4.87521336597982e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 37/71 | LOSS: 4.842292116192862e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 38/71 | LOSS: 4.8618510449252936e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 39/71 | LOSS: 4.85010822330878e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 40/71 | LOSS: 4.8440750261517415e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 41/71 | LOSS: 4.829111438604221e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 42/71 | LOSS: 4.855305114899746e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 43/71 | LOSS: 4.827266288678154e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 44/71 | LOSS: 4.826209063442851e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 45/71 | LOSS: 4.8247901136362445e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 46/71 | LOSS: 4.819860094435268e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 47/71 | LOSS: 4.797566025634599e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 48/71 | LOSS: 4.798074560683893e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 49/71 | LOSS: 4.788244364135608e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 50/71 | LOSS: 4.759997084108238e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 51/71 | LOSS: 4.7622325146571365e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 52/71 | LOSS: 4.737113420320246e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 53/71 | LOSS: 4.727511324105123e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 54/71 | LOSS: 4.697269861770275e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 55/71 | LOSS: 4.6674609604581614e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 56/71 | LOSS: 4.689635069984146e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 57/71 | LOSS: 4.678086772150107e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 58/71 | LOSS: 4.678377983077938e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 59/71 | LOSS: 4.672909979793379e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 60/71 | LOSS: 4.652729859996919e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 61/71 | LOSS: 4.636555867100176e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 62/71 | LOSS: 4.6319388520570345e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 63/71 | LOSS: 4.632369886792276e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 64/71 | LOSS: 4.6315357604516275e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 65/71 | LOSS: 4.6134043610767215e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 66/71 | LOSS: 4.6062701117147865e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 67/71 | LOSS: 4.6005792087806396e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 68/71 | LOSS: 4.600874276086646e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 69/71 | LOSS: 4.58555111241107e-06\n",
      "TRAIN: EPOCH 533/1000 | BATCH 70/71 | LOSS: 4.5920152016650985e-06\n",
      "VAL: EPOCH 533/1000 | BATCH 0/8 | LOSS: 3.83242604584666e-06\n",
      "VAL: EPOCH 533/1000 | BATCH 1/8 | LOSS: 4.194164148429991e-06\n",
      "VAL: EPOCH 533/1000 | BATCH 2/8 | LOSS: 4.378651283332147e-06\n",
      "VAL: EPOCH 533/1000 | BATCH 3/8 | LOSS: 4.381199460112839e-06\n",
      "VAL: EPOCH 533/1000 | BATCH 4/8 | LOSS: 4.401987280289177e-06\n",
      "VAL: EPOCH 533/1000 | BATCH 5/8 | LOSS: 4.405848206564163e-06\n",
      "VAL: EPOCH 533/1000 | BATCH 6/8 | LOSS: 4.28074193743149e-06\n",
      "VAL: EPOCH 533/1000 | BATCH 7/8 | LOSS: 4.206779209425804e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 0/71 | LOSS: 4.309041287342552e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 1/71 | LOSS: 4.2246258544764714e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 2/71 | LOSS: 4.155174944268462e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 3/71 | LOSS: 4.40633755260933e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 4/71 | LOSS: 4.5098463488102425e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 5/71 | LOSS: 4.646330959682625e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 6/71 | LOSS: 4.676810866450458e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 7/71 | LOSS: 4.794417691300623e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 8/71 | LOSS: 4.813458670267008e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 9/71 | LOSS: 4.850705499848118e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 10/71 | LOSS: 4.820093470863702e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 11/71 | LOSS: 4.724343511952611e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 12/71 | LOSS: 4.661131907386544e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 13/71 | LOSS: 4.668480187319801e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 14/71 | LOSS: 4.671751260805952e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 15/71 | LOSS: 4.644722707780602e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 16/71 | LOSS: 4.593222911353223e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 17/71 | LOSS: 4.5573978392591625e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 18/71 | LOSS: 4.621994895914165e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 19/71 | LOSS: 4.626330860446615e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 20/71 | LOSS: 4.62771075555273e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 21/71 | LOSS: 4.61984218831772e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 22/71 | LOSS: 4.584309200926446e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 23/71 | LOSS: 4.551491296448755e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 24/71 | LOSS: 4.502389874687651e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 25/71 | LOSS: 4.503018842045164e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 26/71 | LOSS: 4.4796638407947115e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 27/71 | LOSS: 4.482075837586308e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 28/71 | LOSS: 4.4432411586654025e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 29/71 | LOSS: 4.485504518925154e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 30/71 | LOSS: 4.455323109610735e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 31/71 | LOSS: 4.449353824043101e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 32/71 | LOSS: 4.430680333329788e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 33/71 | LOSS: 4.404506880211343e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 34/71 | LOSS: 4.41212631423176e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 35/71 | LOSS: 4.389311123355179e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 36/71 | LOSS: 4.399627703212507e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 37/71 | LOSS: 4.389773619694001e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 38/71 | LOSS: 4.358513828824821e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 39/71 | LOSS: 4.378730403686859e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 40/71 | LOSS: 4.3676702938594134e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 41/71 | LOSS: 4.353106056615139e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 42/71 | LOSS: 4.361511125966371e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 43/71 | LOSS: 4.342696043253951e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 44/71 | LOSS: 4.353180136402241e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 45/71 | LOSS: 4.357796905938101e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 46/71 | LOSS: 4.363259459422168e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 47/71 | LOSS: 4.350324412409161e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 48/71 | LOSS: 4.333028880641998e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 49/71 | LOSS: 4.335042744969542e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 50/71 | LOSS: 4.349512575536923e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 51/71 | LOSS: 4.3511598251965415e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 52/71 | LOSS: 4.351047042325932e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 53/71 | LOSS: 4.3364792224599145e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 54/71 | LOSS: 4.335154430505512e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 55/71 | LOSS: 4.34432594228643e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 56/71 | LOSS: 4.333046677984703e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 57/71 | LOSS: 4.337969753891982e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 58/71 | LOSS: 4.347109386614293e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 59/71 | LOSS: 4.356031797669857e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 60/71 | LOSS: 4.360781363697034e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 61/71 | LOSS: 4.352117013783549e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 62/71 | LOSS: 4.3596812918708786e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 63/71 | LOSS: 4.368638588658769e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 64/71 | LOSS: 4.362326262707939e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 65/71 | LOSS: 4.360932097590579e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 66/71 | LOSS: 4.358989063300907e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 67/71 | LOSS: 4.346857705888183e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 68/71 | LOSS: 4.343938075101545e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 69/71 | LOSS: 4.350344751661136e-06\n",
      "TRAIN: EPOCH 534/1000 | BATCH 70/71 | LOSS: 4.362020727570216e-06\n",
      "VAL: EPOCH 534/1000 | BATCH 0/8 | LOSS: 4.228358648106223e-06\n",
      "VAL: EPOCH 534/1000 | BATCH 1/8 | LOSS: 4.415790272105369e-06\n",
      "VAL: EPOCH 534/1000 | BATCH 2/8 | LOSS: 4.689136858360143e-06\n",
      "VAL: EPOCH 534/1000 | BATCH 3/8 | LOSS: 4.601581622409867e-06\n",
      "VAL: EPOCH 534/1000 | BATCH 4/8 | LOSS: 4.670680755225476e-06\n",
      "VAL: EPOCH 534/1000 | BATCH 5/8 | LOSS: 4.6719402841214714e-06\n",
      "VAL: EPOCH 534/1000 | BATCH 6/8 | LOSS: 4.613443317274297e-06\n",
      "VAL: EPOCH 534/1000 | BATCH 7/8 | LOSS: 4.524962662344478e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 0/71 | LOSS: 3.825503426924115e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 1/71 | LOSS: 5.169994665266131e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 2/71 | LOSS: 5.281190927538167e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 3/71 | LOSS: 4.999614816370013e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 4/71 | LOSS: 5.1548235205700624e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 5/71 | LOSS: 5.076222502490661e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 6/71 | LOSS: 5.193585301250485e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 7/71 | LOSS: 5.046168951139407e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 8/71 | LOSS: 5.1658400176772074e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 9/71 | LOSS: 5.080776736576808e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 10/71 | LOSS: 5.186650014366023e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 11/71 | LOSS: 5.0247634438467985e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 12/71 | LOSS: 5.301287483934385e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 13/71 | LOSS: 5.164891945241834e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 14/71 | LOSS: 5.371524427270439e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 15/71 | LOSS: 5.298485262983377e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 16/71 | LOSS: 5.326971912676099e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 17/71 | LOSS: 5.395541393227177e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 18/71 | LOSS: 5.3249853791433115e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 19/71 | LOSS: 5.361212629395595e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 20/71 | LOSS: 5.2937970871252696e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 21/71 | LOSS: 5.281305778804711e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 22/71 | LOSS: 5.238669730359739e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 23/71 | LOSS: 5.167281225719004e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 24/71 | LOSS: 5.152858320798259e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 25/71 | LOSS: 5.092877306476741e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 26/71 | LOSS: 5.077548621557071e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 27/71 | LOSS: 5.0542721591487605e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 28/71 | LOSS: 5.040545374868088e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 29/71 | LOSS: 4.991111700292094e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 30/71 | LOSS: 4.9849195860470874e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 31/71 | LOSS: 5.0196114074196885e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 32/71 | LOSS: 5.0283133119096e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 33/71 | LOSS: 4.977662991437041e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 34/71 | LOSS: 4.99025446905372e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 35/71 | LOSS: 5.018937119732274e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 36/71 | LOSS: 5.014608497141445e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 37/71 | LOSS: 4.998110171072767e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 38/71 | LOSS: 4.977278002535548e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 39/71 | LOSS: 4.946538513195264e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 40/71 | LOSS: 4.9108617564875535e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 41/71 | LOSS: 4.898240696266362e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 42/71 | LOSS: 4.868388471148264e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 43/71 | LOSS: 4.860761743491986e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 44/71 | LOSS: 4.843819937781922e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 45/71 | LOSS: 4.844005013143507e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 46/71 | LOSS: 4.807695907576535e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 47/71 | LOSS: 4.8125505145435454e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 48/71 | LOSS: 4.785709817803522e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 49/71 | LOSS: 4.777597787324339e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 50/71 | LOSS: 4.7749931314576635e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 51/71 | LOSS: 4.756897676718207e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 52/71 | LOSS: 4.745437300303854e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 53/71 | LOSS: 4.71682666877179e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 54/71 | LOSS: 4.73988944081198e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 55/71 | LOSS: 4.7385911687212816e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 56/71 | LOSS: 4.7290155124389e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 57/71 | LOSS: 4.706037061466837e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 58/71 | LOSS: 4.724296886860877e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 59/71 | LOSS: 4.7139679926052246e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 60/71 | LOSS: 4.757164583306952e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 61/71 | LOSS: 4.744778611013551e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 62/71 | LOSS: 4.8040246224806495e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 63/71 | LOSS: 4.798265631933418e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 64/71 | LOSS: 4.859146481696651e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 65/71 | LOSS: 4.848526173466808e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 66/71 | LOSS: 4.8360224687413975e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 67/71 | LOSS: 4.871621175585888e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 68/71 | LOSS: 4.8702431180625325e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 69/71 | LOSS: 4.8827807696787725e-06\n",
      "TRAIN: EPOCH 535/1000 | BATCH 70/71 | LOSS: 4.8741580652039e-06\n",
      "VAL: EPOCH 535/1000 | BATCH 0/8 | LOSS: 6.027042218192946e-06\n",
      "VAL: EPOCH 535/1000 | BATCH 1/8 | LOSS: 6.719444627378834e-06\n",
      "VAL: EPOCH 535/1000 | BATCH 2/8 | LOSS: 7.143077103440494e-06\n",
      "VAL: EPOCH 535/1000 | BATCH 3/8 | LOSS: 7.209339855762664e-06\n",
      "VAL: EPOCH 535/1000 | BATCH 4/8 | LOSS: 7.211320189526305e-06\n",
      "VAL: EPOCH 535/1000 | BATCH 5/8 | LOSS: 7.203348104667384e-06\n",
      "VAL: EPOCH 535/1000 | BATCH 6/8 | LOSS: 7.075692727604681e-06\n",
      "VAL: EPOCH 535/1000 | BATCH 7/8 | LOSS: 7.074183542954415e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 0/71 | LOSS: 7.241962521220557e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 1/71 | LOSS: 7.611313321831403e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 2/71 | LOSS: 6.7435660942768054e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 3/71 | LOSS: 6.6693194185063476e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 4/71 | LOSS: 6.884450158395339e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 5/71 | LOSS: 6.601660970773082e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 6/71 | LOSS: 6.339102258477526e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 7/71 | LOSS: 6.246521536468208e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 8/71 | LOSS: 6.1200889326250436e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 9/71 | LOSS: 5.934614773650537e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 10/71 | LOSS: 5.948303623881657e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 11/71 | LOSS: 5.898740710108541e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 12/71 | LOSS: 5.9229306456332916e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 13/71 | LOSS: 5.802659188702819e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 14/71 | LOSS: 5.854275301923432e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 15/71 | LOSS: 5.844772999807901e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 16/71 | LOSS: 5.900894500561716e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 17/71 | LOSS: 6.120543225228579e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 18/71 | LOSS: 6.023257020914523e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 19/71 | LOSS: 6.150880267341563e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 20/71 | LOSS: 6.304031926230805e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 21/71 | LOSS: 6.3371571534596365e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 22/71 | LOSS: 6.2770583376139095e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 23/71 | LOSS: 6.344687240774268e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 24/71 | LOSS: 6.385561846400378e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 25/71 | LOSS: 6.291046877147612e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 26/71 | LOSS: 6.3852669174795865e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 27/71 | LOSS: 6.3202089027722e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 28/71 | LOSS: 6.289725927766493e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 29/71 | LOSS: 6.289838574957685e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 30/71 | LOSS: 6.2809918422464116e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 31/71 | LOSS: 6.325820649522029e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 32/71 | LOSS: 6.254124455705151e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 33/71 | LOSS: 6.316085639756399e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 34/71 | LOSS: 6.248016675921722e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 35/71 | LOSS: 6.2964467638772075e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 36/71 | LOSS: 6.257123508525232e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 37/71 | LOSS: 6.2277561147838195e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 38/71 | LOSS: 6.201237120433526e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 39/71 | LOSS: 6.154090908694343e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 40/71 | LOSS: 6.095660399118859e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 41/71 | LOSS: 6.080292158523142e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 42/71 | LOSS: 6.022506234986332e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 43/71 | LOSS: 5.979998341196981e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 44/71 | LOSS: 5.936237787156844e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 45/71 | LOSS: 5.885248351264834e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 46/71 | LOSS: 5.8467611673203215e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 47/71 | LOSS: 5.828144030791312e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 48/71 | LOSS: 5.796056182923482e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 49/71 | LOSS: 5.758066904490988e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 50/71 | LOSS: 5.716478462287056e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 51/71 | LOSS: 5.6804839582232835e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 52/71 | LOSS: 5.644571068464671e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 53/71 | LOSS: 5.602153279411411e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 54/71 | LOSS: 5.580847147774131e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 55/71 | LOSS: 5.564731765161923e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 56/71 | LOSS: 5.530260491388872e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 57/71 | LOSS: 5.504781472217442e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 58/71 | LOSS: 5.4803704303702616e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 59/71 | LOSS: 5.444560205584518e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 60/71 | LOSS: 5.411614263154705e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 61/71 | LOSS: 5.421263410660744e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 62/71 | LOSS: 5.397686606109288e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 63/71 | LOSS: 5.368525780369282e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 64/71 | LOSS: 5.355765114682771e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 65/71 | LOSS: 5.343172815766152e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 66/71 | LOSS: 5.3505600827697004e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 67/71 | LOSS: 5.326790197640923e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 68/71 | LOSS: 5.323552366699795e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 69/71 | LOSS: 5.319626555839412e-06\n",
      "TRAIN: EPOCH 536/1000 | BATCH 70/71 | LOSS: 5.290441578554365e-06\n",
      "VAL: EPOCH 536/1000 | BATCH 0/8 | LOSS: 3.6730818919750163e-06\n",
      "VAL: EPOCH 536/1000 | BATCH 1/8 | LOSS: 4.20548951751698e-06\n",
      "VAL: EPOCH 536/1000 | BATCH 2/8 | LOSS: 4.390429770258682e-06\n",
      "VAL: EPOCH 536/1000 | BATCH 3/8 | LOSS: 4.46994403091594e-06\n",
      "VAL: EPOCH 536/1000 | BATCH 4/8 | LOSS: 4.480857342059608e-06\n",
      "VAL: EPOCH 536/1000 | BATCH 5/8 | LOSS: 4.439489392401204e-06\n",
      "VAL: EPOCH 536/1000 | BATCH 6/8 | LOSS: 4.293512412394713e-06\n",
      "VAL: EPOCH 536/1000 | BATCH 7/8 | LOSS: 4.1321007131500664e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 0/71 | LOSS: 4.252498001733329e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 1/71 | LOSS: 4.273975719115697e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 2/71 | LOSS: 4.581168165411024e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 3/71 | LOSS: 4.4427218881537556e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 4/71 | LOSS: 4.199817794869887e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 5/71 | LOSS: 4.27646295975137e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 6/71 | LOSS: 4.547158693770013e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 7/71 | LOSS: 4.57721728253091e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 8/71 | LOSS: 4.777719772795939e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 9/71 | LOSS: 4.794886172021506e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 10/71 | LOSS: 4.810375850700604e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 11/71 | LOSS: 4.665292105225187e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 12/71 | LOSS: 4.635337483285604e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 13/71 | LOSS: 4.7041845456468375e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 14/71 | LOSS: 4.6180338965010986e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 15/71 | LOSS: 4.544450604271333e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 16/71 | LOSS: 4.449668909621644e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 17/71 | LOSS: 4.476294634514488e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 18/71 | LOSS: 4.483430290603888e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 19/71 | LOSS: 4.51152495770657e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 20/71 | LOSS: 4.52589721958031e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 21/71 | LOSS: 4.596488576101944e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 22/71 | LOSS: 4.58390438681185e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 23/71 | LOSS: 4.551410550372263e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 24/71 | LOSS: 4.547429371086764e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 25/71 | LOSS: 4.549583475356765e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 26/71 | LOSS: 4.503585875715578e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 27/71 | LOSS: 4.493625088538725e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 28/71 | LOSS: 4.480123215185374e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 29/71 | LOSS: 4.45021008393572e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 30/71 | LOSS: 4.458878955178548e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 31/71 | LOSS: 4.460850036025477e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 32/71 | LOSS: 4.476100292032987e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 33/71 | LOSS: 4.447701792956008e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 34/71 | LOSS: 4.462339165911544e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 35/71 | LOSS: 4.476194969053419e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 36/71 | LOSS: 4.491747072402299e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 37/71 | LOSS: 4.505336691133714e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 38/71 | LOSS: 4.527071373908816e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 39/71 | LOSS: 4.580901554618322e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 40/71 | LOSS: 4.581966494719737e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 41/71 | LOSS: 4.582381474652461e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 42/71 | LOSS: 4.591150109474764e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 43/71 | LOSS: 4.630959996244531e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 44/71 | LOSS: 4.646739378383953e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 45/71 | LOSS: 4.6714503894137404e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 46/71 | LOSS: 4.706544170906628e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 47/71 | LOSS: 4.715612552293654e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 48/71 | LOSS: 4.717634603403727e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 49/71 | LOSS: 4.726377565020812e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 50/71 | LOSS: 4.780649938837479e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 51/71 | LOSS: 4.7986441306700444e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 52/71 | LOSS: 4.8036493885688886e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 53/71 | LOSS: 4.827197446704506e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 54/71 | LOSS: 4.8355608751907395e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 55/71 | LOSS: 4.822952129351117e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 56/71 | LOSS: 4.82360810724857e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 57/71 | LOSS: 4.8286177890735545e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 58/71 | LOSS: 4.8261151462966e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 59/71 | LOSS: 4.817028070647211e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 60/71 | LOSS: 4.81870412960368e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 61/71 | LOSS: 4.795695120617072e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 62/71 | LOSS: 4.796457882808422e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 63/71 | LOSS: 4.787114676219062e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 64/71 | LOSS: 4.785855232442443e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 65/71 | LOSS: 4.786198091592682e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 66/71 | LOSS: 4.770069028327542e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 67/71 | LOSS: 4.765482107332526e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 68/71 | LOSS: 4.7489914922535075e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 69/71 | LOSS: 4.752262056432041e-06\n",
      "TRAIN: EPOCH 537/1000 | BATCH 70/71 | LOSS: 4.7244808536586945e-06\n",
      "VAL: EPOCH 537/1000 | BATCH 0/8 | LOSS: 5.163929927221034e-06\n",
      "VAL: EPOCH 537/1000 | BATCH 1/8 | LOSS: 5.214975317358039e-06\n",
      "VAL: EPOCH 537/1000 | BATCH 2/8 | LOSS: 5.24329758870105e-06\n",
      "VAL: EPOCH 537/1000 | BATCH 3/8 | LOSS: 5.1154555649191025e-06\n",
      "VAL: EPOCH 537/1000 | BATCH 4/8 | LOSS: 5.159173360880231e-06\n",
      "VAL: EPOCH 537/1000 | BATCH 5/8 | LOSS: 5.129335098293571e-06\n",
      "VAL: EPOCH 537/1000 | BATCH 6/8 | LOSS: 5.02916983740371e-06\n",
      "VAL: EPOCH 537/1000 | BATCH 7/8 | LOSS: 4.899796863355732e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 0/71 | LOSS: 6.176323950057849e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 1/71 | LOSS: 5.129881174070761e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 2/71 | LOSS: 4.984461156709585e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 3/71 | LOSS: 4.749279696625308e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 4/71 | LOSS: 5.027156748838024e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 5/71 | LOSS: 5.286253250839461e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 6/71 | LOSS: 4.997118302136576e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 7/71 | LOSS: 5.272441541137596e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 8/71 | LOSS: 5.1426934027808275e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 9/71 | LOSS: 5.3417678600453655e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 10/71 | LOSS: 5.314701638781116e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 11/71 | LOSS: 5.430170536631825e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 12/71 | LOSS: 5.309564043133063e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 13/71 | LOSS: 5.221952034974363e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 14/71 | LOSS: 5.16837309684585e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 15/71 | LOSS: 5.186738277984659e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 16/71 | LOSS: 5.216399572686094e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 17/71 | LOSS: 5.22006675712166e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 18/71 | LOSS: 5.35930847658356e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 19/71 | LOSS: 5.28741330754201e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 20/71 | LOSS: 5.30949343902605e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 21/71 | LOSS: 5.241643283294582e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 22/71 | LOSS: 5.218891947547292e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 23/71 | LOSS: 5.1648927505236015e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 24/71 | LOSS: 5.124665922267013e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 25/71 | LOSS: 5.09054571186705e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 26/71 | LOSS: 5.104078615692776e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 27/71 | LOSS: 5.0622407294602974e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 28/71 | LOSS: 5.040526698864816e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 29/71 | LOSS: 5.019912615959281e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 30/71 | LOSS: 5.019719046392692e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 31/71 | LOSS: 4.993806165032311e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 32/71 | LOSS: 4.989706150643707e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 33/71 | LOSS: 4.969195615825918e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 34/71 | LOSS: 4.960215528626577e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 35/71 | LOSS: 4.960702584513557e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 36/71 | LOSS: 5.026035665110071e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 37/71 | LOSS: 5.007584718441649e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 38/71 | LOSS: 4.980122697909395e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 39/71 | LOSS: 4.983450736517625e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 40/71 | LOSS: 4.969121014588248e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 41/71 | LOSS: 4.948236832682423e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 42/71 | LOSS: 4.923648422718966e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 43/71 | LOSS: 4.9374983590992505e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 44/71 | LOSS: 4.900304328556457e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 45/71 | LOSS: 4.8873116611503065e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 46/71 | LOSS: 4.864192823469181e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 47/71 | LOSS: 4.850218445540122e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 48/71 | LOSS: 4.848326537340858e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 49/71 | LOSS: 4.860417134295858e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 50/71 | LOSS: 4.849626190893925e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 51/71 | LOSS: 4.848232908198742e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 52/71 | LOSS: 4.834782777902976e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 53/71 | LOSS: 4.809805486898027e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 54/71 | LOSS: 4.801733389699207e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 55/71 | LOSS: 4.7978060990772065e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 56/71 | LOSS: 4.7844816092771926e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 57/71 | LOSS: 4.7672364463982505e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 58/71 | LOSS: 4.775279109507811e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 59/71 | LOSS: 4.765176155766919e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 60/71 | LOSS: 4.762962988501922e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 61/71 | LOSS: 4.747107408870614e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 62/71 | LOSS: 4.740354803462174e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 63/71 | LOSS: 4.727481066169048e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 64/71 | LOSS: 4.71234222039549e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 65/71 | LOSS: 4.691677941771173e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 66/71 | LOSS: 4.697675836017771e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 67/71 | LOSS: 4.679812869145441e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 68/71 | LOSS: 4.664548085834806e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 69/71 | LOSS: 4.662724758678191e-06\n",
      "TRAIN: EPOCH 538/1000 | BATCH 70/71 | LOSS: 4.652913839634775e-06\n",
      "VAL: EPOCH 538/1000 | BATCH 0/8 | LOSS: 4.235908818372991e-06\n",
      "VAL: EPOCH 538/1000 | BATCH 1/8 | LOSS: 4.448501385923009e-06\n",
      "VAL: EPOCH 538/1000 | BATCH 2/8 | LOSS: 4.605217630645105e-06\n",
      "VAL: EPOCH 538/1000 | BATCH 3/8 | LOSS: 4.5457120450009825e-06\n",
      "VAL: EPOCH 538/1000 | BATCH 4/8 | LOSS: 4.601678119797725e-06\n",
      "VAL: EPOCH 538/1000 | BATCH 5/8 | LOSS: 4.604081823345041e-06\n",
      "VAL: EPOCH 538/1000 | BATCH 6/8 | LOSS: 4.490192688016188e-06\n",
      "VAL: EPOCH 538/1000 | BATCH 7/8 | LOSS: 4.423490736371605e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 0/71 | LOSS: 4.205167897453066e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 1/71 | LOSS: 4.932149749947712e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 2/71 | LOSS: 4.48415979311297e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 3/71 | LOSS: 4.43957162588049e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 4/71 | LOSS: 4.368473673821427e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 5/71 | LOSS: 4.344153542964098e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 6/71 | LOSS: 4.309202787616024e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 7/71 | LOSS: 4.4775241576644476e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 8/71 | LOSS: 4.439828949721737e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 9/71 | LOSS: 4.458127204998164e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 10/71 | LOSS: 4.416404987172097e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 11/71 | LOSS: 4.282679772889726e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 12/71 | LOSS: 4.315518113938966e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 13/71 | LOSS: 4.332521614612363e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 14/71 | LOSS: 4.337406153354095e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 15/71 | LOSS: 4.32086670798526e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 16/71 | LOSS: 4.277264157709022e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 17/71 | LOSS: 4.24481354204747e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 18/71 | LOSS: 4.229222703974014e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 19/71 | LOSS: 4.2075252167705916e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 20/71 | LOSS: 4.210899554718275e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 21/71 | LOSS: 4.223100032421908e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 22/71 | LOSS: 4.253296419451742e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 23/71 | LOSS: 4.279000904716668e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 24/71 | LOSS: 4.35103771451395e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 25/71 | LOSS: 4.351779108009606e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 26/71 | LOSS: 4.3854166101987684e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 27/71 | LOSS: 4.448387601639427e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 28/71 | LOSS: 4.396882010543527e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 29/71 | LOSS: 4.409725048996431e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 30/71 | LOSS: 4.4007324651184105e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 31/71 | LOSS: 4.367852554310048e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 32/71 | LOSS: 4.358762246490435e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 33/71 | LOSS: 4.344013886485995e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 34/71 | LOSS: 4.353749799130518e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 35/71 | LOSS: 4.367930412425065e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 36/71 | LOSS: 4.371527100718939e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 37/71 | LOSS: 4.351343136477226e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 38/71 | LOSS: 4.360278843215872e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 39/71 | LOSS: 4.372374615968511e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 40/71 | LOSS: 4.3858909443437505e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 41/71 | LOSS: 4.38379923447688e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 42/71 | LOSS: 4.3753035549533985e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 43/71 | LOSS: 4.369628625640458e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 44/71 | LOSS: 4.362801069469747e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 45/71 | LOSS: 4.371224819173538e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 46/71 | LOSS: 4.376826890175505e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 47/71 | LOSS: 4.400095988899011e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 48/71 | LOSS: 4.377648519020651e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 49/71 | LOSS: 4.380039317766205e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 50/71 | LOSS: 4.379532137867429e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 51/71 | LOSS: 4.3760568097730775e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 52/71 | LOSS: 4.38740136025513e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 53/71 | LOSS: 4.405187711300742e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 54/71 | LOSS: 4.402142407012765e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 55/71 | LOSS: 4.372932571478876e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 56/71 | LOSS: 4.37927255996073e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 57/71 | LOSS: 4.361349154131513e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 58/71 | LOSS: 4.370331938751683e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 59/71 | LOSS: 4.363739499998095e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 60/71 | LOSS: 4.374015965308378e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 61/71 | LOSS: 4.369509559681671e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 62/71 | LOSS: 4.378300294168487e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 63/71 | LOSS: 4.367731516907725e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 64/71 | LOSS: 4.3659504661683545e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 65/71 | LOSS: 4.363229873943607e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 66/71 | LOSS: 4.354968278852566e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 67/71 | LOSS: 4.351369264387519e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 68/71 | LOSS: 4.349899216427677e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 69/71 | LOSS: 4.346230590661955e-06\n",
      "TRAIN: EPOCH 539/1000 | BATCH 70/71 | LOSS: 4.3385668671452175e-06\n",
      "VAL: EPOCH 539/1000 | BATCH 0/8 | LOSS: 4.102426373719936e-06\n",
      "VAL: EPOCH 539/1000 | BATCH 1/8 | LOSS: 4.58336421615968e-06\n",
      "VAL: EPOCH 539/1000 | BATCH 2/8 | LOSS: 4.985982892928102e-06\n",
      "VAL: EPOCH 539/1000 | BATCH 3/8 | LOSS: 5.0765098649208085e-06\n",
      "VAL: EPOCH 539/1000 | BATCH 4/8 | LOSS: 5.0975006161024796e-06\n",
      "VAL: EPOCH 539/1000 | BATCH 5/8 | LOSS: 5.172762939764652e-06\n",
      "VAL: EPOCH 539/1000 | BATCH 6/8 | LOSS: 5.091340231696709e-06\n",
      "VAL: EPOCH 539/1000 | BATCH 7/8 | LOSS: 5.0356136966911436e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 0/71 | LOSS: 6.070273229852319e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 1/71 | LOSS: 5.064185188530246e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 2/71 | LOSS: 5.162391062185634e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 3/71 | LOSS: 4.909014478471363e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 4/71 | LOSS: 4.801621616934426e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 5/71 | LOSS: 4.616369892573857e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 6/71 | LOSS: 4.406201924211928e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 7/71 | LOSS: 4.311805895440557e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 8/71 | LOSS: 4.303801107299579e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 9/71 | LOSS: 4.271237753528112e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 10/71 | LOSS: 4.27090135830936e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 11/71 | LOSS: 4.223852272389195e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 12/71 | LOSS: 4.262636926236714e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 13/71 | LOSS: 4.2584476724576755e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 14/71 | LOSS: 4.271033518913706e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 15/71 | LOSS: 4.25810576132335e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 16/71 | LOSS: 4.2213637377298095e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 17/71 | LOSS: 4.2509843878077745e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 18/71 | LOSS: 4.243501555385209e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 19/71 | LOSS: 4.200595037673338e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 20/71 | LOSS: 4.244076221550044e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 21/71 | LOSS: 4.279320747131847e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 22/71 | LOSS: 4.308892179640521e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 23/71 | LOSS: 4.3231072387091745e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 24/71 | LOSS: 4.316354525144561e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 25/71 | LOSS: 4.3981450517094345e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 26/71 | LOSS: 4.381829099926378e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 27/71 | LOSS: 4.46183392049144e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 28/71 | LOSS: 4.441268762595177e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 29/71 | LOSS: 4.491734906271935e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 30/71 | LOSS: 4.534096066685555e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 31/71 | LOSS: 4.596454992622512e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 32/71 | LOSS: 4.693458232864169e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 33/71 | LOSS: 4.725432752232283e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 34/71 | LOSS: 4.7739307839427575e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 35/71 | LOSS: 4.763848064865468e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 36/71 | LOSS: 4.780474185011101e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 37/71 | LOSS: 4.777834042694226e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 38/71 | LOSS: 4.796067567160488e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 39/71 | LOSS: 4.811111426761272e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 40/71 | LOSS: 4.793898619382384e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 41/71 | LOSS: 4.803844468543026e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 42/71 | LOSS: 4.811016228708214e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 43/71 | LOSS: 4.8052523104822535e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 44/71 | LOSS: 4.780795037125548e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 45/71 | LOSS: 4.783444103889886e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 46/71 | LOSS: 4.75514718558809e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 47/71 | LOSS: 4.741039887790066e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 48/71 | LOSS: 4.728817606408075e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 49/71 | LOSS: 4.706974091277516e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 50/71 | LOSS: 4.696744206418825e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 51/71 | LOSS: 4.681953008947368e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 52/71 | LOSS: 4.661151418951944e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 53/71 | LOSS: 4.655179218894369e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 54/71 | LOSS: 4.637905910633642e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 55/71 | LOSS: 4.641315590398725e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 56/71 | LOSS: 4.6402962469297216e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 57/71 | LOSS: 4.633430554851406e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 58/71 | LOSS: 4.617060856123887e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 59/71 | LOSS: 4.61571405973397e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 60/71 | LOSS: 4.606636105668035e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 61/71 | LOSS: 4.578571593488197e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 62/71 | LOSS: 4.577134747691277e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 63/71 | LOSS: 4.5711062774955735e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 64/71 | LOSS: 4.565820070358593e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 65/71 | LOSS: 4.554387074180837e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 66/71 | LOSS: 4.555187836137805e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 67/71 | LOSS: 4.54773491502933e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 68/71 | LOSS: 4.534488277346574e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 69/71 | LOSS: 4.530116302703391e-06\n",
      "TRAIN: EPOCH 540/1000 | BATCH 70/71 | LOSS: 4.536889091340667e-06\n",
      "VAL: EPOCH 540/1000 | BATCH 0/8 | LOSS: 4.049698873132002e-06\n",
      "VAL: EPOCH 540/1000 | BATCH 1/8 | LOSS: 4.3059512790932786e-06\n",
      "VAL: EPOCH 540/1000 | BATCH 2/8 | LOSS: 4.452408575161826e-06\n",
      "VAL: EPOCH 540/1000 | BATCH 3/8 | LOSS: 4.505532160692383e-06\n",
      "VAL: EPOCH 540/1000 | BATCH 4/8 | LOSS: 4.5326723920879886e-06\n",
      "VAL: EPOCH 540/1000 | BATCH 5/8 | LOSS: 4.6311033656820655e-06\n",
      "VAL: EPOCH 540/1000 | BATCH 6/8 | LOSS: 4.5289234549272805e-06\n",
      "VAL: EPOCH 540/1000 | BATCH 7/8 | LOSS: 4.44701788637758e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 0/71 | LOSS: 3.6682556583400583e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 1/71 | LOSS: 3.7821927207914996e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 2/71 | LOSS: 4.199088152745389e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 3/71 | LOSS: 3.933415257506567e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 4/71 | LOSS: 3.892044423992047e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 5/71 | LOSS: 3.96282083177842e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 6/71 | LOSS: 3.948972075055021e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 7/71 | LOSS: 3.9468117165597505e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 8/71 | LOSS: 3.9564098794168485e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 9/71 | LOSS: 3.995258248323808e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 10/71 | LOSS: 4.020619681713552e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 11/71 | LOSS: 4.098657541362627e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 12/71 | LOSS: 4.099534180568298e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 13/71 | LOSS: 4.173608398819592e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 14/71 | LOSS: 4.097564942640019e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 15/71 | LOSS: 4.137715109209239e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 16/71 | LOSS: 4.116714829733075e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 17/71 | LOSS: 4.173162526260259e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 18/71 | LOSS: 4.122721365044861e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 19/71 | LOSS: 4.211323016534152e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 20/71 | LOSS: 4.2485731206570995e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 21/71 | LOSS: 4.383402256280533e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 22/71 | LOSS: 4.38945163177382e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 23/71 | LOSS: 4.469112222219944e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 24/71 | LOSS: 4.540483441815013e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 25/71 | LOSS: 4.66381829238134e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 26/71 | LOSS: 4.6503103274315235e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 27/71 | LOSS: 4.676230495143889e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 28/71 | LOSS: 4.665253355001212e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 29/71 | LOSS: 4.632070332869868e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 30/71 | LOSS: 4.608294023367513e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 31/71 | LOSS: 4.582344217851642e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 32/71 | LOSS: 4.57483025512016e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 33/71 | LOSS: 4.583759030484704e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 34/71 | LOSS: 4.562712014636158e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 35/71 | LOSS: 4.584418345782777e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 36/71 | LOSS: 4.551677095052484e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 37/71 | LOSS: 4.580708910476747e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 38/71 | LOSS: 4.5571701157780644e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 39/71 | LOSS: 4.518889232940637e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 40/71 | LOSS: 4.53954309771844e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 41/71 | LOSS: 4.545513899670152e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 42/71 | LOSS: 4.5267172217156555e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 43/71 | LOSS: 4.512027275764425e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 44/71 | LOSS: 4.4972759496886285e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 45/71 | LOSS: 4.486010489610793e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 46/71 | LOSS: 4.510193185582311e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 47/71 | LOSS: 4.5010293471629366e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 48/71 | LOSS: 4.522719049121833e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 49/71 | LOSS: 4.534174277068814e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 50/71 | LOSS: 4.532088870987268e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 51/71 | LOSS: 4.534775406192728e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 52/71 | LOSS: 4.545359740216057e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 53/71 | LOSS: 4.528551463173008e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 54/71 | LOSS: 4.520844322541962e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 55/71 | LOSS: 4.52373104410851e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 56/71 | LOSS: 4.513511213557203e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 57/71 | LOSS: 4.523229273249383e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 58/71 | LOSS: 4.517696968339466e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 59/71 | LOSS: 4.508224742494349e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 60/71 | LOSS: 4.517819926135058e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 61/71 | LOSS: 4.520220463452842e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 62/71 | LOSS: 4.516180329769018e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 63/71 | LOSS: 4.549871256642746e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 64/71 | LOSS: 4.530776858211119e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 65/71 | LOSS: 4.5271607852941305e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 66/71 | LOSS: 4.556032817801217e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 67/71 | LOSS: 4.559810777744347e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 68/71 | LOSS: 4.576490660521971e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 69/71 | LOSS: 4.568215574961089e-06\n",
      "TRAIN: EPOCH 541/1000 | BATCH 70/71 | LOSS: 4.551728474146164e-06\n",
      "VAL: EPOCH 541/1000 | BATCH 0/8 | LOSS: 4.661742423195392e-06\n",
      "VAL: EPOCH 541/1000 | BATCH 1/8 | LOSS: 5.0328649194852915e-06\n",
      "VAL: EPOCH 541/1000 | BATCH 2/8 | LOSS: 5.081812105345307e-06\n",
      "VAL: EPOCH 541/1000 | BATCH 3/8 | LOSS: 5.105982495479111e-06\n",
      "VAL: EPOCH 541/1000 | BATCH 4/8 | LOSS: 5.103398052597186e-06\n",
      "VAL: EPOCH 541/1000 | BATCH 5/8 | LOSS: 4.917344919401027e-06\n",
      "VAL: EPOCH 541/1000 | BATCH 6/8 | LOSS: 4.7247187272400226e-06\n",
      "VAL: EPOCH 541/1000 | BATCH 7/8 | LOSS: 4.507739419068457e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 0/71 | LOSS: 5.065689038019627e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 1/71 | LOSS: 6.776279406039976e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 2/71 | LOSS: 6.751236166261758e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 3/71 | LOSS: 6.784461220377125e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 4/71 | LOSS: 6.623780791414902e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 5/71 | LOSS: 6.2892192242240226e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 6/71 | LOSS: 6.714126097254588e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 7/71 | LOSS: 6.521675970816432e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 8/71 | LOSS: 6.443473820480803e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 9/71 | LOSS: 6.320252896330203e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 10/71 | LOSS: 6.202073770757786e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 11/71 | LOSS: 6.358906489367655e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 12/71 | LOSS: 6.205792837695649e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 13/71 | LOSS: 6.292420364063998e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 14/71 | LOSS: 6.179477835151677e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 15/71 | LOSS: 6.127508640929591e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 16/71 | LOSS: 5.981099614954848e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 17/71 | LOSS: 5.8920944638884976e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 18/71 | LOSS: 5.825884476220572e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 19/71 | LOSS: 5.734149851832626e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 20/71 | LOSS: 5.6323827785880506e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 21/71 | LOSS: 5.626506849280717e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 22/71 | LOSS: 5.526939098079152e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 23/71 | LOSS: 5.481877299947276e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 24/71 | LOSS: 5.440886670839973e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 25/71 | LOSS: 5.476794845330564e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 26/71 | LOSS: 5.393622308878952e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 27/71 | LOSS: 5.3852787849401854e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 28/71 | LOSS: 5.342712405575279e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 29/71 | LOSS: 5.296478608822023e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 30/71 | LOSS: 5.2792098035898825e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 31/71 | LOSS: 5.215918513101769e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 32/71 | LOSS: 5.16877591480531e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 33/71 | LOSS: 5.166409096091229e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 34/71 | LOSS: 5.141076592605844e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 35/71 | LOSS: 5.100067527210841e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 36/71 | LOSS: 5.0797733316308904e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 37/71 | LOSS: 5.093375666307877e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 38/71 | LOSS: 5.076784179600042e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 39/71 | LOSS: 5.053187493331279e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 40/71 | LOSS: 5.043688433277832e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 41/71 | LOSS: 5.043865447381298e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 42/71 | LOSS: 5.017348543291053e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 43/71 | LOSS: 5.0015066531986205e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 44/71 | LOSS: 4.98950017673552e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 45/71 | LOSS: 4.9759139083898445e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 46/71 | LOSS: 4.9609715644808295e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 47/71 | LOSS: 4.9540684348888435e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 48/71 | LOSS: 4.957107477116087e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 49/71 | LOSS: 4.925295752400416e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 50/71 | LOSS: 4.911769213090427e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 51/71 | LOSS: 4.910921339055326e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 52/71 | LOSS: 4.9269292331992576e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 53/71 | LOSS: 4.905640707875136e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 54/71 | LOSS: 4.890396319917107e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 55/71 | LOSS: 4.868308678851463e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 56/71 | LOSS: 4.8554546705609506e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 57/71 | LOSS: 4.850820953928131e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 58/71 | LOSS: 4.839634970020428e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 59/71 | LOSS: 4.827136869304619e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 60/71 | LOSS: 4.830155234469471e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 61/71 | LOSS: 4.8329430433847345e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 62/71 | LOSS: 4.849265368402629e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 63/71 | LOSS: 4.849570082399168e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 64/71 | LOSS: 4.86210345535745e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 65/71 | LOSS: 4.84795932055876e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 66/71 | LOSS: 4.839449027360334e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 67/71 | LOSS: 4.835544479968067e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 68/71 | LOSS: 4.822574922307968e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 69/71 | LOSS: 4.831852908994603e-06\n",
      "TRAIN: EPOCH 542/1000 | BATCH 70/71 | LOSS: 4.81774566677058e-06\n",
      "VAL: EPOCH 542/1000 | BATCH 0/8 | LOSS: 4.671539954870241e-06\n",
      "VAL: EPOCH 542/1000 | BATCH 1/8 | LOSS: 5.03712476529472e-06\n",
      "VAL: EPOCH 542/1000 | BATCH 2/8 | LOSS: 5.2028503887413535e-06\n",
      "VAL: EPOCH 542/1000 | BATCH 3/8 | LOSS: 5.1592122645161e-06\n",
      "VAL: EPOCH 542/1000 | BATCH 4/8 | LOSS: 5.236795641394565e-06\n",
      "VAL: EPOCH 542/1000 | BATCH 5/8 | LOSS: 5.320702181658514e-06\n",
      "VAL: EPOCH 542/1000 | BATCH 6/8 | LOSS: 5.193559445615392e-06\n",
      "VAL: EPOCH 542/1000 | BATCH 7/8 | LOSS: 5.221498668106506e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 0/71 | LOSS: 4.295719463698333e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 1/71 | LOSS: 4.47781144430337e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 2/71 | LOSS: 4.9262512220593635e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 3/71 | LOSS: 4.835447157347517e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 4/71 | LOSS: 5.233691535977414e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 5/71 | LOSS: 5.190031894623341e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 6/71 | LOSS: 5.317162958817789e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 7/71 | LOSS: 5.256349311366648e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 8/71 | LOSS: 5.101518581795972e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 9/71 | LOSS: 5.068077643954893e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 10/71 | LOSS: 5.02952860353451e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 11/71 | LOSS: 4.915944828098873e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 12/71 | LOSS: 4.857632820158659e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 13/71 | LOSS: 4.794602805954387e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 14/71 | LOSS: 4.766632840376891e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 15/71 | LOSS: 4.749679675342122e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 16/71 | LOSS: 4.692911983618294e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 17/71 | LOSS: 4.654489228212495e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 18/71 | LOSS: 4.725083296139737e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 19/71 | LOSS: 4.764310256177851e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 20/71 | LOSS: 4.817427755673028e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 21/71 | LOSS: 4.818666080940272e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 22/71 | LOSS: 4.804743955743185e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 23/71 | LOSS: 4.843742109036005e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 24/71 | LOSS: 4.8380791304225566e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 25/71 | LOSS: 4.8516740382463186e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 26/71 | LOSS: 4.8329739170353775e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 27/71 | LOSS: 4.820532451179003e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 28/71 | LOSS: 4.7811808270063045e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 29/71 | LOSS: 4.780382460012333e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 30/71 | LOSS: 4.739216783198044e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 31/71 | LOSS: 4.723390510719128e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 32/71 | LOSS: 4.717731640498228e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 33/71 | LOSS: 4.7317294649260206e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 34/71 | LOSS: 4.711927983537732e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 35/71 | LOSS: 4.705697285266473e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 36/71 | LOSS: 4.691905317215699e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 37/71 | LOSS: 4.6777133595816465e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 38/71 | LOSS: 4.649914782744301e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 39/71 | LOSS: 4.607033611137013e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 40/71 | LOSS: 4.588242318721136e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 41/71 | LOSS: 4.584091432964972e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 42/71 | LOSS: 4.568528110275808e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 43/71 | LOSS: 4.583817816307022e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 44/71 | LOSS: 4.5705967649054295e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 45/71 | LOSS: 4.600668865390142e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 46/71 | LOSS: 4.639706798541816e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 47/71 | LOSS: 4.630473900609407e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 48/71 | LOSS: 4.616979924544373e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 49/71 | LOSS: 4.647003579520969e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 50/71 | LOSS: 4.637704808142278e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 51/71 | LOSS: 4.66484868866246e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 52/71 | LOSS: 4.666174210694219e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 53/71 | LOSS: 4.740043340023948e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 54/71 | LOSS: 4.74910660738136e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 55/71 | LOSS: 4.763861552094438e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 56/71 | LOSS: 4.763787976053049e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 57/71 | LOSS: 4.762591123804229e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 58/71 | LOSS: 4.795666065269793e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 59/71 | LOSS: 4.78142583991333e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 60/71 | LOSS: 4.786897899647869e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 61/71 | LOSS: 4.782793065450894e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 62/71 | LOSS: 4.825750209651204e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 63/71 | LOSS: 4.810928935228276e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 64/71 | LOSS: 4.81808864116409e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 65/71 | LOSS: 4.79276522899895e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 66/71 | LOSS: 4.794164994306282e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 67/71 | LOSS: 4.808656466165678e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 68/71 | LOSS: 4.807522297293569e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 69/71 | LOSS: 4.8140442946273e-06\n",
      "TRAIN: EPOCH 543/1000 | BATCH 70/71 | LOSS: 4.797655136967414e-06\n",
      "VAL: EPOCH 543/1000 | BATCH 0/8 | LOSS: 6.763897545170039e-06\n",
      "VAL: EPOCH 543/1000 | BATCH 1/8 | LOSS: 7.089308610375156e-06\n",
      "VAL: EPOCH 543/1000 | BATCH 2/8 | LOSS: 7.239737897180021e-06\n",
      "VAL: EPOCH 543/1000 | BATCH 3/8 | LOSS: 7.23418429515732e-06\n",
      "VAL: EPOCH 543/1000 | BATCH 4/8 | LOSS: 7.334744987019804e-06\n",
      "VAL: EPOCH 543/1000 | BATCH 5/8 | LOSS: 7.143573914921338e-06\n",
      "VAL: EPOCH 543/1000 | BATCH 6/8 | LOSS: 7.13998536282036e-06\n",
      "VAL: EPOCH 543/1000 | BATCH 7/8 | LOSS: 6.977458099299838e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 0/71 | LOSS: 4.998412805434782e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 1/71 | LOSS: 5.2656055231636856e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 2/71 | LOSS: 5.038971115330544e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 3/71 | LOSS: 4.662445633130119e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 4/71 | LOSS: 5.160405862625339e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 5/71 | LOSS: 5.358130958181088e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 6/71 | LOSS: 5.499538117094614e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 7/71 | LOSS: 5.4250897107976925e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 8/71 | LOSS: 5.433215569104908e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 9/71 | LOSS: 5.348336912902596e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 10/71 | LOSS: 5.248161476794683e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 11/71 | LOSS: 5.150623887099452e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 12/71 | LOSS: 5.1117315951369865e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 13/71 | LOSS: 5.001562012044555e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 14/71 | LOSS: 4.918889241404638e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 15/71 | LOSS: 4.8675307908752075e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 16/71 | LOSS: 4.896745863066719e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 17/71 | LOSS: 4.848372542356729e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 18/71 | LOSS: 4.833307840319896e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 19/71 | LOSS: 4.798318116172595e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 20/71 | LOSS: 4.773274418940196e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 21/71 | LOSS: 4.748871943279069e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 22/71 | LOSS: 4.748009488416162e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 23/71 | LOSS: 4.700835925556627e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 24/71 | LOSS: 4.667466046157643e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 25/71 | LOSS: 4.624093653896684e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 26/71 | LOSS: 4.6412237595547964e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 27/71 | LOSS: 4.654166957607231e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 28/71 | LOSS: 4.657868861739126e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 29/71 | LOSS: 4.64428319446597e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 30/71 | LOSS: 4.637873974300625e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 31/71 | LOSS: 4.627416934965822e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 32/71 | LOSS: 4.685836929087545e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 33/71 | LOSS: 4.710398630937989e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 34/71 | LOSS: 4.696418188489458e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 35/71 | LOSS: 4.718369666534272e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 36/71 | LOSS: 4.703531732121593e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 37/71 | LOSS: 4.695504009377098e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 38/71 | LOSS: 4.683632637235673e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 39/71 | LOSS: 4.704457495563474e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 40/71 | LOSS: 4.674624589336795e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 41/71 | LOSS: 4.660005910885784e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 42/71 | LOSS: 4.647092679121226e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 43/71 | LOSS: 4.615576063894299e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 44/71 | LOSS: 4.602566832545563e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 45/71 | LOSS: 4.592040523754764e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 46/71 | LOSS: 4.552860286337896e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 47/71 | LOSS: 4.551304418972298e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 48/71 | LOSS: 4.544334253333378e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 49/71 | LOSS: 4.548770716610306e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 50/71 | LOSS: 4.537136954454849e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 51/71 | LOSS: 4.537416014745059e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 52/71 | LOSS: 4.519668745727033e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 53/71 | LOSS: 4.5104033385238535e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 54/71 | LOSS: 4.503674096089195e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 55/71 | LOSS: 4.505164627387005e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 56/71 | LOSS: 4.492461668728777e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 57/71 | LOSS: 4.49596752238496e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 58/71 | LOSS: 4.485141983161033e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 59/71 | LOSS: 4.501518280145925e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 60/71 | LOSS: 4.51192470075616e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 61/71 | LOSS: 4.51089895690412e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 62/71 | LOSS: 4.518182336935766e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 63/71 | LOSS: 4.521958054226616e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 64/71 | LOSS: 4.527789965672687e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 65/71 | LOSS: 4.518259002548858e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 66/71 | LOSS: 4.5191199911750485e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 67/71 | LOSS: 4.502809213804807e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 68/71 | LOSS: 4.534142722084231e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 69/71 | LOSS: 4.524125837893475e-06\n",
      "TRAIN: EPOCH 544/1000 | BATCH 70/71 | LOSS: 4.531269003333799e-06\n",
      "VAL: EPOCH 544/1000 | BATCH 0/8 | LOSS: 4.3061595533799846e-06\n",
      "VAL: EPOCH 544/1000 | BATCH 1/8 | LOSS: 4.943045041727601e-06\n",
      "VAL: EPOCH 544/1000 | BATCH 2/8 | LOSS: 5.054867566893033e-06\n",
      "VAL: EPOCH 544/1000 | BATCH 3/8 | LOSS: 5.16842840170284e-06\n",
      "VAL: EPOCH 544/1000 | BATCH 4/8 | LOSS: 5.1251280638098254e-06\n",
      "VAL: EPOCH 544/1000 | BATCH 5/8 | LOSS: 5.0757654056117945e-06\n",
      "VAL: EPOCH 544/1000 | BATCH 6/8 | LOSS: 4.9579958353466025e-06\n",
      "VAL: EPOCH 544/1000 | BATCH 7/8 | LOSS: 4.745606673850489e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 0/71 | LOSS: 7.277671102201566e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 1/71 | LOSS: 6.384538210113533e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 2/71 | LOSS: 5.797080423993369e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 3/71 | LOSS: 5.42320617569203e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 4/71 | LOSS: 5.174404668650822e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 5/71 | LOSS: 4.946152709332334e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 6/71 | LOSS: 4.866967434021977e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 7/71 | LOSS: 4.756293606078543e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 8/71 | LOSS: 4.820334248102477e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 9/71 | LOSS: 4.753065513796173e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 10/71 | LOSS: 4.79613601617695e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 11/71 | LOSS: 4.656576019594165e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 12/71 | LOSS: 4.692353373824377e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 13/71 | LOSS: 4.5850886668371715e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 14/71 | LOSS: 4.569684142552432e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 15/71 | LOSS: 4.566685205986687e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 16/71 | LOSS: 4.62541135996563e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 17/71 | LOSS: 4.585777219290321e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 18/71 | LOSS: 4.533468080281684e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 19/71 | LOSS: 4.550474068309996e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 20/71 | LOSS: 4.55856486171667e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 21/71 | LOSS: 4.536630533700438e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 22/71 | LOSS: 4.684008731799069e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 23/71 | LOSS: 4.6714052928109595e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 24/71 | LOSS: 4.745624955830863e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 25/71 | LOSS: 4.772964792629444e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 26/71 | LOSS: 4.911558368837211e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 27/71 | LOSS: 4.949198942735425e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 28/71 | LOSS: 5.02439719451198e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 29/71 | LOSS: 5.032470759639788e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 30/71 | LOSS: 5.006690424770275e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 31/71 | LOSS: 5.0497235122293205e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 32/71 | LOSS: 5.0295042263204586e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 33/71 | LOSS: 5.05324847461666e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 34/71 | LOSS: 5.033229834745206e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 35/71 | LOSS: 5.042088370525259e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 36/71 | LOSS: 5.050713408874455e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 37/71 | LOSS: 5.019184479686756e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 38/71 | LOSS: 5.0391454701289495e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 39/71 | LOSS: 4.999973117492118e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 40/71 | LOSS: 5.008920524920629e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 41/71 | LOSS: 4.9857466895749285e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 42/71 | LOSS: 5.005314500691426e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 43/71 | LOSS: 5.0336385119077356e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 44/71 | LOSS: 5.071596160632908e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 45/71 | LOSS: 5.10564099999022e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 46/71 | LOSS: 5.111135186915759e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 47/71 | LOSS: 5.098363331512701e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 48/71 | LOSS: 5.102074649640803e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 49/71 | LOSS: 5.115584312989085e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 50/71 | LOSS: 5.110339160120359e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 51/71 | LOSS: 5.099651139936703e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 52/71 | LOSS: 5.0881046117522274e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 53/71 | LOSS: 5.077269217205564e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 54/71 | LOSS: 5.05102951716584e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 55/71 | LOSS: 5.035724529177189e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 56/71 | LOSS: 5.020854662992254e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 57/71 | LOSS: 5.0016433894190165e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 58/71 | LOSS: 4.994207483061966e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 59/71 | LOSS: 4.969588345223504e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 60/71 | LOSS: 4.973356908521964e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 61/71 | LOSS: 4.960239776195404e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 62/71 | LOSS: 4.959832635688058e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 63/71 | LOSS: 4.9417831640141685e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 64/71 | LOSS: 4.921354149281307e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 65/71 | LOSS: 4.928177431931104e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 66/71 | LOSS: 4.905631463608093e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 67/71 | LOSS: 4.9304288120979705e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 68/71 | LOSS: 4.921204666628172e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 69/71 | LOSS: 4.975315810692596e-06\n",
      "TRAIN: EPOCH 545/1000 | BATCH 70/71 | LOSS: 4.951477293757243e-06\n",
      "VAL: EPOCH 545/1000 | BATCH 0/8 | LOSS: 5.218410024099285e-06\n",
      "VAL: EPOCH 545/1000 | BATCH 1/8 | LOSS: 5.500637826116872e-06\n",
      "VAL: EPOCH 545/1000 | BATCH 2/8 | LOSS: 5.686237121456846e-06\n",
      "VAL: EPOCH 545/1000 | BATCH 3/8 | LOSS: 5.557240683629061e-06\n",
      "VAL: EPOCH 545/1000 | BATCH 4/8 | LOSS: 5.660309216182213e-06\n",
      "VAL: EPOCH 545/1000 | BATCH 5/8 | LOSS: 5.642064782781138e-06\n",
      "VAL: EPOCH 545/1000 | BATCH 6/8 | LOSS: 5.526311075040472e-06\n",
      "VAL: EPOCH 545/1000 | BATCH 7/8 | LOSS: 5.51424261630018e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 0/71 | LOSS: 5.414999577624258e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 1/71 | LOSS: 5.0095300139219034e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 2/71 | LOSS: 4.674586913703631e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 3/71 | LOSS: 4.952522431267425e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 4/71 | LOSS: 4.5569245230581146e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 5/71 | LOSS: 4.623802169589908e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 6/71 | LOSS: 4.7872682833778005e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 7/71 | LOSS: 4.638279392565892e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 8/71 | LOSS: 4.701704508786659e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 9/71 | LOSS: 4.732995057565858e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 10/71 | LOSS: 4.660399843652902e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 11/71 | LOSS: 4.639158381299542e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 12/71 | LOSS: 4.509048512073395e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 13/71 | LOSS: 4.5011086058366345e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 14/71 | LOSS: 4.473676896547355e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 15/71 | LOSS: 4.461544477862844e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 16/71 | LOSS: 4.411972400596235e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 17/71 | LOSS: 4.425961416624584e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 18/71 | LOSS: 4.4869069418047345e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 19/71 | LOSS: 4.515535636073764e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 20/71 | LOSS: 4.555456414545049e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 21/71 | LOSS: 4.56642015416781e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 22/71 | LOSS: 4.550750111814958e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 23/71 | LOSS: 4.529330160115326e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 24/71 | LOSS: 4.538639232123387e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 25/71 | LOSS: 4.533891823344815e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 26/71 | LOSS: 4.5171298097557155e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 27/71 | LOSS: 4.511350466275742e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 28/71 | LOSS: 4.505765250071598e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 29/71 | LOSS: 4.500372968626228e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 30/71 | LOSS: 4.474023626244161e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 31/71 | LOSS: 4.442840356944089e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 32/71 | LOSS: 4.4502929182327176e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 33/71 | LOSS: 4.470118115558066e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 34/71 | LOSS: 4.465489558112624e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 35/71 | LOSS: 4.468804446686489e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 36/71 | LOSS: 4.488895960143104e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 37/71 | LOSS: 4.483117574014422e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 38/71 | LOSS: 4.4839500787929865e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 39/71 | LOSS: 4.4471849548699535e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 40/71 | LOSS: 4.4480055504852606e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 41/71 | LOSS: 4.426800966852863e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 42/71 | LOSS: 4.396829554871358e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 43/71 | LOSS: 4.402065738676439e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 44/71 | LOSS: 4.39006201607602e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 45/71 | LOSS: 4.37124051290005e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 46/71 | LOSS: 4.3706493457609616e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 47/71 | LOSS: 4.389816609773334e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 48/71 | LOSS: 4.383604493564142e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 49/71 | LOSS: 4.390683034216636e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 50/71 | LOSS: 4.3973367628139245e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 51/71 | LOSS: 4.38499813089248e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 52/71 | LOSS: 4.383887673337188e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 53/71 | LOSS: 4.391784951049313e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 54/71 | LOSS: 4.38550893126045e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 55/71 | LOSS: 4.390707943002781e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 56/71 | LOSS: 4.374122957080162e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 57/71 | LOSS: 4.390477176363267e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 58/71 | LOSS: 4.374365952402113e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 59/71 | LOSS: 4.373321288160999e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 60/71 | LOSS: 4.37199630931766e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 61/71 | LOSS: 4.349781996156267e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 62/71 | LOSS: 4.363821484990435e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 63/71 | LOSS: 4.356544263117712e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 64/71 | LOSS: 4.3536815925248306e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 65/71 | LOSS: 4.338943178729758e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 66/71 | LOSS: 4.319916580419913e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 67/71 | LOSS: 4.342645662050285e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 68/71 | LOSS: 4.3417915753377825e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 69/71 | LOSS: 4.344789343641813e-06\n",
      "TRAIN: EPOCH 546/1000 | BATCH 70/71 | LOSS: 4.3682445215074015e-06\n",
      "VAL: EPOCH 546/1000 | BATCH 0/8 | LOSS: 4.152867404627614e-06\n",
      "VAL: EPOCH 546/1000 | BATCH 1/8 | LOSS: 4.420513960212702e-06\n",
      "VAL: EPOCH 546/1000 | BATCH 2/8 | LOSS: 4.702244344419644e-06\n",
      "VAL: EPOCH 546/1000 | BATCH 3/8 | LOSS: 4.636675953406666e-06\n",
      "VAL: EPOCH 546/1000 | BATCH 4/8 | LOSS: 4.745639580505668e-06\n",
      "VAL: EPOCH 546/1000 | BATCH 5/8 | LOSS: 4.672098460408354e-06\n",
      "VAL: EPOCH 546/1000 | BATCH 6/8 | LOSS: 4.586895946496432e-06\n",
      "VAL: EPOCH 546/1000 | BATCH 7/8 | LOSS: 4.521280402514094e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 0/71 | LOSS: 5.218011210672557e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 1/71 | LOSS: 5.045079433330102e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 2/71 | LOSS: 4.536635515250964e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 3/71 | LOSS: 4.37151868482033e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 4/71 | LOSS: 3.99285640924063e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 5/71 | LOSS: 4.005852777784942e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 6/71 | LOSS: 4.1266335821481855e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 7/71 | LOSS: 4.301788777638649e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 8/71 | LOSS: 4.38075688988546e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 9/71 | LOSS: 4.427356611813593e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 10/71 | LOSS: 4.440336996677533e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 11/71 | LOSS: 4.457501916022011e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 12/71 | LOSS: 4.458307155521023e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 13/71 | LOSS: 4.424086374196382e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 14/71 | LOSS: 4.359776266937843e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 15/71 | LOSS: 4.336858097531149e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 16/71 | LOSS: 4.29931892272982e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 17/71 | LOSS: 4.287643630757682e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 18/71 | LOSS: 4.258603559336686e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 19/71 | LOSS: 4.185799264178059e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 20/71 | LOSS: 4.18048175344579e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 21/71 | LOSS: 4.1721425428807955e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 22/71 | LOSS: 4.21766537289406e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 23/71 | LOSS: 4.181574903820244e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 24/71 | LOSS: 4.17923658460495e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 25/71 | LOSS: 4.122728761593373e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 26/71 | LOSS: 4.099150323276666e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 27/71 | LOSS: 4.1573901268878084e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 28/71 | LOSS: 4.145246372178823e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 29/71 | LOSS: 4.137907823557422e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 30/71 | LOSS: 4.147678399960803e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 31/71 | LOSS: 4.22496499652425e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 32/71 | LOSS: 4.236614668942903e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 33/71 | LOSS: 4.237404749908498e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 34/71 | LOSS: 4.279029170512721e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 35/71 | LOSS: 4.265208569146732e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 36/71 | LOSS: 4.313949117400822e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 37/71 | LOSS: 4.34663881701693e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 38/71 | LOSS: 4.371828498943404e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 39/71 | LOSS: 4.386186890315002e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 40/71 | LOSS: 4.39022605638545e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 41/71 | LOSS: 4.423869226922675e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 42/71 | LOSS: 4.408042778502635e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 43/71 | LOSS: 4.415454885598923e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 44/71 | LOSS: 4.406727516147334e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 45/71 | LOSS: 4.4003735065021115e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 46/71 | LOSS: 4.405909990925872e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 47/71 | LOSS: 4.406719568805784e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 48/71 | LOSS: 4.397660752741159e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 49/71 | LOSS: 4.409139764902648e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 50/71 | LOSS: 4.418842710198506e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 51/71 | LOSS: 4.401588507964614e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 52/71 | LOSS: 4.39375697358216e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 53/71 | LOSS: 4.384001533287153e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 54/71 | LOSS: 4.395285523886824e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 55/71 | LOSS: 4.405802522049791e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 56/71 | LOSS: 4.395932141402195e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 57/71 | LOSS: 4.397362619689877e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 58/71 | LOSS: 4.412919012747036e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 59/71 | LOSS: 4.42741211751733e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 60/71 | LOSS: 4.427563405189459e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 61/71 | LOSS: 4.4239601713284615e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 62/71 | LOSS: 4.431623318894362e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 63/71 | LOSS: 4.439143847179139e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 64/71 | LOSS: 4.461747906903978e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 65/71 | LOSS: 4.475142383963623e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 66/71 | LOSS: 4.467382139599936e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 67/71 | LOSS: 4.471939649540539e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 68/71 | LOSS: 4.472294983276262e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 69/71 | LOSS: 4.453570269496205e-06\n",
      "TRAIN: EPOCH 547/1000 | BATCH 70/71 | LOSS: 4.462515556004061e-06\n",
      "VAL: EPOCH 547/1000 | BATCH 0/8 | LOSS: 4.3158124753972515e-06\n",
      "VAL: EPOCH 547/1000 | BATCH 1/8 | LOSS: 4.4269779664318776e-06\n",
      "VAL: EPOCH 547/1000 | BATCH 2/8 | LOSS: 4.3965421051931726e-06\n",
      "VAL: EPOCH 547/1000 | BATCH 3/8 | LOSS: 4.304255980969174e-06\n",
      "VAL: EPOCH 547/1000 | BATCH 4/8 | LOSS: 4.333861670602346e-06\n",
      "VAL: EPOCH 547/1000 | BATCH 5/8 | LOSS: 4.2754052174132084e-06\n",
      "VAL: EPOCH 547/1000 | BATCH 6/8 | LOSS: 4.103332880081975e-06\n",
      "VAL: EPOCH 547/1000 | BATCH 7/8 | LOSS: 3.987471302480117e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 0/71 | LOSS: 2.6292743768863147e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 1/71 | LOSS: 4.116611648896651e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 2/71 | LOSS: 4.048175848462658e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 3/71 | LOSS: 4.420177845076978e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 4/71 | LOSS: 4.635149662135518e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 5/71 | LOSS: 4.307042559048568e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 6/71 | LOSS: 4.243506119954483e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 7/71 | LOSS: 4.676659983715581e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 8/71 | LOSS: 5.090425904402056e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 9/71 | LOSS: 5.058148576608801e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 10/71 | LOSS: 5.0301369107976575e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 11/71 | LOSS: 5.120969168122731e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 12/71 | LOSS: 5.083644090028918e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 13/71 | LOSS: 5.177544526954339e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 14/71 | LOSS: 5.238310753460003e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 15/71 | LOSS: 5.1630406545655205e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 16/71 | LOSS: 5.0815748476024425e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 17/71 | LOSS: 5.048760448921206e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 18/71 | LOSS: 4.982072093633561e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 19/71 | LOSS: 5.010197844512731e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 20/71 | LOSS: 5.000641619647338e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 21/71 | LOSS: 5.103396223272615e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 22/71 | LOSS: 5.102683318277148e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 23/71 | LOSS: 5.113660269747318e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 24/71 | LOSS: 5.070178094683797e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 25/71 | LOSS: 5.039067437232566e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 26/71 | LOSS: 5.045419778037251e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 27/71 | LOSS: 5.001244736441939e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 28/71 | LOSS: 4.978438067249452e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 29/71 | LOSS: 4.9685081118392795e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 30/71 | LOSS: 4.971930366082233e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 31/71 | LOSS: 4.948615149658053e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 32/71 | LOSS: 4.9090636669764836e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 33/71 | LOSS: 4.901566231798343e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 34/71 | LOSS: 4.891434679978245e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 35/71 | LOSS: 4.878402674219413e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 36/71 | LOSS: 4.854689825299067e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 37/71 | LOSS: 4.834172249215235e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 38/71 | LOSS: 4.839344461661545e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 39/71 | LOSS: 4.806736831142188e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 40/71 | LOSS: 4.787661310263324e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 41/71 | LOSS: 4.781728930105428e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 42/71 | LOSS: 4.769397037307499e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 43/71 | LOSS: 4.767739080142002e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 44/71 | LOSS: 4.722687263994077e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 45/71 | LOSS: 4.723816430115498e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 46/71 | LOSS: 4.69005015068607e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 47/71 | LOSS: 4.664441362933758e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 48/71 | LOSS: 4.649339435134755e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 49/71 | LOSS: 4.668594060603937e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 50/71 | LOSS: 4.655379023140139e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 51/71 | LOSS: 4.65813786394602e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 52/71 | LOSS: 4.65270907728343e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 53/71 | LOSS: 4.659683626144855e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 54/71 | LOSS: 4.639872370469014e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 55/71 | LOSS: 4.639732907207872e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 56/71 | LOSS: 4.629827915044173e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 57/71 | LOSS: 4.643548181618449e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 58/71 | LOSS: 4.6495582502479844e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 59/71 | LOSS: 4.662485203728769e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 60/71 | LOSS: 4.650418077289942e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 61/71 | LOSS: 4.67237061325103e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 62/71 | LOSS: 4.670719543730864e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 63/71 | LOSS: 4.671008770884555e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 64/71 | LOSS: 4.674323361961377e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 65/71 | LOSS: 4.661128703749758e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 66/71 | LOSS: 4.679721222831842e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 67/71 | LOSS: 4.668903252823999e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 68/71 | LOSS: 4.649495098340634e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 69/71 | LOSS: 4.6528077843634364e-06\n",
      "TRAIN: EPOCH 548/1000 | BATCH 70/71 | LOSS: 4.6380128481705724e-06\n",
      "VAL: EPOCH 548/1000 | BATCH 0/8 | LOSS: 4.36961408922798e-06\n",
      "VAL: EPOCH 548/1000 | BATCH 1/8 | LOSS: 4.575538923745626e-06\n",
      "VAL: EPOCH 548/1000 | BATCH 2/8 | LOSS: 4.714167062047636e-06\n",
      "VAL: EPOCH 548/1000 | BATCH 3/8 | LOSS: 4.704662956100947e-06\n",
      "VAL: EPOCH 548/1000 | BATCH 4/8 | LOSS: 4.6944372115831355e-06\n",
      "VAL: EPOCH 548/1000 | BATCH 5/8 | LOSS: 4.627886482921895e-06\n",
      "VAL: EPOCH 548/1000 | BATCH 6/8 | LOSS: 4.494441230105752e-06\n",
      "VAL: EPOCH 548/1000 | BATCH 7/8 | LOSS: 4.341359584714155e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 0/71 | LOSS: 4.074132903042482e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 1/71 | LOSS: 4.1255541418649955e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 2/71 | LOSS: 4.712184969927573e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 3/71 | LOSS: 4.561682317216764e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 4/71 | LOSS: 4.473926492210012e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 5/71 | LOSS: 4.400459223082483e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 6/71 | LOSS: 4.365009837264162e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 7/71 | LOSS: 4.4564276322489604e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 8/71 | LOSS: 4.367132583461676e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 9/71 | LOSS: 4.296193083064281e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 10/71 | LOSS: 4.270728120239007e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 11/71 | LOSS: 4.225929956191976e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 12/71 | LOSS: 4.162462171454816e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 13/71 | LOSS: 4.137073363251277e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 14/71 | LOSS: 4.160154139754013e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 15/71 | LOSS: 4.151977194055689e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 16/71 | LOSS: 4.086318567180356e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 17/71 | LOSS: 4.08664670380353e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 18/71 | LOSS: 4.15951170620438e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 19/71 | LOSS: 4.170425756910845e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 20/71 | LOSS: 4.211558711003385e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 21/71 | LOSS: 4.273389477434235e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 22/71 | LOSS: 4.345244761956881e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 23/71 | LOSS: 4.439214203936596e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 24/71 | LOSS: 4.551648862616275e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 25/71 | LOSS: 4.535257359678396e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 26/71 | LOSS: 4.555555556685755e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 27/71 | LOSS: 4.5859664185497555e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 28/71 | LOSS: 4.616872290390692e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 29/71 | LOSS: 4.596411652831497e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 30/71 | LOSS: 4.608214105187912e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 31/71 | LOSS: 4.605529703383127e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 32/71 | LOSS: 4.616392778078796e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 33/71 | LOSS: 4.621528921020131e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 34/71 | LOSS: 4.658554897624916e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 35/71 | LOSS: 4.68638570636257e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 36/71 | LOSS: 4.675899685320769e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 37/71 | LOSS: 4.682500245603717e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 38/71 | LOSS: 4.692280124520319e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 39/71 | LOSS: 4.72847593186998e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 40/71 | LOSS: 4.717948103262177e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 41/71 | LOSS: 4.732260630849591e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 42/71 | LOSS: 4.706283609562507e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 43/71 | LOSS: 4.698644070024536e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 44/71 | LOSS: 4.70248058566843e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 45/71 | LOSS: 4.681727580685655e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 46/71 | LOSS: 4.687320974784249e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 47/71 | LOSS: 4.681157406594139e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 48/71 | LOSS: 4.70739728463778e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 49/71 | LOSS: 4.721009290733491e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 50/71 | LOSS: 4.781807859342057e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 51/71 | LOSS: 4.7864474569136155e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 52/71 | LOSS: 4.791209860586777e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 53/71 | LOSS: 4.8010900223339886e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 54/71 | LOSS: 4.80291879615503e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 55/71 | LOSS: 4.813148125647006e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 56/71 | LOSS: 4.831051507624165e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 57/71 | LOSS: 4.85656732962525e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 58/71 | LOSS: 4.85304859867645e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 59/71 | LOSS: 4.875502046767603e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 60/71 | LOSS: 4.862967641395803e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 61/71 | LOSS: 4.829406187186138e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 62/71 | LOSS: 4.8548858724224995e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 63/71 | LOSS: 4.881880776963499e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 64/71 | LOSS: 4.913900759744977e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 65/71 | LOSS: 4.919792590127062e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 66/71 | LOSS: 4.973107913985532e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 67/71 | LOSS: 4.973233050055997e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 68/71 | LOSS: 4.981551531576163e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 69/71 | LOSS: 4.994934592364839e-06\n",
      "TRAIN: EPOCH 549/1000 | BATCH 70/71 | LOSS: 4.9638892448110455e-06\n",
      "VAL: EPOCH 549/1000 | BATCH 0/8 | LOSS: 4.46344529336784e-06\n",
      "VAL: EPOCH 549/1000 | BATCH 1/8 | LOSS: 4.829547606277629e-06\n",
      "VAL: EPOCH 549/1000 | BATCH 2/8 | LOSS: 4.95103192103367e-06\n",
      "VAL: EPOCH 549/1000 | BATCH 3/8 | LOSS: 5.054955977357167e-06\n",
      "VAL: EPOCH 549/1000 | BATCH 4/8 | LOSS: 5.042183602199657e-06\n",
      "VAL: EPOCH 549/1000 | BATCH 5/8 | LOSS: 5.057032770613053e-06\n",
      "VAL: EPOCH 549/1000 | BATCH 6/8 | LOSS: 4.952025977088072e-06\n",
      "VAL: EPOCH 549/1000 | BATCH 7/8 | LOSS: 4.749000055426222e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 0/71 | LOSS: 5.469471034302842e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 1/71 | LOSS: 5.579869139182847e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 2/71 | LOSS: 5.087520852005885e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 3/71 | LOSS: 5.611979304376291e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 4/71 | LOSS: 5.65466452826513e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 5/71 | LOSS: 5.513861954871875e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 6/71 | LOSS: 5.56356066486582e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 7/71 | LOSS: 5.669209599545866e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 8/71 | LOSS: 5.912664164497983e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 9/71 | LOSS: 5.7151685723511035e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 10/71 | LOSS: 5.631585323003079e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 11/71 | LOSS: 5.583972703486022e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 12/71 | LOSS: 5.666194563393499e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 13/71 | LOSS: 5.566012045424681e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 14/71 | LOSS: 5.489234081323957e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 15/71 | LOSS: 5.47364609815304e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 16/71 | LOSS: 5.415695287571425e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 17/71 | LOSS: 5.319187746661353e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 18/71 | LOSS: 5.286712047037374e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 19/71 | LOSS: 5.266627545097435e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 20/71 | LOSS: 5.21318242179058e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 21/71 | LOSS: 5.199275681827038e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 22/71 | LOSS: 5.207869731827446e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 23/71 | LOSS: 5.157941283565985e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 24/71 | LOSS: 5.112808557896642e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 25/71 | LOSS: 5.072586803250418e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 26/71 | LOSS: 5.045788973937441e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 27/71 | LOSS: 5.019176325861606e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 28/71 | LOSS: 4.999215026804575e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 29/71 | LOSS: 4.985210474236131e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 30/71 | LOSS: 4.980139824657536e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 31/71 | LOSS: 4.925525985299828e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 32/71 | LOSS: 4.900755260623003e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 33/71 | LOSS: 4.859144717235776e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 34/71 | LOSS: 4.825023266416143e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 35/71 | LOSS: 4.814356581543204e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 36/71 | LOSS: 4.823449610223572e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 37/71 | LOSS: 4.800688956445236e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 38/71 | LOSS: 4.772187926741585e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 39/71 | LOSS: 4.781959376032319e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 40/71 | LOSS: 4.751524939576218e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 41/71 | LOSS: 4.74913281864463e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 42/71 | LOSS: 4.730701437776673e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 43/71 | LOSS: 4.748911418449676e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 44/71 | LOSS: 4.724852588980058e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 45/71 | LOSS: 4.710243012556495e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 46/71 | LOSS: 4.69049230056779e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 47/71 | LOSS: 4.679676280261447e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 48/71 | LOSS: 4.670290749976518e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 49/71 | LOSS: 4.6319378316184156e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 50/71 | LOSS: 4.607162683147608e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 51/71 | LOSS: 4.612898202629787e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 52/71 | LOSS: 4.602278938861828e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 53/71 | LOSS: 4.588617575454582e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 54/71 | LOSS: 4.58030650737452e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 55/71 | LOSS: 4.573204218364221e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 56/71 | LOSS: 4.549755231620797e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 57/71 | LOSS: 4.542899369911977e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 58/71 | LOSS: 4.525633280767795e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 59/71 | LOSS: 4.514498330839463e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 60/71 | LOSS: 4.498646804960549e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 61/71 | LOSS: 4.487337451469249e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 62/71 | LOSS: 4.4913244206356234e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 63/71 | LOSS: 4.486753827137591e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 64/71 | LOSS: 4.4830341756516225e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 65/71 | LOSS: 4.477671447575355e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 66/71 | LOSS: 4.4715327136933116e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 67/71 | LOSS: 4.4612131693397445e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 68/71 | LOSS: 4.4587772096576e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 69/71 | LOSS: 4.445617920672313e-06\n",
      "TRAIN: EPOCH 550/1000 | BATCH 70/71 | LOSS: 4.44700086377516e-06\n",
      "VAL: EPOCH 550/1000 | BATCH 0/8 | LOSS: 5.022207005822565e-06\n",
      "VAL: EPOCH 550/1000 | BATCH 1/8 | LOSS: 5.022657660447294e-06\n",
      "VAL: EPOCH 550/1000 | BATCH 2/8 | LOSS: 4.972012296396618e-06\n",
      "VAL: EPOCH 550/1000 | BATCH 3/8 | LOSS: 4.8507549763598945e-06\n",
      "VAL: EPOCH 550/1000 | BATCH 4/8 | LOSS: 4.96717084388365e-06\n",
      "VAL: EPOCH 550/1000 | BATCH 5/8 | LOSS: 4.864842367169331e-06\n",
      "VAL: EPOCH 550/1000 | BATCH 6/8 | LOSS: 4.71721614303533e-06\n",
      "VAL: EPOCH 550/1000 | BATCH 7/8 | LOSS: 4.605589367656648e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 0/71 | LOSS: 4.5076512833475135e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 1/71 | LOSS: 4.771648491441738e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 2/71 | LOSS: 5.0067766702947365e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 3/71 | LOSS: 5.106622779749159e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 4/71 | LOSS: 4.890276522928616e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 5/71 | LOSS: 4.681531777350756e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 6/71 | LOSS: 4.4922473015114e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 7/71 | LOSS: 4.507947949150548e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 8/71 | LOSS: 4.516459335314317e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 9/71 | LOSS: 4.5024567043583374e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 10/71 | LOSS: 4.416650550741576e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 11/71 | LOSS: 4.488187641982222e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 12/71 | LOSS: 4.471506599657005e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 13/71 | LOSS: 4.516679025203173e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 14/71 | LOSS: 4.462237393454416e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 15/71 | LOSS: 4.389204434573912e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 16/71 | LOSS: 4.31463534850456e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 17/71 | LOSS: 4.356384503504766e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 18/71 | LOSS: 4.348242447648724e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 19/71 | LOSS: 4.300899672671221e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 20/71 | LOSS: 4.24972185577471e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 21/71 | LOSS: 4.190215783885585e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 22/71 | LOSS: 4.186229271790185e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 23/71 | LOSS: 4.23085500263672e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 24/71 | LOSS: 4.228989982948406e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 25/71 | LOSS: 4.178466051248856e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 26/71 | LOSS: 4.131222634963773e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 27/71 | LOSS: 4.155877239294828e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 28/71 | LOSS: 4.162314945492183e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 29/71 | LOSS: 4.154298411170506e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 30/71 | LOSS: 4.138416450645778e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 31/71 | LOSS: 4.148834030104354e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 32/71 | LOSS: 4.156120274439507e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 33/71 | LOSS: 4.167819785085585e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 34/71 | LOSS: 4.156698618576879e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 35/71 | LOSS: 4.157796922754642e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 36/71 | LOSS: 4.152085835271832e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 37/71 | LOSS: 4.176279958301166e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 38/71 | LOSS: 4.18120874620889e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 39/71 | LOSS: 4.182491585424941e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 40/71 | LOSS: 4.194991289495658e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 41/71 | LOSS: 4.188277297803974e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 42/71 | LOSS: 4.2047434754051714e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 43/71 | LOSS: 4.213250396002298e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 44/71 | LOSS: 4.232956320669877e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 45/71 | LOSS: 4.256542687788011e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 46/71 | LOSS: 4.239024253772012e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 47/71 | LOSS: 4.257128599268374e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 48/71 | LOSS: 4.286351194961605e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 49/71 | LOSS: 4.2749118620122316e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 50/71 | LOSS: 4.368377970012279e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 51/71 | LOSS: 4.403978305862648e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 52/71 | LOSS: 4.4385779750333515e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 53/71 | LOSS: 4.470257198152939e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 54/71 | LOSS: 4.49532088334143e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 55/71 | LOSS: 4.511789654770837e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 56/71 | LOSS: 4.581695185678607e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 57/71 | LOSS: 4.56304831945529e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 58/71 | LOSS: 4.5972641855461025e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 59/71 | LOSS: 4.602222819964178e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 60/71 | LOSS: 4.597993207336585e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 61/71 | LOSS: 4.606559473945855e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 62/71 | LOSS: 4.610997429774707e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 63/71 | LOSS: 4.607489383801067e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 64/71 | LOSS: 4.631292425144168e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 65/71 | LOSS: 4.681744275275736e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 66/71 | LOSS: 4.674993635435656e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 67/71 | LOSS: 4.6726701658799896e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 68/71 | LOSS: 4.7157326418422656e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 69/71 | LOSS: 4.7226835606904515e-06\n",
      "TRAIN: EPOCH 551/1000 | BATCH 70/71 | LOSS: 4.731041003715675e-06\n",
      "VAL: EPOCH 551/1000 | BATCH 0/8 | LOSS: 6.051544914953411e-06\n",
      "VAL: EPOCH 551/1000 | BATCH 1/8 | LOSS: 6.065305115043884e-06\n",
      "VAL: EPOCH 551/1000 | BATCH 2/8 | LOSS: 6.207869925371294e-06\n",
      "VAL: EPOCH 551/1000 | BATCH 3/8 | LOSS: 6.171405857458012e-06\n",
      "VAL: EPOCH 551/1000 | BATCH 4/8 | LOSS: 6.139164543128572e-06\n",
      "VAL: EPOCH 551/1000 | BATCH 5/8 | LOSS: 6.054091651700825e-06\n",
      "VAL: EPOCH 551/1000 | BATCH 6/8 | LOSS: 5.861014480095557e-06\n",
      "VAL: EPOCH 551/1000 | BATCH 7/8 | LOSS: 5.745718851812853e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 0/71 | LOSS: 5.807019988424145e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 1/71 | LOSS: 6.184460517033585e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 2/71 | LOSS: 5.347057064379139e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 3/71 | LOSS: 5.4126726922731905e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 4/71 | LOSS: 5.440621453089989e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 5/71 | LOSS: 5.330922363100399e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 6/71 | LOSS: 5.29694875694986e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 7/71 | LOSS: 5.4085053591279575e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 8/71 | LOSS: 5.3608184771292144e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 9/71 | LOSS: 5.340911980056262e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 10/71 | LOSS: 5.191168322033429e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 11/71 | LOSS: 5.146426948006895e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 12/71 | LOSS: 5.1890940540033625e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 13/71 | LOSS: 5.1380352650604825e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 14/71 | LOSS: 5.119121897223522e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 15/71 | LOSS: 5.019847080234285e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 16/71 | LOSS: 4.9944323617477064e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 17/71 | LOSS: 4.962772171287118e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 18/71 | LOSS: 5.019688698762605e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 19/71 | LOSS: 4.981838185358356e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 20/71 | LOSS: 4.954439960090981e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 21/71 | LOSS: 5.025047988221393e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 22/71 | LOSS: 5.057049639103554e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 23/71 | LOSS: 5.018770584304851e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 24/71 | LOSS: 5.00870271025633e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 25/71 | LOSS: 4.9640726029582975e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 26/71 | LOSS: 4.940377620086009e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 27/71 | LOSS: 4.891410989265361e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 28/71 | LOSS: 4.857666736229319e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 29/71 | LOSS: 4.881581139670743e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 30/71 | LOSS: 4.856837302858983e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 31/71 | LOSS: 4.8059826838198205e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 32/71 | LOSS: 4.866287285536104e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 33/71 | LOSS: 4.833938680135093e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 34/71 | LOSS: 4.853118161918246e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 35/71 | LOSS: 4.820653430215316e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 36/71 | LOSS: 4.824911708989954e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 37/71 | LOSS: 4.8253833052800286e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 38/71 | LOSS: 4.79907188468459e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 39/71 | LOSS: 4.794300485855274e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 40/71 | LOSS: 4.787846553260418e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 41/71 | LOSS: 4.801865895301273e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 42/71 | LOSS: 4.780962314704761e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 43/71 | LOSS: 4.754176028530889e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 44/71 | LOSS: 4.784261389229667e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 45/71 | LOSS: 4.769992276875279e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 46/71 | LOSS: 4.769839192026956e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 47/71 | LOSS: 4.766550825744768e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 48/71 | LOSS: 4.778572841188383e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 49/71 | LOSS: 4.770333730448329e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 50/71 | LOSS: 4.777124082002225e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 51/71 | LOSS: 4.766852607819881e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 52/71 | LOSS: 4.78064544898549e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 53/71 | LOSS: 4.754399204405975e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 54/71 | LOSS: 4.781629001048383e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 55/71 | LOSS: 4.773338612754482e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 56/71 | LOSS: 4.760078047439221e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 57/71 | LOSS: 4.756046435212393e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 58/71 | LOSS: 4.77180936024401e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 59/71 | LOSS: 4.7617943550903876e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 60/71 | LOSS: 4.7446599816540865e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 61/71 | LOSS: 4.74927454037712e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 62/71 | LOSS: 4.7840906243811965e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 63/71 | LOSS: 4.82827653769391e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 64/71 | LOSS: 4.837838262644184e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 65/71 | LOSS: 4.8539477161698e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 66/71 | LOSS: 4.860420624515712e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 67/71 | LOSS: 4.866051841311085e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 68/71 | LOSS: 4.880375557980227e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 69/71 | LOSS: 4.885308167753725e-06\n",
      "TRAIN: EPOCH 552/1000 | BATCH 70/71 | LOSS: 4.9064869734804596e-06\n",
      "VAL: EPOCH 552/1000 | BATCH 0/8 | LOSS: 3.6893839023832697e-06\n",
      "VAL: EPOCH 552/1000 | BATCH 1/8 | LOSS: 4.295164444556576e-06\n",
      "VAL: EPOCH 552/1000 | BATCH 2/8 | LOSS: 4.5042371918195085e-06\n",
      "VAL: EPOCH 552/1000 | BATCH 3/8 | LOSS: 4.588551178130729e-06\n",
      "VAL: EPOCH 552/1000 | BATCH 4/8 | LOSS: 4.633713706425624e-06\n",
      "VAL: EPOCH 552/1000 | BATCH 5/8 | LOSS: 4.6339146895964705e-06\n",
      "VAL: EPOCH 552/1000 | BATCH 6/8 | LOSS: 4.507441774746569e-06\n",
      "VAL: EPOCH 552/1000 | BATCH 7/8 | LOSS: 4.345667434790812e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 0/71 | LOSS: 4.194936536805471e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 1/71 | LOSS: 4.9591430979489814e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 2/71 | LOSS: 5.005992231114457e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 3/71 | LOSS: 5.238226322035189e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 4/71 | LOSS: 4.868138285019086e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 5/71 | LOSS: 4.768674936409904e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 6/71 | LOSS: 4.803001534777909e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 7/71 | LOSS: 4.794875650304675e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 8/71 | LOSS: 4.9357840655122545e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 9/71 | LOSS: 4.948567880092014e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 10/71 | LOSS: 4.904023936350396e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 11/71 | LOSS: 4.817794414672487e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 12/71 | LOSS: 4.783905489183291e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 13/71 | LOSS: 4.774485286621452e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 14/71 | LOSS: 4.768405127227501e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 15/71 | LOSS: 4.739856549917931e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 16/71 | LOSS: 4.848060986178632e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 17/71 | LOSS: 4.812855890021537e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 18/71 | LOSS: 4.815526979559524e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 19/71 | LOSS: 4.791949561422371e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 20/71 | LOSS: 4.82412208059811e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 21/71 | LOSS: 4.803911524091249e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 22/71 | LOSS: 4.746984112226409e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 23/71 | LOSS: 4.7781633630468905e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 24/71 | LOSS: 4.8188341770583065e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 25/71 | LOSS: 4.790780385869416e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 26/71 | LOSS: 4.788795792962817e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 27/71 | LOSS: 4.827033429981904e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 28/71 | LOSS: 4.769574184922084e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 29/71 | LOSS: 4.769976233850078e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 30/71 | LOSS: 4.775315368709515e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 31/71 | LOSS: 4.7744954372319626e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 32/71 | LOSS: 4.724011915007423e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 33/71 | LOSS: 4.666790914598096e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 34/71 | LOSS: 4.640640918296413e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 35/71 | LOSS: 4.607981742942178e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 36/71 | LOSS: 4.561760742699089e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 37/71 | LOSS: 4.552204395143858e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 38/71 | LOSS: 4.528102023799035e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 39/71 | LOSS: 4.537837190810023e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 40/71 | LOSS: 4.559248298658243e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 41/71 | LOSS: 4.554174898614638e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 42/71 | LOSS: 4.544774165604098e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 43/71 | LOSS: 4.5453113040655975e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 44/71 | LOSS: 4.545144712311514e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 45/71 | LOSS: 4.527814825371974e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 46/71 | LOSS: 4.50493953297716e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 47/71 | LOSS: 4.49351926571732e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 48/71 | LOSS: 4.47280135817056e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 49/71 | LOSS: 4.450232563613099e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 50/71 | LOSS: 4.450215114689747e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 51/71 | LOSS: 4.43988834866524e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 52/71 | LOSS: 4.432913075741556e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 53/71 | LOSS: 4.418787444525084e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 54/71 | LOSS: 4.392976180414817e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 55/71 | LOSS: 4.377675582288378e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 56/71 | LOSS: 4.364741622945682e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 57/71 | LOSS: 4.362464872836677e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 58/71 | LOSS: 4.360307646154177e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 59/71 | LOSS: 4.3584793881260945e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 60/71 | LOSS: 4.34913050546621e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 61/71 | LOSS: 4.374507277075722e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 62/71 | LOSS: 4.394067352064118e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 63/71 | LOSS: 4.41055459887707e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 64/71 | LOSS: 4.4426353025091185e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 65/71 | LOSS: 4.4518743331509265e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 66/71 | LOSS: 4.448935367609417e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 67/71 | LOSS: 4.478513310058427e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 68/71 | LOSS: 4.465449691174992e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 69/71 | LOSS: 4.496280095216727e-06\n",
      "TRAIN: EPOCH 553/1000 | BATCH 70/71 | LOSS: 4.493672820540845e-06\n",
      "VAL: EPOCH 553/1000 | BATCH 0/8 | LOSS: 6.0459142332547344e-06\n",
      "VAL: EPOCH 553/1000 | BATCH 1/8 | LOSS: 6.826179287600098e-06\n",
      "VAL: EPOCH 553/1000 | BATCH 2/8 | LOSS: 7.038976946205366e-06\n",
      "VAL: EPOCH 553/1000 | BATCH 3/8 | LOSS: 7.073089591358439e-06\n",
      "VAL: EPOCH 553/1000 | BATCH 4/8 | LOSS: 7.076397014316171e-06\n",
      "VAL: EPOCH 553/1000 | BATCH 5/8 | LOSS: 7.225465348407549e-06\n",
      "VAL: EPOCH 553/1000 | BATCH 6/8 | LOSS: 7.140195780916006e-06\n",
      "VAL: EPOCH 553/1000 | BATCH 7/8 | LOSS: 7.1222895599021285e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 0/71 | LOSS: 7.649018698430154e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 1/71 | LOSS: 5.7937452311307425e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 2/71 | LOSS: 5.809825324831763e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 3/71 | LOSS: 5.5201178383867955e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 4/71 | LOSS: 5.663042611558922e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 5/71 | LOSS: 5.477659139311679e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 6/71 | LOSS: 5.656776595839931e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 7/71 | LOSS: 5.407920582456427e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 8/71 | LOSS: 5.3799184974599885e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 9/71 | LOSS: 5.244967360340524e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 10/71 | LOSS: 5.081338878715707e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 11/71 | LOSS: 5.042019059449861e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 12/71 | LOSS: 4.90369886672572e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 13/71 | LOSS: 4.842330911612537e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 14/71 | LOSS: 4.781033870434233e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 15/71 | LOSS: 4.741373004435445e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 16/71 | LOSS: 4.707793884924339e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 17/71 | LOSS: 4.713090472958154e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 18/71 | LOSS: 4.645341983919743e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 19/71 | LOSS: 4.6510283254974635e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 20/71 | LOSS: 4.59524024639298e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 21/71 | LOSS: 4.5703095255786055e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 22/71 | LOSS: 4.613796956777072e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 23/71 | LOSS: 4.615459857859605e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 24/71 | LOSS: 4.603136576406542e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 25/71 | LOSS: 4.536472575777085e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 26/71 | LOSS: 4.511646457777479e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 27/71 | LOSS: 4.5260413961451766e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 28/71 | LOSS: 4.5155975228759605e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 29/71 | LOSS: 4.512231513823887e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 30/71 | LOSS: 4.5303805974629455e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 31/71 | LOSS: 4.5074832257796515e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 32/71 | LOSS: 4.484412074541042e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 33/71 | LOSS: 4.485013579035106e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 34/71 | LOSS: 4.455458260313858e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 35/71 | LOSS: 4.466210049890327e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 36/71 | LOSS: 4.48640632901122e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 37/71 | LOSS: 4.470844972179054e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 38/71 | LOSS: 4.503988101043013e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 39/71 | LOSS: 4.490135296464359e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 40/71 | LOSS: 4.501132396887065e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 41/71 | LOSS: 4.51684800173964e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 42/71 | LOSS: 4.535652991297295e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 43/71 | LOSS: 4.5422752660587404e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 44/71 | LOSS: 4.556838348435122e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 45/71 | LOSS: 4.543842919702001e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 46/71 | LOSS: 4.542725825577942e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 47/71 | LOSS: 4.55106847141451e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 48/71 | LOSS: 4.52265624294434e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 49/71 | LOSS: 4.52233990017703e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 50/71 | LOSS: 4.507527743650523e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 51/71 | LOSS: 4.537120074788758e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 52/71 | LOSS: 4.5418482927143145e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 53/71 | LOSS: 4.5672981775203e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 54/71 | LOSS: 4.5701282198768405e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 55/71 | LOSS: 4.572095666130086e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 56/71 | LOSS: 4.575690869195999e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 57/71 | LOSS: 4.56289498727309e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 58/71 | LOSS: 4.55616369220217e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 59/71 | LOSS: 4.52875952987597e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 60/71 | LOSS: 4.543230145782963e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 61/71 | LOSS: 4.537477571654064e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 62/71 | LOSS: 4.541734384359202e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 63/71 | LOSS: 4.532960939940267e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 64/71 | LOSS: 4.529315569575724e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 65/71 | LOSS: 4.530172766477115e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 66/71 | LOSS: 4.52310616052653e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 67/71 | LOSS: 4.55045692901609e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 68/71 | LOSS: 4.543568537342978e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 69/71 | LOSS: 4.536820015995805e-06\n",
      "TRAIN: EPOCH 554/1000 | BATCH 70/71 | LOSS: 4.517160550036146e-06\n",
      "VAL: EPOCH 554/1000 | BATCH 0/8 | LOSS: 3.620891220634803e-06\n",
      "VAL: EPOCH 554/1000 | BATCH 1/8 | LOSS: 3.885849309881451e-06\n",
      "VAL: EPOCH 554/1000 | BATCH 2/8 | LOSS: 3.941053061377413e-06\n",
      "VAL: EPOCH 554/1000 | BATCH 3/8 | LOSS: 3.93131540477043e-06\n",
      "VAL: EPOCH 554/1000 | BATCH 4/8 | LOSS: 3.987707623309689e-06\n",
      "VAL: EPOCH 554/1000 | BATCH 5/8 | LOSS: 3.994402201594009e-06\n",
      "VAL: EPOCH 554/1000 | BATCH 6/8 | LOSS: 3.873278826306757e-06\n",
      "VAL: EPOCH 554/1000 | BATCH 7/8 | LOSS: 3.737106595735895e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 0/71 | LOSS: 3.972189915657509e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 1/71 | LOSS: 4.239954932927503e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 2/71 | LOSS: 4.489039686935333e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 3/71 | LOSS: 4.699045916822797e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 4/71 | LOSS: 4.423995142133208e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 5/71 | LOSS: 4.5548344284422155e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 6/71 | LOSS: 4.4150705044947765e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 7/71 | LOSS: 4.535770926850091e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 8/71 | LOSS: 4.467843710800581e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 9/71 | LOSS: 4.496195106185041e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 10/71 | LOSS: 4.5070933496886445e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 11/71 | LOSS: 4.425053153530219e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 12/71 | LOSS: 4.435095276606332e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 13/71 | LOSS: 4.376307694136423e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 14/71 | LOSS: 4.41642191617575e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 15/71 | LOSS: 4.432526210962351e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 16/71 | LOSS: 4.4272035478677615e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 17/71 | LOSS: 4.424026795075204e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 18/71 | LOSS: 4.565232553719206e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 19/71 | LOSS: 4.496214830851386e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 20/71 | LOSS: 4.51156597591762e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 21/71 | LOSS: 4.475088944259782e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 22/71 | LOSS: 4.452797512662389e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 23/71 | LOSS: 4.4411212816915695e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 24/71 | LOSS: 4.509795917329029e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 25/71 | LOSS: 4.531983248713144e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 26/71 | LOSS: 4.4989692553456365e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 27/71 | LOSS: 4.51763406772443e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 28/71 | LOSS: 4.541352084210115e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 29/71 | LOSS: 4.528661891830173e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 30/71 | LOSS: 4.5223438548653635e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 31/71 | LOSS: 4.586671273898446e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 32/71 | LOSS: 4.562579389047724e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 33/71 | LOSS: 4.558897364190403e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 34/71 | LOSS: 4.554937303120304e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 35/71 | LOSS: 4.547450000952975e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 36/71 | LOSS: 4.52260949075296e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 37/71 | LOSS: 4.527683113577184e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 38/71 | LOSS: 4.507477039320899e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 39/71 | LOSS: 4.489074285629613e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 40/71 | LOSS: 4.471344077390357e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 41/71 | LOSS: 4.461915739825816e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 42/71 | LOSS: 4.444000941750642e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 43/71 | LOSS: 4.428014687040228e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 44/71 | LOSS: 4.434648326423485e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 45/71 | LOSS: 4.41431000027755e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 46/71 | LOSS: 4.4205210523362805e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 47/71 | LOSS: 4.420860714541656e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 48/71 | LOSS: 4.439414466588107e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 49/71 | LOSS: 4.433919084476656e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 50/71 | LOSS: 4.4467318123982596e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 51/71 | LOSS: 4.44860454361053e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 52/71 | LOSS: 4.4394749598929065e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 53/71 | LOSS: 4.459584303091375e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 54/71 | LOSS: 4.464131548460997e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 55/71 | LOSS: 4.434418493864152e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 56/71 | LOSS: 4.452443598685868e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 57/71 | LOSS: 4.457889103989523e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 58/71 | LOSS: 4.462706621810774e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 59/71 | LOSS: 4.451198844890314e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 60/71 | LOSS: 4.449227993913543e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 61/71 | LOSS: 4.433921947477962e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 62/71 | LOSS: 4.4238163968296405e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 63/71 | LOSS: 4.4121252678053224e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 64/71 | LOSS: 4.404230069737353e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 65/71 | LOSS: 4.415247093620013e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 66/71 | LOSS: 4.4331793017577256e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 67/71 | LOSS: 4.4331594775779675e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 68/71 | LOSS: 4.433022731841513e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 69/71 | LOSS: 4.440834316353305e-06\n",
      "TRAIN: EPOCH 555/1000 | BATCH 70/71 | LOSS: 4.472983378869224e-06\n",
      "VAL: EPOCH 555/1000 | BATCH 0/8 | LOSS: 5.620692263619276e-06\n",
      "VAL: EPOCH 555/1000 | BATCH 1/8 | LOSS: 6.488969347628881e-06\n",
      "VAL: EPOCH 555/1000 | BATCH 2/8 | LOSS: 6.7091170118753025e-06\n",
      "VAL: EPOCH 555/1000 | BATCH 3/8 | LOSS: 6.899732966303418e-06\n",
      "VAL: EPOCH 555/1000 | BATCH 4/8 | LOSS: 6.8458765781542755e-06\n",
      "VAL: EPOCH 555/1000 | BATCH 5/8 | LOSS: 7.005537630296506e-06\n",
      "VAL: EPOCH 555/1000 | BATCH 6/8 | LOSS: 6.9777373547757245e-06\n",
      "VAL: EPOCH 555/1000 | BATCH 7/8 | LOSS: 6.832994813521509e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 0/71 | LOSS: 6.5284625634376425e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 1/71 | LOSS: 5.320388027030276e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 2/71 | LOSS: 5.436261744762305e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 3/71 | LOSS: 4.987667125533335e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 4/71 | LOSS: 5.315545422490686e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 5/71 | LOSS: 5.2310834538123645e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 6/71 | LOSS: 5.1328280018684125e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 7/71 | LOSS: 5.106652963604574e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 8/71 | LOSS: 5.149651214095583e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 9/71 | LOSS: 5.03225096508686e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 10/71 | LOSS: 5.0415968127543405e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 11/71 | LOSS: 4.9207062223407165e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 12/71 | LOSS: 4.824375292418364e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 13/71 | LOSS: 4.778286958233886e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 14/71 | LOSS: 4.81074445512301e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 15/71 | LOSS: 4.868035233585033e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 16/71 | LOSS: 4.759531293530017e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 17/71 | LOSS: 4.8980912955206195e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 18/71 | LOSS: 4.898507493060989e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 19/71 | LOSS: 5.016809268454381e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 20/71 | LOSS: 4.9698426101669955e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 21/71 | LOSS: 5.110811281900334e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 22/71 | LOSS: 5.034510116314303e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 23/71 | LOSS: 5.0847056816868035e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 24/71 | LOSS: 5.050629388279049e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 25/71 | LOSS: 5.083950397350306e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 26/71 | LOSS: 5.041730724365658e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 27/71 | LOSS: 5.058450158555518e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 28/71 | LOSS: 5.013197496423502e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 29/71 | LOSS: 4.977480004223859e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 30/71 | LOSS: 4.910338512056642e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 31/71 | LOSS: 4.891720891464502e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 32/71 | LOSS: 4.8791370125289895e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 33/71 | LOSS: 4.8791250318412065e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 34/71 | LOSS: 4.888943918298797e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 35/71 | LOSS: 4.880287254612388e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 36/71 | LOSS: 4.934999909695565e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 37/71 | LOSS: 4.887778154146585e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 38/71 | LOSS: 4.926015062431898e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 39/71 | LOSS: 4.898591981827849e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 40/71 | LOSS: 4.938413290224938e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 41/71 | LOSS: 4.937100719698287e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 42/71 | LOSS: 4.926019538859607e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 43/71 | LOSS: 4.9583801300782815e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 44/71 | LOSS: 4.9952272244505975e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 45/71 | LOSS: 4.982793741518158e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 46/71 | LOSS: 4.968542653125738e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 47/71 | LOSS: 4.939760448981663e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 48/71 | LOSS: 4.9150653188805834e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 49/71 | LOSS: 4.93615482810128e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 50/71 | LOSS: 4.910031039424425e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 51/71 | LOSS: 4.8946867230789885e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 52/71 | LOSS: 4.888542125970487e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 53/71 | LOSS: 4.874242917038727e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 54/71 | LOSS: 4.881948986869909e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 55/71 | LOSS: 4.85959976767195e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 56/71 | LOSS: 4.848110927668402e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 57/71 | LOSS: 4.8366848777105e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 58/71 | LOSS: 4.825700443689129e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 59/71 | LOSS: 4.823575413107998e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 60/71 | LOSS: 4.8180738236832906e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 61/71 | LOSS: 4.808493174330637e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 62/71 | LOSS: 4.80434934096203e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 63/71 | LOSS: 4.817535593559796e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 64/71 | LOSS: 4.820347871478029e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 65/71 | LOSS: 4.801335476810037e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 66/71 | LOSS: 4.808767389884234e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 67/71 | LOSS: 4.800986600377441e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 68/71 | LOSS: 4.803609957546713e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 69/71 | LOSS: 4.800326031987165e-06\n",
      "TRAIN: EPOCH 556/1000 | BATCH 70/71 | LOSS: 4.802385591843426e-06\n",
      "VAL: EPOCH 556/1000 | BATCH 0/8 | LOSS: 4.022302618977847e-06\n",
      "VAL: EPOCH 556/1000 | BATCH 1/8 | LOSS: 4.490983656069147e-06\n",
      "VAL: EPOCH 556/1000 | BATCH 2/8 | LOSS: 4.835396642495955e-06\n",
      "VAL: EPOCH 556/1000 | BATCH 3/8 | LOSS: 4.881621975982853e-06\n",
      "VAL: EPOCH 556/1000 | BATCH 4/8 | LOSS: 4.9362201934854966e-06\n",
      "VAL: EPOCH 556/1000 | BATCH 5/8 | LOSS: 4.926125635999294e-06\n",
      "VAL: EPOCH 556/1000 | BATCH 6/8 | LOSS: 4.827377844256782e-06\n",
      "VAL: EPOCH 556/1000 | BATCH 7/8 | LOSS: 4.75128200605468e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 0/71 | LOSS: 3.870462478516856e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 1/71 | LOSS: 4.380584414320765e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 2/71 | LOSS: 4.9717130726397345e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 3/71 | LOSS: 4.7379642182932e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 4/71 | LOSS: 4.9196338295587335e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 5/71 | LOSS: 4.798375357495388e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 6/71 | LOSS: 4.535633544427193e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 7/71 | LOSS: 4.463130522935899e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 8/71 | LOSS: 4.593466428559623e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 9/71 | LOSS: 4.5677519892706185e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 10/71 | LOSS: 4.562024245883962e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 11/71 | LOSS: 4.551785593776003e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 12/71 | LOSS: 4.543598799080848e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 13/71 | LOSS: 4.5171283643087786e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 14/71 | LOSS: 4.5268954181665325e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 15/71 | LOSS: 4.455699553318482e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 16/71 | LOSS: 4.381875040802269e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 17/71 | LOSS: 4.530890515121024e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 18/71 | LOSS: 4.4806122452045155e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 19/71 | LOSS: 4.44678992153058e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 20/71 | LOSS: 4.3976273922280165e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 21/71 | LOSS: 4.372969876633513e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 22/71 | LOSS: 4.4091943100679405e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 23/71 | LOSS: 4.411749406093198e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 24/71 | LOSS: 4.39144385381951e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 25/71 | LOSS: 4.4805377596136305e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 26/71 | LOSS: 4.446258959365811e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 27/71 | LOSS: 4.537392750795074e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 28/71 | LOSS: 4.528390099008463e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 29/71 | LOSS: 4.5146823898297345e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 30/71 | LOSS: 4.603190899938307e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 31/71 | LOSS: 4.703481224055395e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 32/71 | LOSS: 4.747369309894346e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 33/71 | LOSS: 4.828166699072051e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 34/71 | LOSS: 4.887393034940552e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 35/71 | LOSS: 4.885308789602681e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 36/71 | LOSS: 4.947310596459853e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 37/71 | LOSS: 4.9450369667618745e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 38/71 | LOSS: 4.9812445887745034e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 39/71 | LOSS: 4.9985408395514245e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 40/71 | LOSS: 4.9806421773654424e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 41/71 | LOSS: 4.968083766514264e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 42/71 | LOSS: 4.953144969815269e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 43/71 | LOSS: 4.956347869334753e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 44/71 | LOSS: 4.959325704274104e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 45/71 | LOSS: 4.965327335689242e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 46/71 | LOSS: 4.9370527681566404e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 47/71 | LOSS: 4.942559551561014e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 48/71 | LOSS: 4.925833504135621e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 49/71 | LOSS: 4.922293528579757e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 50/71 | LOSS: 4.9075615339354364e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 51/71 | LOSS: 4.894414132650127e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 52/71 | LOSS: 4.871366807595412e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 53/71 | LOSS: 4.8563916413547334e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 54/71 | LOSS: 4.868505696074201e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 55/71 | LOSS: 4.867972673342333e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 56/71 | LOSS: 4.852167321906452e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 57/71 | LOSS: 4.8783501864002986e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 58/71 | LOSS: 4.862974365250458e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 59/71 | LOSS: 4.8503943276045906e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 60/71 | LOSS: 4.827670304844925e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 61/71 | LOSS: 4.8456879283314206e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 62/71 | LOSS: 4.833342913231703e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 63/71 | LOSS: 4.841899905017044e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 64/71 | LOSS: 4.824085188094893e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 65/71 | LOSS: 4.824641795301321e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 66/71 | LOSS: 4.829571368523531e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 67/71 | LOSS: 4.8184200221947e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 68/71 | LOSS: 4.822079415754305e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 69/71 | LOSS: 4.815087553262336e-06\n",
      "TRAIN: EPOCH 557/1000 | BATCH 70/71 | LOSS: 4.864517440579784e-06\n",
      "VAL: EPOCH 557/1000 | BATCH 0/8 | LOSS: 3.730186790562584e-06\n",
      "VAL: EPOCH 557/1000 | BATCH 1/8 | LOSS: 4.2606703800629475e-06\n",
      "VAL: EPOCH 557/1000 | BATCH 2/8 | LOSS: 4.403657158036367e-06\n",
      "VAL: EPOCH 557/1000 | BATCH 3/8 | LOSS: 4.470644341836305e-06\n",
      "VAL: EPOCH 557/1000 | BATCH 4/8 | LOSS: 4.458070861801389e-06\n",
      "VAL: EPOCH 557/1000 | BATCH 5/8 | LOSS: 4.482008042335413e-06\n",
      "VAL: EPOCH 557/1000 | BATCH 6/8 | LOSS: 4.3808024981574685e-06\n",
      "VAL: EPOCH 557/1000 | BATCH 7/8 | LOSS: 4.210760465639396e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 0/71 | LOSS: 4.1843459257506765e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 1/71 | LOSS: 6.241871687961975e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 2/71 | LOSS: 5.646635751569799e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 3/71 | LOSS: 5.587747295976442e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 4/71 | LOSS: 5.3832483899896035e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 5/71 | LOSS: 5.268892133850993e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 6/71 | LOSS: 5.361396558458052e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 7/71 | LOSS: 5.222871607202251e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 8/71 | LOSS: 5.21094544738945e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 9/71 | LOSS: 5.168556026546867e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 10/71 | LOSS: 5.170191756902072e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 11/71 | LOSS: 5.099570178875486e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 12/71 | LOSS: 5.132453749795856e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 13/71 | LOSS: 5.176034684544513e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 14/71 | LOSS: 5.0862595647534665e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 15/71 | LOSS: 5.132738067459286e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 16/71 | LOSS: 5.129711205560906e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 17/71 | LOSS: 5.140708860077818e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 18/71 | LOSS: 5.225627032096351e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 19/71 | LOSS: 5.1952520607301265e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 20/71 | LOSS: 5.230924952705945e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 21/71 | LOSS: 5.182878567211446e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 22/71 | LOSS: 5.1529447536386614e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 23/71 | LOSS: 5.0875594107916795e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 24/71 | LOSS: 5.072659077995922e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 25/71 | LOSS: 5.061147686109717e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 26/71 | LOSS: 5.021440773378816e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 27/71 | LOSS: 5.06418281734763e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 28/71 | LOSS: 5.003325974218475e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 29/71 | LOSS: 4.998128270017333e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 30/71 | LOSS: 4.973491169009996e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 31/71 | LOSS: 4.962952075970861e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 32/71 | LOSS: 4.983815263888876e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 33/71 | LOSS: 4.968752685218694e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 34/71 | LOSS: 4.979227638354392e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 35/71 | LOSS: 4.9590124338768265e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 36/71 | LOSS: 4.936522944606966e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 37/71 | LOSS: 4.926448881585537e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 38/71 | LOSS: 4.901988655220544e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 39/71 | LOSS: 4.9388979846298755e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 40/71 | LOSS: 4.935707504667268e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 41/71 | LOSS: 4.929257470177531e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 42/71 | LOSS: 4.9025917485059205e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 43/71 | LOSS: 4.947399087417249e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 44/71 | LOSS: 4.933773910427893e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 45/71 | LOSS: 4.963781466556058e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 46/71 | LOSS: 4.951808211850956e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 47/71 | LOSS: 4.937039255044813e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 48/71 | LOSS: 4.944159121694202e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 49/71 | LOSS: 4.9525940949024515e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 50/71 | LOSS: 4.932752981225278e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 51/71 | LOSS: 4.913166780904878e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 52/71 | LOSS: 4.939686668190264e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 53/71 | LOSS: 4.928379578612092e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 54/71 | LOSS: 4.9356652652932095e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 55/71 | LOSS: 4.942531165040366e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 56/71 | LOSS: 4.934489349276798e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 57/71 | LOSS: 4.923488306523158e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 58/71 | LOSS: 4.917810475351378e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 59/71 | LOSS: 4.9241182485578365e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 60/71 | LOSS: 4.916237107640062e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 61/71 | LOSS: 4.903223971415437e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 62/71 | LOSS: 4.894505500776595e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 63/71 | LOSS: 4.873410745176443e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 64/71 | LOSS: 4.8686201038295765e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 65/71 | LOSS: 4.85140690491817e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 66/71 | LOSS: 4.841673244164437e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 67/71 | LOSS: 4.827495344858e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 68/71 | LOSS: 4.830925882888023e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 69/71 | LOSS: 4.814899261873506e-06\n",
      "TRAIN: EPOCH 558/1000 | BATCH 70/71 | LOSS: 4.811032337310165e-06\n",
      "VAL: EPOCH 558/1000 | BATCH 0/8 | LOSS: 4.588407591654686e-06\n",
      "VAL: EPOCH 558/1000 | BATCH 1/8 | LOSS: 4.94117989546794e-06\n",
      "VAL: EPOCH 558/1000 | BATCH 2/8 | LOSS: 4.900455526997878e-06\n",
      "VAL: EPOCH 558/1000 | BATCH 3/8 | LOSS: 4.855462293562596e-06\n",
      "VAL: EPOCH 558/1000 | BATCH 4/8 | LOSS: 4.846928823099006e-06\n",
      "VAL: EPOCH 558/1000 | BATCH 5/8 | LOSS: 4.74088559106652e-06\n",
      "VAL: EPOCH 558/1000 | BATCH 6/8 | LOSS: 4.595701284415554e-06\n",
      "VAL: EPOCH 558/1000 | BATCH 7/8 | LOSS: 4.385138623774765e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 0/71 | LOSS: 4.458640432858374e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 1/71 | LOSS: 3.8315926076393225e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 2/71 | LOSS: 3.861589220832684e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 3/71 | LOSS: 4.349533298864117e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 4/71 | LOSS: 4.2874802147707666e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 5/71 | LOSS: 4.469883416883628e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 6/71 | LOSS: 4.448284244062961e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 7/71 | LOSS: 4.47311603579692e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 8/71 | LOSS: 4.4319978719108294e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 9/71 | LOSS: 4.3848008090208165e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 10/71 | LOSS: 4.373941361667345e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 11/71 | LOSS: 4.341444442464611e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 12/71 | LOSS: 4.266282146020855e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 13/71 | LOSS: 4.220380511859341e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 14/71 | LOSS: 4.230173559941856e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 15/71 | LOSS: 4.253967304634898e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 16/71 | LOSS: 4.326307469134062e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 17/71 | LOSS: 4.405338068459565e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 18/71 | LOSS: 4.38977364961159e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 19/71 | LOSS: 4.412006967413618e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 20/71 | LOSS: 4.42458341846456e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 21/71 | LOSS: 4.4144097314942466e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 22/71 | LOSS: 4.395178532080457e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 23/71 | LOSS: 4.392622931466879e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 24/71 | LOSS: 4.369870757727767e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 25/71 | LOSS: 4.353879769671319e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 26/71 | LOSS: 4.35023022233778e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 27/71 | LOSS: 4.345052023576759e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 28/71 | LOSS: 4.315141393754889e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 29/71 | LOSS: 4.316928031281956e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 30/71 | LOSS: 4.3066423337046625e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 31/71 | LOSS: 4.292113629844607e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 32/71 | LOSS: 4.291871005768895e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 33/71 | LOSS: 4.270365352835249e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 34/71 | LOSS: 4.2482186862408916e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 35/71 | LOSS: 4.255418275533884e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 36/71 | LOSS: 4.27339982461244e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 37/71 | LOSS: 4.2889239705944325e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 38/71 | LOSS: 4.271536022898568e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 39/71 | LOSS: 4.28871613848969e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 40/71 | LOSS: 4.285838114322737e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 41/71 | LOSS: 4.315931283556329e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 42/71 | LOSS: 4.307947366151195e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 43/71 | LOSS: 4.321197748553955e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 44/71 | LOSS: 4.34664585150636e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 45/71 | LOSS: 4.338512682174358e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 46/71 | LOSS: 4.3748980405848186e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 47/71 | LOSS: 4.369713157833151e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 48/71 | LOSS: 4.3643617202030085e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 49/71 | LOSS: 4.39655656919058e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 50/71 | LOSS: 4.437147628563894e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 51/71 | LOSS: 4.482356031915585e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 52/71 | LOSS: 4.480791279281718e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 53/71 | LOSS: 4.542156872317989e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 54/71 | LOSS: 4.5607139575374525e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 55/71 | LOSS: 4.566666210134177e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 56/71 | LOSS: 4.606973051155811e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 57/71 | LOSS: 4.636820229844206e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 58/71 | LOSS: 4.65168704977259e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 59/71 | LOSS: 4.6866558553423e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 60/71 | LOSS: 4.744245751546677e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 61/71 | LOSS: 4.805884945228407e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 62/71 | LOSS: 4.8133076436190295e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 63/71 | LOSS: 4.870710377247178e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 64/71 | LOSS: 4.881319574241598e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 65/71 | LOSS: 4.884457184468347e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 66/71 | LOSS: 4.913019261921499e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 67/71 | LOSS: 4.899587947072766e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 68/71 | LOSS: 4.879845573009663e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 69/71 | LOSS: 4.867442813909812e-06\n",
      "TRAIN: EPOCH 559/1000 | BATCH 70/71 | LOSS: 4.846339965630429e-06\n",
      "VAL: EPOCH 559/1000 | BATCH 0/8 | LOSS: 3.829031811619643e-06\n",
      "VAL: EPOCH 559/1000 | BATCH 1/8 | LOSS: 4.235406095176586e-06\n",
      "VAL: EPOCH 559/1000 | BATCH 2/8 | LOSS: 4.48938696232896e-06\n",
      "VAL: EPOCH 559/1000 | BATCH 3/8 | LOSS: 4.4356569333103835e-06\n",
      "VAL: EPOCH 559/1000 | BATCH 4/8 | LOSS: 4.523788993537891e-06\n",
      "VAL: EPOCH 559/1000 | BATCH 5/8 | LOSS: 4.448442875097196e-06\n",
      "VAL: EPOCH 559/1000 | BATCH 6/8 | LOSS: 4.341189878687146e-06\n",
      "VAL: EPOCH 559/1000 | BATCH 7/8 | LOSS: 4.240470445893152e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 0/71 | LOSS: 3.516209744702792e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 1/71 | LOSS: 4.2160020257142605e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 2/71 | LOSS: 4.326982889324427e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 3/71 | LOSS: 4.32141860073898e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 4/71 | LOSS: 4.116299169254489e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 5/71 | LOSS: 4.190052322883275e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 6/71 | LOSS: 4.330892352299998e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 7/71 | LOSS: 4.319204265357257e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 8/71 | LOSS: 4.3904311092369935e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 9/71 | LOSS: 4.354640850579017e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 10/71 | LOSS: 4.392022095668256e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 11/71 | LOSS: 4.327002898207866e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 12/71 | LOSS: 4.312325717574612e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 13/71 | LOSS: 4.28292056702568e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 14/71 | LOSS: 4.367936041186719e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 15/71 | LOSS: 4.3399545859301725e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 16/71 | LOSS: 4.375893989644657e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 17/71 | LOSS: 4.3971089982935355e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 18/71 | LOSS: 4.422841069945686e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 19/71 | LOSS: 4.497662462199514e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 20/71 | LOSS: 4.5325469467830515e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 21/71 | LOSS: 4.5011869050623234e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 22/71 | LOSS: 4.454006359636601e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 23/71 | LOSS: 4.451207416877878e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 24/71 | LOSS: 4.445867089089006e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 25/71 | LOSS: 4.481126989748401e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 26/71 | LOSS: 4.452330678624868e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 27/71 | LOSS: 4.513337233090299e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 28/71 | LOSS: 4.485577854775956e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 29/71 | LOSS: 4.469311966204259e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 30/71 | LOSS: 4.465637050910324e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 31/71 | LOSS: 4.480511570648105e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 32/71 | LOSS: 4.4568876367548276e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 33/71 | LOSS: 4.470640874387755e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 34/71 | LOSS: 4.430798779659589e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 35/71 | LOSS: 4.419557178102372e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 36/71 | LOSS: 4.403149463345275e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 37/71 | LOSS: 4.397245142603137e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 38/71 | LOSS: 4.372599173271178e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 39/71 | LOSS: 4.361680117881405e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 40/71 | LOSS: 4.3493613558406115e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 41/71 | LOSS: 4.3250559678026115e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 42/71 | LOSS: 4.318033403293622e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 43/71 | LOSS: 4.343578196271252e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 44/71 | LOSS: 4.328351502004403e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 45/71 | LOSS: 4.3144434881548435e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 46/71 | LOSS: 4.313187528555802e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 47/71 | LOSS: 4.294958633484687e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 48/71 | LOSS: 4.309143262115849e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 49/71 | LOSS: 4.2925882780764365e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 50/71 | LOSS: 4.292419250715266e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 51/71 | LOSS: 4.268292203645475e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 52/71 | LOSS: 4.260413990503469e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 53/71 | LOSS: 4.245601387622257e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 54/71 | LOSS: 4.2660715983154e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 55/71 | LOSS: 4.275551191312843e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 56/71 | LOSS: 4.273147767794159e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 57/71 | LOSS: 4.282774257105835e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 58/71 | LOSS: 4.2943177165505985e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 59/71 | LOSS: 4.296073567881346e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 60/71 | LOSS: 4.292339729202423e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 61/71 | LOSS: 4.280185833287701e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 62/71 | LOSS: 4.2795149641148915e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 63/71 | LOSS: 4.272680168782017e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 64/71 | LOSS: 4.256785880967912e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 65/71 | LOSS: 4.246648869053443e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 66/71 | LOSS: 4.243069331245514e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 67/71 | LOSS: 4.2517466989225155e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 68/71 | LOSS: 4.2483838629066115e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 69/71 | LOSS: 4.254475710955116e-06\n",
      "TRAIN: EPOCH 560/1000 | BATCH 70/71 | LOSS: 4.270909966193434e-06\n",
      "VAL: EPOCH 560/1000 | BATCH 0/8 | LOSS: 3.93903064832557e-06\n",
      "VAL: EPOCH 560/1000 | BATCH 1/8 | LOSS: 4.289740672902553e-06\n",
      "VAL: EPOCH 560/1000 | BATCH 2/8 | LOSS: 4.436377669965926e-06\n",
      "VAL: EPOCH 560/1000 | BATCH 3/8 | LOSS: 4.424290523274976e-06\n",
      "VAL: EPOCH 560/1000 | BATCH 4/8 | LOSS: 4.4849187361251095e-06\n",
      "VAL: EPOCH 560/1000 | BATCH 5/8 | LOSS: 4.420737013788312e-06\n",
      "VAL: EPOCH 560/1000 | BATCH 6/8 | LOSS: 4.278565386682333e-06\n",
      "VAL: EPOCH 560/1000 | BATCH 7/8 | LOSS: 4.153043164478731e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 0/71 | LOSS: 3.988527168985456e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 1/71 | LOSS: 4.064172571816016e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 2/71 | LOSS: 4.038617134938249e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 3/71 | LOSS: 3.93586611835417e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 4/71 | LOSS: 4.186309206488659e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 5/71 | LOSS: 4.064600640655651e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 6/71 | LOSS: 4.046065571726233e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 7/71 | LOSS: 4.046678327540576e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 8/71 | LOSS: 4.038051126068846e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 9/71 | LOSS: 4.02192222281883e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 10/71 | LOSS: 4.060275262996914e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 11/71 | LOSS: 4.073436798535113e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 12/71 | LOSS: 4.027773194628446e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 13/71 | LOSS: 4.105326622396076e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 14/71 | LOSS: 4.101938066014554e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 15/71 | LOSS: 4.12844798347578e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 16/71 | LOSS: 4.234493805116097e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 17/71 | LOSS: 4.211526402286836e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 18/71 | LOSS: 4.15529123194191e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 19/71 | LOSS: 4.132921003474621e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 20/71 | LOSS: 4.143464030632804e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 21/71 | LOSS: 4.140262324819394e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 22/71 | LOSS: 4.119820548863651e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 23/71 | LOSS: 4.195094552036001e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 24/71 | LOSS: 4.194790881228982e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 25/71 | LOSS: 4.168206347598103e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 26/71 | LOSS: 4.165424157960203e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 27/71 | LOSS: 4.133096906246335e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 28/71 | LOSS: 4.123164515100331e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 29/71 | LOSS: 4.11774888865087e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 30/71 | LOSS: 4.155180587047418e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 31/71 | LOSS: 4.183703950388917e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 32/71 | LOSS: 4.188638878545272e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 33/71 | LOSS: 4.165736152786207e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 34/71 | LOSS: 4.228848917721605e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 35/71 | LOSS: 4.19821750736244e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 36/71 | LOSS: 4.2476041152120754e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 37/71 | LOSS: 4.282382280363986e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 38/71 | LOSS: 4.2885533042490115e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 39/71 | LOSS: 4.307900593403247e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 40/71 | LOSS: 4.386244693417145e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 41/71 | LOSS: 4.377413737139639e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 42/71 | LOSS: 4.470631438380224e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 43/71 | LOSS: 4.529968407496199e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 44/71 | LOSS: 4.60278952737604e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 45/71 | LOSS: 4.602329795488913e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 46/71 | LOSS: 4.7053577399059034e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 47/71 | LOSS: 4.719898124487069e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 48/71 | LOSS: 4.729046047347709e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 49/71 | LOSS: 4.713154044111434e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 50/71 | LOSS: 4.695098952794854e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 51/71 | LOSS: 4.699066135589629e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 52/71 | LOSS: 4.691887341941079e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 53/71 | LOSS: 4.692514595363819e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 54/71 | LOSS: 4.6850631158370316e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 55/71 | LOSS: 4.709838037797454e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 56/71 | LOSS: 4.693836943484361e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 57/71 | LOSS: 4.671846117444461e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 58/71 | LOSS: 4.668558122537403e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 59/71 | LOSS: 4.664858973531712e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 60/71 | LOSS: 4.654389083982812e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 61/71 | LOSS: 4.638708017615324e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 62/71 | LOSS: 4.643455393682142e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 63/71 | LOSS: 4.634154016969205e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 64/71 | LOSS: 4.621531832946437e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 65/71 | LOSS: 4.605871267951002e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 66/71 | LOSS: 4.58317030713817e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 67/71 | LOSS: 4.584734383302823e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 68/71 | LOSS: 4.5742473391157175e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 69/71 | LOSS: 4.5741476697003234e-06\n",
      "TRAIN: EPOCH 561/1000 | BATCH 70/71 | LOSS: 4.549234793391316e-06\n",
      "VAL: EPOCH 561/1000 | BATCH 0/8 | LOSS: 6.246618795557879e-06\n",
      "VAL: EPOCH 561/1000 | BATCH 1/8 | LOSS: 6.27115559836966e-06\n",
      "VAL: EPOCH 561/1000 | BATCH 2/8 | LOSS: 6.230941532218519e-06\n",
      "VAL: EPOCH 561/1000 | BATCH 3/8 | LOSS: 6.099800657466403e-06\n",
      "VAL: EPOCH 561/1000 | BATCH 4/8 | LOSS: 6.284801747824531e-06\n",
      "VAL: EPOCH 561/1000 | BATCH 5/8 | LOSS: 6.112437252644061e-06\n",
      "VAL: EPOCH 561/1000 | BATCH 6/8 | LOSS: 6.0137474195341516e-06\n",
      "VAL: EPOCH 561/1000 | BATCH 7/8 | LOSS: 5.854222479229065e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 0/71 | LOSS: 6.7264290919411e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 1/71 | LOSS: 5.5403866099368315e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 2/71 | LOSS: 5.4780181623452035e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 3/71 | LOSS: 5.271217787594651e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 4/71 | LOSS: 5.215681903791847e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 5/71 | LOSS: 5.138901087775594e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 6/71 | LOSS: 5.208372645678797e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 7/71 | LOSS: 5.2219156714272685e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 8/71 | LOSS: 5.103066743888323e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 9/71 | LOSS: 5.0655579343583664e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 10/71 | LOSS: 4.946821449762369e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 11/71 | LOSS: 4.95197929240021e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 12/71 | LOSS: 4.884996909682881e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 13/71 | LOSS: 4.867211487180612e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 14/71 | LOSS: 4.803068729112662e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 15/71 | LOSS: 4.740762349797478e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 16/71 | LOSS: 4.760701799836277e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 17/71 | LOSS: 4.7050677570344606e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 18/71 | LOSS: 4.641938116229109e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 19/71 | LOSS: 4.680617655594687e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 20/71 | LOSS: 4.622274846475366e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 21/71 | LOSS: 4.626552574584573e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 22/71 | LOSS: 4.600086106659981e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 23/71 | LOSS: 4.59733863067413e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 24/71 | LOSS: 4.630274661394651e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 25/71 | LOSS: 4.677281743589936e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 26/71 | LOSS: 4.689886441998102e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 27/71 | LOSS: 4.734493545908793e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 28/71 | LOSS: 4.7246038822526445e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 29/71 | LOSS: 4.8261312334337465e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 30/71 | LOSS: 4.80617725975941e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 31/71 | LOSS: 4.773936424840031e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 32/71 | LOSS: 4.799215262072252e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 33/71 | LOSS: 4.868106757652747e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 34/71 | LOSS: 4.903979411860097e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 35/71 | LOSS: 4.913520481548201e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 36/71 | LOSS: 5.004203423005812e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 37/71 | LOSS: 4.988214820849335e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 38/71 | LOSS: 4.984823123940702e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 39/71 | LOSS: 5.007394327094516e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 40/71 | LOSS: 5.01945978834467e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 41/71 | LOSS: 5.063818867884471e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 42/71 | LOSS: 5.0951452765222585e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 43/71 | LOSS: 5.142390395204447e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 44/71 | LOSS: 5.124573332674724e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 45/71 | LOSS: 5.138671717166131e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 46/71 | LOSS: 5.112204293099955e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 47/71 | LOSS: 5.159152474713362e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 48/71 | LOSS: 5.163223481851711e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 49/71 | LOSS: 5.189897792661213e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 50/71 | LOSS: 5.2313480320627405e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 51/71 | LOSS: 5.219042428996965e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 52/71 | LOSS: 5.232947198649372e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 53/71 | LOSS: 5.215212956533867e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 54/71 | LOSS: 5.246324177609164e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 55/71 | LOSS: 5.230465360130308e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 56/71 | LOSS: 5.2051207787148434e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 57/71 | LOSS: 5.208434854780378e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 58/71 | LOSS: 5.225439404596928e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 59/71 | LOSS: 5.208950729714464e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 60/71 | LOSS: 5.173945286604394e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 61/71 | LOSS: 5.178377562280671e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 62/71 | LOSS: 5.155223554596425e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 63/71 | LOSS: 5.173528936808225e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 64/71 | LOSS: 5.148507153325437e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 65/71 | LOSS: 5.115823317723742e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 66/71 | LOSS: 5.096279366300033e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 67/71 | LOSS: 5.0825599942072586e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 68/71 | LOSS: 5.06551928215165e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 69/71 | LOSS: 5.0390279381775015e-06\n",
      "TRAIN: EPOCH 562/1000 | BATCH 70/71 | LOSS: 5.03553295741985e-06\n",
      "VAL: EPOCH 562/1000 | BATCH 0/8 | LOSS: 4.906433787255082e-06\n",
      "VAL: EPOCH 562/1000 | BATCH 1/8 | LOSS: 5.243329496806837e-06\n",
      "VAL: EPOCH 562/1000 | BATCH 2/8 | LOSS: 5.571198319861044e-06\n",
      "VAL: EPOCH 562/1000 | BATCH 3/8 | LOSS: 5.511075983122282e-06\n",
      "VAL: EPOCH 562/1000 | BATCH 4/8 | LOSS: 5.6090412726916835e-06\n",
      "VAL: EPOCH 562/1000 | BATCH 5/8 | LOSS: 5.692673918626194e-06\n",
      "VAL: EPOCH 562/1000 | BATCH 6/8 | LOSS: 5.61756210767531e-06\n",
      "VAL: EPOCH 562/1000 | BATCH 7/8 | LOSS: 5.655657332681585e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 0/71 | LOSS: 4.639996859623352e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 1/71 | LOSS: 4.341585736256093e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 2/71 | LOSS: 4.3996442400384694e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 3/71 | LOSS: 4.871949840890011e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 4/71 | LOSS: 4.680693564296234e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 5/71 | LOSS: 4.893728449436215e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 6/71 | LOSS: 4.799671647301043e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 7/71 | LOSS: 4.71333208906799e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 8/71 | LOSS: 4.683531743568084e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 9/71 | LOSS: 4.6476158331643095e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 10/71 | LOSS: 4.716682889755413e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 11/71 | LOSS: 4.624393871684636e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 12/71 | LOSS: 4.6161032794491175e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 13/71 | LOSS: 4.549211049769448e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 14/71 | LOSS: 4.5220300459429074e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 15/71 | LOSS: 4.473680562000482e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 16/71 | LOSS: 4.468124095236656e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 17/71 | LOSS: 4.44564641889479e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 18/71 | LOSS: 4.373714809365706e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 19/71 | LOSS: 4.426408622748568e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 20/71 | LOSS: 4.43009135086045e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 21/71 | LOSS: 4.4453395689329644e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 22/71 | LOSS: 4.403689079323237e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 23/71 | LOSS: 4.43591776881173e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 24/71 | LOSS: 4.408645199873718e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 25/71 | LOSS: 4.4298401637486395e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 26/71 | LOSS: 4.427802027580414e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 27/71 | LOSS: 4.400220940884278e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 28/71 | LOSS: 4.366903230502744e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 29/71 | LOSS: 4.367752580947126e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 30/71 | LOSS: 4.348298379256683e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 31/71 | LOSS: 4.352304330268453e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 32/71 | LOSS: 4.4031061351592085e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 33/71 | LOSS: 4.361033133778512e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 34/71 | LOSS: 4.348056186894869e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 35/71 | LOSS: 4.351854177760591e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 36/71 | LOSS: 4.3400817700072564e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 37/71 | LOSS: 4.339329608994472e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 38/71 | LOSS: 4.370180686936902e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 39/71 | LOSS: 4.387751829426634e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 40/71 | LOSS: 4.3679884949996474e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 41/71 | LOSS: 4.4064899795904195e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 42/71 | LOSS: 4.380506700113602e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 43/71 | LOSS: 4.363298651035373e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 44/71 | LOSS: 4.355506340895469e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 45/71 | LOSS: 4.343349879174797e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 46/71 | LOSS: 4.345602896950019e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 47/71 | LOSS: 4.346731382535533e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 48/71 | LOSS: 4.352258972120377e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 49/71 | LOSS: 4.327503115746367e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 50/71 | LOSS: 4.350440286882576e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 51/71 | LOSS: 4.331513515318455e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 52/71 | LOSS: 4.344367361450425e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 53/71 | LOSS: 4.338589629124154e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 54/71 | LOSS: 4.340226244246217e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 55/71 | LOSS: 4.335822272391202e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 56/71 | LOSS: 4.331355305048259e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 57/71 | LOSS: 4.326797724827664e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 58/71 | LOSS: 4.318508946059826e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 59/71 | LOSS: 4.340334790716346e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 60/71 | LOSS: 4.3423408889614874e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 61/71 | LOSS: 4.328913544873837e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 62/71 | LOSS: 4.331392058283821e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 63/71 | LOSS: 4.325103912350414e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 64/71 | LOSS: 4.303678801639203e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 65/71 | LOSS: 4.3049825914446655e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 66/71 | LOSS: 4.302988701911361e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 67/71 | LOSS: 4.289239651664984e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 68/71 | LOSS: 4.30603123883581e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 69/71 | LOSS: 4.2962376255673e-06\n",
      "TRAIN: EPOCH 563/1000 | BATCH 70/71 | LOSS: 4.289978435313252e-06\n",
      "VAL: EPOCH 563/1000 | BATCH 0/8 | LOSS: 3.6356871078169206e-06\n",
      "VAL: EPOCH 563/1000 | BATCH 1/8 | LOSS: 4.077981088812521e-06\n",
      "VAL: EPOCH 563/1000 | BATCH 2/8 | LOSS: 4.238617672550997e-06\n",
      "VAL: EPOCH 563/1000 | BATCH 3/8 | LOSS: 4.263329913101188e-06\n",
      "VAL: EPOCH 563/1000 | BATCH 4/8 | LOSS: 4.301817034502164e-06\n",
      "VAL: EPOCH 563/1000 | BATCH 5/8 | LOSS: 4.3160481482118485e-06\n",
      "VAL: EPOCH 563/1000 | BATCH 6/8 | LOSS: 4.207978008707869e-06\n",
      "VAL: EPOCH 563/1000 | BATCH 7/8 | LOSS: 4.0760242256965284e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 0/71 | LOSS: 4.0707309381105006e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 1/71 | LOSS: 3.893494749718229e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 2/71 | LOSS: 4.037638973386493e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 3/71 | LOSS: 4.153012355345709e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 4/71 | LOSS: 3.93274149246281e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 5/71 | LOSS: 3.7671293663758356e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 6/71 | LOSS: 3.7099226964138713e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 7/71 | LOSS: 3.8144091263347946e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 8/71 | LOSS: 3.820405102790876e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 9/71 | LOSS: 3.8909042814339045e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 10/71 | LOSS: 3.945875670813786e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 11/71 | LOSS: 3.999602843881196e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 12/71 | LOSS: 3.991135459867879e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 13/71 | LOSS: 3.989127630380348e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 14/71 | LOSS: 4.033639136954056e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 15/71 | LOSS: 3.999816954092239e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 16/71 | LOSS: 3.9530232243123464e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 17/71 | LOSS: 3.9411985047384706e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 18/71 | LOSS: 3.944532325381021e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 19/71 | LOSS: 3.9888888750283515e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 20/71 | LOSS: 4.0739618747292774e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 21/71 | LOSS: 4.12463732986494e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 22/71 | LOSS: 4.125699552273347e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 23/71 | LOSS: 4.170140906959811e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 24/71 | LOSS: 4.293629754101857e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 25/71 | LOSS: 4.300192673466741e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 26/71 | LOSS: 4.460095523326244e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 27/71 | LOSS: 4.481155883695465e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 28/71 | LOSS: 4.615408623318092e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 29/71 | LOSS: 4.612033156566516e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 30/71 | LOSS: 4.62039666202937e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 31/71 | LOSS: 4.603475687758873e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 32/71 | LOSS: 4.681383595170425e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 33/71 | LOSS: 4.7123398069422415e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 34/71 | LOSS: 4.739310861623381e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 35/71 | LOSS: 4.791985449830665e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 36/71 | LOSS: 4.833622443658061e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 37/71 | LOSS: 4.956156700293497e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 38/71 | LOSS: 5.036258838887219e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 39/71 | LOSS: 5.098602605357882e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 40/71 | LOSS: 5.124059975414861e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 41/71 | LOSS: 5.103132913236983e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 42/71 | LOSS: 5.098781785332099e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 43/71 | LOSS: 5.11429582481717e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 44/71 | LOSS: 5.091006116547053e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 45/71 | LOSS: 5.087988592608979e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 46/71 | LOSS: 5.06349646882959e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 47/71 | LOSS: 5.040861869550402e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 48/71 | LOSS: 5.043952216733394e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 49/71 | LOSS: 5.0275905869057165e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 50/71 | LOSS: 5.00378186919917e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 51/71 | LOSS: 5.035837519839193e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 52/71 | LOSS: 5.023840067910639e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 53/71 | LOSS: 5.00017120817507e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 54/71 | LOSS: 4.999063751463175e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 55/71 | LOSS: 5.038740754668782e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 56/71 | LOSS: 5.03536437447525e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 57/71 | LOSS: 5.029435101278068e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 58/71 | LOSS: 5.077601503278672e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 59/71 | LOSS: 5.068595726243075e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 60/71 | LOSS: 5.0827448853479075e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 61/71 | LOSS: 5.079430567797493e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 62/71 | LOSS: 5.07760384087825e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 63/71 | LOSS: 5.056546008574969e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 64/71 | LOSS: 5.047433919221825e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 65/71 | LOSS: 5.028461042787637e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 66/71 | LOSS: 5.013167001787007e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 67/71 | LOSS: 5.032777868804902e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 68/71 | LOSS: 5.028535744476477e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 69/71 | LOSS: 5.024429096270719e-06\n",
      "TRAIN: EPOCH 564/1000 | BATCH 70/71 | LOSS: 5.03553735437839e-06\n",
      "VAL: EPOCH 564/1000 | BATCH 0/8 | LOSS: 4.103388164367061e-06\n",
      "VAL: EPOCH 564/1000 | BATCH 1/8 | LOSS: 4.368969257484423e-06\n",
      "VAL: EPOCH 564/1000 | BATCH 2/8 | LOSS: 4.368740216402027e-06\n",
      "VAL: EPOCH 564/1000 | BATCH 3/8 | LOSS: 4.369228008727077e-06\n",
      "VAL: EPOCH 564/1000 | BATCH 4/8 | LOSS: 4.3697184082702735e-06\n",
      "VAL: EPOCH 564/1000 | BATCH 5/8 | LOSS: 4.36498172954695e-06\n",
      "VAL: EPOCH 564/1000 | BATCH 6/8 | LOSS: 4.213717504951221e-06\n",
      "VAL: EPOCH 564/1000 | BATCH 7/8 | LOSS: 4.014324616719023e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 0/71 | LOSS: 5.344892088032793e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 1/71 | LOSS: 4.357425268608495e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 2/71 | LOSS: 4.834645703037192e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 3/71 | LOSS: 4.4876450715491956e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 4/71 | LOSS: 4.47943762083014e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 5/71 | LOSS: 4.406879384077911e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 6/71 | LOSS: 4.366811578750328e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 7/71 | LOSS: 4.3602850325896725e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 8/71 | LOSS: 4.370077148349891e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 9/71 | LOSS: 4.514473016570264e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 10/71 | LOSS: 4.3816062936292095e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 11/71 | LOSS: 4.445716342615924e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 12/71 | LOSS: 4.407376851444356e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 13/71 | LOSS: 4.424105473585119e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 14/71 | LOSS: 4.3982544563429355e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 15/71 | LOSS: 4.3625159236171385e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 16/71 | LOSS: 4.344323729931014e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 17/71 | LOSS: 4.3207199951211805e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 18/71 | LOSS: 4.319012448736191e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 19/71 | LOSS: 4.2950724377988084e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 20/71 | LOSS: 4.339094402894261e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 21/71 | LOSS: 4.324747072297958e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 22/71 | LOSS: 4.356491316407894e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 23/71 | LOSS: 4.351687294956719e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 24/71 | LOSS: 4.340902878539055e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 25/71 | LOSS: 4.3393408672133e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 26/71 | LOSS: 4.291898322763801e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 27/71 | LOSS: 4.279555225496422e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 28/71 | LOSS: 4.290955844069309e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 29/71 | LOSS: 4.27101243379487e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 30/71 | LOSS: 4.296767186925506e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 31/71 | LOSS: 4.299371596516721e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 32/71 | LOSS: 4.314044321235445e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 33/71 | LOSS: 4.3234468675483884e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 34/71 | LOSS: 4.351134703028947e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 35/71 | LOSS: 4.398531220633433e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 36/71 | LOSS: 4.412197159655153e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 37/71 | LOSS: 4.462043845801657e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 38/71 | LOSS: 4.502472702136938e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 39/71 | LOSS: 4.5091758124726765e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 40/71 | LOSS: 4.484604931615361e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 41/71 | LOSS: 4.476461537619582e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 42/71 | LOSS: 4.4844647840533116e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 43/71 | LOSS: 4.526129797258446e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 44/71 | LOSS: 4.506478767830737e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 45/71 | LOSS: 4.505430845948348e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 46/71 | LOSS: 4.5013280843240284e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 47/71 | LOSS: 4.506310683420149e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 48/71 | LOSS: 4.500256130884328e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 49/71 | LOSS: 4.49721121640323e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 50/71 | LOSS: 4.484595178034985e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 51/71 | LOSS: 4.4832691602235945e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 52/71 | LOSS: 4.476781793339299e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 53/71 | LOSS: 4.4570404716742575e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 54/71 | LOSS: 4.4442489111547316e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 55/71 | LOSS: 4.440077719079584e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 56/71 | LOSS: 4.418152656670141e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 57/71 | LOSS: 4.414804834595998e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 58/71 | LOSS: 4.412981124298343e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 59/71 | LOSS: 4.425217359009063e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 60/71 | LOSS: 4.409046603720272e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 61/71 | LOSS: 4.410664433312524e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 62/71 | LOSS: 4.396980008846546e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 63/71 | LOSS: 4.386166704506422e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 64/71 | LOSS: 4.3729565767045225e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 65/71 | LOSS: 4.392886601387793e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 66/71 | LOSS: 4.3871672521312735e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 67/71 | LOSS: 4.388617375806301e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 68/71 | LOSS: 4.3758703615848376e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 69/71 | LOSS: 4.3823686967568524e-06\n",
      "TRAIN: EPOCH 565/1000 | BATCH 70/71 | LOSS: 4.372928771490607e-06\n",
      "VAL: EPOCH 565/1000 | BATCH 0/8 | LOSS: 4.355259989097249e-06\n",
      "VAL: EPOCH 565/1000 | BATCH 1/8 | LOSS: 5.080439905214007e-06\n",
      "VAL: EPOCH 565/1000 | BATCH 2/8 | LOSS: 5.195545630461614e-06\n",
      "VAL: EPOCH 565/1000 | BATCH 3/8 | LOSS: 5.301193709783547e-06\n",
      "VAL: EPOCH 565/1000 | BATCH 4/8 | LOSS: 5.292073637974681e-06\n",
      "VAL: EPOCH 565/1000 | BATCH 5/8 | LOSS: 5.272116065195102e-06\n",
      "VAL: EPOCH 565/1000 | BATCH 6/8 | LOSS: 5.200806689182562e-06\n",
      "VAL: EPOCH 565/1000 | BATCH 7/8 | LOSS: 5.04173038962108e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 0/71 | LOSS: 5.059845079813385e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 1/71 | LOSS: 4.620510935637867e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 2/71 | LOSS: 5.268048577515098e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 3/71 | LOSS: 5.370704229790135e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 4/71 | LOSS: 5.548556509893388e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 5/71 | LOSS: 5.758295552974839e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 6/71 | LOSS: 5.57360583895518e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 7/71 | LOSS: 6.035311571395141e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 8/71 | LOSS: 5.792857033763236e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 9/71 | LOSS: 5.944602889940143e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 10/71 | LOSS: 5.838082870468497e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 11/71 | LOSS: 5.856910471872349e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 12/71 | LOSS: 5.720560973434029e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 13/71 | LOSS: 5.7258174105559845e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 14/71 | LOSS: 5.730104536875539e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 15/71 | LOSS: 5.763387918023e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 16/71 | LOSS: 5.73949420654304e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 17/71 | LOSS: 5.7368980984999025e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 18/71 | LOSS: 5.73747313051411e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 19/71 | LOSS: 5.654657297782251e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 20/71 | LOSS: 5.660062058831543e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 21/71 | LOSS: 5.618530477394498e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 22/71 | LOSS: 5.73604954653369e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 23/71 | LOSS: 5.68879835327607e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 24/71 | LOSS: 5.802643227070803e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 25/71 | LOSS: 5.769664365009074e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 26/71 | LOSS: 5.850605921969637e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 27/71 | LOSS: 5.820008124958674e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 28/71 | LOSS: 5.869564951165652e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 29/71 | LOSS: 5.874676147262411e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 30/71 | LOSS: 5.8576339846871225e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 31/71 | LOSS: 5.841123837058149e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 32/71 | LOSS: 5.840056611773572e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 33/71 | LOSS: 5.789124436299008e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 34/71 | LOSS: 5.724266936470356e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 35/71 | LOSS: 5.663317311801721e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 36/71 | LOSS: 5.627780865324894e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 37/71 | LOSS: 5.6096177774490664e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 38/71 | LOSS: 5.56747958258851e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 39/71 | LOSS: 5.512332523949226e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 40/71 | LOSS: 5.507158770031798e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 41/71 | LOSS: 5.487760361012035e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 42/71 | LOSS: 5.462681883727782e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 43/71 | LOSS: 5.4577338553165396e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 44/71 | LOSS: 5.4295167451022686e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 45/71 | LOSS: 5.392541291494917e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 46/71 | LOSS: 5.3825932219355155e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 47/71 | LOSS: 5.3788575324157746e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 48/71 | LOSS: 5.338903023659106e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 49/71 | LOSS: 5.310986402946582e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 50/71 | LOSS: 5.3243247971384374e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 51/71 | LOSS: 5.29053758450223e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 52/71 | LOSS: 5.270937551684702e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 53/71 | LOSS: 5.250602292242682e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 54/71 | LOSS: 5.2343324527596865e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 55/71 | LOSS: 5.222341245923547e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 56/71 | LOSS: 5.228427952667851e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 57/71 | LOSS: 5.2049926290269e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 58/71 | LOSS: 5.22066681573421e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 59/71 | LOSS: 5.212923698157586e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 60/71 | LOSS: 5.192369733359541e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 61/71 | LOSS: 5.155397862073451e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 62/71 | LOSS: 5.146337304130799e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 63/71 | LOSS: 5.136953170392644e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 64/71 | LOSS: 5.119065064298831e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 65/71 | LOSS: 5.112964841332128e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 66/71 | LOSS: 5.111080079233889e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 67/71 | LOSS: 5.11584103686105e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 68/71 | LOSS: 5.100181387324878e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 69/71 | LOSS: 5.091283719593776e-06\n",
      "TRAIN: EPOCH 566/1000 | BATCH 70/71 | LOSS: 5.0750420445516465e-06\n",
      "VAL: EPOCH 566/1000 | BATCH 0/8 | LOSS: 5.65405935049057e-06\n",
      "VAL: EPOCH 566/1000 | BATCH 1/8 | LOSS: 5.758687620982528e-06\n",
      "VAL: EPOCH 566/1000 | BATCH 2/8 | LOSS: 5.8015093600261025e-06\n",
      "VAL: EPOCH 566/1000 | BATCH 3/8 | LOSS: 5.713035761800711e-06\n",
      "VAL: EPOCH 566/1000 | BATCH 4/8 | LOSS: 5.851810601598117e-06\n",
      "VAL: EPOCH 566/1000 | BATCH 5/8 | LOSS: 5.7028558633949915e-06\n",
      "VAL: EPOCH 566/1000 | BATCH 6/8 | LOSS: 5.565399923008434e-06\n",
      "VAL: EPOCH 566/1000 | BATCH 7/8 | LOSS: 5.3982772669769474e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 0/71 | LOSS: 6.038398169039283e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 1/71 | LOSS: 5.845130544912536e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 2/71 | LOSS: 5.356296242098324e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 3/71 | LOSS: 5.195784751776955e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 4/71 | LOSS: 5.131413672643248e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 5/71 | LOSS: 5.177465482120169e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 6/71 | LOSS: 5.0156235634598744e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 7/71 | LOSS: 5.119231218486675e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 8/71 | LOSS: 5.059040581222183e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 9/71 | LOSS: 4.94128794343851e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 10/71 | LOSS: 4.827886186831165e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 11/71 | LOSS: 4.87025931761309e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 12/71 | LOSS: 4.830430183434286e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 13/71 | LOSS: 4.8637446720281985e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 14/71 | LOSS: 4.9131389156779426e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 15/71 | LOSS: 4.87726902065333e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 16/71 | LOSS: 5.073949603898013e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 17/71 | LOSS: 5.043170454478564e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 18/71 | LOSS: 5.114560592214969e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 19/71 | LOSS: 5.1090533588649125e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 20/71 | LOSS: 5.190850818811062e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 21/71 | LOSS: 5.21790099645097e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 22/71 | LOSS: 5.231589432736181e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 23/71 | LOSS: 5.302981946897489e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 24/71 | LOSS: 5.293506892485311e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 25/71 | LOSS: 5.324070868412561e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 26/71 | LOSS: 5.3381415994038064e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 27/71 | LOSS: 5.309818707896089e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 28/71 | LOSS: 5.240253264266107e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 29/71 | LOSS: 5.245605719513454e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 30/71 | LOSS: 5.255002281867952e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 31/71 | LOSS: 5.268581659834126e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 32/71 | LOSS: 5.264067925908779e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 33/71 | LOSS: 5.2512368176180374e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 34/71 | LOSS: 5.205847498603232e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 35/71 | LOSS: 5.240126483840868e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 36/71 | LOSS: 5.203029374840568e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 37/71 | LOSS: 5.190077600721117e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 38/71 | LOSS: 5.165852703962278e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 39/71 | LOSS: 5.142272800640057e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 40/71 | LOSS: 5.132784957734147e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 41/71 | LOSS: 5.112367023771401e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 42/71 | LOSS: 5.087909801997824e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 43/71 | LOSS: 5.070504884811684e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 44/71 | LOSS: 5.0460384651766315e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 45/71 | LOSS: 5.02691189111552e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 46/71 | LOSS: 5.021974107450582e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 47/71 | LOSS: 5.002168611175269e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 48/71 | LOSS: 4.9917642785175655e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 49/71 | LOSS: 4.970909212715924e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 50/71 | LOSS: 4.9427070262383635e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 51/71 | LOSS: 4.921130054159361e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 52/71 | LOSS: 4.930749063755941e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 53/71 | LOSS: 4.9203907961041465e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 54/71 | LOSS: 4.901151032754569e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 55/71 | LOSS: 4.926183346957e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 56/71 | LOSS: 4.92530472129466e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 57/71 | LOSS: 4.948229980856985e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 58/71 | LOSS: 4.9391351278584695e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 59/71 | LOSS: 4.9668480149496945e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 60/71 | LOSS: 4.949554850696348e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 61/71 | LOSS: 4.990256908029316e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 62/71 | LOSS: 4.985349960406893e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 63/71 | LOSS: 4.998083799279129e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 64/71 | LOSS: 4.998683873334533e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 65/71 | LOSS: 5.010779311691515e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 66/71 | LOSS: 5.0349635954778175e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 67/71 | LOSS: 5.0293514912963844e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 68/71 | LOSS: 5.075370019266552e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 69/71 | LOSS: 5.084491843782806e-06\n",
      "TRAIN: EPOCH 567/1000 | BATCH 70/71 | LOSS: 5.0943005596347806e-06\n",
      "VAL: EPOCH 567/1000 | BATCH 0/8 | LOSS: 3.9694805309409276e-06\n",
      "VAL: EPOCH 567/1000 | BATCH 1/8 | LOSS: 4.329786861489993e-06\n",
      "VAL: EPOCH 567/1000 | BATCH 2/8 | LOSS: 4.466334000123122e-06\n",
      "VAL: EPOCH 567/1000 | BATCH 3/8 | LOSS: 4.471432816899323e-06\n",
      "VAL: EPOCH 567/1000 | BATCH 4/8 | LOSS: 4.510551480052527e-06\n",
      "VAL: EPOCH 567/1000 | BATCH 5/8 | LOSS: 4.408337645145366e-06\n",
      "VAL: EPOCH 567/1000 | BATCH 6/8 | LOSS: 4.293579357701154e-06\n",
      "VAL: EPOCH 567/1000 | BATCH 7/8 | LOSS: 4.144375111536647e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 0/71 | LOSS: 3.805276264756685e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 1/71 | LOSS: 4.596244934873539e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 2/71 | LOSS: 4.355100069612187e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 3/71 | LOSS: 4.59009197584237e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 4/71 | LOSS: 4.6316767111420635e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 5/71 | LOSS: 4.732812840302358e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 6/71 | LOSS: 4.588345616087151e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 7/71 | LOSS: 4.462888483658389e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 8/71 | LOSS: 4.411409438236862e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 9/71 | LOSS: 4.382337033348449e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 10/71 | LOSS: 4.370685205272324e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 11/71 | LOSS: 4.336309056422276e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 12/71 | LOSS: 4.359511859337755e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 13/71 | LOSS: 4.3661847257681075e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 14/71 | LOSS: 4.460270019990276e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 15/71 | LOSS: 4.484908046720193e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 16/71 | LOSS: 4.491377841772833e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 17/71 | LOSS: 4.482803016495988e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 18/71 | LOSS: 4.5137482127182395e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 19/71 | LOSS: 4.424891903909156e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 20/71 | LOSS: 4.429948841702537e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 21/71 | LOSS: 4.396602941431576e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 22/71 | LOSS: 4.3436502496858695e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 23/71 | LOSS: 4.372409023289947e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 24/71 | LOSS: 4.395219875732437e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 25/71 | LOSS: 4.399120003794311e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 26/71 | LOSS: 4.405457140684895e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 27/71 | LOSS: 4.496973734343815e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 28/71 | LOSS: 4.503961118361671e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 29/71 | LOSS: 4.553711460175691e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 30/71 | LOSS: 4.602956514352099e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 31/71 | LOSS: 4.598744439476832e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 32/71 | LOSS: 4.580730216151675e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 33/71 | LOSS: 4.622927690434153e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 34/71 | LOSS: 4.617821819660353e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 35/71 | LOSS: 4.632257299716811e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 36/71 | LOSS: 4.608637826439352e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 37/71 | LOSS: 4.60688358160903e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 38/71 | LOSS: 4.585987239690253e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 39/71 | LOSS: 4.592802963543363e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 40/71 | LOSS: 4.559016277686701e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 41/71 | LOSS: 4.526325526620938e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 42/71 | LOSS: 4.495338396017634e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 43/71 | LOSS: 4.4947759595908625e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 44/71 | LOSS: 4.459851308840573e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 45/71 | LOSS: 4.460649600569804e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 46/71 | LOSS: 4.456256216686365e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 47/71 | LOSS: 4.455815007039139e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 48/71 | LOSS: 4.460840941368476e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 49/71 | LOSS: 4.458972230168002e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 50/71 | LOSS: 4.4579121371427e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 51/71 | LOSS: 4.447463403231757e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 52/71 | LOSS: 4.439151758938437e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 53/71 | LOSS: 4.426257010823907e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 54/71 | LOSS: 4.417149685567975e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 55/71 | LOSS: 4.412685545470854e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 56/71 | LOSS: 4.410517699956504e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 57/71 | LOSS: 4.409405364483597e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 58/71 | LOSS: 4.396229145240178e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 59/71 | LOSS: 4.397181188172302e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 60/71 | LOSS: 4.391760288943588e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 61/71 | LOSS: 4.38045040687763e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 62/71 | LOSS: 4.3894044160374e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 63/71 | LOSS: 4.380031949580143e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 64/71 | LOSS: 4.382369588011687e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 65/71 | LOSS: 4.399359916147439e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 66/71 | LOSS: 4.391473180136805e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 67/71 | LOSS: 4.381667338293482e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 68/71 | LOSS: 4.375773131328785e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 69/71 | LOSS: 4.378492501798194e-06\n",
      "TRAIN: EPOCH 568/1000 | BATCH 70/71 | LOSS: 4.3496153863652325e-06\n",
      "VAL: EPOCH 568/1000 | BATCH 0/8 | LOSS: 4.936418463330483e-06\n",
      "VAL: EPOCH 568/1000 | BATCH 1/8 | LOSS: 4.963392939316691e-06\n",
      "VAL: EPOCH 568/1000 | BATCH 2/8 | LOSS: 4.946559026090351e-06\n",
      "VAL: EPOCH 568/1000 | BATCH 3/8 | LOSS: 4.848694857173541e-06\n",
      "VAL: EPOCH 568/1000 | BATCH 4/8 | LOSS: 4.971773159923032e-06\n",
      "VAL: EPOCH 568/1000 | BATCH 5/8 | LOSS: 4.8990248918319894e-06\n",
      "VAL: EPOCH 568/1000 | BATCH 6/8 | LOSS: 4.765431705371677e-06\n",
      "VAL: EPOCH 568/1000 | BATCH 7/8 | LOSS: 4.624393909580249e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 0/71 | LOSS: 4.714884198619984e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 1/71 | LOSS: 4.433991307450924e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 2/71 | LOSS: 4.216460486835179e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 3/71 | LOSS: 4.071425792062655e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 4/71 | LOSS: 4.085660475539044e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 5/71 | LOSS: 4.077764818551562e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 6/71 | LOSS: 4.047673363467246e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 7/71 | LOSS: 4.017929683186594e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 8/71 | LOSS: 3.998546592103796e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 9/71 | LOSS: 3.92406034279702e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 10/71 | LOSS: 4.033985760543146e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 11/71 | LOSS: 4.0796210024988495e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 12/71 | LOSS: 4.116580997176173e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 13/71 | LOSS: 4.108912159089024e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 14/71 | LOSS: 4.1557926882281515e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 15/71 | LOSS: 4.120513608540932e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 16/71 | LOSS: 4.167584098896066e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 17/71 | LOSS: 4.168810973068402e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 18/71 | LOSS: 4.222373072001287e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 19/71 | LOSS: 4.197137036499043e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 20/71 | LOSS: 4.161519481862169e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 21/71 | LOSS: 4.15042186639105e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 22/71 | LOSS: 4.21641465950613e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 23/71 | LOSS: 4.216057230147878e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 24/71 | LOSS: 4.266786681910162e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 25/71 | LOSS: 4.259801410346252e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 26/71 | LOSS: 4.303351867444387e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 27/71 | LOSS: 4.308453713162765e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 28/71 | LOSS: 4.379113361523948e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 29/71 | LOSS: 4.442479462340998e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 30/71 | LOSS: 4.488600061156303e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 31/71 | LOSS: 4.548327687814435e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 32/71 | LOSS: 4.574898791058028e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 33/71 | LOSS: 4.581166776648281e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 34/71 | LOSS: 4.590004787782424e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 35/71 | LOSS: 4.611733042262737e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 36/71 | LOSS: 4.618455139623964e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 37/71 | LOSS: 4.64381388323282e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 38/71 | LOSS: 4.631932236010564e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 39/71 | LOSS: 4.649353849117688e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 40/71 | LOSS: 4.634399535306573e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 41/71 | LOSS: 4.642179955486167e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 42/71 | LOSS: 4.6970511173344144e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 43/71 | LOSS: 4.700291619676798e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 44/71 | LOSS: 4.6970638676510945e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 45/71 | LOSS: 4.699557260447245e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 46/71 | LOSS: 4.745852691562601e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 47/71 | LOSS: 4.75067848526578e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 48/71 | LOSS: 4.726877579964728e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 49/71 | LOSS: 4.744587417917501e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 50/71 | LOSS: 4.737838494025602e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 51/71 | LOSS: 4.755539193341135e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 52/71 | LOSS: 4.7811298407155706e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 53/71 | LOSS: 4.763775488451453e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 54/71 | LOSS: 4.76214278767822e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 55/71 | LOSS: 4.730851730170928e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 56/71 | LOSS: 4.737108838526183e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 57/71 | LOSS: 4.730928724021741e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 58/71 | LOSS: 4.7033729823836485e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 59/71 | LOSS: 4.73202439555583e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 60/71 | LOSS: 4.730916275473676e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 61/71 | LOSS: 4.738179390803834e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 62/71 | LOSS: 4.714169151719987e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 63/71 | LOSS: 4.723100413883685e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 64/71 | LOSS: 4.7506916521692455e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 65/71 | LOSS: 4.7689575136577e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 66/71 | LOSS: 4.7680032740070386e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 67/71 | LOSS: 4.769787307502745e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 68/71 | LOSS: 4.774999099580505e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 69/71 | LOSS: 4.7555794123711945e-06\n",
      "TRAIN: EPOCH 569/1000 | BATCH 70/71 | LOSS: 4.815063762244265e-06\n",
      "VAL: EPOCH 569/1000 | BATCH 0/8 | LOSS: 3.984942395618418e-06\n",
      "VAL: EPOCH 569/1000 | BATCH 1/8 | LOSS: 4.322195536587969e-06\n",
      "VAL: EPOCH 569/1000 | BATCH 2/8 | LOSS: 4.401226609237104e-06\n",
      "VAL: EPOCH 569/1000 | BATCH 3/8 | LOSS: 4.425502424965089e-06\n",
      "VAL: EPOCH 569/1000 | BATCH 4/8 | LOSS: 4.47457687187125e-06\n",
      "VAL: EPOCH 569/1000 | BATCH 5/8 | LOSS: 4.5727215365332086e-06\n",
      "VAL: EPOCH 569/1000 | BATCH 6/8 | LOSS: 4.458309279341068e-06\n",
      "VAL: EPOCH 569/1000 | BATCH 7/8 | LOSS: 4.36136096482187e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 0/71 | LOSS: 4.173152774455957e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 1/71 | LOSS: 4.826494887311128e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 2/71 | LOSS: 5.354104511449502e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 3/71 | LOSS: 5.102488103148062e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 4/71 | LOSS: 5.3419504183693786e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 5/71 | LOSS: 5.256941221887246e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 6/71 | LOSS: 5.276602678350173e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 7/71 | LOSS: 5.148771492713422e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 8/71 | LOSS: 5.322288011989764e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 9/71 | LOSS: 5.301227338350145e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 10/71 | LOSS: 5.090431469315345e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 11/71 | LOSS: 5.043360980986715e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 12/71 | LOSS: 5.067465630977845e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 13/71 | LOSS: 5.043512179067225e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 14/71 | LOSS: 5.05142409868616e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 15/71 | LOSS: 4.961317472407245e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 16/71 | LOSS: 4.950156193551854e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 17/71 | LOSS: 4.904541330890626e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 18/71 | LOSS: 4.938825464992796e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 19/71 | LOSS: 4.854071130466764e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 20/71 | LOSS: 4.865769347989477e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 21/71 | LOSS: 4.829281413714687e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 22/71 | LOSS: 4.781703864864539e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 23/71 | LOSS: 4.786493529233364e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 24/71 | LOSS: 4.772714291902958e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 25/71 | LOSS: 4.7417863173065525e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 26/71 | LOSS: 4.7156437614359205e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 27/71 | LOSS: 4.665426599461041e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 28/71 | LOSS: 4.618503092795686e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 29/71 | LOSS: 4.595408965239282e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 30/71 | LOSS: 4.5703114122466835e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 31/71 | LOSS: 4.551302417610259e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 32/71 | LOSS: 4.536419282885618e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 33/71 | LOSS: 4.520441353526942e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 34/71 | LOSS: 4.48360345736936e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 35/71 | LOSS: 4.458836725815571e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 36/71 | LOSS: 4.47570707021097e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 37/71 | LOSS: 4.449261113174878e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 38/71 | LOSS: 4.459784926998859e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 39/71 | LOSS: 4.445555055099248e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 40/71 | LOSS: 4.4350906758087905e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 41/71 | LOSS: 4.416167750302217e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 42/71 | LOSS: 4.3953446620011965e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 43/71 | LOSS: 4.387353633526138e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 44/71 | LOSS: 4.378680331582372e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 45/71 | LOSS: 4.366546506943173e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 46/71 | LOSS: 4.345227415932583e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 47/71 | LOSS: 4.316860739095318e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 48/71 | LOSS: 4.337044188383627e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 49/71 | LOSS: 4.346050341155205e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 50/71 | LOSS: 4.377018334636494e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 51/71 | LOSS: 4.366073763041921e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 52/71 | LOSS: 4.382811865517002e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 53/71 | LOSS: 4.382566418017733e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 54/71 | LOSS: 4.3879923189458415e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 55/71 | LOSS: 4.366891423452606e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 56/71 | LOSS: 4.3771899367640255e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 57/71 | LOSS: 4.364278095573386e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 58/71 | LOSS: 4.348514024105359e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 59/71 | LOSS: 4.337309417223878e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 60/71 | LOSS: 4.3355599573856795e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 61/71 | LOSS: 4.333382310417825e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 62/71 | LOSS: 4.32184509239931e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 63/71 | LOSS: 4.34029019302784e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 64/71 | LOSS: 4.3442985945428234e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 65/71 | LOSS: 4.36785059622652e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 66/71 | LOSS: 4.39303366942683e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 67/71 | LOSS: 4.379781113156616e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 68/71 | LOSS: 4.3725439769806815e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 69/71 | LOSS: 4.370795461129871e-06\n",
      "TRAIN: EPOCH 570/1000 | BATCH 70/71 | LOSS: 4.401672313947593e-06\n",
      "VAL: EPOCH 570/1000 | BATCH 0/8 | LOSS: 4.600601641868707e-06\n",
      "VAL: EPOCH 570/1000 | BATCH 1/8 | LOSS: 5.156201950740069e-06\n",
      "VAL: EPOCH 570/1000 | BATCH 2/8 | LOSS: 5.273391252558213e-06\n",
      "VAL: EPOCH 570/1000 | BATCH 3/8 | LOSS: 5.288087550070486e-06\n",
      "VAL: EPOCH 570/1000 | BATCH 4/8 | LOSS: 5.2614987907873e-06\n",
      "VAL: EPOCH 570/1000 | BATCH 5/8 | LOSS: 5.221944396301599e-06\n",
      "VAL: EPOCH 570/1000 | BATCH 6/8 | LOSS: 5.045669175680294e-06\n",
      "VAL: EPOCH 570/1000 | BATCH 7/8 | LOSS: 4.946584056142456e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 0/71 | LOSS: 6.370546543621458e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 1/71 | LOSS: 5.7270242450613296e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 2/71 | LOSS: 5.325984147930285e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 3/71 | LOSS: 5.031191676607705e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 4/71 | LOSS: 5.1620539124996865e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 5/71 | LOSS: 5.003863179808832e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 6/71 | LOSS: 4.942669030632325e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 7/71 | LOSS: 4.968837004071247e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 8/71 | LOSS: 4.85120103298363e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 9/71 | LOSS: 4.7376627207995625e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 10/71 | LOSS: 4.642835681972263e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 11/71 | LOSS: 4.598754458129406e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 12/71 | LOSS: 4.5603494352410334e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 13/71 | LOSS: 4.541621527615851e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 14/71 | LOSS: 4.516832298880521e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 15/71 | LOSS: 4.510387526579507e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 16/71 | LOSS: 4.42970538054343e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 17/71 | LOSS: 4.456386869201702e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 18/71 | LOSS: 4.416549185598821e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 19/71 | LOSS: 4.422875895215838e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 20/71 | LOSS: 4.374605309532767e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 21/71 | LOSS: 4.41930656465543e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 22/71 | LOSS: 4.419524845181276e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 23/71 | LOSS: 4.425611109581951e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 24/71 | LOSS: 4.411071186041226e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 25/71 | LOSS: 4.4102099799112275e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 26/71 | LOSS: 4.380485094130833e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 27/71 | LOSS: 4.365057439567213e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 28/71 | LOSS: 4.3776310811718475e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 29/71 | LOSS: 4.355139147567874e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 30/71 | LOSS: 4.3610618611696294e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 31/71 | LOSS: 4.347432494000714e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 32/71 | LOSS: 4.3770504072943535e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 33/71 | LOSS: 4.358846862327054e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 34/71 | LOSS: 4.400815565272102e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 35/71 | LOSS: 4.3787726440314145e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 36/71 | LOSS: 4.449703338478513e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 37/71 | LOSS: 4.427271882808495e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 38/71 | LOSS: 4.478327232740971e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 39/71 | LOSS: 4.546434200847216e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 40/71 | LOSS: 4.5974927616043715e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 41/71 | LOSS: 4.619445024088366e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 42/71 | LOSS: 4.6098560307105205e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 43/71 | LOSS: 4.730060301095694e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 44/71 | LOSS: 4.732904871060681e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 45/71 | LOSS: 4.74514876674695e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 46/71 | LOSS: 4.763909712292321e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 47/71 | LOSS: 4.77499979467666e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 48/71 | LOSS: 4.794262914791554e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 49/71 | LOSS: 4.826283834518108e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 50/71 | LOSS: 4.8601007967911705e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 51/71 | LOSS: 4.83531105233991e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 52/71 | LOSS: 4.875496128044928e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 53/71 | LOSS: 4.864615107180725e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 54/71 | LOSS: 4.8906356947885465e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 55/71 | LOSS: 4.89007882720216e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 56/71 | LOSS: 4.903746928488417e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 57/71 | LOSS: 4.891610383938e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 58/71 | LOSS: 4.903867181645306e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 59/71 | LOSS: 4.879495440945902e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 60/71 | LOSS: 4.873587319006682e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 61/71 | LOSS: 4.855888582530879e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 62/71 | LOSS: 4.865537903242512e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 63/71 | LOSS: 4.858041947386482e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 64/71 | LOSS: 4.830694856886672e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 65/71 | LOSS: 4.838157265525793e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 66/71 | LOSS: 4.849787908607317e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 67/71 | LOSS: 4.849410171412755e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 68/71 | LOSS: 4.82868826889055e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 69/71 | LOSS: 4.861170860489698e-06\n",
      "TRAIN: EPOCH 571/1000 | BATCH 70/71 | LOSS: 4.83535426559376e-06\n",
      "VAL: EPOCH 571/1000 | BATCH 0/8 | LOSS: 4.2507945181569085e-06\n",
      "VAL: EPOCH 571/1000 | BATCH 1/8 | LOSS: 4.347984486230416e-06\n",
      "VAL: EPOCH 571/1000 | BATCH 2/8 | LOSS: 4.3673414135507e-06\n",
      "VAL: EPOCH 571/1000 | BATCH 3/8 | LOSS: 4.344495664554415e-06\n",
      "VAL: EPOCH 571/1000 | BATCH 4/8 | LOSS: 4.377358891360927e-06\n",
      "VAL: EPOCH 571/1000 | BATCH 5/8 | LOSS: 4.488354610051222e-06\n",
      "VAL: EPOCH 571/1000 | BATCH 6/8 | LOSS: 4.376336456906367e-06\n",
      "VAL: EPOCH 571/1000 | BATCH 7/8 | LOSS: 4.272585186981814e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 0/71 | LOSS: 4.515454293141374e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 1/71 | LOSS: 4.6215325255616335e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 2/71 | LOSS: 4.7393938681731624e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 3/71 | LOSS: 4.306371465645498e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 4/71 | LOSS: 4.368412555777468e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 5/71 | LOSS: 4.335487119533354e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 6/71 | LOSS: 4.207816281060721e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 7/71 | LOSS: 4.1763547073969676e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 8/71 | LOSS: 4.1811593089530815e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 9/71 | LOSS: 4.339687939136638e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 10/71 | LOSS: 4.296193393119293e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 11/71 | LOSS: 4.321436601154953e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 12/71 | LOSS: 4.314012270557354e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 13/71 | LOSS: 4.281307838027715e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 14/71 | LOSS: 4.235996281446812e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 15/71 | LOSS: 4.234927416746359e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 16/71 | LOSS: 4.21431278642986e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 17/71 | LOSS: 4.205162491012339e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 18/71 | LOSS: 4.1647022523025765e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 19/71 | LOSS: 4.103932963062107e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 20/71 | LOSS: 4.064350865259517e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 21/71 | LOSS: 4.062966116758947e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 22/71 | LOSS: 4.085312579975478e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 23/71 | LOSS: 4.044093420437396e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 24/71 | LOSS: 4.06565085540933e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 25/71 | LOSS: 4.039078161300635e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 26/71 | LOSS: 4.04012470661891e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 27/71 | LOSS: 4.046974059487443e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 28/71 | LOSS: 4.1157048865992195e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 29/71 | LOSS: 4.12919842650202e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 30/71 | LOSS: 4.188567169377723e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 31/71 | LOSS: 4.187343122907805e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 32/71 | LOSS: 4.167996008398019e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 33/71 | LOSS: 4.143882860904234e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 34/71 | LOSS: 4.120780243153734e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 35/71 | LOSS: 4.115323621236813e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 36/71 | LOSS: 4.126512663247381e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 37/71 | LOSS: 4.137551220657387e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 38/71 | LOSS: 4.135214176597238e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 39/71 | LOSS: 4.1905376235717995e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 40/71 | LOSS: 4.201126674205425e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 41/71 | LOSS: 4.228040113827904e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 42/71 | LOSS: 4.223213694743652e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 43/71 | LOSS: 4.220491896169485e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 44/71 | LOSS: 4.2257395004223024e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 45/71 | LOSS: 4.207530033138186e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 46/71 | LOSS: 4.201892923137688e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 47/71 | LOSS: 4.231559917874013e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 48/71 | LOSS: 4.243072045046945e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 49/71 | LOSS: 4.270371910024551e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 50/71 | LOSS: 4.319889980893897e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 51/71 | LOSS: 4.317092318090615e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 52/71 | LOSS: 4.336734208372259e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 53/71 | LOSS: 4.333245139302259e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 54/71 | LOSS: 4.332675912750843e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 55/71 | LOSS: 4.354997007080133e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 56/71 | LOSS: 4.393782299366808e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 57/71 | LOSS: 4.417965712867766e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 58/71 | LOSS: 4.418466171231002e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 59/71 | LOSS: 4.4777014106027005e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 60/71 | LOSS: 4.475126574941926e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 61/71 | LOSS: 4.526970755133304e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 62/71 | LOSS: 4.543327858776874e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 63/71 | LOSS: 4.5592011730377635e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 64/71 | LOSS: 4.54712091176099e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 65/71 | LOSS: 4.566047554169817e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 66/71 | LOSS: 4.557151441986207e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 67/71 | LOSS: 4.552163224669878e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 68/71 | LOSS: 4.561477196504079e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 69/71 | LOSS: 4.550646057006296e-06\n",
      "TRAIN: EPOCH 572/1000 | BATCH 70/71 | LOSS: 4.546583091134483e-06\n",
      "VAL: EPOCH 572/1000 | BATCH 0/8 | LOSS: 4.0003137655730825e-06\n",
      "VAL: EPOCH 572/1000 | BATCH 1/8 | LOSS: 4.267920303391293e-06\n",
      "VAL: EPOCH 572/1000 | BATCH 2/8 | LOSS: 4.501582983114834e-06\n",
      "VAL: EPOCH 572/1000 | BATCH 3/8 | LOSS: 4.447917831384984e-06\n",
      "VAL: EPOCH 572/1000 | BATCH 4/8 | LOSS: 4.60030087197083e-06\n",
      "VAL: EPOCH 572/1000 | BATCH 5/8 | LOSS: 4.61581604819609e-06\n",
      "VAL: EPOCH 572/1000 | BATCH 6/8 | LOSS: 4.550403446046403e-06\n",
      "VAL: EPOCH 572/1000 | BATCH 7/8 | LOSS: 4.459668332401634e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 0/71 | LOSS: 4.412011548993178e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 1/71 | LOSS: 4.704503226093948e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 2/71 | LOSS: 4.113564273211523e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 3/71 | LOSS: 4.01033292973807e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 4/71 | LOSS: 4.051552014061599e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 5/71 | LOSS: 4.146570859120402e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 6/71 | LOSS: 4.080302492833912e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 7/71 | LOSS: 4.006725532690325e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 8/71 | LOSS: 3.9913147197189064e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 9/71 | LOSS: 3.956684554395906e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 10/71 | LOSS: 3.965325194340866e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 11/71 | LOSS: 3.939678398031295e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 12/71 | LOSS: 3.971349892358726e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 13/71 | LOSS: 3.951607043615825e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 14/71 | LOSS: 3.986993851867737e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 15/71 | LOSS: 4.071377048830982e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 16/71 | LOSS: 4.039711792061366e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 17/71 | LOSS: 4.0277332623696165e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 18/71 | LOSS: 4.0634298257875916e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 19/71 | LOSS: 4.080587791577272e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 20/71 | LOSS: 4.129496357477148e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 21/71 | LOSS: 4.102422012279434e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 22/71 | LOSS: 4.121634664561893e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 23/71 | LOSS: 4.092307932523909e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 24/71 | LOSS: 4.124180732105742e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 25/71 | LOSS: 4.089151415288283e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 26/71 | LOSS: 4.092708869157596e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 27/71 | LOSS: 4.183607830197746e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 28/71 | LOSS: 4.1372398943073265e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 29/71 | LOSS: 4.140208276718719e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 30/71 | LOSS: 4.178628556004579e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 31/71 | LOSS: 4.17393290064183e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 32/71 | LOSS: 4.159174454109619e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 33/71 | LOSS: 4.157371871074515e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 34/71 | LOSS: 4.205497115045936e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 35/71 | LOSS: 4.190791281012531e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 36/71 | LOSS: 4.204229907299049e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 37/71 | LOSS: 4.2034679740697216e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 38/71 | LOSS: 4.180775745109527e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 39/71 | LOSS: 4.21581173100094e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 40/71 | LOSS: 4.213491920159956e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 41/71 | LOSS: 4.204918896210161e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 42/71 | LOSS: 4.218653594064161e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 43/71 | LOSS: 4.201831907060048e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 44/71 | LOSS: 4.1897994833561825e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 45/71 | LOSS: 4.188556970249287e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 46/71 | LOSS: 4.209444027087847e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 47/71 | LOSS: 4.195165852631059e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 48/71 | LOSS: 4.203670952698971e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 49/71 | LOSS: 4.21636289502203e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 50/71 | LOSS: 4.241906606016439e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 51/71 | LOSS: 4.237249570975949e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 52/71 | LOSS: 4.230305258492733e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 53/71 | LOSS: 4.247628171301416e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 54/71 | LOSS: 4.258082208252745e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 55/71 | LOSS: 4.247249129158652e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 56/71 | LOSS: 4.249598867701072e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 57/71 | LOSS: 4.255088767955573e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 58/71 | LOSS: 4.232521975717322e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 59/71 | LOSS: 4.227030058245873e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 60/71 | LOSS: 4.214562582614026e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 61/71 | LOSS: 4.269096668776324e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 62/71 | LOSS: 4.276922655186527e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 63/71 | LOSS: 4.2853823742916575e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 64/71 | LOSS: 4.27543399242635e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 65/71 | LOSS: 4.293574506570594e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 66/71 | LOSS: 4.293438610487223e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 67/71 | LOSS: 4.2912333844255685e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 68/71 | LOSS: 4.279221588992674e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 69/71 | LOSS: 4.263791648944399e-06\n",
      "TRAIN: EPOCH 573/1000 | BATCH 70/71 | LOSS: 4.2682051705821615e-06\n",
      "VAL: EPOCH 573/1000 | BATCH 0/8 | LOSS: 3.6105177514400566e-06\n",
      "VAL: EPOCH 573/1000 | BATCH 1/8 | LOSS: 3.992100459981884e-06\n",
      "VAL: EPOCH 573/1000 | BATCH 2/8 | LOSS: 4.051732730658841e-06\n",
      "VAL: EPOCH 573/1000 | BATCH 3/8 | LOSS: 4.017409708012565e-06\n",
      "VAL: EPOCH 573/1000 | BATCH 4/8 | LOSS: 4.079410837221076e-06\n",
      "VAL: EPOCH 573/1000 | BATCH 5/8 | LOSS: 4.108088546672661e-06\n",
      "VAL: EPOCH 573/1000 | BATCH 6/8 | LOSS: 3.972292623594902e-06\n",
      "VAL: EPOCH 573/1000 | BATCH 7/8 | LOSS: 3.880312391402185e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 0/71 | LOSS: 4.550706762529444e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 1/71 | LOSS: 4.4930156946065836e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 2/71 | LOSS: 4.222435033322351e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 3/71 | LOSS: 4.211892644434556e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 4/71 | LOSS: 4.2164512251474665e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 5/71 | LOSS: 4.46623179565601e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 6/71 | LOSS: 4.421760650075157e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 7/71 | LOSS: 4.563454552908297e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 8/71 | LOSS: 4.522762411498762e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 9/71 | LOSS: 4.6619814611403855e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 10/71 | LOSS: 4.619010248709608e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 11/71 | LOSS: 4.5434610266662885e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 12/71 | LOSS: 4.4655858066387455e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 13/71 | LOSS: 4.475963367310344e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 14/71 | LOSS: 4.490961312815974e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 15/71 | LOSS: 4.479770083776202e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 16/71 | LOSS: 4.493007281780592e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 17/71 | LOSS: 4.477992547435861e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 18/71 | LOSS: 4.4189601004817585e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 19/71 | LOSS: 4.442516126346163e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 20/71 | LOSS: 4.4698757565562295e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 21/71 | LOSS: 4.423149396808929e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 22/71 | LOSS: 4.42824530775555e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 23/71 | LOSS: 4.430077098049878e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 24/71 | LOSS: 4.397696948217344e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 25/71 | LOSS: 4.370156244266792e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 26/71 | LOSS: 4.312182825203679e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 27/71 | LOSS: 4.30522348031965e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 28/71 | LOSS: 4.272697032288776e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 29/71 | LOSS: 4.277736434232793e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 30/71 | LOSS: 4.276521952218196e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 31/71 | LOSS: 4.233593742242192e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 32/71 | LOSS: 4.260684914752898e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 33/71 | LOSS: 4.2441412032905435e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 34/71 | LOSS: 4.256181156441536e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 35/71 | LOSS: 4.257412216348813e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 36/71 | LOSS: 4.249234525805396e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 37/71 | LOSS: 4.245166122162022e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 38/71 | LOSS: 4.298074775131401e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 39/71 | LOSS: 4.299332772461639e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 40/71 | LOSS: 4.299696961314156e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 41/71 | LOSS: 4.331077878391841e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 42/71 | LOSS: 4.3345094973331065e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 43/71 | LOSS: 4.343453331950348e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 44/71 | LOSS: 4.352749177390554e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 45/71 | LOSS: 4.35601742023921e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 46/71 | LOSS: 4.348378539323173e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 47/71 | LOSS: 4.355493369227285e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 48/71 | LOSS: 4.3659108773774315e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 49/71 | LOSS: 4.386625805636868e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 50/71 | LOSS: 4.416134805639968e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 51/71 | LOSS: 4.409875215899844e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 52/71 | LOSS: 4.414704820209288e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 53/71 | LOSS: 4.422846359375399e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 54/71 | LOSS: 4.449502723053394e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 55/71 | LOSS: 4.457899982363804e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 56/71 | LOSS: 4.478811772793977e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 57/71 | LOSS: 4.462542921734486e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 58/71 | LOSS: 4.50894008743921e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 59/71 | LOSS: 4.516685866443974e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 60/71 | LOSS: 4.529919288259288e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 61/71 | LOSS: 4.535903955119495e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 62/71 | LOSS: 4.576033789918357e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 63/71 | LOSS: 4.5834642428133066e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 64/71 | LOSS: 4.578751098121687e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 65/71 | LOSS: 4.57565364754511e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 66/71 | LOSS: 4.566327194893279e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 67/71 | LOSS: 4.589983481818147e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 68/71 | LOSS: 4.581900483295296e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 69/71 | LOSS: 4.576362571892137e-06\n",
      "TRAIN: EPOCH 574/1000 | BATCH 70/71 | LOSS: 4.603505988303419e-06\n",
      "VAL: EPOCH 574/1000 | BATCH 0/8 | LOSS: 4.34322055298253e-06\n",
      "VAL: EPOCH 574/1000 | BATCH 1/8 | LOSS: 4.494065478866105e-06\n",
      "VAL: EPOCH 574/1000 | BATCH 2/8 | LOSS: 4.4875528146803845e-06\n",
      "VAL: EPOCH 574/1000 | BATCH 3/8 | LOSS: 4.40916824118176e-06\n",
      "VAL: EPOCH 574/1000 | BATCH 4/8 | LOSS: 4.4263797462917864e-06\n",
      "VAL: EPOCH 574/1000 | BATCH 5/8 | LOSS: 4.389744844957022e-06\n",
      "VAL: EPOCH 574/1000 | BATCH 6/8 | LOSS: 4.210663519188529e-06\n",
      "VAL: EPOCH 574/1000 | BATCH 7/8 | LOSS: 4.059375015685873e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 0/71 | LOSS: 4.6591085265390575e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 1/71 | LOSS: 4.72749661639682e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 2/71 | LOSS: 4.548614621550466e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 3/71 | LOSS: 4.944831516695558e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 4/71 | LOSS: 5.16599566253717e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 5/71 | LOSS: 5.244697604211979e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 6/71 | LOSS: 5.373054851328821e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 7/71 | LOSS: 5.392437515183701e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 8/71 | LOSS: 5.393463602053493e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 9/71 | LOSS: 5.670435393767548e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 10/71 | LOSS: 5.546632192735243e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 11/71 | LOSS: 5.497274097857978e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 12/71 | LOSS: 5.378705701822093e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 13/71 | LOSS: 5.366252090815189e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 14/71 | LOSS: 5.30518518644385e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 15/71 | LOSS: 5.282574022658082e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 16/71 | LOSS: 5.173916901292293e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 17/71 | LOSS: 5.200912002894458e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 18/71 | LOSS: 5.176166699751966e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 19/71 | LOSS: 5.2207730277586965e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 20/71 | LOSS: 5.137110076161445e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 21/71 | LOSS: 5.185027661851067e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 22/71 | LOSS: 5.1302667602810645e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 23/71 | LOSS: 5.147068956527316e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 24/71 | LOSS: 5.1052521484962195e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 25/71 | LOSS: 5.069755563734081e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 26/71 | LOSS: 5.036372958447491e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 27/71 | LOSS: 5.001903722196792e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 28/71 | LOSS: 4.971222504787749e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 29/71 | LOSS: 4.944952373383179e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 30/71 | LOSS: 4.9178456280177696e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 31/71 | LOSS: 4.902526569594556e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 32/71 | LOSS: 4.860026668514668e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 33/71 | LOSS: 4.810647998751579e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 34/71 | LOSS: 4.79244537408314e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 35/71 | LOSS: 4.763687507470928e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 36/71 | LOSS: 4.788698124930552e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 37/71 | LOSS: 4.7653483977084855e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 38/71 | LOSS: 4.750244862897e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 39/71 | LOSS: 4.718661375591182e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 40/71 | LOSS: 4.733172184986448e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 41/71 | LOSS: 4.707615641477571e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 42/71 | LOSS: 4.731095111500783e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 43/71 | LOSS: 4.73056555640803e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 44/71 | LOSS: 4.7313209860375435e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 45/71 | LOSS: 4.744561915290631e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 46/71 | LOSS: 4.722740225182125e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 47/71 | LOSS: 4.733880307602097e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 48/71 | LOSS: 4.714595044268728e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 49/71 | LOSS: 4.699369756053784e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 50/71 | LOSS: 4.716215743446453e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 51/71 | LOSS: 4.704958987881232e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 52/71 | LOSS: 4.698569236492369e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 53/71 | LOSS: 4.7128792680774095e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 54/71 | LOSS: 4.712804175968896e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 55/71 | LOSS: 4.701171121723746e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 56/71 | LOSS: 4.6990969821579936e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 57/71 | LOSS: 4.684180919804053e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 58/71 | LOSS: 4.672723685347345e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 59/71 | LOSS: 4.6625013510492865e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 60/71 | LOSS: 4.666097128556728e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 61/71 | LOSS: 4.665502112930558e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 62/71 | LOSS: 4.673304280754925e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 63/71 | LOSS: 4.682347935158759e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 64/71 | LOSS: 4.6783045036136174e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 65/71 | LOSS: 4.685907532058826e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 66/71 | LOSS: 4.690565837431202e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 67/71 | LOSS: 4.701529231918839e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 68/71 | LOSS: 4.69575127720853e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 69/71 | LOSS: 4.7062154733014595e-06\n",
      "TRAIN: EPOCH 575/1000 | BATCH 70/71 | LOSS: 4.715945412409754e-06\n",
      "VAL: EPOCH 575/1000 | BATCH 0/8 | LOSS: 5.7543029470252804e-06\n",
      "VAL: EPOCH 575/1000 | BATCH 1/8 | LOSS: 5.93524964642711e-06\n",
      "VAL: EPOCH 575/1000 | BATCH 2/8 | LOSS: 5.96526994437833e-06\n",
      "VAL: EPOCH 575/1000 | BATCH 3/8 | LOSS: 5.860421424586093e-06\n",
      "VAL: EPOCH 575/1000 | BATCH 4/8 | LOSS: 6.023756122885971e-06\n",
      "VAL: EPOCH 575/1000 | BATCH 5/8 | LOSS: 5.824007227298959e-06\n",
      "VAL: EPOCH 575/1000 | BATCH 6/8 | LOSS: 5.742342961769152e-06\n",
      "VAL: EPOCH 575/1000 | BATCH 7/8 | LOSS: 5.599827545665903e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 0/71 | LOSS: 5.544162377191242e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 1/71 | LOSS: 5.6357482662861e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 2/71 | LOSS: 5.8927585087076295e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 3/71 | LOSS: 5.65752077363868e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 4/71 | LOSS: 5.995402443659259e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 5/71 | LOSS: 5.88398643230903e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 6/71 | LOSS: 5.768320339224634e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 7/71 | LOSS: 5.85686086651549e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 8/71 | LOSS: 5.831826304832551e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 9/71 | LOSS: 5.9751556364062704e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 10/71 | LOSS: 5.781576295372691e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 11/71 | LOSS: 5.960101134405704e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 12/71 | LOSS: 5.822964759807712e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 13/71 | LOSS: 5.864375972513309e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 14/71 | LOSS: 5.827012864756398e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 15/71 | LOSS: 5.74133895270279e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 16/71 | LOSS: 5.623344457169023e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 17/71 | LOSS: 5.585734697888256e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 18/71 | LOSS: 5.503625791454014e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 19/71 | LOSS: 5.448549904940591e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 20/71 | LOSS: 5.4447990632546685e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 21/71 | LOSS: 5.429130004069355e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 22/71 | LOSS: 5.365176587909683e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 23/71 | LOSS: 5.3254832437232835e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 24/71 | LOSS: 5.2703625260619446e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 25/71 | LOSS: 5.257397438421881e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 26/71 | LOSS: 5.217216716523075e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 27/71 | LOSS: 5.228941550480418e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 28/71 | LOSS: 5.158980049349464e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 29/71 | LOSS: 5.131099472540275e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 30/71 | LOSS: 5.078204961342459e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 31/71 | LOSS: 5.057902527028091e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 32/71 | LOSS: 5.036695118619581e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 33/71 | LOSS: 5.065080599439063e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 34/71 | LOSS: 5.05566382084258e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 35/71 | LOSS: 4.996029986159556e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 36/71 | LOSS: 4.964653732492258e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 37/71 | LOSS: 4.962772167298106e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 38/71 | LOSS: 4.919858960979932e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 39/71 | LOSS: 4.868127797408306e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 40/71 | LOSS: 4.851497151765038e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 41/71 | LOSS: 4.810299423124456e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 42/71 | LOSS: 4.8104852847372544e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 43/71 | LOSS: 4.806156565254324e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 44/71 | LOSS: 4.773319910277173e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 45/71 | LOSS: 4.777579751253538e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 46/71 | LOSS: 4.760115585823053e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 47/71 | LOSS: 4.7437541752515244e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 48/71 | LOSS: 4.726629793701565e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 49/71 | LOSS: 4.748071305584745e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 50/71 | LOSS: 4.771783925902769e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 51/71 | LOSS: 4.777864983924911e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 52/71 | LOSS: 4.757875438303498e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 53/71 | LOSS: 4.7812839562134795e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 54/71 | LOSS: 4.760422623142156e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 55/71 | LOSS: 4.749192620628102e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 56/71 | LOSS: 4.7404778901213915e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 57/71 | LOSS: 4.718286257637624e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 58/71 | LOSS: 4.702649621938659e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 59/71 | LOSS: 4.681618160399618e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 60/71 | LOSS: 4.6784869400504305e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 61/71 | LOSS: 4.670725241985272e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 62/71 | LOSS: 4.688100023572804e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 63/71 | LOSS: 4.686961457167627e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 64/71 | LOSS: 4.6804940036683825e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 65/71 | LOSS: 4.684543961792025e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 66/71 | LOSS: 4.6694431555589465e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 67/71 | LOSS: 4.662780758521608e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 68/71 | LOSS: 4.644618295482149e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 69/71 | LOSS: 4.6421590436044164e-06\n",
      "TRAIN: EPOCH 576/1000 | BATCH 70/71 | LOSS: 4.642386848374858e-06\n",
      "VAL: EPOCH 576/1000 | BATCH 0/8 | LOSS: 3.775911181946867e-06\n",
      "VAL: EPOCH 576/1000 | BATCH 1/8 | LOSS: 4.030726927339856e-06\n",
      "VAL: EPOCH 576/1000 | BATCH 2/8 | LOSS: 4.15956787946925e-06\n",
      "VAL: EPOCH 576/1000 | BATCH 3/8 | LOSS: 4.13260698906015e-06\n",
      "VAL: EPOCH 576/1000 | BATCH 4/8 | LOSS: 4.17769665546075e-06\n",
      "VAL: EPOCH 576/1000 | BATCH 5/8 | LOSS: 4.195035103293776e-06\n",
      "VAL: EPOCH 576/1000 | BATCH 6/8 | LOSS: 4.064415764202879e-06\n",
      "VAL: EPOCH 576/1000 | BATCH 7/8 | LOSS: 3.932946441409513e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 0/71 | LOSS: 3.449188170634443e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 1/71 | LOSS: 3.306534836156061e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 2/71 | LOSS: 3.4999570743821096e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 3/71 | LOSS: 3.5012137118428655e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 4/71 | LOSS: 3.448142706474755e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 5/71 | LOSS: 3.620238658186281e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 6/71 | LOSS: 3.6225201906095857e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 7/71 | LOSS: 3.8058070117585885e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 8/71 | LOSS: 3.822712794014175e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 9/71 | LOSS: 3.872612751365523e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 10/71 | LOSS: 3.9798448845183225e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 11/71 | LOSS: 3.951827238779515e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 12/71 | LOSS: 4.157075776534979e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 13/71 | LOSS: 4.227376653683937e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 14/71 | LOSS: 4.332704641759241e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 15/71 | LOSS: 4.330123346107939e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 16/71 | LOSS: 4.480972567013369e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 17/71 | LOSS: 4.456980756610089e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 18/71 | LOSS: 4.480672080382264e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 19/71 | LOSS: 4.4731540583597965e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 20/71 | LOSS: 4.43745198892776e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 21/71 | LOSS: 4.468870129544071e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 22/71 | LOSS: 4.478508374276039e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 23/71 | LOSS: 4.48680338157222e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 24/71 | LOSS: 4.5447059437719875e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 25/71 | LOSS: 4.573665583288507e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 26/71 | LOSS: 4.579386237916576e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 27/71 | LOSS: 4.609931214158028e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 28/71 | LOSS: 4.614556795284694e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 29/71 | LOSS: 4.608935970888221e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 30/71 | LOSS: 4.60558751382773e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 31/71 | LOSS: 4.591070158710409e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 32/71 | LOSS: 4.54040880895539e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 33/71 | LOSS: 4.517414441472604e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 34/71 | LOSS: 4.489266198756273e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 35/71 | LOSS: 4.4553571948805866e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 36/71 | LOSS: 4.441899621982688e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 37/71 | LOSS: 4.42597246020389e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 38/71 | LOSS: 4.4391457520713375e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 39/71 | LOSS: 4.440078419065685e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 40/71 | LOSS: 4.429641571399904e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 41/71 | LOSS: 4.4169405988387076e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 42/71 | LOSS: 4.413276486915027e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 43/71 | LOSS: 4.423310470387647e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 44/71 | LOSS: 4.41057774070133e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 45/71 | LOSS: 4.412059851070492e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 46/71 | LOSS: 4.401144761164262e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 47/71 | LOSS: 4.389289330219981e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 48/71 | LOSS: 4.374708029844238e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 49/71 | LOSS: 4.344958892943396e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 50/71 | LOSS: 4.3227530746733854e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 51/71 | LOSS: 4.337552647589641e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 52/71 | LOSS: 4.331686714410678e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 53/71 | LOSS: 4.3190133535770455e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 54/71 | LOSS: 4.299958767329703e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 55/71 | LOSS: 4.329683325262782e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 56/71 | LOSS: 4.339771924589928e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 57/71 | LOSS: 4.327024647675993e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 58/71 | LOSS: 4.33806219962978e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 59/71 | LOSS: 4.323145954761761e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 60/71 | LOSS: 4.318201870803738e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 61/71 | LOSS: 4.33483098145042e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 62/71 | LOSS: 4.327689353770453e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 63/71 | LOSS: 4.313742071104798e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 64/71 | LOSS: 4.308910148700726e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 65/71 | LOSS: 4.296251807483548e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 66/71 | LOSS: 4.291807964540594e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 67/71 | LOSS: 4.288376444224816e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 68/71 | LOSS: 4.308759225355259e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 69/71 | LOSS: 4.311539604324415e-06\n",
      "TRAIN: EPOCH 577/1000 | BATCH 70/71 | LOSS: 4.3263981290654325e-06\n",
      "VAL: EPOCH 577/1000 | BATCH 0/8 | LOSS: 3.778897280426463e-06\n",
      "VAL: EPOCH 577/1000 | BATCH 1/8 | LOSS: 4.229593059790204e-06\n",
      "VAL: EPOCH 577/1000 | BATCH 2/8 | LOSS: 4.35397305409424e-06\n",
      "VAL: EPOCH 577/1000 | BATCH 3/8 | LOSS: 4.403898856253363e-06\n",
      "VAL: EPOCH 577/1000 | BATCH 4/8 | LOSS: 4.446312323125312e-06\n",
      "VAL: EPOCH 577/1000 | BATCH 5/8 | LOSS: 4.524437902849361e-06\n",
      "VAL: EPOCH 577/1000 | BATCH 6/8 | LOSS: 4.4294597500993405e-06\n",
      "VAL: EPOCH 577/1000 | BATCH 7/8 | LOSS: 4.3315602908933215e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 0/71 | LOSS: 4.372380317363422e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 1/71 | LOSS: 4.0498696307622595e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 2/71 | LOSS: 4.403002246059866e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 3/71 | LOSS: 4.1846473095574765e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 4/71 | LOSS: 4.036685641040094e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 5/71 | LOSS: 4.055855849098104e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 6/71 | LOSS: 4.138652846969697e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 7/71 | LOSS: 4.379806853194168e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 8/71 | LOSS: 4.570225226214259e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 9/71 | LOSS: 4.566222287394339e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 10/71 | LOSS: 4.523583150082479e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 11/71 | LOSS: 4.4825856851578765e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 12/71 | LOSS: 4.5364575691145055e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 13/71 | LOSS: 4.513486406462367e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 14/71 | LOSS: 4.483757796454786e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 15/71 | LOSS: 4.507372324269454e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 16/71 | LOSS: 4.411803461955381e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 17/71 | LOSS: 4.356662758356025e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 18/71 | LOSS: 4.40224888925992e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 19/71 | LOSS: 4.379250708552718e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 20/71 | LOSS: 4.417032111329415e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 21/71 | LOSS: 4.433037102816425e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 22/71 | LOSS: 4.454160914421031e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 23/71 | LOSS: 4.43847269101146e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 24/71 | LOSS: 4.436578656168422e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 25/71 | LOSS: 4.456315257182443e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 26/71 | LOSS: 4.448105402035354e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 27/71 | LOSS: 4.440915599194081e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 28/71 | LOSS: 4.420663368238883e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 29/71 | LOSS: 4.415567840017805e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 30/71 | LOSS: 4.407979128201635e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 31/71 | LOSS: 4.3835544474291055e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 32/71 | LOSS: 4.367401984519816e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 33/71 | LOSS: 4.344035807983292e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 34/71 | LOSS: 4.314247741733977e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 35/71 | LOSS: 4.343280794374651e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 36/71 | LOSS: 4.3470360676135014e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 37/71 | LOSS: 4.335381558312771e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 38/71 | LOSS: 4.342843928894487e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 39/71 | LOSS: 4.3692385474969345e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 40/71 | LOSS: 4.358728591153234e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 41/71 | LOSS: 4.385857263990883e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 42/71 | LOSS: 4.3891308408536475e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 43/71 | LOSS: 4.392995730416839e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 44/71 | LOSS: 4.407592395081237e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 45/71 | LOSS: 4.431456545717083e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 46/71 | LOSS: 4.433183927718114e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 47/71 | LOSS: 4.450918491253712e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 48/71 | LOSS: 4.46793441072686e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 49/71 | LOSS: 4.47998314484721e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 50/71 | LOSS: 4.460791677618045e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 51/71 | LOSS: 4.46394997110334e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 52/71 | LOSS: 4.445564704752233e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 53/71 | LOSS: 4.455247023913587e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 54/71 | LOSS: 4.445082099251025e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 55/71 | LOSS: 4.453631553198127e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 56/71 | LOSS: 4.472890379889705e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 57/71 | LOSS: 4.460912530954249e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 58/71 | LOSS: 4.461246667002127e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 59/71 | LOSS: 4.469312178419689e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 60/71 | LOSS: 4.4917991261649685e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 61/71 | LOSS: 4.503040775988108e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 62/71 | LOSS: 4.530383713634885e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 63/71 | LOSS: 4.549741426274068e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 64/71 | LOSS: 4.543503298639105e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 65/71 | LOSS: 4.589128876074083e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 66/71 | LOSS: 4.597475640705974e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 67/71 | LOSS: 4.645300190104001e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 68/71 | LOSS: 4.649236943541968e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 69/71 | LOSS: 4.69432017910419e-06\n",
      "TRAIN: EPOCH 578/1000 | BATCH 70/71 | LOSS: 4.680557297010829e-06\n",
      "VAL: EPOCH 578/1000 | BATCH 0/8 | LOSS: 4.538563644018723e-06\n",
      "VAL: EPOCH 578/1000 | BATCH 1/8 | LOSS: 5.0429041493771365e-06\n",
      "VAL: EPOCH 578/1000 | BATCH 2/8 | LOSS: 5.121461678451548e-06\n",
      "VAL: EPOCH 578/1000 | BATCH 3/8 | LOSS: 5.0617652505025035e-06\n",
      "VAL: EPOCH 578/1000 | BATCH 4/8 | LOSS: 5.170256463316036e-06\n",
      "VAL: EPOCH 578/1000 | BATCH 5/8 | LOSS: 5.032197577747866e-06\n",
      "VAL: EPOCH 578/1000 | BATCH 6/8 | LOSS: 4.9248776252040575e-06\n",
      "VAL: EPOCH 578/1000 | BATCH 7/8 | LOSS: 4.759433181789063e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 0/71 | LOSS: 5.430392320704414e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 1/71 | LOSS: 5.864191280124942e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 2/71 | LOSS: 5.266196543137387e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 3/71 | LOSS: 5.288341071718605e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 4/71 | LOSS: 5.349159346224042e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 5/71 | LOSS: 5.198667243651774e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 6/71 | LOSS: 5.025221526011592e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 7/71 | LOSS: 5.1303422878845595e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 8/71 | LOSS: 5.0277025265030616e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 9/71 | LOSS: 5.063991147835623e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 10/71 | LOSS: 4.962318143952871e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 11/71 | LOSS: 4.979592138928031e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 12/71 | LOSS: 4.907730132030198e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 13/71 | LOSS: 4.895211174828416e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 14/71 | LOSS: 4.91697489148161e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 15/71 | LOSS: 4.911545602226397e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 16/71 | LOSS: 4.902618644230054e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 17/71 | LOSS: 4.807756719552951e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 18/71 | LOSS: 4.7658182474582355e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 19/71 | LOSS: 4.792617278326361e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 20/71 | LOSS: 4.798977442987962e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 21/71 | LOSS: 4.741616873037525e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 22/71 | LOSS: 4.705356510068826e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 23/71 | LOSS: 4.684297948642779e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 24/71 | LOSS: 4.644414111680817e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 25/71 | LOSS: 4.727454814621104e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 26/71 | LOSS: 4.677220069290208e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 27/71 | LOSS: 4.727988441897781e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 28/71 | LOSS: 4.699449791273921e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 29/71 | LOSS: 4.753874812498301e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 30/71 | LOSS: 4.748493607152223e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 31/71 | LOSS: 4.793710232320336e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 32/71 | LOSS: 4.775658494922523e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 33/71 | LOSS: 4.763839422808173e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 34/71 | LOSS: 4.733296522577543e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 35/71 | LOSS: 4.7206495676416834e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 36/71 | LOSS: 4.6930595704612345e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 37/71 | LOSS: 4.648119209760855e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 38/71 | LOSS: 4.637967590380681e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 39/71 | LOSS: 4.642799683551857e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 40/71 | LOSS: 4.634134944442449e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 41/71 | LOSS: 4.6062331518572004e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 42/71 | LOSS: 4.58661755273656e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 43/71 | LOSS: 4.603907556925564e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 44/71 | LOSS: 4.602188897074989e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 45/71 | LOSS: 4.5920358477656995e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 46/71 | LOSS: 4.611153971904485e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 47/71 | LOSS: 4.618782341481165e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 48/71 | LOSS: 4.609563333991789e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 49/71 | LOSS: 4.632729701370408e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 50/71 | LOSS: 4.5980652885191375e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 51/71 | LOSS: 4.633128613617233e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 52/71 | LOSS: 4.614850566659303e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 53/71 | LOSS: 4.629466856284934e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 54/71 | LOSS: 4.621636890078662e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 55/71 | LOSS: 4.632652730508978e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 56/71 | LOSS: 4.6362806084257416e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 57/71 | LOSS: 4.64381999900697e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 58/71 | LOSS: 4.646560706008794e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 59/71 | LOSS: 4.6243442284321645e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 60/71 | LOSS: 4.666324606600416e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 61/71 | LOSS: 4.659754370462202e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 62/71 | LOSS: 4.723184579306112e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 63/71 | LOSS: 4.707727818242802e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 64/71 | LOSS: 4.7466453417235665e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 65/71 | LOSS: 4.726717937349652e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 66/71 | LOSS: 4.7596783737552965e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 67/71 | LOSS: 4.744099896925036e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 68/71 | LOSS: 4.7778316977622826e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 69/71 | LOSS: 4.773495207440906e-06\n",
      "TRAIN: EPOCH 579/1000 | BATCH 70/71 | LOSS: 4.766152709583968e-06\n",
      "VAL: EPOCH 579/1000 | BATCH 0/8 | LOSS: 3.7738850551249925e-06\n",
      "VAL: EPOCH 579/1000 | BATCH 1/8 | LOSS: 4.322081395002897e-06\n",
      "VAL: EPOCH 579/1000 | BATCH 2/8 | LOSS: 4.72698366138502e-06\n",
      "VAL: EPOCH 579/1000 | BATCH 3/8 | LOSS: 4.822857476938225e-06\n",
      "VAL: EPOCH 579/1000 | BATCH 4/8 | LOSS: 4.852362235396868e-06\n",
      "VAL: EPOCH 579/1000 | BATCH 5/8 | LOSS: 4.962002397708905e-06\n",
      "VAL: EPOCH 579/1000 | BATCH 6/8 | LOSS: 4.878129987316372e-06\n",
      "VAL: EPOCH 579/1000 | BATCH 7/8 | LOSS: 4.819828177460295e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 0/71 | LOSS: 4.931741386826616e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 1/71 | LOSS: 5.196977326704655e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 2/71 | LOSS: 5.654894872956599e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 3/71 | LOSS: 5.566990694205742e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 4/71 | LOSS: 5.480177878780523e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 5/71 | LOSS: 5.1769904606165556e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 6/71 | LOSS: 5.3276266628797335e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 7/71 | LOSS: 5.100792606071991e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 8/71 | LOSS: 4.992627332790613e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 9/71 | LOSS: 4.988467230759852e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 10/71 | LOSS: 4.905178209148951e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 11/71 | LOSS: 4.943495412135235e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 12/71 | LOSS: 4.803661812939726e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 13/71 | LOSS: 4.749091155125436e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 14/71 | LOSS: 4.7099136736505894e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 15/71 | LOSS: 4.710542796715345e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 16/71 | LOSS: 4.6182059149406785e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 17/71 | LOSS: 4.548557588653542e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 18/71 | LOSS: 4.547231427702507e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 19/71 | LOSS: 4.55216098771416e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 20/71 | LOSS: 4.507447556534316e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 21/71 | LOSS: 4.590360504153068e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 22/71 | LOSS: 4.615562078391162e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 23/71 | LOSS: 4.614618812108044e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 24/71 | LOSS: 4.6155113886925396e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 25/71 | LOSS: 4.649549369927487e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 26/71 | LOSS: 4.650466322615372e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 27/71 | LOSS: 4.667654154997893e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 28/71 | LOSS: 4.682398255305276e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 29/71 | LOSS: 4.7054181322891965e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 30/71 | LOSS: 4.6970740479610545e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 31/71 | LOSS: 4.67042802654305e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 32/71 | LOSS: 4.680692131153068e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 33/71 | LOSS: 4.6759525957895676e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 34/71 | LOSS: 4.686118336394429e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 35/71 | LOSS: 4.67357041442382e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 36/71 | LOSS: 4.67778586057279e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 37/71 | LOSS: 4.67867157806866e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 38/71 | LOSS: 4.65360635760776e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 39/71 | LOSS: 4.653095533058149e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 40/71 | LOSS: 4.68164747450451e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 41/71 | LOSS: 4.683987510491404e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 42/71 | LOSS: 4.689221288022371e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 43/71 | LOSS: 4.679893462170267e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 44/71 | LOSS: 4.645825411999896e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 45/71 | LOSS: 4.638760554806377e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 46/71 | LOSS: 4.668369306369989e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 47/71 | LOSS: 4.667124173352022e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 48/71 | LOSS: 4.654151237501741e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 49/71 | LOSS: 4.6556079405490895e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 50/71 | LOSS: 4.642003735474039e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 51/71 | LOSS: 4.6517236787971906e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 52/71 | LOSS: 4.635632782289045e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 53/71 | LOSS: 4.674820885426285e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 54/71 | LOSS: 4.651745823800659e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 55/71 | LOSS: 4.6724028501492806e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 56/71 | LOSS: 4.725979503443038e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 57/71 | LOSS: 4.770065504152594e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 58/71 | LOSS: 4.747543860472526e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 59/71 | LOSS: 4.771691874339013e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 60/71 | LOSS: 4.78183775441244e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 61/71 | LOSS: 4.7773546832045655e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 62/71 | LOSS: 4.794616343710841e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 63/71 | LOSS: 4.780448314534169e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 64/71 | LOSS: 4.795507342910591e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 65/71 | LOSS: 4.786704625020678e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 66/71 | LOSS: 4.7742390038433425e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 67/71 | LOSS: 4.784336644714879e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 68/71 | LOSS: 4.76315741924855e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 69/71 | LOSS: 4.76698324811358e-06\n",
      "TRAIN: EPOCH 580/1000 | BATCH 70/71 | LOSS: 4.776253938832948e-06\n",
      "VAL: EPOCH 580/1000 | BATCH 0/8 | LOSS: 4.599600742949406e-06\n",
      "VAL: EPOCH 580/1000 | BATCH 1/8 | LOSS: 5.053969971413608e-06\n",
      "VAL: EPOCH 580/1000 | BATCH 2/8 | LOSS: 5.33117827217211e-06\n",
      "VAL: EPOCH 580/1000 | BATCH 3/8 | LOSS: 5.21486902016477e-06\n",
      "VAL: EPOCH 580/1000 | BATCH 4/8 | LOSS: 5.3039182603242805e-06\n",
      "VAL: EPOCH 580/1000 | BATCH 5/8 | LOSS: 5.286580820514549e-06\n",
      "VAL: EPOCH 580/1000 | BATCH 6/8 | LOSS: 5.223038767456144e-06\n",
      "VAL: EPOCH 580/1000 | BATCH 7/8 | LOSS: 5.183096334349102e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 0/71 | LOSS: 5.20108369528316e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 1/71 | LOSS: 4.8878621328185545e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 2/71 | LOSS: 4.52056625969514e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 3/71 | LOSS: 4.491137019613234e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 4/71 | LOSS: 4.6315925828821495e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 5/71 | LOSS: 4.72264105155773e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 6/71 | LOSS: 4.708700998889981e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 7/71 | LOSS: 4.662154367451876e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 8/71 | LOSS: 4.558905680419735e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 9/71 | LOSS: 4.613815622178663e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 10/71 | LOSS: 4.595515024034698e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 11/71 | LOSS: 4.5620807706351725e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 12/71 | LOSS: 4.437935208793183e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 13/71 | LOSS: 4.413255591576412e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 14/71 | LOSS: 4.319645843982774e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 15/71 | LOSS: 4.2434207330188656e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 16/71 | LOSS: 4.24075096816523e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 17/71 | LOSS: 4.298749975835865e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 18/71 | LOSS: 4.278976001315689e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 19/71 | LOSS: 4.3244595190117255e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 20/71 | LOSS: 4.3391721322093095e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 21/71 | LOSS: 4.368415437222046e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 22/71 | LOSS: 4.458982264464873e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 23/71 | LOSS: 4.4951690938432876e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 24/71 | LOSS: 4.478019855014282e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 25/71 | LOSS: 4.493133421699615e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 26/71 | LOSS: 4.4944570247564655e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 27/71 | LOSS: 4.540396243360842e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 28/71 | LOSS: 4.492432512490958e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 29/71 | LOSS: 4.545738352135231e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 30/71 | LOSS: 4.541461770889826e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 31/71 | LOSS: 4.540023077481692e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 32/71 | LOSS: 4.646281488682937e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 33/71 | LOSS: 4.641829003262769e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 34/71 | LOSS: 4.6244635801226e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 35/71 | LOSS: 4.6424996816717285e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 36/71 | LOSS: 4.66029804113999e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 37/71 | LOSS: 4.662116722149796e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 38/71 | LOSS: 4.661374258574096e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 39/71 | LOSS: 4.674111181657281e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 40/71 | LOSS: 4.6555565005269825e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 41/71 | LOSS: 4.659956061914272e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 42/71 | LOSS: 4.670389153245704e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 43/71 | LOSS: 4.69786377524625e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 44/71 | LOSS: 4.654812058409637e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 45/71 | LOSS: 4.653166506020922e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 46/71 | LOSS: 4.647039285121904e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 47/71 | LOSS: 4.6307854925468446e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 48/71 | LOSS: 4.6319873527696765e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 49/71 | LOSS: 4.617644913196273e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 50/71 | LOSS: 4.602727697191785e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 51/71 | LOSS: 4.591110430632957e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 52/71 | LOSS: 4.582766360675484e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 53/71 | LOSS: 4.574116840356308e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 54/71 | LOSS: 4.5695693229804006e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 55/71 | LOSS: 4.575892981100489e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 56/71 | LOSS: 4.569119406975612e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 57/71 | LOSS: 4.5577481371545785e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 58/71 | LOSS: 4.538566229912557e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 59/71 | LOSS: 4.5310907239581866e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 60/71 | LOSS: 4.523750521166519e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 61/71 | LOSS: 4.512576046797603e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 62/71 | LOSS: 4.496207840554508e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 63/71 | LOSS: 4.501086714014946e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 64/71 | LOSS: 4.493275201429452e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 65/71 | LOSS: 4.502067902263441e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 66/71 | LOSS: 4.4912492081602435e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 67/71 | LOSS: 4.486047049233414e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 68/71 | LOSS: 4.489538745790766e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 69/71 | LOSS: 4.471387949576768e-06\n",
      "TRAIN: EPOCH 581/1000 | BATCH 70/71 | LOSS: 4.455366633023081e-06\n",
      "VAL: EPOCH 581/1000 | BATCH 0/8 | LOSS: 3.92295351048233e-06\n",
      "VAL: EPOCH 581/1000 | BATCH 1/8 | LOSS: 4.2200294956273865e-06\n",
      "VAL: EPOCH 581/1000 | BATCH 2/8 | LOSS: 4.400320600931688e-06\n",
      "VAL: EPOCH 581/1000 | BATCH 3/8 | LOSS: 4.329909870648407e-06\n",
      "VAL: EPOCH 581/1000 | BATCH 4/8 | LOSS: 4.4279258872848e-06\n",
      "VAL: EPOCH 581/1000 | BATCH 5/8 | LOSS: 4.392266873765038e-06\n",
      "VAL: EPOCH 581/1000 | BATCH 6/8 | LOSS: 4.300397222323227e-06\n",
      "VAL: EPOCH 581/1000 | BATCH 7/8 | LOSS: 4.226356736580783e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 0/71 | LOSS: 3.7214924759609858e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 1/71 | LOSS: 4.029352680845477e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 2/71 | LOSS: 4.157631110501825e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 3/71 | LOSS: 3.929673823677149e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 4/71 | LOSS: 3.836456971839652e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 5/71 | LOSS: 4.019342857948989e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 6/71 | LOSS: 4.0795063601503245e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 7/71 | LOSS: 4.0457837826579635e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 8/71 | LOSS: 4.245619240666403e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 9/71 | LOSS: 4.208524251225753e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 10/71 | LOSS: 4.265107856388733e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 11/71 | LOSS: 4.261380847007483e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 12/71 | LOSS: 4.32071947915784e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 13/71 | LOSS: 4.351160051945564e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 14/71 | LOSS: 4.298535835308334e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 15/71 | LOSS: 4.305371049895257e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 16/71 | LOSS: 4.293001171283643e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 17/71 | LOSS: 4.342416003863845e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 18/71 | LOSS: 4.268435436403563e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 19/71 | LOSS: 4.4894897882841175e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 20/71 | LOSS: 4.520232540103496e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 21/71 | LOSS: 4.593320888737005e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 22/71 | LOSS: 4.608629079366042e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 23/71 | LOSS: 4.7769000275366125e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 24/71 | LOSS: 4.766381134686526e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 25/71 | LOSS: 4.772624309295609e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 26/71 | LOSS: 4.753930237779126e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 27/71 | LOSS: 4.78573539502187e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 28/71 | LOSS: 4.798002464667661e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 29/71 | LOSS: 4.787583460104846e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 30/71 | LOSS: 4.822842041199516e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 31/71 | LOSS: 4.818199130340872e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 32/71 | LOSS: 4.80702485881259e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 33/71 | LOSS: 4.83714738413041e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 34/71 | LOSS: 4.830217091823995e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 35/71 | LOSS: 4.835977493813617e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 36/71 | LOSS: 4.814797525673262e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 37/71 | LOSS: 4.7900392402018614e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 38/71 | LOSS: 4.779965668907001e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 39/71 | LOSS: 4.789988690845348e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 40/71 | LOSS: 4.804946379075906e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 41/71 | LOSS: 4.828571112132825e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 42/71 | LOSS: 4.832922503166713e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 43/71 | LOSS: 4.802758367490738e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 44/71 | LOSS: 4.795922510917686e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 45/71 | LOSS: 4.802597736077429e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 46/71 | LOSS: 4.793734844613627e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 47/71 | LOSS: 4.7550830686304835e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 48/71 | LOSS: 4.723321116940423e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 49/71 | LOSS: 4.7124016236921306e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 50/71 | LOSS: 4.70844117765356e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 51/71 | LOSS: 4.7007738955401764e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 52/71 | LOSS: 4.701807603919835e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 53/71 | LOSS: 4.682158019191034e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 54/71 | LOSS: 4.664125468810777e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 55/71 | LOSS: 4.657652924931556e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 56/71 | LOSS: 4.660576327328352e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 57/71 | LOSS: 4.662427386392665e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 58/71 | LOSS: 4.685696729334807e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 59/71 | LOSS: 4.683615334973486e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 60/71 | LOSS: 4.6838200604292605e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 61/71 | LOSS: 4.667959538276209e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 62/71 | LOSS: 4.6794907936442e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 63/71 | LOSS: 4.67795175751462e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 64/71 | LOSS: 4.66424488737315e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 65/71 | LOSS: 4.6868941305519636e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 66/71 | LOSS: 4.687060668606028e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 67/71 | LOSS: 4.6691485856760715e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 68/71 | LOSS: 4.67166458859709e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 69/71 | LOSS: 4.693417603058541e-06\n",
      "TRAIN: EPOCH 582/1000 | BATCH 70/71 | LOSS: 4.675938549860828e-06\n",
      "VAL: EPOCH 582/1000 | BATCH 0/8 | LOSS: 7.620149517606478e-06\n",
      "VAL: EPOCH 582/1000 | BATCH 1/8 | LOSS: 7.901457593106898e-06\n",
      "VAL: EPOCH 582/1000 | BATCH 2/8 | LOSS: 7.913748959254008e-06\n",
      "VAL: EPOCH 582/1000 | BATCH 3/8 | LOSS: 7.771281389068463e-06\n",
      "VAL: EPOCH 582/1000 | BATCH 4/8 | LOSS: 7.93115632404806e-06\n",
      "VAL: EPOCH 582/1000 | BATCH 5/8 | LOSS: 7.674257631151704e-06\n",
      "VAL: EPOCH 582/1000 | BATCH 6/8 | LOSS: 7.505201600516946e-06\n",
      "VAL: EPOCH 582/1000 | BATCH 7/8 | LOSS: 7.281799923930521e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 0/71 | LOSS: 5.799499831482535e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 1/71 | LOSS: 4.851451876675128e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 2/71 | LOSS: 5.2916325330443215e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 3/71 | LOSS: 5.409939944911457e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 4/71 | LOSS: 5.389718717196957e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 5/71 | LOSS: 5.2571009897898575e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 6/71 | LOSS: 5.147934708864861e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 7/71 | LOSS: 5.305277568368183e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 8/71 | LOSS: 5.254534496796421e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 9/71 | LOSS: 5.356308065529447e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 10/71 | LOSS: 5.249610942634965e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 11/71 | LOSS: 5.1183342482848575e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 12/71 | LOSS: 5.040832635271587e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 13/71 | LOSS: 5.062372062119331e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 14/71 | LOSS: 4.982012539282247e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 15/71 | LOSS: 4.918167761047698e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 16/71 | LOSS: 4.876937763958053e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 17/71 | LOSS: 4.811410601885453e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 18/71 | LOSS: 4.8064824376955294e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 19/71 | LOSS: 4.749851029828278e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 20/71 | LOSS: 4.701860494346225e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 21/71 | LOSS: 4.692067126267251e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 22/71 | LOSS: 4.6718864921232406e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 23/71 | LOSS: 4.70486588710628e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 24/71 | LOSS: 4.681572354456876e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 25/71 | LOSS: 4.650812010927789e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 26/71 | LOSS: 4.629524634462238e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 27/71 | LOSS: 4.602938458577098e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 28/71 | LOSS: 4.620840972448009e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 29/71 | LOSS: 4.610215228240122e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 30/71 | LOSS: 4.711787735892182e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 31/71 | LOSS: 4.689115073119865e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 32/71 | LOSS: 4.712975141680517e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 33/71 | LOSS: 4.719546107962207e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 34/71 | LOSS: 4.782701242740066e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 35/71 | LOSS: 4.786066609893977e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 36/71 | LOSS: 4.769881367817131e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 37/71 | LOSS: 4.756683426278055e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 38/71 | LOSS: 4.744519290584438e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 39/71 | LOSS: 4.706768748974355e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 40/71 | LOSS: 4.675935282116038e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 41/71 | LOSS: 4.712184867068052e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 42/71 | LOSS: 4.719131850855097e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 43/71 | LOSS: 4.684750634127837e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 44/71 | LOSS: 4.6858575337359474e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 45/71 | LOSS: 4.680294503724207e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 46/71 | LOSS: 4.676799794942007e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 47/71 | LOSS: 4.660338930762009e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 48/71 | LOSS: 4.665593353690513e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 49/71 | LOSS: 4.657516565202968e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 50/71 | LOSS: 4.689165302361111e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 51/71 | LOSS: 4.6801375044826554e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 52/71 | LOSS: 4.678258280690434e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 53/71 | LOSS: 4.651890785007035e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 54/71 | LOSS: 4.662237171776889e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 55/71 | LOSS: 4.656339992184907e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 56/71 | LOSS: 4.663972775456945e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 57/71 | LOSS: 4.660052746689401e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 58/71 | LOSS: 4.653719146116893e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 59/71 | LOSS: 4.633773119167017e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 60/71 | LOSS: 4.618820527911503e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 61/71 | LOSS: 4.608445586258782e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 62/71 | LOSS: 4.603337798933295e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 63/71 | LOSS: 4.5922540934384415e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 64/71 | LOSS: 4.589042887696101e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 65/71 | LOSS: 4.591176947900694e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 66/71 | LOSS: 4.585685453650458e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 67/71 | LOSS: 4.588986699031118e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 68/71 | LOSS: 4.584435393316317e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 69/71 | LOSS: 4.587714407274948e-06\n",
      "TRAIN: EPOCH 583/1000 | BATCH 70/71 | LOSS: 4.612677822850356e-06\n",
      "VAL: EPOCH 583/1000 | BATCH 0/8 | LOSS: 3.286828359705396e-06\n",
      "VAL: EPOCH 583/1000 | BATCH 1/8 | LOSS: 3.854554506688146e-06\n",
      "VAL: EPOCH 583/1000 | BATCH 2/8 | LOSS: 4.070506292919163e-06\n",
      "VAL: EPOCH 583/1000 | BATCH 3/8 | LOSS: 4.145698994761915e-06\n",
      "VAL: EPOCH 583/1000 | BATCH 4/8 | LOSS: 4.182041266176384e-06\n",
      "VAL: EPOCH 583/1000 | BATCH 5/8 | LOSS: 4.210304117198878e-06\n",
      "VAL: EPOCH 583/1000 | BATCH 6/8 | LOSS: 4.099200525940562e-06\n",
      "VAL: EPOCH 583/1000 | BATCH 7/8 | LOSS: 3.967988021713609e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 0/71 | LOSS: 4.74557418783661e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 1/71 | LOSS: 5.235268190517672e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 2/71 | LOSS: 5.007264765784687e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 3/71 | LOSS: 5.0961635906787706e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 4/71 | LOSS: 5.36595898665837e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 5/71 | LOSS: 5.23504907808577e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 6/71 | LOSS: 5.552234207633384e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 7/71 | LOSS: 5.399840290465363e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 8/71 | LOSS: 5.524152483202569e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 9/71 | LOSS: 5.596898654403049e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 10/71 | LOSS: 5.586487201402303e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 11/71 | LOSS: 5.517136514754384e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 12/71 | LOSS: 5.602407607232006e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 13/71 | LOSS: 5.52386092879585e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 14/71 | LOSS: 5.718680919623391e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 15/71 | LOSS: 5.62345590537916e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 16/71 | LOSS: 5.827282326636658e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 17/71 | LOSS: 5.824391109854332e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 18/71 | LOSS: 5.959876728555077e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 19/71 | LOSS: 5.852812955708942e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 20/71 | LOSS: 5.800078638241671e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 21/71 | LOSS: 5.895904988309749e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 22/71 | LOSS: 5.844766569087211e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 23/71 | LOSS: 5.865275625183131e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 24/71 | LOSS: 5.799303871754091e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 25/71 | LOSS: 5.796261610577438e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 26/71 | LOSS: 5.761461529295451e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 27/71 | LOSS: 5.752623889877993e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 28/71 | LOSS: 5.699927551804335e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 29/71 | LOSS: 5.640096120866171e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 30/71 | LOSS: 5.659100503591265e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 31/71 | LOSS: 5.607250471939551e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 32/71 | LOSS: 5.63096105135187e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 33/71 | LOSS: 5.576814480852644e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 34/71 | LOSS: 5.599113618310574e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 35/71 | LOSS: 5.546039511096347e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 36/71 | LOSS: 5.516844020029247e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 37/71 | LOSS: 5.459774372770641e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 38/71 | LOSS: 5.424705903304783e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 39/71 | LOSS: 5.409182483617769e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 40/71 | LOSS: 5.414536012611323e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 41/71 | LOSS: 5.370848953134555e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 42/71 | LOSS: 5.346180667529129e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 43/71 | LOSS: 5.306658470710152e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 44/71 | LOSS: 5.272658533309974e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 45/71 | LOSS: 5.246848470209364e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 46/71 | LOSS: 5.2273692338705125e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 47/71 | LOSS: 5.210473545957939e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 48/71 | LOSS: 5.170685737390827e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 49/71 | LOSS: 5.1526068909879545e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 50/71 | LOSS: 5.1579583455082574e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 51/71 | LOSS: 5.134302332757775e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 52/71 | LOSS: 5.124902926118526e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 53/71 | LOSS: 5.107441918678685e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 54/71 | LOSS: 5.0884913434856575e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 55/71 | LOSS: 5.079486754766549e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 56/71 | LOSS: 5.050661504930376e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 57/71 | LOSS: 5.032073925670782e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 58/71 | LOSS: 5.004062956486142e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 59/71 | LOSS: 4.999987171080041e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 60/71 | LOSS: 5.004913813924058e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 61/71 | LOSS: 4.9819771249765895e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 62/71 | LOSS: 4.9779384736457215e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 63/71 | LOSS: 4.9703741815676494e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 64/71 | LOSS: 4.949262763777649e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 65/71 | LOSS: 4.9305740813823915e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 66/71 | LOSS: 4.916004165840889e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 67/71 | LOSS: 4.9070210633651776e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 68/71 | LOSS: 4.890401791324594e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 69/71 | LOSS: 4.866415913186626e-06\n",
      "TRAIN: EPOCH 584/1000 | BATCH 70/71 | LOSS: 4.861288804842305e-06\n",
      "VAL: EPOCH 584/1000 | BATCH 0/8 | LOSS: 4.455015186977107e-06\n",
      "VAL: EPOCH 584/1000 | BATCH 1/8 | LOSS: 4.831563728657784e-06\n",
      "VAL: EPOCH 584/1000 | BATCH 2/8 | LOSS: 5.035399832801583e-06\n",
      "VAL: EPOCH 584/1000 | BATCH 3/8 | LOSS: 4.9724898190106614e-06\n",
      "VAL: EPOCH 584/1000 | BATCH 4/8 | LOSS: 5.070672250440111e-06\n",
      "VAL: EPOCH 584/1000 | BATCH 5/8 | LOSS: 5.168064186970393e-06\n",
      "VAL: EPOCH 584/1000 | BATCH 6/8 | LOSS: 5.0636232507323645e-06\n",
      "VAL: EPOCH 584/1000 | BATCH 7/8 | LOSS: 5.0379913432152534e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 0/71 | LOSS: 4.654967597161885e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 1/71 | LOSS: 4.142498369219538e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 2/71 | LOSS: 3.986531586027316e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 3/71 | LOSS: 4.335815560807532e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 4/71 | LOSS: 4.23362553192419e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 5/71 | LOSS: 4.218267880181277e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 6/71 | LOSS: 4.269716880246831e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 7/71 | LOSS: 4.277057541912654e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 8/71 | LOSS: 4.20746624109193e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 9/71 | LOSS: 4.175626008873223e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 10/71 | LOSS: 4.241972956978398e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 11/71 | LOSS: 4.250119104654004e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 12/71 | LOSS: 4.268809649287365e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 13/71 | LOSS: 4.2498347738728626e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 14/71 | LOSS: 4.308903711110664e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 15/71 | LOSS: 4.291618665774877e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 16/71 | LOSS: 4.22117693019305e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 17/71 | LOSS: 4.2137281879048614e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 18/71 | LOSS: 4.289397230949314e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 19/71 | LOSS: 4.212881844978256e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 20/71 | LOSS: 4.230552111125906e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 21/71 | LOSS: 4.234035424650012e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 22/71 | LOSS: 4.233482088792421e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 23/71 | LOSS: 4.206043759798679e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 24/71 | LOSS: 4.2017486157419625e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 25/71 | LOSS: 4.192742643191685e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 26/71 | LOSS: 4.269468549696116e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 27/71 | LOSS: 4.285505692028632e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 28/71 | LOSS: 4.30915500554216e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 29/71 | LOSS: 4.282511781639187e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 30/71 | LOSS: 4.3092390144245715e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 31/71 | LOSS: 4.324858522863906e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 32/71 | LOSS: 4.376896730250199e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 33/71 | LOSS: 4.370804056045873e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 34/71 | LOSS: 4.368843526338293e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 35/71 | LOSS: 4.3939239983754105e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 36/71 | LOSS: 4.403952196888575e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 37/71 | LOSS: 4.437782787423011e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 38/71 | LOSS: 4.4344927305363135e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 39/71 | LOSS: 4.474686409139394e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 40/71 | LOSS: 4.478021199291816e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 41/71 | LOSS: 4.478327301036361e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 42/71 | LOSS: 4.462536422621735e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 43/71 | LOSS: 4.47873778532746e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 44/71 | LOSS: 4.4557733821824915e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 45/71 | LOSS: 4.4704101049358975e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 46/71 | LOSS: 4.485251106126135e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 47/71 | LOSS: 4.4581654776720825e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 48/71 | LOSS: 4.47913128352541e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 49/71 | LOSS: 4.505532988332561e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 50/71 | LOSS: 4.5528557290940536e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 51/71 | LOSS: 4.584874222228581e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 52/71 | LOSS: 4.5722156515556944e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 53/71 | LOSS: 4.60219331654554e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 54/71 | LOSS: 4.597324302515269e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 55/71 | LOSS: 4.618140272733788e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 56/71 | LOSS: 4.6009979501959925e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 57/71 | LOSS: 4.589818805972504e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 58/71 | LOSS: 4.571495071366495e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 59/71 | LOSS: 4.557232773549913e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 60/71 | LOSS: 4.544676462277583e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 61/71 | LOSS: 4.532224696988358e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 62/71 | LOSS: 4.536604047456104e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 63/71 | LOSS: 4.529376436579469e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 64/71 | LOSS: 4.534360795542643e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 65/71 | LOSS: 4.526475456920955e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 66/71 | LOSS: 4.522616872738798e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 67/71 | LOSS: 4.515692323285989e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 68/71 | LOSS: 4.519999891641234e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 69/71 | LOSS: 4.511823827410159e-06\n",
      "TRAIN: EPOCH 585/1000 | BATCH 70/71 | LOSS: 4.535275875113398e-06\n",
      "VAL: EPOCH 585/1000 | BATCH 0/8 | LOSS: 3.6902092688251287e-06\n",
      "VAL: EPOCH 585/1000 | BATCH 1/8 | LOSS: 4.266164069122169e-06\n",
      "VAL: EPOCH 585/1000 | BATCH 2/8 | LOSS: 4.444010907415456e-06\n",
      "VAL: EPOCH 585/1000 | BATCH 3/8 | LOSS: 4.478114533412736e-06\n",
      "VAL: EPOCH 585/1000 | BATCH 4/8 | LOSS: 4.528036970441462e-06\n",
      "VAL: EPOCH 585/1000 | BATCH 5/8 | LOSS: 4.507027900520673e-06\n",
      "VAL: EPOCH 585/1000 | BATCH 6/8 | LOSS: 4.407739587415043e-06\n",
      "VAL: EPOCH 585/1000 | BATCH 7/8 | LOSS: 4.251994795367864e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 0/71 | LOSS: 4.53904976893682e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 1/71 | LOSS: 4.306651362639968e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 2/71 | LOSS: 4.248314326105174e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 3/71 | LOSS: 4.619561877916567e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 4/71 | LOSS: 4.302806428313488e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 5/71 | LOSS: 4.489057649455693e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 6/71 | LOSS: 4.612604048556282e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 7/71 | LOSS: 4.713477380846598e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 8/71 | LOSS: 4.592966484374807e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 9/71 | LOSS: 4.693347364082001e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 10/71 | LOSS: 4.62898647542302e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 11/71 | LOSS: 4.6205128304184955e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 12/71 | LOSS: 4.604727039874818e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 13/71 | LOSS: 4.748792857104231e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 14/71 | LOSS: 4.68035548995734e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 15/71 | LOSS: 4.699609760905332e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 16/71 | LOSS: 4.723635263015988e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 17/71 | LOSS: 4.7212065889349715e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 18/71 | LOSS: 4.738857796837692e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 19/71 | LOSS: 4.820840365482581e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 20/71 | LOSS: 4.808474679001458e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 21/71 | LOSS: 4.774673935902353e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 22/71 | LOSS: 4.826067859777021e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 23/71 | LOSS: 4.803263787304483e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 24/71 | LOSS: 4.775575880557881e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 25/71 | LOSS: 4.797805148043083e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 26/71 | LOSS: 4.766527606787147e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 27/71 | LOSS: 4.743994338696211e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 28/71 | LOSS: 4.763308809209587e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 29/71 | LOSS: 4.804511119497571e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 30/71 | LOSS: 4.750388869421929e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 31/71 | LOSS: 4.793386786161591e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 32/71 | LOSS: 4.799945414064992e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 33/71 | LOSS: 4.790802884546044e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 34/71 | LOSS: 4.801331494620952e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 35/71 | LOSS: 4.793264963609545e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 36/71 | LOSS: 4.8054265408683095e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 37/71 | LOSS: 4.8076723619248555e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 38/71 | LOSS: 4.793404659955204e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 39/71 | LOSS: 4.801836422529959e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 40/71 | LOSS: 4.7949207472389925e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 41/71 | LOSS: 4.788439934513701e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 42/71 | LOSS: 4.7806984977881275e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 43/71 | LOSS: 4.781419043162962e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 44/71 | LOSS: 4.773883215926213e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 45/71 | LOSS: 4.781007068350331e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 46/71 | LOSS: 4.7555450895201155e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 47/71 | LOSS: 4.75514185893644e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 48/71 | LOSS: 4.723882279811695e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 49/71 | LOSS: 4.7664868407082395e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 50/71 | LOSS: 4.752332219426285e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 51/71 | LOSS: 4.749030377017614e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 52/71 | LOSS: 4.76633338295423e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 53/71 | LOSS: 4.7441585677545475e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 54/71 | LOSS: 4.745633999515452e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 55/71 | LOSS: 4.784308868823116e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 56/71 | LOSS: 4.770860948810239e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 57/71 | LOSS: 4.763692874709297e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 58/71 | LOSS: 4.773950676882138e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 59/71 | LOSS: 4.786370899031074e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 60/71 | LOSS: 4.767071459355066e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 61/71 | LOSS: 4.7666074336912945e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 62/71 | LOSS: 4.758822320755349e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 63/71 | LOSS: 4.7667363958225906e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 64/71 | LOSS: 4.7690033450355535e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 65/71 | LOSS: 4.771374402811158e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 66/71 | LOSS: 4.768420199274152e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 67/71 | LOSS: 4.774788093902102e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 68/71 | LOSS: 4.762741776874569e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 69/71 | LOSS: 4.758639092220359e-06\n",
      "TRAIN: EPOCH 586/1000 | BATCH 70/71 | LOSS: 4.7626165301345524e-06\n",
      "VAL: EPOCH 586/1000 | BATCH 0/8 | LOSS: 4.861030902247876e-06\n",
      "VAL: EPOCH 586/1000 | BATCH 1/8 | LOSS: 4.980138101018383e-06\n",
      "VAL: EPOCH 586/1000 | BATCH 2/8 | LOSS: 4.940886962382744e-06\n",
      "VAL: EPOCH 586/1000 | BATCH 3/8 | LOSS: 4.8534266170463525e-06\n",
      "VAL: EPOCH 586/1000 | BATCH 4/8 | LOSS: 4.985043960914481e-06\n",
      "VAL: EPOCH 586/1000 | BATCH 5/8 | LOSS: 4.8915568034620565e-06\n",
      "VAL: EPOCH 586/1000 | BATCH 6/8 | LOSS: 4.78096321135776e-06\n",
      "VAL: EPOCH 586/1000 | BATCH 7/8 | LOSS: 4.662772369101731e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 0/71 | LOSS: 4.618360435415525e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 1/71 | LOSS: 4.415865760165616e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 2/71 | LOSS: 4.437143161339918e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 3/71 | LOSS: 4.379589995551214e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 4/71 | LOSS: 4.438451924215769e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 5/71 | LOSS: 4.464874640082901e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 6/71 | LOSS: 4.4761689293539216e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 7/71 | LOSS: 4.472129091936949e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 8/71 | LOSS: 5.029819931223756e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 9/71 | LOSS: 4.878558502241503e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 10/71 | LOSS: 4.987009147457271e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 11/71 | LOSS: 4.9821259532715585e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 12/71 | LOSS: 4.912074823201133e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 13/71 | LOSS: 4.944879557504984e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 14/71 | LOSS: 5.011409181558217e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 15/71 | LOSS: 5.0813283678508014e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 16/71 | LOSS: 5.042948433744159e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 17/71 | LOSS: 5.039115346032001e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 18/71 | LOSS: 5.055141867303878e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 19/71 | LOSS: 5.205556453802273e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 20/71 | LOSS: 5.138693051689881e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 21/71 | LOSS: 5.302089200333946e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 22/71 | LOSS: 5.2673003665404394e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 23/71 | LOSS: 5.282953509322397e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 24/71 | LOSS: 5.271957088552881e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 25/71 | LOSS: 5.338183270326296e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 26/71 | LOSS: 5.288711556069622e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 27/71 | LOSS: 5.304525270081026e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 28/71 | LOSS: 5.283685509468718e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 29/71 | LOSS: 5.353312720520383e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 30/71 | LOSS: 5.291042344329218e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 31/71 | LOSS: 5.299989339846434e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 32/71 | LOSS: 5.291066096988013e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 33/71 | LOSS: 5.2962221108079095e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 34/71 | LOSS: 5.276509312222645e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 35/71 | LOSS: 5.29467476755801e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 36/71 | LOSS: 5.282999498720872e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 37/71 | LOSS: 5.240301860746848e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 38/71 | LOSS: 5.244548085615045e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 39/71 | LOSS: 5.234675347765005e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 40/71 | LOSS: 5.228320355299951e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 41/71 | LOSS: 5.236582009750973e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 42/71 | LOSS: 5.275033567328179e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 43/71 | LOSS: 5.291109625266637e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 44/71 | LOSS: 5.283768779109879e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 45/71 | LOSS: 5.2681128913125415e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 46/71 | LOSS: 5.248939903546497e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 47/71 | LOSS: 5.250044684620055e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 48/71 | LOSS: 5.232032584231371e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 49/71 | LOSS: 5.22478586390207e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 50/71 | LOSS: 5.210925765448551e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 51/71 | LOSS: 5.177811737247304e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 52/71 | LOSS: 5.2012355894784945e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 53/71 | LOSS: 5.164321742591306e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 54/71 | LOSS: 5.17726808869735e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 55/71 | LOSS: 5.167469298937379e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 56/71 | LOSS: 5.141951473170163e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 57/71 | LOSS: 5.18230035987503e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 58/71 | LOSS: 5.1630087825087376e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 59/71 | LOSS: 5.160883191971758e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 60/71 | LOSS: 5.140992288197893e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 61/71 | LOSS: 5.138704728602944e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 62/71 | LOSS: 5.103033529286179e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 63/71 | LOSS: 5.088136891373551e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 64/71 | LOSS: 5.080713782053163e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 65/71 | LOSS: 5.070374165618639e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 66/71 | LOSS: 5.063367169312377e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 67/71 | LOSS: 5.044211497854977e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 68/71 | LOSS: 5.048728467770253e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 69/71 | LOSS: 5.033009831225042e-06\n",
      "TRAIN: EPOCH 587/1000 | BATCH 70/71 | LOSS: 5.037005807230664e-06\n",
      "VAL: EPOCH 587/1000 | BATCH 0/8 | LOSS: 3.897850547218695e-06\n",
      "VAL: EPOCH 587/1000 | BATCH 1/8 | LOSS: 4.1782659536693245e-06\n",
      "VAL: EPOCH 587/1000 | BATCH 2/8 | LOSS: 4.2068641050718725e-06\n",
      "VAL: EPOCH 587/1000 | BATCH 3/8 | LOSS: 4.137134396842157e-06\n",
      "VAL: EPOCH 587/1000 | BATCH 4/8 | LOSS: 4.151680877839681e-06\n",
      "VAL: EPOCH 587/1000 | BATCH 5/8 | LOSS: 4.071305511388346e-06\n",
      "VAL: EPOCH 587/1000 | BATCH 6/8 | LOSS: 3.913397384686894e-06\n",
      "VAL: EPOCH 587/1000 | BATCH 7/8 | LOSS: 3.7866258537633257e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 0/71 | LOSS: 4.5582228267448954e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 1/71 | LOSS: 5.397065706347348e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 2/71 | LOSS: 5.013568321980226e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 3/71 | LOSS: 4.749926119984593e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 4/71 | LOSS: 4.371793602331309e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 5/71 | LOSS: 4.409247973550616e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 6/71 | LOSS: 4.40653697556367e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 7/71 | LOSS: 4.60792352896533e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 8/71 | LOSS: 4.509804815218861e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 9/71 | LOSS: 4.881682571067358e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 10/71 | LOSS: 4.811265377859606e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 11/71 | LOSS: 4.953972355300114e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 12/71 | LOSS: 4.919115081765295e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 13/71 | LOSS: 4.9912478711381226e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 14/71 | LOSS: 4.935034242710875e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 15/71 | LOSS: 5.047367181987283e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 16/71 | LOSS: 4.970190302093911e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 17/71 | LOSS: 5.0093397021555575e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 18/71 | LOSS: 4.936506851855053e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 19/71 | LOSS: 4.9107114818980335e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 20/71 | LOSS: 4.8788463226671976e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 21/71 | LOSS: 4.811139247480738e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 22/71 | LOSS: 4.8552923861121675e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 23/71 | LOSS: 4.781219615779264e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 24/71 | LOSS: 4.817004682990955e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 25/71 | LOSS: 4.8174403078659315e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 26/71 | LOSS: 4.841605190180147e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 27/71 | LOSS: 4.80784092360409e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 28/71 | LOSS: 4.812254074154068e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 29/71 | LOSS: 4.795716722583165e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 30/71 | LOSS: 4.75526902729114e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 31/71 | LOSS: 4.728013827559607e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 32/71 | LOSS: 4.71903245218555e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 33/71 | LOSS: 4.686295275913674e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 34/71 | LOSS: 4.676222033594968e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 35/71 | LOSS: 4.692230820384995e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 36/71 | LOSS: 4.682001847843797e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 37/71 | LOSS: 4.659041271799269e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 38/71 | LOSS: 4.633181100498172e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 39/71 | LOSS: 4.655132369180137e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 40/71 | LOSS: 4.656052019859865e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 41/71 | LOSS: 4.677981099207716e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 42/71 | LOSS: 4.698417210103037e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 43/71 | LOSS: 4.733996036065946e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 44/71 | LOSS: 4.73615871972773e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 45/71 | LOSS: 4.746850184188667e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 46/71 | LOSS: 4.775790441581581e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 47/71 | LOSS: 4.850066768350795e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 48/71 | LOSS: 4.84983941681335e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 49/71 | LOSS: 4.8827868477019365e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 50/71 | LOSS: 4.889686757522606e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 51/71 | LOSS: 4.871785830136315e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 52/71 | LOSS: 4.865217319230345e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 53/71 | LOSS: 4.8561095800997225e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 54/71 | LOSS: 4.85477024250775e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 55/71 | LOSS: 4.840862910019236e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 56/71 | LOSS: 4.848681356362935e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 57/71 | LOSS: 4.8772428844413615e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 58/71 | LOSS: 4.8780736652603945e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 59/71 | LOSS: 4.881424695213355e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 60/71 | LOSS: 4.871355676201705e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 61/71 | LOSS: 4.89124133471177e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 62/71 | LOSS: 4.884878636504355e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 63/71 | LOSS: 4.892200767159238e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 64/71 | LOSS: 4.879994562893094e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 65/71 | LOSS: 4.8808909076053e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 66/71 | LOSS: 4.8785418890311934e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 67/71 | LOSS: 4.873350566386387e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 68/71 | LOSS: 4.897291404816222e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 69/71 | LOSS: 4.914795424935749e-06\n",
      "TRAIN: EPOCH 588/1000 | BATCH 70/71 | LOSS: 4.900391850626452e-06\n",
      "VAL: EPOCH 588/1000 | BATCH 0/8 | LOSS: 3.6181936593493447e-06\n",
      "VAL: EPOCH 588/1000 | BATCH 1/8 | LOSS: 4.016187858724152e-06\n",
      "VAL: EPOCH 588/1000 | BATCH 2/8 | LOSS: 4.093195002496941e-06\n",
      "VAL: EPOCH 588/1000 | BATCH 3/8 | LOSS: 4.115100978197006e-06\n",
      "VAL: EPOCH 588/1000 | BATCH 4/8 | LOSS: 4.124468705413164e-06\n",
      "VAL: EPOCH 588/1000 | BATCH 5/8 | LOSS: 4.1288489380046185e-06\n",
      "VAL: EPOCH 588/1000 | BATCH 6/8 | LOSS: 3.9940186330308535e-06\n",
      "VAL: EPOCH 588/1000 | BATCH 7/8 | LOSS: 3.8461418228052935e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 0/71 | LOSS: 3.5680391192727257e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 1/71 | LOSS: 4.279677114027436e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 2/71 | LOSS: 4.296649876778247e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 3/71 | LOSS: 4.164909114479087e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 4/71 | LOSS: 4.161670221947133e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 5/71 | LOSS: 4.4628576082080445e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 6/71 | LOSS: 4.388055978259737e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 7/71 | LOSS: 4.3699377556549734e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 8/71 | LOSS: 4.615384669806291e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 9/71 | LOSS: 5.066198718850501e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 10/71 | LOSS: 5.018644928482403e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 11/71 | LOSS: 5.005721050110878e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 12/71 | LOSS: 5.058303311391053e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 13/71 | LOSS: 5.044149474997539e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 14/71 | LOSS: 5.03631672472693e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 15/71 | LOSS: 5.120905484545801e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 16/71 | LOSS: 5.087551578660222e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 17/71 | LOSS: 5.070362274434754e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 18/71 | LOSS: 5.072434736288606e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 19/71 | LOSS: 5.1044815108980405e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 20/71 | LOSS: 5.107429582754516e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 21/71 | LOSS: 5.075400147027474e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 22/71 | LOSS: 5.132891542218757e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 23/71 | LOSS: 5.108052505420346e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 24/71 | LOSS: 5.2081515605095776e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 25/71 | LOSS: 5.133228193279558e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 26/71 | LOSS: 5.158737259854981e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 27/71 | LOSS: 5.0915816619375875e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 28/71 | LOSS: 5.077795031897479e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 29/71 | LOSS: 5.092276349690413e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 30/71 | LOSS: 5.011405604701365e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 31/71 | LOSS: 5.063913498304373e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 32/71 | LOSS: 5.092026702351921e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 33/71 | LOSS: 5.1321407802200105e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 34/71 | LOSS: 5.114236325555664e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 35/71 | LOSS: 5.113905236460899e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 36/71 | LOSS: 5.122489581579528e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 37/71 | LOSS: 5.0974256222808e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 38/71 | LOSS: 5.111787365818506e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 39/71 | LOSS: 5.08774084551078e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 40/71 | LOSS: 5.0725162542753755e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 41/71 | LOSS: 5.058002903708603e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 42/71 | LOSS: 5.055633987150179e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 43/71 | LOSS: 5.019586333012492e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 44/71 | LOSS: 5.014856302902141e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 45/71 | LOSS: 5.03425895243234e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 46/71 | LOSS: 4.99778373020316e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 47/71 | LOSS: 4.997201306385553e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 48/71 | LOSS: 4.950675131521148e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 49/71 | LOSS: 4.941303509440331e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 50/71 | LOSS: 4.9310983874473845e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 51/71 | LOSS: 4.952640171478169e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 52/71 | LOSS: 4.949086827901686e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 53/71 | LOSS: 4.9501113481811544e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 54/71 | LOSS: 4.9779981095939135e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 55/71 | LOSS: 5.007728105965725e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 56/71 | LOSS: 5.058666538229374e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 57/71 | LOSS: 5.058126515089905e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 58/71 | LOSS: 5.129645310493504e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 59/71 | LOSS: 5.132674342197182e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 60/71 | LOSS: 5.166861292011282e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 61/71 | LOSS: 5.174945752720129e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 62/71 | LOSS: 5.196632220383709e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 63/71 | LOSS: 5.235996223262873e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 64/71 | LOSS: 5.2409499508380224e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 65/71 | LOSS: 5.2756797031454905e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 66/71 | LOSS: 5.271615901932167e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 67/71 | LOSS: 5.3325788237944995e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 68/71 | LOSS: 5.3192643364404235e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 69/71 | LOSS: 5.3205434109518785e-06\n",
      "TRAIN: EPOCH 589/1000 | BATCH 70/71 | LOSS: 5.3168492760166155e-06\n",
      "VAL: EPOCH 589/1000 | BATCH 0/8 | LOSS: 5.307904757501092e-06\n",
      "VAL: EPOCH 589/1000 | BATCH 1/8 | LOSS: 5.5616869758523535e-06\n",
      "VAL: EPOCH 589/1000 | BATCH 2/8 | LOSS: 5.547216157234895e-06\n",
      "VAL: EPOCH 589/1000 | BATCH 3/8 | LOSS: 5.599785936283297e-06\n",
      "VAL: EPOCH 589/1000 | BATCH 4/8 | LOSS: 5.503280863194959e-06\n",
      "VAL: EPOCH 589/1000 | BATCH 5/8 | LOSS: 5.396374414582776e-06\n",
      "VAL: EPOCH 589/1000 | BATCH 6/8 | LOSS: 5.178276751394151e-06\n",
      "VAL: EPOCH 589/1000 | BATCH 7/8 | LOSS: 5.011026530610252e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 0/71 | LOSS: 6.5250942498096265e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 1/71 | LOSS: 6.416574706236133e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 2/71 | LOSS: 5.7860247579810675e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 3/71 | LOSS: 5.417147690423008e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 4/71 | LOSS: 5.228579084359808e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 5/71 | LOSS: 4.986461552410522e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 6/71 | LOSS: 4.833281406847943e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 7/71 | LOSS: 4.811788102188075e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 8/71 | LOSS: 4.745738983223822e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 9/71 | LOSS: 4.806967467629875e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 10/71 | LOSS: 4.767419113704818e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 11/71 | LOSS: 4.828553358038334e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 12/71 | LOSS: 5.023515751208134e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 13/71 | LOSS: 4.916395791302161e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 14/71 | LOSS: 4.940853447502983e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 15/71 | LOSS: 4.9370998027598034e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 16/71 | LOSS: 4.941342133272829e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 17/71 | LOSS: 4.912910059020861e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 18/71 | LOSS: 4.915020769514836e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 19/71 | LOSS: 4.930954253268282e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 20/71 | LOSS: 4.9034473423013145e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 21/71 | LOSS: 4.844759628626476e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 22/71 | LOSS: 4.857734201441812e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 23/71 | LOSS: 4.803568212234192e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 24/71 | LOSS: 4.743481495097512e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 25/71 | LOSS: 4.715737147237703e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 26/71 | LOSS: 4.746874765260145e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 27/71 | LOSS: 4.735572961830517e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 28/71 | LOSS: 4.716104411209958e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 29/71 | LOSS: 4.699437249655603e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 30/71 | LOSS: 4.66217452119435e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 31/71 | LOSS: 4.63027207331379e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 32/71 | LOSS: 4.622183827119773e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 33/71 | LOSS: 4.606266670367297e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 34/71 | LOSS: 4.618008097168058e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 35/71 | LOSS: 4.619759579327365e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 36/71 | LOSS: 4.628788677085116e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 37/71 | LOSS: 4.634916505988497e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 38/71 | LOSS: 4.624618132089754e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 39/71 | LOSS: 4.5909182404102465e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 40/71 | LOSS: 4.56762345825931e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 41/71 | LOSS: 4.597357414717235e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 42/71 | LOSS: 4.580662435241826e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 43/71 | LOSS: 4.583081079090334e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 44/71 | LOSS: 4.570873963732286e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 45/71 | LOSS: 4.574846170528516e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 46/71 | LOSS: 4.551156415742453e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 47/71 | LOSS: 4.537863498891663e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 48/71 | LOSS: 4.520485938010839e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 49/71 | LOSS: 4.509505938585789e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 50/71 | LOSS: 4.507135506227153e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 51/71 | LOSS: 4.4902851728833275e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 52/71 | LOSS: 4.486918433545759e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 53/71 | LOSS: 4.47032387337795e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 54/71 | LOSS: 4.446397935515216e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 55/71 | LOSS: 4.436344880787146e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 56/71 | LOSS: 4.432496493073383e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 57/71 | LOSS: 4.4094894025780875e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 58/71 | LOSS: 4.427070831236236e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 59/71 | LOSS: 4.45624893927743e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 60/71 | LOSS: 4.489106589464994e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 61/71 | LOSS: 4.470402055014875e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 62/71 | LOSS: 4.5197604762622165e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 63/71 | LOSS: 4.52934038719377e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 64/71 | LOSS: 4.559641722433018e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 65/71 | LOSS: 4.5682144321869025e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 66/71 | LOSS: 4.5563112928618075e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 67/71 | LOSS: 4.5765962313988195e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 68/71 | LOSS: 4.574588254636973e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 69/71 | LOSS: 4.5939695545322525e-06\n",
      "TRAIN: EPOCH 590/1000 | BATCH 70/71 | LOSS: 4.587986034455522e-06\n",
      "VAL: EPOCH 590/1000 | BATCH 0/8 | LOSS: 4.462922333914321e-06\n",
      "VAL: EPOCH 590/1000 | BATCH 1/8 | LOSS: 4.7687140067864675e-06\n",
      "VAL: EPOCH 590/1000 | BATCH 2/8 | LOSS: 4.857233003955723e-06\n",
      "VAL: EPOCH 590/1000 | BATCH 3/8 | LOSS: 4.8330496156268055e-06\n",
      "VAL: EPOCH 590/1000 | BATCH 4/8 | LOSS: 4.896247992292046e-06\n",
      "VAL: EPOCH 590/1000 | BATCH 5/8 | LOSS: 4.856315626966534e-06\n",
      "VAL: EPOCH 590/1000 | BATCH 6/8 | LOSS: 4.740211417291513e-06\n",
      "VAL: EPOCH 590/1000 | BATCH 7/8 | LOSS: 4.539391369462464e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 0/71 | LOSS: 4.787909801962087e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 1/71 | LOSS: 4.989990657122689e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 2/71 | LOSS: 4.567495883141722e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 3/71 | LOSS: 4.707666562353552e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 4/71 | LOSS: 4.502859474087017e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 5/71 | LOSS: 4.52099648858469e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 6/71 | LOSS: 4.453227412731005e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 7/71 | LOSS: 4.303217139067783e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 8/71 | LOSS: 4.275137674388437e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 9/71 | LOSS: 4.259744105183927e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 10/71 | LOSS: 4.151223100217811e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 11/71 | LOSS: 4.091683858102139e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 12/71 | LOSS: 4.040285515517238e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 13/71 | LOSS: 4.0223181940746144e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 14/71 | LOSS: 4.1190592279842045e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 15/71 | LOSS: 4.149739325498558e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 16/71 | LOSS: 4.14117602411031e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 17/71 | LOSS: 4.121549800907411e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 18/71 | LOSS: 4.133829945299351e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 19/71 | LOSS: 4.169859300873213e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 20/71 | LOSS: 4.181022924447981e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 21/71 | LOSS: 4.1879160335156485e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 22/71 | LOSS: 4.239493403909601e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 23/71 | LOSS: 4.256933749502423e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 24/71 | LOSS: 4.272411124475184e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 25/71 | LOSS: 4.271220850918885e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 26/71 | LOSS: 4.319831119843272e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 27/71 | LOSS: 4.340978120710913e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 28/71 | LOSS: 4.325939894913282e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 29/71 | LOSS: 4.325129801448687e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 30/71 | LOSS: 4.353476712030017e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 31/71 | LOSS: 4.295831779188575e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 32/71 | LOSS: 4.287880907819878e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 33/71 | LOSS: 4.26305978314357e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 34/71 | LOSS: 4.2484233940090885e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 35/71 | LOSS: 4.261292708128571e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 36/71 | LOSS: 4.288247774221e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 37/71 | LOSS: 4.308330164205568e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 38/71 | LOSS: 4.295103823236646e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 39/71 | LOSS: 4.33026620498822e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 40/71 | LOSS: 4.3563253124064916e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 41/71 | LOSS: 4.362878106197361e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 42/71 | LOSS: 4.387785248730134e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 43/71 | LOSS: 4.3749339666150755e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 44/71 | LOSS: 4.390880849314271e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 45/71 | LOSS: 4.387785126077095e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 46/71 | LOSS: 4.388345089071354e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 47/71 | LOSS: 4.375832593458047e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 48/71 | LOSS: 4.368105780430393e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 49/71 | LOSS: 4.361598998912087e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 50/71 | LOSS: 4.350633153259039e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 51/71 | LOSS: 4.3465961381955894e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 52/71 | LOSS: 4.35688997091553e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 53/71 | LOSS: 4.345451838229632e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 54/71 | LOSS: 4.328485014907942e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 55/71 | LOSS: 4.309017498371759e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 56/71 | LOSS: 4.3423713681509155e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 57/71 | LOSS: 4.3608482028816825e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 58/71 | LOSS: 4.378460069778681e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 59/71 | LOSS: 4.369682206591581e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 60/71 | LOSS: 4.381385921436452e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 61/71 | LOSS: 4.3890927482578025e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 62/71 | LOSS: 4.39986955291441e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 63/71 | LOSS: 4.3973369407979135e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 64/71 | LOSS: 4.397094880914665e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 65/71 | LOSS: 4.399290587846763e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 66/71 | LOSS: 4.389497825517293e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 67/71 | LOSS: 4.38917009446146e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 68/71 | LOSS: 4.389511002665276e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 69/71 | LOSS: 4.403447724143916e-06\n",
      "TRAIN: EPOCH 591/1000 | BATCH 70/71 | LOSS: 4.391330792355469e-06\n",
      "VAL: EPOCH 591/1000 | BATCH 0/8 | LOSS: 5.452416189655196e-06\n",
      "VAL: EPOCH 591/1000 | BATCH 1/8 | LOSS: 5.518231319001643e-06\n",
      "VAL: EPOCH 591/1000 | BATCH 2/8 | LOSS: 5.604676213503505e-06\n",
      "VAL: EPOCH 591/1000 | BATCH 3/8 | LOSS: 5.521193884305831e-06\n",
      "VAL: EPOCH 591/1000 | BATCH 4/8 | LOSS: 5.544722898775944e-06\n",
      "VAL: EPOCH 591/1000 | BATCH 5/8 | LOSS: 5.439412916530273e-06\n",
      "VAL: EPOCH 591/1000 | BATCH 6/8 | LOSS: 5.2763056633661364e-06\n",
      "VAL: EPOCH 591/1000 | BATCH 7/8 | LOSS: 5.091854546890318e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 0/71 | LOSS: 5.714627150155138e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 1/71 | LOSS: 4.198476290184772e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 2/71 | LOSS: 4.319501082742742e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 3/71 | LOSS: 4.306327582526137e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 4/71 | LOSS: 4.328466820879839e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 5/71 | LOSS: 4.590854359776131e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 6/71 | LOSS: 4.536574838961574e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 7/71 | LOSS: 4.446322350304399e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 8/71 | LOSS: 4.551221839695548e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 9/71 | LOSS: 4.539414931059582e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 10/71 | LOSS: 4.505589996287282e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 11/71 | LOSS: 4.522688603477339e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 12/71 | LOSS: 4.521230284108941e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 13/71 | LOSS: 4.4472559790977225e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 14/71 | LOSS: 4.440731557527518e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 15/71 | LOSS: 4.385345917512495e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 16/71 | LOSS: 4.360494625978069e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 17/71 | LOSS: 4.4312713245946925e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 18/71 | LOSS: 4.408879213339165e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 19/71 | LOSS: 4.3566294380070755e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 20/71 | LOSS: 4.343550353085101e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 21/71 | LOSS: 4.2869999312890945e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 22/71 | LOSS: 4.25812690398585e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 23/71 | LOSS: 4.267076557577336e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 24/71 | LOSS: 4.284461119823391e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 25/71 | LOSS: 4.277673374767451e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 26/71 | LOSS: 4.352489600865671e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 27/71 | LOSS: 4.364488177088788e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 28/71 | LOSS: 4.38315241965405e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 29/71 | LOSS: 4.3784745230368575e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 30/71 | LOSS: 4.355778445085068e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 31/71 | LOSS: 4.337051962011174e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 32/71 | LOSS: 4.318239379327596e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 33/71 | LOSS: 4.305985826517024e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 34/71 | LOSS: 4.325804625945498e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 35/71 | LOSS: 4.316536660553538e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 36/71 | LOSS: 4.331465002119453e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 37/71 | LOSS: 4.313500719370878e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 38/71 | LOSS: 4.312545500467352e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 39/71 | LOSS: 4.322046618199238e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 40/71 | LOSS: 4.31531073378964e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 41/71 | LOSS: 4.30807651918301e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 42/71 | LOSS: 4.305937478039955e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 43/71 | LOSS: 4.314490090771208e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 44/71 | LOSS: 4.298716076947231e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 45/71 | LOSS: 4.301132805820721e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 46/71 | LOSS: 4.299564517033162e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 47/71 | LOSS: 4.33361750159141e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 48/71 | LOSS: 4.345003060511865e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 49/71 | LOSS: 4.373063256934984e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 50/71 | LOSS: 4.369163176021425e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 51/71 | LOSS: 4.378698638272628e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 52/71 | LOSS: 4.385234815173426e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 53/71 | LOSS: 4.377353543868931e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 54/71 | LOSS: 4.373742541991471e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 55/71 | LOSS: 4.3757644415823705e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 56/71 | LOSS: 4.382069427696192e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 57/71 | LOSS: 4.3775527548608935e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 58/71 | LOSS: 4.3785373498517945e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 59/71 | LOSS: 4.367235662054251e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 60/71 | LOSS: 4.36034490688099e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 61/71 | LOSS: 4.348032784799861e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 62/71 | LOSS: 4.353646090748953e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 63/71 | LOSS: 4.3596777530297e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 64/71 | LOSS: 4.357952492192943e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 65/71 | LOSS: 4.3839713519198335e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 66/71 | LOSS: 4.3824066955521494e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 67/71 | LOSS: 4.377973234969592e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 68/71 | LOSS: 4.388008250501024e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 69/71 | LOSS: 4.386266878749926e-06\n",
      "TRAIN: EPOCH 592/1000 | BATCH 70/71 | LOSS: 4.384461833620329e-06\n",
      "VAL: EPOCH 592/1000 | BATCH 0/8 | LOSS: 4.344683475210331e-06\n",
      "VAL: EPOCH 592/1000 | BATCH 1/8 | LOSS: 4.4996168071520515e-06\n",
      "VAL: EPOCH 592/1000 | BATCH 2/8 | LOSS: 4.6574623411288485e-06\n",
      "VAL: EPOCH 592/1000 | BATCH 3/8 | LOSS: 4.62678247004078e-06\n",
      "VAL: EPOCH 592/1000 | BATCH 4/8 | LOSS: 4.715511204267387e-06\n",
      "VAL: EPOCH 592/1000 | BATCH 5/8 | LOSS: 4.696325201318056e-06\n",
      "VAL: EPOCH 592/1000 | BATCH 6/8 | LOSS: 4.5745936664129005e-06\n",
      "VAL: EPOCH 592/1000 | BATCH 7/8 | LOSS: 4.3984535693653015e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 0/71 | LOSS: 4.480186362343375e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 1/71 | LOSS: 4.671814622270176e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 2/71 | LOSS: 4.294577517309032e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 3/71 | LOSS: 4.550907704015117e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 4/71 | LOSS: 4.360570437711431e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 5/71 | LOSS: 4.515731158486839e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 6/71 | LOSS: 4.5578579244777624e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 7/71 | LOSS: 4.517831371231296e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 8/71 | LOSS: 4.4396353789327095e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 9/71 | LOSS: 4.34768621744297e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 10/71 | LOSS: 4.312250646101099e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 11/71 | LOSS: 4.320197869371138e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 12/71 | LOSS: 4.2450406174164246e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 13/71 | LOSS: 4.223914580896755e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 14/71 | LOSS: 4.2442412601909984e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 15/71 | LOSS: 4.152711497340533e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 16/71 | LOSS: 4.15644055518736e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 17/71 | LOSS: 4.155407762280245e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 18/71 | LOSS: 4.200401439160678e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 19/71 | LOSS: 4.215749333980056e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 20/71 | LOSS: 4.2531274262035456e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 21/71 | LOSS: 4.2684105520410496e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 22/71 | LOSS: 4.243244575263816e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 23/71 | LOSS: 4.181685521113347e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 24/71 | LOSS: 4.157702314842027e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 25/71 | LOSS: 4.20091852832299e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 26/71 | LOSS: 4.221635781168179e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 27/71 | LOSS: 4.304382254563929e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 28/71 | LOSS: 4.314918724362385e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 29/71 | LOSS: 4.412140090911028e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 30/71 | LOSS: 4.399106097891138e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 31/71 | LOSS: 4.456505735106475e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 32/71 | LOSS: 4.449416585637002e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 33/71 | LOSS: 4.464457694720134e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 34/71 | LOSS: 4.443024278901118e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 35/71 | LOSS: 4.43492455006991e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 36/71 | LOSS: 4.454803852363634e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 37/71 | LOSS: 4.422037842519594e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 38/71 | LOSS: 4.479882462850909e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 39/71 | LOSS: 4.522630581504927e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 40/71 | LOSS: 4.574257295976943e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 41/71 | LOSS: 4.60597024833814e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 42/71 | LOSS: 4.6558362069041575e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 43/71 | LOSS: 4.701027054662435e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 44/71 | LOSS: 4.772134949841226e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 45/71 | LOSS: 4.823461109560372e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 46/71 | LOSS: 4.815555151234297e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 47/71 | LOSS: 4.901109103154037e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 48/71 | LOSS: 4.926526975284606e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 49/71 | LOSS: 4.937718940709601e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 50/71 | LOSS: 4.9236852075909175e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 51/71 | LOSS: 4.972154845115885e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 52/71 | LOSS: 4.963428482541239e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 53/71 | LOSS: 5.020290911017111e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 54/71 | LOSS: 4.990255721085387e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 55/71 | LOSS: 5.02246958181526e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 56/71 | LOSS: 5.040041343275797e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 57/71 | LOSS: 5.049419534206491e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 58/71 | LOSS: 5.039611258294919e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 59/71 | LOSS: 5.046129664757851e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 60/71 | LOSS: 5.025350640712308e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 61/71 | LOSS: 4.995704517246087e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 62/71 | LOSS: 4.9758352454932355e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 63/71 | LOSS: 4.963834303595149e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 64/71 | LOSS: 4.9460086512016666e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 65/71 | LOSS: 4.930199951779672e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 66/71 | LOSS: 4.9199316388477376e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 67/71 | LOSS: 4.909502730002072e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 68/71 | LOSS: 4.90773377812044e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 69/71 | LOSS: 4.9045138179540766e-06\n",
      "TRAIN: EPOCH 593/1000 | BATCH 70/71 | LOSS: 4.910572731115652e-06\n",
      "VAL: EPOCH 593/1000 | BATCH 0/8 | LOSS: 4.0131344576366246e-06\n",
      "VAL: EPOCH 593/1000 | BATCH 1/8 | LOSS: 4.591151991917286e-06\n",
      "VAL: EPOCH 593/1000 | BATCH 2/8 | LOSS: 4.7897374315653e-06\n",
      "VAL: EPOCH 593/1000 | BATCH 3/8 | LOSS: 4.853114432989969e-06\n",
      "VAL: EPOCH 593/1000 | BATCH 4/8 | LOSS: 4.859324508288409e-06\n",
      "VAL: EPOCH 593/1000 | BATCH 5/8 | LOSS: 4.900468638879829e-06\n",
      "VAL: EPOCH 593/1000 | BATCH 6/8 | LOSS: 4.7987100515456405e-06\n",
      "VAL: EPOCH 593/1000 | BATCH 7/8 | LOSS: 4.667289914550565e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 0/71 | LOSS: 5.160473847354297e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 1/71 | LOSS: 4.922944071950042e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 2/71 | LOSS: 4.6775101812575786e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 3/71 | LOSS: 4.499710371419496e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 4/71 | LOSS: 4.429013006301829e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 5/71 | LOSS: 4.455603781631605e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 6/71 | LOSS: 4.508963619238264e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 7/71 | LOSS: 4.658537420709763e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 8/71 | LOSS: 4.6613080384203286e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 9/71 | LOSS: 4.919782122669858e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 10/71 | LOSS: 4.869966935736804e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 11/71 | LOSS: 4.844452822302022e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 12/71 | LOSS: 4.969593401955745e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 13/71 | LOSS: 4.98554654119029e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 14/71 | LOSS: 4.946040310945439e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 15/71 | LOSS: 4.837455122697065e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 16/71 | LOSS: 4.859466250362696e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 17/71 | LOSS: 4.788275772524584e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 18/71 | LOSS: 4.7780337425999615e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 19/71 | LOSS: 4.749806225845532e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 20/71 | LOSS: 4.757646409069864e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 21/71 | LOSS: 4.778818596274835e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 22/71 | LOSS: 4.760248588275357e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 23/71 | LOSS: 4.733466482775839e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 24/71 | LOSS: 4.777603116963291e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 25/71 | LOSS: 4.785744557431406e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 26/71 | LOSS: 4.750433601185688e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 27/71 | LOSS: 4.746769978477719e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 28/71 | LOSS: 4.70336154366719e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 29/71 | LOSS: 4.681080205652203e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 30/71 | LOSS: 4.693378888341906e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 31/71 | LOSS: 4.668895826398511e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 32/71 | LOSS: 4.6570329907284975e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 33/71 | LOSS: 4.639128693430821e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 34/71 | LOSS: 4.598711172677992e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 35/71 | LOSS: 4.570808338636804e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 36/71 | LOSS: 4.550629037603849e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 37/71 | LOSS: 4.529079905966539e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 38/71 | LOSS: 4.506732921072398e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 39/71 | LOSS: 4.480146719743061e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 40/71 | LOSS: 4.453477236982108e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 41/71 | LOSS: 4.458116645115037e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 42/71 | LOSS: 4.489609187488853e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 43/71 | LOSS: 4.4773404035616045e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 44/71 | LOSS: 4.475860396269127e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 45/71 | LOSS: 4.463039001324353e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 46/71 | LOSS: 4.457474755752706e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 47/71 | LOSS: 4.464488649584079e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 48/71 | LOSS: 4.454014413345342e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 49/71 | LOSS: 4.438101100276981e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 50/71 | LOSS: 4.441427688338901e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 51/71 | LOSS: 4.4297904519925605e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 52/71 | LOSS: 4.42150099791794e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 53/71 | LOSS: 4.403741728411761e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 54/71 | LOSS: 4.420854288996038e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 55/71 | LOSS: 4.405452005227874e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 56/71 | LOSS: 4.395491926021478e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 57/71 | LOSS: 4.416661066239487e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 58/71 | LOSS: 4.4144552610582725e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 59/71 | LOSS: 4.408670876424973e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 60/71 | LOSS: 4.403062158402105e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 61/71 | LOSS: 4.460607149139246e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 62/71 | LOSS: 4.449904512483275e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 63/71 | LOSS: 4.447688823461249e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 64/71 | LOSS: 4.4419615873126675e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 65/71 | LOSS: 4.436844585253504e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 66/71 | LOSS: 4.432445465097961e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 67/71 | LOSS: 4.433137077927206e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 68/71 | LOSS: 4.4461492876323154e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 69/71 | LOSS: 4.468821290564457e-06\n",
      "TRAIN: EPOCH 594/1000 | BATCH 70/71 | LOSS: 4.501747304895847e-06\n",
      "VAL: EPOCH 594/1000 | BATCH 0/8 | LOSS: 3.8438865885837e-06\n",
      "VAL: EPOCH 594/1000 | BATCH 1/8 | LOSS: 4.341279236541595e-06\n",
      "VAL: EPOCH 594/1000 | BATCH 2/8 | LOSS: 4.647930836654268e-06\n",
      "VAL: EPOCH 594/1000 | BATCH 3/8 | LOSS: 4.6665230684084236e-06\n",
      "VAL: EPOCH 594/1000 | BATCH 4/8 | LOSS: 4.721498498838628e-06\n",
      "VAL: EPOCH 594/1000 | BATCH 5/8 | LOSS: 4.7195411904491875e-06\n",
      "VAL: EPOCH 594/1000 | BATCH 6/8 | LOSS: 4.635670393327018e-06\n",
      "VAL: EPOCH 594/1000 | BATCH 7/8 | LOSS: 4.5660718797080335e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 0/71 | LOSS: 4.170283773419214e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 1/71 | LOSS: 5.549523166337167e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 2/71 | LOSS: 5.629423715921196e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 3/71 | LOSS: 5.397391987571609e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 4/71 | LOSS: 5.691079422831535e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 5/71 | LOSS: 5.853627044416498e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 6/71 | LOSS: 5.7953706605725785e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 7/71 | LOSS: 5.860367650711851e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 8/71 | LOSS: 5.7730921475417236e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 9/71 | LOSS: 5.574825627263635e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 10/71 | LOSS: 5.550621897947911e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 11/71 | LOSS: 5.5018329779462265e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 12/71 | LOSS: 5.385959341835517e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 13/71 | LOSS: 5.431491834834949e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 14/71 | LOSS: 5.285893500210174e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 15/71 | LOSS: 5.258587776779677e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 16/71 | LOSS: 5.211545746586874e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 17/71 | LOSS: 5.1275603962292534e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 18/71 | LOSS: 5.084741324504935e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 19/71 | LOSS: 5.121184153722425e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 20/71 | LOSS: 5.02789775748083e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 21/71 | LOSS: 5.022987600320696e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 22/71 | LOSS: 4.983382812339266e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 23/71 | LOSS: 4.9472191676613875e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 24/71 | LOSS: 4.923928645439446e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 25/71 | LOSS: 4.909060635327478e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 26/71 | LOSS: 4.907533366349526e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 27/71 | LOSS: 4.882101783160968e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 28/71 | LOSS: 4.842364825012267e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 29/71 | LOSS: 4.787054566198397e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 30/71 | LOSS: 4.741467063790731e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 31/71 | LOSS: 4.742339307028942e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 32/71 | LOSS: 4.710927669293596e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 33/71 | LOSS: 4.719672226790007e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 34/71 | LOSS: 4.688082006915855e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 35/71 | LOSS: 4.692410376113306e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 36/71 | LOSS: 4.709308940791006e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 37/71 | LOSS: 4.704262000574856e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 38/71 | LOSS: 4.6666218331199325e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 39/71 | LOSS: 4.682630918750874e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 40/71 | LOSS: 4.699408796890256e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 41/71 | LOSS: 4.712600073054048e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 42/71 | LOSS: 4.678609282691045e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 43/71 | LOSS: 4.701207821901996e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 44/71 | LOSS: 4.712610274913863e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 45/71 | LOSS: 4.732233813466218e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 46/71 | LOSS: 4.748942545004266e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 47/71 | LOSS: 4.730401575632944e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 48/71 | LOSS: 4.730751461280232e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 49/71 | LOSS: 4.734595563604671e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 50/71 | LOSS: 4.740165471886547e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 51/71 | LOSS: 4.735179603498042e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 52/71 | LOSS: 4.720366456789428e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 53/71 | LOSS: 4.749640517595118e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 54/71 | LOSS: 4.7490941845105475e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 55/71 | LOSS: 4.751866595954977e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 56/71 | LOSS: 4.754440967365538e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 57/71 | LOSS: 4.75110429588626e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 58/71 | LOSS: 4.734028033130078e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 59/71 | LOSS: 4.729000628837335e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 60/71 | LOSS: 4.742922693948771e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 61/71 | LOSS: 4.752251048821856e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 62/71 | LOSS: 4.75822410783347e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 63/71 | LOSS: 4.759543120513854e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 64/71 | LOSS: 4.7705947193977545e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 65/71 | LOSS: 4.759009212565698e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 66/71 | LOSS: 4.757901136267108e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 67/71 | LOSS: 4.749448993045328e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 68/71 | LOSS: 4.731086784121926e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 69/71 | LOSS: 4.725354694658433e-06\n",
      "TRAIN: EPOCH 595/1000 | BATCH 70/71 | LOSS: 4.728616771207668e-06\n",
      "VAL: EPOCH 595/1000 | BATCH 0/8 | LOSS: 5.963572220935021e-06\n",
      "VAL: EPOCH 595/1000 | BATCH 1/8 | LOSS: 6.65197921989602e-06\n",
      "VAL: EPOCH 595/1000 | BATCH 2/8 | LOSS: 7.1185116515456075e-06\n",
      "VAL: EPOCH 595/1000 | BATCH 3/8 | LOSS: 7.144834512473608e-06\n",
      "VAL: EPOCH 595/1000 | BATCH 4/8 | LOSS: 7.2410709435644096e-06\n",
      "VAL: EPOCH 595/1000 | BATCH 5/8 | LOSS: 7.404164762192522e-06\n",
      "VAL: EPOCH 595/1000 | BATCH 6/8 | LOSS: 7.316286818032884e-06\n",
      "VAL: EPOCH 595/1000 | BATCH 7/8 | LOSS: 7.382406124634144e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 0/71 | LOSS: 6.39736663288204e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 1/71 | LOSS: 5.609175786958076e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 2/71 | LOSS: 5.582818024170895e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 3/71 | LOSS: 5.8078976508113556e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 4/71 | LOSS: 5.944671283941716e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 5/71 | LOSS: 6.062785814719973e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 6/71 | LOSS: 5.753218699412953e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 7/71 | LOSS: 5.974753491955198e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 8/71 | LOSS: 5.781640235606271e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 9/71 | LOSS: 5.935453782512923e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 10/71 | LOSS: 5.8062347224305e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 11/71 | LOSS: 5.856795117627674e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 12/71 | LOSS: 5.874899670743616e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 13/71 | LOSS: 5.885546508060153e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 14/71 | LOSS: 5.876461545994971e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 15/71 | LOSS: 5.748364458213473e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 16/71 | LOSS: 5.88133030653219e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 17/71 | LOSS: 5.846500648153273e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 18/71 | LOSS: 5.81775169082807e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 19/71 | LOSS: 5.768626260760357e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 20/71 | LOSS: 5.65205003849336e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 21/71 | LOSS: 5.690070917229274e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 22/71 | LOSS: 5.598820102339074e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 23/71 | LOSS: 5.571615417920839e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 24/71 | LOSS: 5.515198990906356e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 25/71 | LOSS: 5.479180572365294e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 26/71 | LOSS: 5.446971600677999e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 27/71 | LOSS: 5.3982924847722254e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 28/71 | LOSS: 5.392905316919746e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 29/71 | LOSS: 5.364050730349846e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 30/71 | LOSS: 5.300595339283445e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 31/71 | LOSS: 5.277011240423235e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 32/71 | LOSS: 5.266767705810101e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 33/71 | LOSS: 5.303925076184575e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 34/71 | LOSS: 5.274611521599582e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 35/71 | LOSS: 5.230233240480528e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 36/71 | LOSS: 5.2221295378773434e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 37/71 | LOSS: 5.239202090179836e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 38/71 | LOSS: 5.210197298602464e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 39/71 | LOSS: 5.183835037314566e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 40/71 | LOSS: 5.15643341714167e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 41/71 | LOSS: 5.1502681257954376e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 42/71 | LOSS: 5.115581765240614e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 43/71 | LOSS: 5.0988628831047125e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 44/71 | LOSS: 5.071284830490994e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 45/71 | LOSS: 5.071173562931148e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 46/71 | LOSS: 5.053715003281893e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 47/71 | LOSS: 5.037386969775071e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 48/71 | LOSS: 5.037761225974798e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 49/71 | LOSS: 5.0208705079057835e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 50/71 | LOSS: 5.002097984341911e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 51/71 | LOSS: 4.9786903822188415e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 52/71 | LOSS: 4.970119544970726e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 53/71 | LOSS: 4.943786604127564e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 54/71 | LOSS: 4.927974751725825e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 55/71 | LOSS: 4.910532981382078e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 56/71 | LOSS: 4.896832032292814e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 57/71 | LOSS: 4.899955287924215e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 58/71 | LOSS: 4.904975524260731e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 59/71 | LOSS: 4.890831750496242e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 60/71 | LOSS: 4.869298995576263e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 61/71 | LOSS: 4.8742267132007736e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 62/71 | LOSS: 4.858091830418931e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 63/71 | LOSS: 4.847443339173196e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 64/71 | LOSS: 4.828891944551008e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 65/71 | LOSS: 4.8304396311020055e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 66/71 | LOSS: 4.809946159496834e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 67/71 | LOSS: 4.7993757347927276e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 68/71 | LOSS: 4.802153299628433e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 69/71 | LOSS: 4.800039817252712e-06\n",
      "TRAIN: EPOCH 596/1000 | BATCH 70/71 | LOSS: 4.80965322630011e-06\n",
      "VAL: EPOCH 596/1000 | BATCH 0/8 | LOSS: 6.289172233664431e-06\n",
      "VAL: EPOCH 596/1000 | BATCH 1/8 | LOSS: 6.424790171877248e-06\n",
      "VAL: EPOCH 596/1000 | BATCH 2/8 | LOSS: 6.565547361484884e-06\n",
      "VAL: EPOCH 596/1000 | BATCH 3/8 | LOSS: 6.497011668216146e-06\n",
      "VAL: EPOCH 596/1000 | BATCH 4/8 | LOSS: 6.585312439710833e-06\n",
      "VAL: EPOCH 596/1000 | BATCH 5/8 | LOSS: 6.380065315170214e-06\n",
      "VAL: EPOCH 596/1000 | BATCH 6/8 | LOSS: 6.201483886668159e-06\n",
      "VAL: EPOCH 596/1000 | BATCH 7/8 | LOSS: 6.002892178003094e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 0/71 | LOSS: 5.4392321544582956e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 1/71 | LOSS: 5.42692009730672e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 2/71 | LOSS: 5.609197917995819e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 3/71 | LOSS: 5.604306920758972e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 4/71 | LOSS: 5.887308088858845e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 5/71 | LOSS: 5.389554720143981e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 6/71 | LOSS: 5.0580219976836815e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 7/71 | LOSS: 5.105185095999332e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 8/71 | LOSS: 5.058699369126569e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 9/71 | LOSS: 4.959384295943892e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 10/71 | LOSS: 4.884239140003708e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 11/71 | LOSS: 4.804466016139486e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 12/71 | LOSS: 4.732885197601998e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 13/71 | LOSS: 4.7783830398527376e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 14/71 | LOSS: 4.809854696456266e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 15/71 | LOSS: 4.748122904629781e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 16/71 | LOSS: 4.72817309320603e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 17/71 | LOSS: 4.721681004108784e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 18/71 | LOSS: 4.715724882867346e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 19/71 | LOSS: 4.723840152109915e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 20/71 | LOSS: 4.675296904246179e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 21/71 | LOSS: 4.691596772814095e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 22/71 | LOSS: 4.702572230440716e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 23/71 | LOSS: 4.667695236548752e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 24/71 | LOSS: 4.682247054006439e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 25/71 | LOSS: 4.6424986696645365e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 26/71 | LOSS: 4.676286809828727e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 27/71 | LOSS: 4.70471785060259e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 28/71 | LOSS: 4.74135836627503e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 29/71 | LOSS: 4.772294778376817e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 30/71 | LOSS: 4.8222544489450934e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 31/71 | LOSS: 4.8045483680425605e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 32/71 | LOSS: 4.833951290052165e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 33/71 | LOSS: 4.8501883210359236e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 34/71 | LOSS: 4.858631252448374e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 35/71 | LOSS: 4.838997395179629e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 36/71 | LOSS: 4.850743558514246e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 37/71 | LOSS: 4.870543056025972e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 38/71 | LOSS: 4.832259964342274e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 39/71 | LOSS: 4.889841869726297e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 40/71 | LOSS: 4.8791221327452906e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 41/71 | LOSS: 4.8722592910486304e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 42/71 | LOSS: 4.935289849146461e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 43/71 | LOSS: 4.939256453698445e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 44/71 | LOSS: 4.962403297920698e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 45/71 | LOSS: 4.9701423044414535e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 46/71 | LOSS: 4.998997634716711e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 47/71 | LOSS: 4.984661250280927e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 48/71 | LOSS: 4.981903142537221e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 49/71 | LOSS: 4.9490425908516045e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 50/71 | LOSS: 4.937866829184749e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 51/71 | LOSS: 4.92863738197859e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 52/71 | LOSS: 4.928462576915406e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 53/71 | LOSS: 4.918967580057075e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 54/71 | LOSS: 4.919223097617346e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 55/71 | LOSS: 4.921893861364879e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 56/71 | LOSS: 4.892056778229298e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 57/71 | LOSS: 4.878178056687516e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 58/71 | LOSS: 4.888083833713485e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 59/71 | LOSS: 4.869693987075152e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 60/71 | LOSS: 4.854052636413845e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 61/71 | LOSS: 4.834811207846572e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 62/71 | LOSS: 4.846443108897166e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 63/71 | LOSS: 4.844007246873616e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 64/71 | LOSS: 4.828270053062274e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 65/71 | LOSS: 4.817302290189907e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 66/71 | LOSS: 4.834417502445546e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 67/71 | LOSS: 4.831555613423809e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 68/71 | LOSS: 4.817512599006675e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 69/71 | LOSS: 4.82123278158854e-06\n",
      "TRAIN: EPOCH 597/1000 | BATCH 70/71 | LOSS: 4.820714401587242e-06\n",
      "VAL: EPOCH 597/1000 | BATCH 0/8 | LOSS: 3.6301012187323067e-06\n",
      "VAL: EPOCH 597/1000 | BATCH 1/8 | LOSS: 3.949965048377635e-06\n",
      "VAL: EPOCH 597/1000 | BATCH 2/8 | LOSS: 4.137363248446491e-06\n",
      "VAL: EPOCH 597/1000 | BATCH 3/8 | LOSS: 4.136138841204229e-06\n",
      "VAL: EPOCH 597/1000 | BATCH 4/8 | LOSS: 4.2041086999233815e-06\n",
      "VAL: EPOCH 597/1000 | BATCH 5/8 | LOSS: 4.24606044665173e-06\n",
      "VAL: EPOCH 597/1000 | BATCH 6/8 | LOSS: 4.137511008853575e-06\n",
      "VAL: EPOCH 597/1000 | BATCH 7/8 | LOSS: 4.069812547413676e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 0/71 | LOSS: 3.5996829410578357e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 1/71 | LOSS: 4.0572041370978695e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 2/71 | LOSS: 4.448251047506346e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 3/71 | LOSS: 4.369674059034878e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 4/71 | LOSS: 4.594346091835178e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 5/71 | LOSS: 4.6107655104303076e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 6/71 | LOSS: 4.79056503926196e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 7/71 | LOSS: 4.571287547605607e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 8/71 | LOSS: 4.653035577777902e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 9/71 | LOSS: 4.641508485292434e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 10/71 | LOSS: 4.701619218394626e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 11/71 | LOSS: 4.726707250786906e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 12/71 | LOSS: 4.739252989775895e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 13/71 | LOSS: 4.811976006229608e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 14/71 | LOSS: 4.887388968199957e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 15/71 | LOSS: 4.90358470983665e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 16/71 | LOSS: 5.012395109481397e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 17/71 | LOSS: 4.99203189166211e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 18/71 | LOSS: 5.0558317908373855e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 19/71 | LOSS: 5.057599923929956e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 20/71 | LOSS: 4.988346886204367e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 21/71 | LOSS: 4.954338051406508e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 22/71 | LOSS: 4.9283336090371925e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 23/71 | LOSS: 4.879011347232638e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 24/71 | LOSS: 4.8298732872353865e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 25/71 | LOSS: 4.7644405360467945e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 26/71 | LOSS: 4.7806195450700925e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 27/71 | LOSS: 4.746063698998374e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 28/71 | LOSS: 4.72022918137523e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 29/71 | LOSS: 4.72246781555441e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 30/71 | LOSS: 4.7052538452736635e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 31/71 | LOSS: 4.684389800502231e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 32/71 | LOSS: 4.646390696948264e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 33/71 | LOSS: 4.657558500131178e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 34/71 | LOSS: 4.646482238968019e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 35/71 | LOSS: 4.615342839365945e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 36/71 | LOSS: 4.645317495807241e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 37/71 | LOSS: 4.6278535077554376e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 38/71 | LOSS: 4.617346494575404e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 39/71 | LOSS: 4.58525763065154e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 40/71 | LOSS: 4.592350866759505e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 41/71 | LOSS: 4.587013049352945e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 42/71 | LOSS: 4.564593556879541e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 43/71 | LOSS: 4.553864078554232e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 44/71 | LOSS: 4.520913439086548e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 45/71 | LOSS: 4.521321389090546e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 46/71 | LOSS: 4.537894618748551e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 47/71 | LOSS: 4.536365305322458e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 48/71 | LOSS: 4.54754644013111e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 49/71 | LOSS: 4.530843066277157e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 50/71 | LOSS: 4.5444367194303435e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 51/71 | LOSS: 4.537635745166951e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 52/71 | LOSS: 4.537945003148535e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 53/71 | LOSS: 4.5211793475472305e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 54/71 | LOSS: 4.506045773020808e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 55/71 | LOSS: 4.509088914314166e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 56/71 | LOSS: 4.50428892531419e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 57/71 | LOSS: 4.5176355084110806e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 58/71 | LOSS: 4.499953578395712e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 59/71 | LOSS: 4.484066399375782e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 60/71 | LOSS: 4.466248121210155e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 61/71 | LOSS: 4.459633670751739e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 62/71 | LOSS: 4.462566385839073e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 63/71 | LOSS: 4.450412813383764e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 64/71 | LOSS: 4.4516058656322104e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 65/71 | LOSS: 4.4555731309711435e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 66/71 | LOSS: 4.443778546224913e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 67/71 | LOSS: 4.430670203954137e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 68/71 | LOSS: 4.417169975567958e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 69/71 | LOSS: 4.412231461563871e-06\n",
      "TRAIN: EPOCH 598/1000 | BATCH 70/71 | LOSS: 4.398753876749895e-06\n",
      "VAL: EPOCH 598/1000 | BATCH 0/8 | LOSS: 3.5361961181479273e-06\n",
      "VAL: EPOCH 598/1000 | BATCH 1/8 | LOSS: 3.973001980739355e-06\n",
      "VAL: EPOCH 598/1000 | BATCH 2/8 | LOSS: 4.0710478212228436e-06\n",
      "VAL: EPOCH 598/1000 | BATCH 3/8 | LOSS: 4.119925108625466e-06\n",
      "VAL: EPOCH 598/1000 | BATCH 4/8 | LOSS: 4.1326031805510866e-06\n",
      "VAL: EPOCH 598/1000 | BATCH 5/8 | LOSS: 4.146604283050692e-06\n",
      "VAL: EPOCH 598/1000 | BATCH 6/8 | LOSS: 4.016866731555118e-06\n",
      "VAL: EPOCH 598/1000 | BATCH 7/8 | LOSS: 3.849043253012496e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 0/71 | LOSS: 3.3164019441755954e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 1/71 | LOSS: 3.264782662881771e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 2/71 | LOSS: 3.245588535113105e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 3/71 | LOSS: 3.2270835390590946e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 4/71 | LOSS: 3.365846714586951e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 5/71 | LOSS: 3.5865314202965237e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 6/71 | LOSS: 3.761984122060572e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 7/71 | LOSS: 3.802029254984518e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 8/71 | LOSS: 3.931551974447858e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 9/71 | LOSS: 4.037496773889871e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 10/71 | LOSS: 4.040227850055089e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 11/71 | LOSS: 3.990386062469042e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 12/71 | LOSS: 4.0077953389509976e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 13/71 | LOSS: 4.0003053852290445e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 14/71 | LOSS: 4.038353047993345e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 15/71 | LOSS: 4.081268031086438e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 16/71 | LOSS: 4.102685740209006e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 17/71 | LOSS: 4.10525902023235e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 18/71 | LOSS: 4.158690540191999e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 19/71 | LOSS: 4.18706176787964e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 20/71 | LOSS: 4.192084816168062e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 21/71 | LOSS: 4.209933525668351e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 22/71 | LOSS: 4.221100227922484e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 23/71 | LOSS: 4.211145854545369e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 24/71 | LOSS: 4.1973435145337135e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 25/71 | LOSS: 4.249949877423485e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 26/71 | LOSS: 4.24515795842766e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 27/71 | LOSS: 4.268384567954594e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 28/71 | LOSS: 4.276352260133316e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 29/71 | LOSS: 4.289064994130361e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 30/71 | LOSS: 4.337531590276593e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 31/71 | LOSS: 4.334210316869758e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 32/71 | LOSS: 4.354912176274935e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 33/71 | LOSS: 4.369946001294439e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 34/71 | LOSS: 4.4443550905270964e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 35/71 | LOSS: 4.414375452041794e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 36/71 | LOSS: 4.4479626209264295e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 37/71 | LOSS: 4.442663590740076e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 38/71 | LOSS: 4.436881075283357e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 39/71 | LOSS: 4.441585929271241e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 40/71 | LOSS: 4.468518499179925e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 41/71 | LOSS: 4.440604741484137e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 42/71 | LOSS: 4.48397935711952e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 43/71 | LOSS: 4.4888735870203656e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 44/71 | LOSS: 4.523477158121144e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 45/71 | LOSS: 4.548130272783253e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 46/71 | LOSS: 4.5541329742031555e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 47/71 | LOSS: 4.670291910239636e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 48/71 | LOSS: 4.672053967865674e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 49/71 | LOSS: 4.6997479967103575e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 50/71 | LOSS: 4.6886413977464304e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 51/71 | LOSS: 4.700200607798111e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 52/71 | LOSS: 4.692777265346486e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 53/71 | LOSS: 4.739070643861649e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 54/71 | LOSS: 4.7247159851725556e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 55/71 | LOSS: 4.778130443940297e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 56/71 | LOSS: 4.760247799838493e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 57/71 | LOSS: 4.816730387542991e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 58/71 | LOSS: 4.7869817358409584e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 59/71 | LOSS: 4.779680144414063e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 60/71 | LOSS: 4.759171107536054e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 61/71 | LOSS: 4.759160751452709e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 62/71 | LOSS: 4.737262503430617e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 63/71 | LOSS: 4.736358349077818e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 64/71 | LOSS: 4.735772787311222e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 65/71 | LOSS: 4.717458916109701e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 66/71 | LOSS: 4.701049379240322e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 67/71 | LOSS: 4.6951169866481316e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 68/71 | LOSS: 4.677696947312696e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 69/71 | LOSS: 4.668998563569662e-06\n",
      "TRAIN: EPOCH 599/1000 | BATCH 70/71 | LOSS: 4.70227770857094e-06\n",
      "VAL: EPOCH 599/1000 | BATCH 0/8 | LOSS: 4.062908374180552e-06\n",
      "VAL: EPOCH 599/1000 | BATCH 1/8 | LOSS: 4.694556992035359e-06\n",
      "VAL: EPOCH 599/1000 | BATCH 2/8 | LOSS: 4.863082115965274e-06\n",
      "VAL: EPOCH 599/1000 | BATCH 3/8 | LOSS: 4.92675985697133e-06\n",
      "VAL: EPOCH 599/1000 | BATCH 4/8 | LOSS: 4.974340026819846e-06\n",
      "VAL: EPOCH 599/1000 | BATCH 5/8 | LOSS: 5.071542924876364e-06\n",
      "VAL: EPOCH 599/1000 | BATCH 6/8 | LOSS: 4.9831705837277696e-06\n",
      "VAL: EPOCH 599/1000 | BATCH 7/8 | LOSS: 4.9061188178711745e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 0/71 | LOSS: 5.800678081868682e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 1/71 | LOSS: 4.6609858372903545e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 2/71 | LOSS: 4.292725786096223e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 3/71 | LOSS: 4.352283610842278e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 4/71 | LOSS: 4.355201008365839e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 5/71 | LOSS: 4.624662134726047e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 6/71 | LOSS: 4.4828586526689056e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 7/71 | LOSS: 4.5712007477050065e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 8/71 | LOSS: 4.60552453457947e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 9/71 | LOSS: 4.582362498695147e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 10/71 | LOSS: 4.5728269965779456e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 11/71 | LOSS: 4.521390640851071e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 12/71 | LOSS: 4.596470681952689e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 13/71 | LOSS: 4.558053888104041e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 14/71 | LOSS: 4.566688069947608e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 15/71 | LOSS: 4.621846159125198e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 16/71 | LOSS: 4.591542566392009e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 17/71 | LOSS: 4.640305456228412e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 18/71 | LOSS: 4.62200741343335e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 19/71 | LOSS: 4.673076091421535e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 20/71 | LOSS: 4.609171074185897e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 21/71 | LOSS: 4.642393378162524e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 22/71 | LOSS: 4.632342571984855e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 23/71 | LOSS: 4.628211532538747e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 24/71 | LOSS: 4.595294622049551e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 25/71 | LOSS: 4.6104370487507995e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 26/71 | LOSS: 4.551360217630173e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 27/71 | LOSS: 4.530561942114478e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 28/71 | LOSS: 4.5149193848091864e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 29/71 | LOSS: 4.483548870363544e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 30/71 | LOSS: 4.462375419543898e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 31/71 | LOSS: 4.437190639805522e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 32/71 | LOSS: 4.440911170328918e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 33/71 | LOSS: 4.450257660316899e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 34/71 | LOSS: 4.4378289593233995e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 35/71 | LOSS: 4.440884873070496e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 36/71 | LOSS: 4.425261719203459e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 37/71 | LOSS: 4.402028600069523e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 38/71 | LOSS: 4.4397532362311795e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 39/71 | LOSS: 4.440652594439598e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 40/71 | LOSS: 4.4003549214167145e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 41/71 | LOSS: 4.419327037144935e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 42/71 | LOSS: 4.447766619959174e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 43/71 | LOSS: 4.4213377040333315e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 44/71 | LOSS: 4.430116809493888e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 45/71 | LOSS: 4.444224892406213e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 46/71 | LOSS: 4.429942701461457e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 47/71 | LOSS: 4.414294058581921e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 48/71 | LOSS: 4.414303819022298e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 49/71 | LOSS: 4.400218826958735e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 50/71 | LOSS: 4.39616334077465e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 51/71 | LOSS: 4.386715899657317e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 52/71 | LOSS: 4.359954581954264e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 53/71 | LOSS: 4.367848730537473e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 54/71 | LOSS: 4.382304281204018e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 55/71 | LOSS: 4.376123444314674e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 56/71 | LOSS: 4.420985936105768e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 57/71 | LOSS: 4.4080371669130315e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 58/71 | LOSS: 4.436655325338568e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 59/71 | LOSS: 4.4167006914600885e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 60/71 | LOSS: 4.450374706452674e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 61/71 | LOSS: 4.434263253701389e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 62/71 | LOSS: 4.456769169164112e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 63/71 | LOSS: 4.4679706832084776e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 64/71 | LOSS: 4.464605318879387e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 65/71 | LOSS: 4.476176717148381e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 66/71 | LOSS: 4.475330705876087e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 67/71 | LOSS: 4.510738663424597e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 68/71 | LOSS: 4.504231879843206e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 69/71 | LOSS: 4.54815025737584e-06\n",
      "TRAIN: EPOCH 600/1000 | BATCH 70/71 | LOSS: 4.542454705738813e-06\n",
      "VAL: EPOCH 600/1000 | BATCH 0/8 | LOSS: 4.407832875585882e-06\n",
      "VAL: EPOCH 600/1000 | BATCH 1/8 | LOSS: 4.8567214889772e-06\n",
      "VAL: EPOCH 600/1000 | BATCH 2/8 | LOSS: 4.933383479510667e-06\n",
      "VAL: EPOCH 600/1000 | BATCH 3/8 | LOSS: 4.955732379130495e-06\n",
      "VAL: EPOCH 600/1000 | BATCH 4/8 | LOSS: 4.9756502448872196e-06\n",
      "VAL: EPOCH 600/1000 | BATCH 5/8 | LOSS: 5.06188666804519e-06\n",
      "VAL: EPOCH 600/1000 | BATCH 6/8 | LOSS: 4.960033038514666e-06\n",
      "VAL: EPOCH 600/1000 | BATCH 7/8 | LOSS: 4.86354008444323e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 0/71 | LOSS: 4.352011274022516e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 1/71 | LOSS: 4.3901354729314335e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 2/71 | LOSS: 4.7479288696195e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 3/71 | LOSS: 4.845333023695275e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 4/71 | LOSS: 4.677675678976811e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 5/71 | LOSS: 4.748514584207442e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 6/71 | LOSS: 4.652186232436049e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 7/71 | LOSS: 4.573559692744311e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 8/71 | LOSS: 4.503284647701851e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 9/71 | LOSS: 4.538604389381362e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 10/71 | LOSS: 4.527245685246519e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 11/71 | LOSS: 4.473644442744747e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 12/71 | LOSS: 4.485941504893932e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 13/71 | LOSS: 4.493845738449766e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 14/71 | LOSS: 4.4425234136724615e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 15/71 | LOSS: 4.419495567731246e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 16/71 | LOSS: 4.490987053299357e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 17/71 | LOSS: 4.462841578363926e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 18/71 | LOSS: 4.4529768537789715e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 19/71 | LOSS: 4.496151007060689e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 20/71 | LOSS: 4.465042853293458e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 21/71 | LOSS: 4.442439055971838e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 22/71 | LOSS: 4.449184070440622e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 23/71 | LOSS: 4.483678850419892e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 24/71 | LOSS: 4.477600878090016e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 25/71 | LOSS: 4.484279779367521e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 26/71 | LOSS: 4.5016166091550585e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 27/71 | LOSS: 4.450339710209457e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 28/71 | LOSS: 4.408636406314427e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 29/71 | LOSS: 4.381815271396287e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 30/71 | LOSS: 4.3575691007947995e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 31/71 | LOSS: 4.357892940731745e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 32/71 | LOSS: 4.3436790967847845e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 33/71 | LOSS: 4.353341603960229e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 34/71 | LOSS: 4.398390676994625e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 35/71 | LOSS: 4.396308649271911e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 36/71 | LOSS: 4.407214609690945e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 37/71 | LOSS: 4.455724706531328e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 38/71 | LOSS: 4.45795445813975e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 39/71 | LOSS: 4.4812071053002e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 40/71 | LOSS: 4.496214714668983e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 41/71 | LOSS: 4.533547861943329e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 42/71 | LOSS: 4.53177414445151e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 43/71 | LOSS: 4.532098878164768e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 44/71 | LOSS: 4.515616329727992e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 45/71 | LOSS: 4.528702185740732e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 46/71 | LOSS: 4.53547650454795e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 47/71 | LOSS: 4.534805455591595e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 48/71 | LOSS: 4.5307821047962145e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 49/71 | LOSS: 4.514193137765687e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 50/71 | LOSS: 4.528731333088146e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 51/71 | LOSS: 4.508271834206075e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 52/71 | LOSS: 4.514611453247061e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 53/71 | LOSS: 4.511137018636401e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 54/71 | LOSS: 4.497932621399575e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 55/71 | LOSS: 4.481019601598746e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 56/71 | LOSS: 4.492576907292109e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 57/71 | LOSS: 4.474089643014755e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 58/71 | LOSS: 4.460990285841004e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 59/71 | LOSS: 4.458799545166888e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 60/71 | LOSS: 4.456817727199838e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 61/71 | LOSS: 4.4599804229414254e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 62/71 | LOSS: 4.448146477270279e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 63/71 | LOSS: 4.447814117014559e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 64/71 | LOSS: 4.436272953311206e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 65/71 | LOSS: 4.425139915535207e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 66/71 | LOSS: 4.418519421363529e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 67/71 | LOSS: 4.437650769726615e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 68/71 | LOSS: 4.433357643379629e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 69/71 | LOSS: 4.4245537700200135e-06\n",
      "TRAIN: EPOCH 601/1000 | BATCH 70/71 | LOSS: 4.419543173894389e-06\n",
      "VAL: EPOCH 601/1000 | BATCH 0/8 | LOSS: 3.889746494678548e-06\n",
      "VAL: EPOCH 601/1000 | BATCH 1/8 | LOSS: 4.177401251581614e-06\n",
      "VAL: EPOCH 601/1000 | BATCH 2/8 | LOSS: 4.383269545845299e-06\n",
      "VAL: EPOCH 601/1000 | BATCH 3/8 | LOSS: 4.4334810809232295e-06\n",
      "VAL: EPOCH 601/1000 | BATCH 4/8 | LOSS: 4.472464206628502e-06\n",
      "VAL: EPOCH 601/1000 | BATCH 5/8 | LOSS: 4.451091475251208e-06\n",
      "VAL: EPOCH 601/1000 | BATCH 6/8 | LOSS: 4.302269124829243e-06\n",
      "VAL: EPOCH 601/1000 | BATCH 7/8 | LOSS: 4.134138208655713e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 0/71 | LOSS: 3.231139089621138e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 1/71 | LOSS: 4.65461857857008e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 2/71 | LOSS: 4.438034769312556e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 3/71 | LOSS: 4.362486947684374e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 4/71 | LOSS: 4.310465283197118e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 5/71 | LOSS: 4.324455630921875e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 6/71 | LOSS: 4.372795046947431e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 7/71 | LOSS: 4.542213218883262e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 8/71 | LOSS: 4.634477338388226e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 9/71 | LOSS: 4.590318485497846e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 10/71 | LOSS: 4.678505361384437e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 11/71 | LOSS: 4.692388150336531e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 12/71 | LOSS: 4.7697715042606714e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 13/71 | LOSS: 4.782863795428836e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 14/71 | LOSS: 4.881052215447805e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 15/71 | LOSS: 4.8272510184688144e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 16/71 | LOSS: 4.877894940257059e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 17/71 | LOSS: 4.904205954719348e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 18/71 | LOSS: 4.953100654491113e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 19/71 | LOSS: 5.035848903389706e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 20/71 | LOSS: 5.071156292182249e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 21/71 | LOSS: 5.078212221964019e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 22/71 | LOSS: 5.06384442459144e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 23/71 | LOSS: 5.050073923484888e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 24/71 | LOSS: 4.976704231012263e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 25/71 | LOSS: 4.946259152277959e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 26/71 | LOSS: 4.880440538154626e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 27/71 | LOSS: 4.83673391369978e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 28/71 | LOSS: 4.786598843754197e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 29/71 | LOSS: 4.773580152080589e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 30/71 | LOSS: 4.752646433306866e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 31/71 | LOSS: 4.711142679525437e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 32/71 | LOSS: 4.740968189721708e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 33/71 | LOSS: 4.723241612309186e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 34/71 | LOSS: 4.713789179991831e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 35/71 | LOSS: 4.697117320675817e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 36/71 | LOSS: 4.722450235882157e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 37/71 | LOSS: 4.708320327489146e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 38/71 | LOSS: 4.748146168892093e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 39/71 | LOSS: 4.727629038825398e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 40/71 | LOSS: 4.765547024229508e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 41/71 | LOSS: 4.731433158494405e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 42/71 | LOSS: 4.740312538900938e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 43/71 | LOSS: 4.7426825631191605e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 44/71 | LOSS: 4.722198577332569e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 45/71 | LOSS: 4.730953328955524e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 46/71 | LOSS: 4.721131788639778e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 47/71 | LOSS: 4.722463643247465e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 48/71 | LOSS: 4.730368383039901e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 49/71 | LOSS: 4.763534607263864e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 50/71 | LOSS: 4.759422385997787e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 51/71 | LOSS: 4.771426172458003e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 52/71 | LOSS: 4.789433668915046e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 53/71 | LOSS: 4.77102058640513e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 54/71 | LOSS: 4.755186587730846e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 55/71 | LOSS: 4.746102994041783e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 56/71 | LOSS: 4.736069018818262e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 57/71 | LOSS: 4.744065924161736e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 58/71 | LOSS: 4.746412903810871e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 59/71 | LOSS: 4.752076824843243e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 60/71 | LOSS: 4.749548843690599e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 61/71 | LOSS: 4.738504384799706e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 62/71 | LOSS: 4.72975492061417e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 63/71 | LOSS: 4.719007030473676e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 64/71 | LOSS: 4.711986345612856e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 65/71 | LOSS: 4.715374161330119e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 66/71 | LOSS: 4.718392058599257e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 67/71 | LOSS: 4.717557370735021e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 68/71 | LOSS: 4.720796843447834e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 69/71 | LOSS: 4.721249888461898e-06\n",
      "TRAIN: EPOCH 602/1000 | BATCH 70/71 | LOSS: 4.716387458859277e-06\n",
      "VAL: EPOCH 602/1000 | BATCH 0/8 | LOSS: 3.7671529753424693e-06\n",
      "VAL: EPOCH 602/1000 | BATCH 1/8 | LOSS: 4.113931936444715e-06\n",
      "VAL: EPOCH 602/1000 | BATCH 2/8 | LOSS: 4.1944019055032795e-06\n",
      "VAL: EPOCH 602/1000 | BATCH 3/8 | LOSS: 4.147017875766323e-06\n",
      "VAL: EPOCH 602/1000 | BATCH 4/8 | LOSS: 4.203681328363018e-06\n",
      "VAL: EPOCH 602/1000 | BATCH 5/8 | LOSS: 4.125528979178246e-06\n",
      "VAL: EPOCH 602/1000 | BATCH 6/8 | LOSS: 3.989190775298214e-06\n",
      "VAL: EPOCH 602/1000 | BATCH 7/8 | LOSS: 3.860008774836388e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 0/71 | LOSS: 4.068846465088427e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 1/71 | LOSS: 3.968173132307129e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 2/71 | LOSS: 3.9714689895239035e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 3/71 | LOSS: 3.986761385021964e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 4/71 | LOSS: 3.8946197491895875e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 5/71 | LOSS: 4.01200130302944e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 6/71 | LOSS: 4.157033864820343e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 7/71 | LOSS: 4.162220420766971e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 8/71 | LOSS: 4.327190153061464e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 9/71 | LOSS: 4.332026128395227e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 10/71 | LOSS: 4.367661445443942e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 11/71 | LOSS: 4.2919577367683814e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 12/71 | LOSS: 4.258718123450168e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 13/71 | LOSS: 4.328599857217341e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 14/71 | LOSS: 4.2703560211521104e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 15/71 | LOSS: 4.317802620334987e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 16/71 | LOSS: 4.3213489441447614e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 17/71 | LOSS: 4.295612446488424e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 18/71 | LOSS: 4.304259331739128e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 19/71 | LOSS: 4.273494721473981e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 20/71 | LOSS: 4.248693520431905e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 21/71 | LOSS: 4.2624270587442546e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 22/71 | LOSS: 4.2697092003831845e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 23/71 | LOSS: 4.2904479187200195e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 24/71 | LOSS: 4.2903246139758265e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 25/71 | LOSS: 4.276271318723537e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 26/71 | LOSS: 4.268244976157331e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 27/71 | LOSS: 4.285884951319271e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 28/71 | LOSS: 4.3006554796161726e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 29/71 | LOSS: 4.322986069382751e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 30/71 | LOSS: 4.292469135003968e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 31/71 | LOSS: 4.3271812444345414e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 32/71 | LOSS: 4.329587985315205e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 33/71 | LOSS: 4.322091118571252e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 34/71 | LOSS: 4.303843148331257e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 35/71 | LOSS: 4.33789076244769e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 36/71 | LOSS: 4.323538866407722e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 37/71 | LOSS: 4.320497629650115e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 38/71 | LOSS: 4.3596974953685185e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 39/71 | LOSS: 4.352581140665279e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 40/71 | LOSS: 4.34236583869944e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 41/71 | LOSS: 4.341374213774023e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 42/71 | LOSS: 4.3452222027638545e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 43/71 | LOSS: 4.342445219084436e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 44/71 | LOSS: 4.3399127005411885e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 45/71 | LOSS: 4.320294198370665e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 46/71 | LOSS: 4.303460850388411e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 47/71 | LOSS: 4.302384079816572e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 48/71 | LOSS: 4.287973339085845e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 49/71 | LOSS: 4.31301448315935e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 50/71 | LOSS: 4.293975895480424e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 51/71 | LOSS: 4.291621346160705e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 52/71 | LOSS: 4.278537645868195e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 53/71 | LOSS: 4.270704666682663e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 54/71 | LOSS: 4.258933549036473e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 55/71 | LOSS: 4.268844816936378e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 56/71 | LOSS: 4.2650291593497715e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 57/71 | LOSS: 4.260204326752061e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 58/71 | LOSS: 4.280747396601867e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 59/71 | LOSS: 4.281544295281492e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 60/71 | LOSS: 4.28030521703238e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 61/71 | LOSS: 4.292291801873709e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 62/71 | LOSS: 4.284791022280207e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 63/71 | LOSS: 4.2867094229848135e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 64/71 | LOSS: 4.299585869095888e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 65/71 | LOSS: 4.308407669255261e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 66/71 | LOSS: 4.299239503440609e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 67/71 | LOSS: 4.2965694165907145e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 68/71 | LOSS: 4.3041676776013356e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 69/71 | LOSS: 4.308754773774126e-06\n",
      "TRAIN: EPOCH 603/1000 | BATCH 70/71 | LOSS: 4.302083041206778e-06\n",
      "VAL: EPOCH 603/1000 | BATCH 0/8 | LOSS: 5.585840426647337e-06\n",
      "VAL: EPOCH 603/1000 | BATCH 1/8 | LOSS: 5.841443680765224e-06\n",
      "VAL: EPOCH 603/1000 | BATCH 2/8 | LOSS: 5.918073081071877e-06\n",
      "VAL: EPOCH 603/1000 | BATCH 3/8 | LOSS: 5.802553801004251e-06\n",
      "VAL: EPOCH 603/1000 | BATCH 4/8 | LOSS: 5.896007951378124e-06\n",
      "VAL: EPOCH 603/1000 | BATCH 5/8 | LOSS: 5.713215387004311e-06\n",
      "VAL: EPOCH 603/1000 | BATCH 6/8 | LOSS: 5.609998814699273e-06\n",
      "VAL: EPOCH 603/1000 | BATCH 7/8 | LOSS: 5.4129291697790904e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 0/71 | LOSS: 6.961195140320342e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 1/71 | LOSS: 5.573289854510222e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 2/71 | LOSS: 5.7768741802040795e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 3/71 | LOSS: 5.580111292147194e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 4/71 | LOSS: 5.564462844631634e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 5/71 | LOSS: 5.4865489194829325e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 6/71 | LOSS: 5.287843253297199e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 7/71 | LOSS: 5.381631467571424e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 8/71 | LOSS: 5.215853080800217e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 9/71 | LOSS: 5.225333416092326e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 10/71 | LOSS: 5.111162863613572e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 11/71 | LOSS: 5.17550715054919e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 12/71 | LOSS: 5.029309984540585e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 13/71 | LOSS: 5.003736045442306e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 14/71 | LOSS: 4.905937385046855e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 15/71 | LOSS: 4.811758515188558e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 16/71 | LOSS: 4.848567026355715e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 17/71 | LOSS: 4.772989667698211e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 18/71 | LOSS: 4.686244162593012e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 19/71 | LOSS: 4.814657700080716e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 20/71 | LOSS: 4.815095139963974e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 21/71 | LOSS: 4.903029052498327e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 22/71 | LOSS: 4.8683510296699675e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 23/71 | LOSS: 5.037607328025236e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 24/71 | LOSS: 5.020443713874556e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 25/71 | LOSS: 5.087209956400329e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 26/71 | LOSS: 5.027053820985775e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 27/71 | LOSS: 5.0672381673523755e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 28/71 | LOSS: 5.06747418794789e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 29/71 | LOSS: 5.074473422913191e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 30/71 | LOSS: 5.047111288501671e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 31/71 | LOSS: 5.056692856442169e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 32/71 | LOSS: 5.067240677321521e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 33/71 | LOSS: 5.093642464677995e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 34/71 | LOSS: 5.109655311181476e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 35/71 | LOSS: 5.144080710629674e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 36/71 | LOSS: 5.093810392970005e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 37/71 | LOSS: 5.0753271807479905e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 38/71 | LOSS: 5.079497721173819e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 39/71 | LOSS: 5.054979476426525e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 40/71 | LOSS: 5.0558652197296175e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 41/71 | LOSS: 5.039626741910026e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 42/71 | LOSS: 5.0236634875588165e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 43/71 | LOSS: 5.011179826975752e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 44/71 | LOSS: 4.992206696543791e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 45/71 | LOSS: 4.9718069417197155e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 46/71 | LOSS: 4.978628247896422e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 47/71 | LOSS: 4.961370270469463e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 48/71 | LOSS: 4.974273767346654e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 49/71 | LOSS: 4.9782653786678565e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 50/71 | LOSS: 5.00800832228646e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 51/71 | LOSS: 5.0127956158646084e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 52/71 | LOSS: 5.0209851511715656e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 53/71 | LOSS: 5.014556024805104e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 54/71 | LOSS: 5.035402078977892e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 55/71 | LOSS: 5.040721126598717e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 56/71 | LOSS: 5.062373650600835e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 57/71 | LOSS: 5.11890146393351e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 58/71 | LOSS: 5.104511505339622e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 59/71 | LOSS: 5.109154843315385e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 60/71 | LOSS: 5.132866709707324e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 61/71 | LOSS: 5.15708688175026e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 62/71 | LOSS: 5.143689288651952e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 63/71 | LOSS: 5.157509413322714e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 64/71 | LOSS: 5.205569665961845e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 65/71 | LOSS: 5.220880225489523e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 66/71 | LOSS: 5.259344446253024e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 67/71 | LOSS: 5.245683891028902e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 68/71 | LOSS: 5.300147486063972e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 69/71 | LOSS: 5.308367449937837e-06\n",
      "TRAIN: EPOCH 604/1000 | BATCH 70/71 | LOSS: 5.396354327773148e-06\n",
      "VAL: EPOCH 604/1000 | BATCH 0/8 | LOSS: 4.249118319421541e-06\n",
      "VAL: EPOCH 604/1000 | BATCH 1/8 | LOSS: 4.464600351639092e-06\n",
      "VAL: EPOCH 604/1000 | BATCH 2/8 | LOSS: 4.65868894631664e-06\n",
      "VAL: EPOCH 604/1000 | BATCH 3/8 | LOSS: 4.649489255825756e-06\n",
      "VAL: EPOCH 604/1000 | BATCH 4/8 | LOSS: 4.7382593947986605e-06\n",
      "VAL: EPOCH 604/1000 | BATCH 5/8 | LOSS: 4.769246061187005e-06\n",
      "VAL: EPOCH 604/1000 | BATCH 6/8 | LOSS: 4.7019777541988465e-06\n",
      "VAL: EPOCH 604/1000 | BATCH 7/8 | LOSS: 4.630975297459372e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 0/71 | LOSS: 4.291789991839323e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 1/71 | LOSS: 6.06898129262845e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 2/71 | LOSS: 6.077601634994305e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 3/71 | LOSS: 5.9548602848735754e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 4/71 | LOSS: 5.710869754693704e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 5/71 | LOSS: 5.976373737818601e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 6/71 | LOSS: 5.7942432119619165e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 7/71 | LOSS: 5.6211832202279766e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 8/71 | LOSS: 5.674780570390996e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 9/71 | LOSS: 5.76808402001916e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 10/71 | LOSS: 5.683837984675351e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 11/71 | LOSS: 5.624145842375583e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 12/71 | LOSS: 5.6043917748656195e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 13/71 | LOSS: 5.556717236946237e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 14/71 | LOSS: 5.4728297072870195e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 15/71 | LOSS: 5.4896913752600085e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 16/71 | LOSS: 5.366135022032883e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 17/71 | LOSS: 5.334585163533727e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 18/71 | LOSS: 5.268090166952645e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 19/71 | LOSS: 5.237241316535801e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 20/71 | LOSS: 5.206049904483648e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 21/71 | LOSS: 5.252643508025688e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 22/71 | LOSS: 5.248524698602167e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 23/71 | LOSS: 5.25798022484499e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 24/71 | LOSS: 5.3129752086533695e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 25/71 | LOSS: 5.317574312777231e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 26/71 | LOSS: 5.301291737312029e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 27/71 | LOSS: 5.340492780955433e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 28/71 | LOSS: 5.472392033347017e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 29/71 | LOSS: 5.501451687450753e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 30/71 | LOSS: 5.605395692269198e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 31/71 | LOSS: 5.579792087928581e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 32/71 | LOSS: 5.74352482847947e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 33/71 | LOSS: 5.720793524233159e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 34/71 | LOSS: 5.7593095984235075e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 35/71 | LOSS: 5.782342150774235e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 36/71 | LOSS: 5.783800736173509e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 37/71 | LOSS: 5.773345588079935e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 38/71 | LOSS: 5.741862435407268e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 39/71 | LOSS: 5.735235424708662e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 40/71 | LOSS: 5.7182578086390205e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 41/71 | LOSS: 5.678242652200944e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 42/71 | LOSS: 5.660789981426883e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 43/71 | LOSS: 5.632542803885537e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 44/71 | LOSS: 5.602887914493395e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 45/71 | LOSS: 5.617057537244324e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 46/71 | LOSS: 5.615489199238042e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 47/71 | LOSS: 5.579376249897905e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 48/71 | LOSS: 5.5462514493989814e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 49/71 | LOSS: 5.557316144404467e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 50/71 | LOSS: 5.553850484170207e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 51/71 | LOSS: 5.563119403772222e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 52/71 | LOSS: 5.537826433832173e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 53/71 | LOSS: 5.543850260504836e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 54/71 | LOSS: 5.5388990486707454e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 55/71 | LOSS: 5.515805867162271e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 56/71 | LOSS: 5.50543643584477e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 57/71 | LOSS: 5.468711994330204e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 58/71 | LOSS: 5.494881826838907e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 59/71 | LOSS: 5.484144962035013e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 60/71 | LOSS: 5.461316635267052e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 61/71 | LOSS: 5.455521127553384e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 62/71 | LOSS: 5.4478543410772945e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 63/71 | LOSS: 5.443677384420198e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 64/71 | LOSS: 5.413873325848656e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 65/71 | LOSS: 5.4026484569862294e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 66/71 | LOSS: 5.425432014654594e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 67/71 | LOSS: 5.395195634772629e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 68/71 | LOSS: 5.414169482469004e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 69/71 | LOSS: 5.415444330278011e-06\n",
      "TRAIN: EPOCH 605/1000 | BATCH 70/71 | LOSS: 5.375777362414609e-06\n",
      "VAL: EPOCH 605/1000 | BATCH 0/8 | LOSS: 4.908873506792588e-06\n",
      "VAL: EPOCH 605/1000 | BATCH 1/8 | LOSS: 4.8245103698718594e-06\n",
      "VAL: EPOCH 605/1000 | BATCH 2/8 | LOSS: 4.874950870240961e-06\n",
      "VAL: EPOCH 605/1000 | BATCH 3/8 | LOSS: 4.763800689033815e-06\n",
      "VAL: EPOCH 605/1000 | BATCH 4/8 | LOSS: 4.827410157304257e-06\n",
      "VAL: EPOCH 605/1000 | BATCH 5/8 | LOSS: 4.781603062535093e-06\n",
      "VAL: EPOCH 605/1000 | BATCH 6/8 | LOSS: 4.661093043978326e-06\n",
      "VAL: EPOCH 605/1000 | BATCH 7/8 | LOSS: 4.505873789639736e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 0/71 | LOSS: 3.4285071706108283e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 1/71 | LOSS: 4.282729378246586e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 2/71 | LOSS: 4.3224919560695225e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 3/71 | LOSS: 4.149541268816392e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 4/71 | LOSS: 4.347057074483018e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 5/71 | LOSS: 4.287241229879631e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 6/71 | LOSS: 4.1142385660870266e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 7/71 | LOSS: 4.1872291376421344e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 8/71 | LOSS: 4.0255868548734324e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 9/71 | LOSS: 4.062912330482504e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 10/71 | LOSS: 4.169102091757602e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 11/71 | LOSS: 4.182836354023796e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 12/71 | LOSS: 4.294579167225703e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 13/71 | LOSS: 4.32726272135499e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 14/71 | LOSS: 4.390541137884914e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 15/71 | LOSS: 4.379013574862256e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 16/71 | LOSS: 4.364228757100029e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 17/71 | LOSS: 4.390782906840387e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 18/71 | LOSS: 4.439538291702966e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 19/71 | LOSS: 4.483390853238234e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 20/71 | LOSS: 4.478517894312972e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 21/71 | LOSS: 4.573442517787705e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 22/71 | LOSS: 4.6085559639006694e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 23/71 | LOSS: 4.684106064208511e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 24/71 | LOSS: 4.6582421236962545e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 25/71 | LOSS: 4.763681195422228e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 26/71 | LOSS: 4.792223451120124e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 27/71 | LOSS: 4.786916682470681e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 28/71 | LOSS: 4.767640269886309e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 29/71 | LOSS: 4.7734790162697514e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 30/71 | LOSS: 4.7633079556098594e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 31/71 | LOSS: 4.7281277844035685e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 32/71 | LOSS: 4.713569186413114e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 33/71 | LOSS: 4.7291792685320935e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 34/71 | LOSS: 4.708599986510568e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 35/71 | LOSS: 4.716786337313452e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 36/71 | LOSS: 4.708792384671806e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 37/71 | LOSS: 4.709111833203918e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 38/71 | LOSS: 4.6843676605371565e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 39/71 | LOSS: 4.6730566111818915e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 40/71 | LOSS: 4.662245645504743e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 41/71 | LOSS: 4.63923683063123e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 42/71 | LOSS: 4.611356659954127e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 43/71 | LOSS: 4.628922232024541e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 44/71 | LOSS: 4.631713838736889e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 45/71 | LOSS: 4.625545700590642e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 46/71 | LOSS: 4.604158317654746e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 47/71 | LOSS: 4.60939184184402e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 48/71 | LOSS: 4.588279176571131e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 49/71 | LOSS: 4.5642761142516976e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 50/71 | LOSS: 4.563010941851912e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 51/71 | LOSS: 4.555429654828913e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 52/71 | LOSS: 4.549726438782736e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 53/71 | LOSS: 4.53998763482054e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 54/71 | LOSS: 4.548484726414741e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 55/71 | LOSS: 4.547409032186676e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 56/71 | LOSS: 4.532295805435288e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 57/71 | LOSS: 4.546148673022077e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 58/71 | LOSS: 4.542908169180194e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 59/71 | LOSS: 4.539101655609556e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 60/71 | LOSS: 4.521293152213387e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 61/71 | LOSS: 4.541189032498815e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 62/71 | LOSS: 4.543441155831152e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 63/71 | LOSS: 4.538074364290878e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 64/71 | LOSS: 4.528531483699155e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 65/71 | LOSS: 4.514729670463015e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 66/71 | LOSS: 4.528827995313601e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 67/71 | LOSS: 4.52663195962005e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 68/71 | LOSS: 4.545908370636281e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 69/71 | LOSS: 4.5425218136837686e-06\n",
      "TRAIN: EPOCH 606/1000 | BATCH 70/71 | LOSS: 4.5723098460705805e-06\n",
      "VAL: EPOCH 606/1000 | BATCH 0/8 | LOSS: 5.3695557653554715e-06\n",
      "VAL: EPOCH 606/1000 | BATCH 1/8 | LOSS: 5.8139298744208645e-06\n",
      "VAL: EPOCH 606/1000 | BATCH 2/8 | LOSS: 6.129490581467205e-06\n",
      "VAL: EPOCH 606/1000 | BATCH 3/8 | LOSS: 6.207128421920061e-06\n",
      "VAL: EPOCH 606/1000 | BATCH 4/8 | LOSS: 6.310354820016073e-06\n",
      "VAL: EPOCH 606/1000 | BATCH 5/8 | LOSS: 6.2915058454867294e-06\n",
      "VAL: EPOCH 606/1000 | BATCH 6/8 | LOSS: 6.309983226466491e-06\n",
      "VAL: EPOCH 606/1000 | BATCH 7/8 | LOSS: 6.167014248603664e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 0/71 | LOSS: 6.2454510043608025e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 1/71 | LOSS: 6.313448011496803e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 2/71 | LOSS: 6.164226154699766e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 3/71 | LOSS: 5.863881142431637e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 4/71 | LOSS: 5.7733877838472836e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 5/71 | LOSS: 5.558594997031226e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 6/71 | LOSS: 5.692438272879892e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 7/71 | LOSS: 5.516962744422926e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 8/71 | LOSS: 5.548006255493318e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 9/71 | LOSS: 5.548511171582504e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 10/71 | LOSS: 5.564114458552054e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 11/71 | LOSS: 5.447154270162476e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 12/71 | LOSS: 5.343070545047969e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 13/71 | LOSS: 5.290119556125969e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 14/71 | LOSS: 5.200835281963615e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 15/71 | LOSS: 5.220359525992535e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 16/71 | LOSS: 5.2086344272966555e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 17/71 | LOSS: 5.253117706160992e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 18/71 | LOSS: 5.166265988296609e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 19/71 | LOSS: 5.154192263034929e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 20/71 | LOSS: 5.096457871765042e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 21/71 | LOSS: 5.0370226538441125e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 22/71 | LOSS: 5.060215145298887e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 23/71 | LOSS: 5.006329511540268e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 24/71 | LOSS: 4.992426629542024e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 25/71 | LOSS: 4.947485395999907e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 26/71 | LOSS: 4.8898574884030195e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 27/71 | LOSS: 4.908453401445253e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 28/71 | LOSS: 4.8849236730442e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 29/71 | LOSS: 4.9533894222501354e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 30/71 | LOSS: 4.930689453519129e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 31/71 | LOSS: 5.006021552844686e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 32/71 | LOSS: 4.985873767344112e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 33/71 | LOSS: 4.962064488555531e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 34/71 | LOSS: 4.959768336578106e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 35/71 | LOSS: 4.9227465473854354e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 36/71 | LOSS: 4.927401757438044e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 37/71 | LOSS: 4.892366572867583e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 38/71 | LOSS: 4.887210847160672e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 39/71 | LOSS: 4.869080385105917e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 40/71 | LOSS: 4.8568304675161305e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 41/71 | LOSS: 4.847796357613074e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 42/71 | LOSS: 4.849753897656757e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 43/71 | LOSS: 4.8620183757851585e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 44/71 | LOSS: 4.878636304460492e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 45/71 | LOSS: 4.832142839523326e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 46/71 | LOSS: 4.864428271328971e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 47/71 | LOSS: 4.862578142213654e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 48/71 | LOSS: 4.875384742522978e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 49/71 | LOSS: 4.880065653196652e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 50/71 | LOSS: 4.866116734203887e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 51/71 | LOSS: 4.857905537392071e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 52/71 | LOSS: 4.84215597851907e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 53/71 | LOSS: 4.849658504705682e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 54/71 | LOSS: 4.854391218858919e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 55/71 | LOSS: 4.83062612082514e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 56/71 | LOSS: 4.8340257228374595e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 57/71 | LOSS: 4.8263064807793724e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 58/71 | LOSS: 4.8156793959837375e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 59/71 | LOSS: 4.807086118792843e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 60/71 | LOSS: 4.7893974072797045e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 61/71 | LOSS: 4.776774880332185e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 62/71 | LOSS: 4.765022107936342e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 63/71 | LOSS: 4.746943535138826e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 64/71 | LOSS: 4.722345129966225e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 65/71 | LOSS: 4.721787590686731e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 66/71 | LOSS: 4.731948053373724e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 67/71 | LOSS: 4.72177032741135e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 68/71 | LOSS: 4.7109998752642675e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 69/71 | LOSS: 4.7151303825947775e-06\n",
      "TRAIN: EPOCH 607/1000 | BATCH 70/71 | LOSS: 4.711601681726318e-06\n",
      "VAL: EPOCH 607/1000 | BATCH 0/8 | LOSS: 5.784292625321541e-06\n",
      "VAL: EPOCH 607/1000 | BATCH 1/8 | LOSS: 5.858697704752558e-06\n",
      "VAL: EPOCH 607/1000 | BATCH 2/8 | LOSS: 5.8032821167823085e-06\n",
      "VAL: EPOCH 607/1000 | BATCH 3/8 | LOSS: 5.649913532579376e-06\n",
      "VAL: EPOCH 607/1000 | BATCH 4/8 | LOSS: 5.786428391729714e-06\n",
      "VAL: EPOCH 607/1000 | BATCH 5/8 | LOSS: 5.61039890574951e-06\n",
      "VAL: EPOCH 607/1000 | BATCH 6/8 | LOSS: 5.482868605862937e-06\n",
      "VAL: EPOCH 607/1000 | BATCH 7/8 | LOSS: 5.3252638849699e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 0/71 | LOSS: 5.6048520491458476e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 1/71 | LOSS: 5.116647116665263e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 2/71 | LOSS: 5.368837264541071e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 3/71 | LOSS: 5.2272291668487014e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 4/71 | LOSS: 5.439585947897285e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 5/71 | LOSS: 5.298460716100332e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 6/71 | LOSS: 5.200201420458532e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 7/71 | LOSS: 5.4115651551001065e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 8/71 | LOSS: 5.198740533766492e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 9/71 | LOSS: 5.2020515795447865e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 10/71 | LOSS: 5.276988386785061e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 11/71 | LOSS: 5.447213425213704e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 12/71 | LOSS: 5.329800142834966e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 13/71 | LOSS: 5.3682377126408805e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 14/71 | LOSS: 5.43714143835435e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 15/71 | LOSS: 5.3221135800640695e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 16/71 | LOSS: 5.318633865874076e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 17/71 | LOSS: 5.256797281718819e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 18/71 | LOSS: 5.212275542171004e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 19/71 | LOSS: 5.13533049115722e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 20/71 | LOSS: 5.0654076360279716e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 21/71 | LOSS: 5.0361007776121944e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 22/71 | LOSS: 5.001486146205065e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 23/71 | LOSS: 4.961114124550174e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 24/71 | LOSS: 4.977802163921297e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 25/71 | LOSS: 4.916487586552438e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 26/71 | LOSS: 4.941429374917385e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 27/71 | LOSS: 4.931263755939394e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 28/71 | LOSS: 4.872330229208556e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 29/71 | LOSS: 4.872137757653642e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 30/71 | LOSS: 4.8972422724370016e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 31/71 | LOSS: 4.886860835995321e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 32/71 | LOSS: 4.876967909005929e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 33/71 | LOSS: 4.896115061616429e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 34/71 | LOSS: 4.88133539745052e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 35/71 | LOSS: 4.854634122845407e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 36/71 | LOSS: 4.841128847564923e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 37/71 | LOSS: 4.830122730039109e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 38/71 | LOSS: 4.798825545712577e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 39/71 | LOSS: 4.83009573599702e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 40/71 | LOSS: 4.828518496385474e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 41/71 | LOSS: 4.817259986382505e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 42/71 | LOSS: 4.789567842529978e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 43/71 | LOSS: 4.785987564239556e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 44/71 | LOSS: 4.747513341701253e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 45/71 | LOSS: 4.716771657157274e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 46/71 | LOSS: 4.705790006289348e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 47/71 | LOSS: 4.691957656177692e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 48/71 | LOSS: 4.664772418715186e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 49/71 | LOSS: 4.6592611079177e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 50/71 | LOSS: 4.656891931910079e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 51/71 | LOSS: 4.6568630934346474e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 52/71 | LOSS: 4.640687221872899e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 53/71 | LOSS: 4.6154739097632786e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 54/71 | LOSS: 4.619658284354955e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 55/71 | LOSS: 4.607577758568888e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 56/71 | LOSS: 4.615953274192738e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 57/71 | LOSS: 4.60949321489446e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 58/71 | LOSS: 4.609706708941889e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 59/71 | LOSS: 4.593011146880599e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 60/71 | LOSS: 4.5776346008201246e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 61/71 | LOSS: 4.553617882705904e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 62/71 | LOSS: 4.562102518205407e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 63/71 | LOSS: 4.565027129643795e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 64/71 | LOSS: 4.5587545768145135e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 65/71 | LOSS: 4.546761557101105e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 66/71 | LOSS: 4.539832249988581e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 67/71 | LOSS: 4.537441531524296e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 68/71 | LOSS: 4.541117231633836e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 69/71 | LOSS: 4.546050867507542e-06\n",
      "TRAIN: EPOCH 608/1000 | BATCH 70/71 | LOSS: 4.555112883268444e-06\n",
      "VAL: EPOCH 608/1000 | BATCH 0/8 | LOSS: 3.7641998460458126e-06\n",
      "VAL: EPOCH 608/1000 | BATCH 1/8 | LOSS: 3.9761155221640365e-06\n",
      "VAL: EPOCH 608/1000 | BATCH 2/8 | LOSS: 4.084081562420276e-06\n",
      "VAL: EPOCH 608/1000 | BATCH 3/8 | LOSS: 4.042492378175666e-06\n",
      "VAL: EPOCH 608/1000 | BATCH 4/8 | LOSS: 4.153060581302271e-06\n",
      "VAL: EPOCH 608/1000 | BATCH 5/8 | LOSS: 4.173267977118182e-06\n",
      "VAL: EPOCH 608/1000 | BATCH 6/8 | LOSS: 4.0999098018801305e-06\n",
      "VAL: EPOCH 608/1000 | BATCH 7/8 | LOSS: 4.002742485909039e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 0/71 | LOSS: 3.579545136744855e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 1/71 | LOSS: 4.256760348653188e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 2/71 | LOSS: 4.0082665388278356e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 3/71 | LOSS: 4.1762611999729415e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 4/71 | LOSS: 4.230428021401167e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 5/71 | LOSS: 4.189557178809385e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 6/71 | LOSS: 4.157640432822518e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 7/71 | LOSS: 4.095051451713516e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 8/71 | LOSS: 4.109152188094514e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 9/71 | LOSS: 4.106248889002018e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 10/71 | LOSS: 4.099578952512027e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 11/71 | LOSS: 4.066228181424473e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 12/71 | LOSS: 4.015999226024947e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 13/71 | LOSS: 4.074162656512012e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 14/71 | LOSS: 4.06457484132261e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 15/71 | LOSS: 4.116593544267744e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 16/71 | LOSS: 4.080922414740438e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 17/71 | LOSS: 4.062821732178337e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 18/71 | LOSS: 4.05195565259706e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 19/71 | LOSS: 4.110862562356488e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 20/71 | LOSS: 4.094797044932161e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 21/71 | LOSS: 4.047265003299055e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 22/71 | LOSS: 4.07430115956231e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 23/71 | LOSS: 4.1130891380210715e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 24/71 | LOSS: 4.136888055654708e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 25/71 | LOSS: 4.134652062230788e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 26/71 | LOSS: 4.157526257552456e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 27/71 | LOSS: 4.168096617312196e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 28/71 | LOSS: 4.173377153071262e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 29/71 | LOSS: 4.169780125569863e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 30/71 | LOSS: 4.173162294811787e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 31/71 | LOSS: 4.137717155572318e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 32/71 | LOSS: 4.134418249481259e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 33/71 | LOSS: 4.1086771435563125e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 34/71 | LOSS: 4.1053206197310435e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 35/71 | LOSS: 4.144436590852112e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 36/71 | LOSS: 4.15603678472532e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 37/71 | LOSS: 4.147784585783436e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 38/71 | LOSS: 4.159616718168717e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 39/71 | LOSS: 4.192800088276272e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 40/71 | LOSS: 4.180592618578841e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 41/71 | LOSS: 4.185549382132844e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 42/71 | LOSS: 4.205591087452905e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 43/71 | LOSS: 4.178231082815645e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 44/71 | LOSS: 4.1796532261489325e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 45/71 | LOSS: 4.191631185913929e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 46/71 | LOSS: 4.1931800800004105e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 47/71 | LOSS: 4.18836935030716e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 48/71 | LOSS: 4.176899233707631e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 49/71 | LOSS: 4.183737437415402e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 50/71 | LOSS: 4.1729765375659514e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 51/71 | LOSS: 4.208309327133555e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 52/71 | LOSS: 4.201814144320417e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 53/71 | LOSS: 4.201551274928971e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 54/71 | LOSS: 4.200149057644674e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 55/71 | LOSS: 4.211742358555577e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 56/71 | LOSS: 4.232814876958034e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 57/71 | LOSS: 4.224631319285154e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 58/71 | LOSS: 4.225752398547306e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 59/71 | LOSS: 4.243782776332713e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 60/71 | LOSS: 4.220487068375748e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 61/71 | LOSS: 4.209726416921777e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 62/71 | LOSS: 4.20834931713808e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 63/71 | LOSS: 4.204545959396455e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 64/71 | LOSS: 4.190110401116097e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 65/71 | LOSS: 4.1822562135359585e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 66/71 | LOSS: 4.178354313794511e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 67/71 | LOSS: 4.183385756812854e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 68/71 | LOSS: 4.179168716133142e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 69/71 | LOSS: 4.19360073919961e-06\n",
      "TRAIN: EPOCH 609/1000 | BATCH 70/71 | LOSS: 4.2010962401881966e-06\n",
      "VAL: EPOCH 609/1000 | BATCH 0/8 | LOSS: 3.5986493003292708e-06\n",
      "VAL: EPOCH 609/1000 | BATCH 1/8 | LOSS: 4.186391720395477e-06\n",
      "VAL: EPOCH 609/1000 | BATCH 2/8 | LOSS: 4.457252468152244e-06\n",
      "VAL: EPOCH 609/1000 | BATCH 3/8 | LOSS: 4.500573993482249e-06\n",
      "VAL: EPOCH 609/1000 | BATCH 4/8 | LOSS: 4.5235124616738174e-06\n",
      "VAL: EPOCH 609/1000 | BATCH 5/8 | LOSS: 4.579563475696584e-06\n",
      "VAL: EPOCH 609/1000 | BATCH 6/8 | LOSS: 4.494740648754357e-06\n",
      "VAL: EPOCH 609/1000 | BATCH 7/8 | LOSS: 4.395878875129711e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 0/71 | LOSS: 5.151778168510646e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 1/71 | LOSS: 4.617291324393591e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 2/71 | LOSS: 4.55834833701374e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 3/71 | LOSS: 4.368689928924141e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 4/71 | LOSS: 4.243988723828807e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 5/71 | LOSS: 4.014015947480705e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 6/71 | LOSS: 3.872582997895993e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 7/71 | LOSS: 3.911057348204849e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 8/71 | LOSS: 4.0022008912880365e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 9/71 | LOSS: 3.955447209591512e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 10/71 | LOSS: 3.942845317148815e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 11/71 | LOSS: 4.0545284794764784e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 12/71 | LOSS: 3.994001085280056e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 13/71 | LOSS: 4.0645170916572426e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 14/71 | LOSS: 4.125253614499039e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 15/71 | LOSS: 4.086816758785972e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 16/71 | LOSS: 4.0758345810044215e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 17/71 | LOSS: 4.0306561004399555e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 18/71 | LOSS: 4.06336120680575e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 19/71 | LOSS: 4.064652853230655e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 20/71 | LOSS: 4.011269246413056e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 21/71 | LOSS: 3.985058469879732e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 22/71 | LOSS: 3.998836894464583e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 23/71 | LOSS: 4.001980045131859e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 24/71 | LOSS: 4.0010728389461295e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 25/71 | LOSS: 3.964659937922149e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 26/71 | LOSS: 3.941212820858776e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 27/71 | LOSS: 3.951959610861065e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 28/71 | LOSS: 3.935240729019981e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 29/71 | LOSS: 3.9467112325534496e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 30/71 | LOSS: 3.975557532495229e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 31/71 | LOSS: 3.991319552199002e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 32/71 | LOSS: 3.9975022073948905e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 33/71 | LOSS: 4.0294272460361e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 34/71 | LOSS: 4.028934556897314e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 35/71 | LOSS: 4.0331272417562204e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 36/71 | LOSS: 4.058684269054808e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 37/71 | LOSS: 4.062803979745934e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 38/71 | LOSS: 4.0747492547756665e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 39/71 | LOSS: 4.087133567054479e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 40/71 | LOSS: 4.081027355357769e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 41/71 | LOSS: 4.076768104390621e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 42/71 | LOSS: 4.094594614375532e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 43/71 | LOSS: 4.10611346007334e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 44/71 | LOSS: 4.130667836458694e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 45/71 | LOSS: 4.126398484122942e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 46/71 | LOSS: 4.1307701617079925e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 47/71 | LOSS: 4.147516425708393e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 48/71 | LOSS: 4.130899293270385e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 49/71 | LOSS: 4.126781141167157e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 50/71 | LOSS: 4.124730429455167e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 51/71 | LOSS: 4.101831638518678e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 52/71 | LOSS: 4.1000954031503326e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 53/71 | LOSS: 4.119657698235339e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 54/71 | LOSS: 4.107921726741055e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 55/71 | LOSS: 4.124039065800389e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 56/71 | LOSS: 4.122085871578045e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 57/71 | LOSS: 4.125534905268006e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 58/71 | LOSS: 4.113868013611586e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 59/71 | LOSS: 4.114880277938937e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 60/71 | LOSS: 4.117404487856891e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 61/71 | LOSS: 4.111414997873174e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 62/71 | LOSS: 4.1281628505708395e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 63/71 | LOSS: 4.136442733226886e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 64/71 | LOSS: 4.134672571336313e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 65/71 | LOSS: 4.134501688730035e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 66/71 | LOSS: 4.15990831215958e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 67/71 | LOSS: 4.143234377806948e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 68/71 | LOSS: 4.130167415338371e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 69/71 | LOSS: 4.127817331729083e-06\n",
      "TRAIN: EPOCH 610/1000 | BATCH 70/71 | LOSS: 4.117113681965914e-06\n",
      "VAL: EPOCH 610/1000 | BATCH 0/8 | LOSS: 3.938474492315436e-06\n",
      "VAL: EPOCH 610/1000 | BATCH 1/8 | LOSS: 4.235007054376183e-06\n",
      "VAL: EPOCH 610/1000 | BATCH 2/8 | LOSS: 4.43907780815304e-06\n",
      "VAL: EPOCH 610/1000 | BATCH 3/8 | LOSS: 4.42595944605273e-06\n",
      "VAL: EPOCH 610/1000 | BATCH 4/8 | LOSS: 4.5567408051283564e-06\n",
      "VAL: EPOCH 610/1000 | BATCH 5/8 | LOSS: 4.534825848168111e-06\n",
      "VAL: EPOCH 610/1000 | BATCH 6/8 | LOSS: 4.451077724557503e-06\n",
      "VAL: EPOCH 610/1000 | BATCH 7/8 | LOSS: 4.326320663494698e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 0/71 | LOSS: 3.9491978895966895e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 1/71 | LOSS: 3.7050975834063138e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 2/71 | LOSS: 3.7152004400316705e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 3/71 | LOSS: 3.640279999217455e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 4/71 | LOSS: 3.809789404840558e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 5/71 | LOSS: 3.7704688414426832e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 6/71 | LOSS: 4.066740789962101e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 7/71 | LOSS: 4.0371248246628966e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 8/71 | LOSS: 4.050276503322594e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 9/71 | LOSS: 4.064595941599691e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 10/71 | LOSS: 4.07687664185439e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 11/71 | LOSS: 4.084113697899738e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 12/71 | LOSS: 4.072064012479342e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 13/71 | LOSS: 4.0502660730063714e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 14/71 | LOSS: 4.036321115563624e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 15/71 | LOSS: 4.024187489903852e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 16/71 | LOSS: 4.093041244392653e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 17/71 | LOSS: 4.105373363927356e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 18/71 | LOSS: 4.0990699074571145e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 19/71 | LOSS: 4.075211063536699e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 20/71 | LOSS: 4.112490820435119e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 21/71 | LOSS: 4.1579619805402634e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 22/71 | LOSS: 4.122596692468505e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 23/71 | LOSS: 4.131379076473725e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 24/71 | LOSS: 4.102248994968249e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 25/71 | LOSS: 4.1229815399044756e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 26/71 | LOSS: 4.087594207804715e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 27/71 | LOSS: 4.077209910389813e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 28/71 | LOSS: 4.072846007241831e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 29/71 | LOSS: 4.054622005848311e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 30/71 | LOSS: 4.039044428751485e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 31/71 | LOSS: 4.0362111732861194e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 32/71 | LOSS: 4.01266385613946e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 33/71 | LOSS: 3.998031803241568e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 34/71 | LOSS: 4.000132308387297e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 35/71 | LOSS: 3.994782540909607e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 36/71 | LOSS: 3.994876861552994e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 37/71 | LOSS: 3.99188102045101e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 38/71 | LOSS: 3.998896638735553e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 39/71 | LOSS: 3.983081762726215e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 40/71 | LOSS: 3.9906513160442445e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 41/71 | LOSS: 3.982619876383285e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 42/71 | LOSS: 3.991156502305002e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 43/71 | LOSS: 3.9908674693833746e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 44/71 | LOSS: 3.985215860464248e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 45/71 | LOSS: 3.9927208193978956e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 46/71 | LOSS: 3.982276405409682e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 47/71 | LOSS: 3.984941846132036e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 48/71 | LOSS: 3.997549449232625e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 49/71 | LOSS: 4.002351233793888e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 50/71 | LOSS: 4.014197050613196e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 51/71 | LOSS: 4.066763521083116e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 52/71 | LOSS: 4.060370055979221e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 53/71 | LOSS: 4.04535320218134e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 54/71 | LOSS: 4.08120068972163e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 55/71 | LOSS: 4.117381385932666e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 56/71 | LOSS: 4.130036179623475e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 57/71 | LOSS: 4.16048403465162e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 58/71 | LOSS: 4.169917775023782e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 59/71 | LOSS: 4.166662805952607e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 60/71 | LOSS: 4.175055948030977e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 61/71 | LOSS: 4.16788524152282e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 62/71 | LOSS: 4.159793152645025e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 63/71 | LOSS: 4.159773059342342e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 64/71 | LOSS: 4.147848438571181e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 65/71 | LOSS: 4.159974626859229e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 66/71 | LOSS: 4.146768899894894e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 67/71 | LOSS: 4.1594735695499e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 68/71 | LOSS: 4.170115045675682e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 69/71 | LOSS: 4.171573011392736e-06\n",
      "TRAIN: EPOCH 611/1000 | BATCH 70/71 | LOSS: 4.1597802560894525e-06\n",
      "VAL: EPOCH 611/1000 | BATCH 0/8 | LOSS: 4.149851520196535e-06\n",
      "VAL: EPOCH 611/1000 | BATCH 1/8 | LOSS: 4.334006916906219e-06\n",
      "VAL: EPOCH 611/1000 | BATCH 2/8 | LOSS: 4.467094489276254e-06\n",
      "VAL: EPOCH 611/1000 | BATCH 3/8 | LOSS: 4.455312250684074e-06\n",
      "VAL: EPOCH 611/1000 | BATCH 4/8 | LOSS: 4.4997303120908326e-06\n",
      "VAL: EPOCH 611/1000 | BATCH 5/8 | LOSS: 4.409091843626811e-06\n",
      "VAL: EPOCH 611/1000 | BATCH 6/8 | LOSS: 4.253179278228865e-06\n",
      "VAL: EPOCH 611/1000 | BATCH 7/8 | LOSS: 4.147392587583454e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 0/71 | LOSS: 3.2909601941355504e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 1/71 | LOSS: 3.721590019267751e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 2/71 | LOSS: 4.046107581719601e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 3/71 | LOSS: 4.191573225398315e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 4/71 | LOSS: 4.139138582104351e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 5/71 | LOSS: 4.093491573560944e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 6/71 | LOSS: 4.088508116443076e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 7/71 | LOSS: 4.073420939221251e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 8/71 | LOSS: 4.142853413213743e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 9/71 | LOSS: 4.147849222135847e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 10/71 | LOSS: 4.237657487299822e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 11/71 | LOSS: 4.33351370550857e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 12/71 | LOSS: 4.318400792376801e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 13/71 | LOSS: 4.306614300730871e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 14/71 | LOSS: 4.2756877519423145e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 15/71 | LOSS: 4.214033538119111e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 16/71 | LOSS: 4.240687490785015e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 17/71 | LOSS: 4.273359031180411e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 18/71 | LOSS: 4.284754715244114e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 19/71 | LOSS: 4.2916883558064e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 20/71 | LOSS: 4.257590565733456e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 21/71 | LOSS: 4.2628078269692855e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 22/71 | LOSS: 4.229639493449055e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 23/71 | LOSS: 4.234464256569481e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 24/71 | LOSS: 4.2611087155819406e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 25/71 | LOSS: 4.3215665510405624e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 26/71 | LOSS: 4.348942689426218e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 27/71 | LOSS: 4.344276346403474e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 28/71 | LOSS: 4.354271979913204e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 29/71 | LOSS: 4.437711610686771e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 30/71 | LOSS: 4.418820994502429e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 31/71 | LOSS: 4.45303832918853e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 32/71 | LOSS: 4.423030476931617e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 33/71 | LOSS: 4.415855220727013e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 34/71 | LOSS: 4.394438643170621e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 35/71 | LOSS: 4.3927593146185646e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 36/71 | LOSS: 4.372261634450075e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 37/71 | LOSS: 4.39530224651351e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 38/71 | LOSS: 4.396111278718961e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 39/71 | LOSS: 4.391001795056582e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 40/71 | LOSS: 4.409062429237188e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 41/71 | LOSS: 4.390609595767663e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 42/71 | LOSS: 4.395749926494995e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 43/71 | LOSS: 4.391911452537236e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 44/71 | LOSS: 4.391361537475152e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 45/71 | LOSS: 4.381250090298873e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 46/71 | LOSS: 4.397363599832231e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 47/71 | LOSS: 4.379843109821498e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 48/71 | LOSS: 4.3919742299239055e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 49/71 | LOSS: 4.368919080661726e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 50/71 | LOSS: 4.3626768648905185e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 51/71 | LOSS: 4.364437429033457e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 52/71 | LOSS: 4.335038755805658e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 53/71 | LOSS: 4.350759935631585e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 54/71 | LOSS: 4.344952718714591e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 55/71 | LOSS: 4.343621770073826e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 56/71 | LOSS: 4.339428510559772e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 57/71 | LOSS: 4.337809231997355e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 58/71 | LOSS: 4.330079857520444e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 59/71 | LOSS: 4.314405699309039e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 60/71 | LOSS: 4.317954387604049e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 61/71 | LOSS: 4.31011375029254e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 62/71 | LOSS: 4.319217991689497e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 63/71 | LOSS: 4.324659677479303e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 64/71 | LOSS: 4.3152398540349585e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 65/71 | LOSS: 4.312298818314792e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 66/71 | LOSS: 4.303630550648043e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 67/71 | LOSS: 4.335942017352115e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 68/71 | LOSS: 4.323232442929757e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 69/71 | LOSS: 4.335989271047376e-06\n",
      "TRAIN: EPOCH 612/1000 | BATCH 70/71 | LOSS: 4.3484671140773365e-06\n",
      "VAL: EPOCH 612/1000 | BATCH 0/8 | LOSS: 3.949769052269403e-06\n",
      "VAL: EPOCH 612/1000 | BATCH 1/8 | LOSS: 4.3440782064863015e-06\n",
      "VAL: EPOCH 612/1000 | BATCH 2/8 | LOSS: 4.463449840841349e-06\n",
      "VAL: EPOCH 612/1000 | BATCH 3/8 | LOSS: 4.450170536074438e-06\n",
      "VAL: EPOCH 612/1000 | BATCH 4/8 | LOSS: 4.474636352824746e-06\n",
      "VAL: EPOCH 612/1000 | BATCH 5/8 | LOSS: 4.380513094777901e-06\n",
      "VAL: EPOCH 612/1000 | BATCH 6/8 | LOSS: 4.251031896274071e-06\n",
      "VAL: EPOCH 612/1000 | BATCH 7/8 | LOSS: 4.080175358467386e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 0/71 | LOSS: 4.742000783153344e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 1/71 | LOSS: 4.563841230265098e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 2/71 | LOSS: 4.453771301389982e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 3/71 | LOSS: 4.31738385486824e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 4/71 | LOSS: 4.539295332506299e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 5/71 | LOSS: 4.571520321405842e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 6/71 | LOSS: 4.464991109541318e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 7/71 | LOSS: 4.552790585421462e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 8/71 | LOSS: 4.57910214714098e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 9/71 | LOSS: 4.461049866222311e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 10/71 | LOSS: 4.511076026987708e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 11/71 | LOSS: 4.608470362654771e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 12/71 | LOSS: 4.542474471236346e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 13/71 | LOSS: 4.53424344673944e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 14/71 | LOSS: 4.603984264880031e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 15/71 | LOSS: 4.705203792809698e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 16/71 | LOSS: 4.691712547355564e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 17/71 | LOSS: 4.772011859838838e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 18/71 | LOSS: 4.739333570270039e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 19/71 | LOSS: 4.732899924420053e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 20/71 | LOSS: 4.738953932593133e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 21/71 | LOSS: 4.746605265443742e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 22/71 | LOSS: 4.701743512708598e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 23/71 | LOSS: 4.719481173272773e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 24/71 | LOSS: 4.710731982413563e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 25/71 | LOSS: 4.703966265669106e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 26/71 | LOSS: 4.698082538770568e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 27/71 | LOSS: 4.6401954786493405e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 28/71 | LOSS: 4.6083902800781855e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 29/71 | LOSS: 4.604690328354385e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 30/71 | LOSS: 4.612769236055394e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 31/71 | LOSS: 4.558480007688104e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 32/71 | LOSS: 4.547757994673289e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 33/71 | LOSS: 4.53738433032627e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 34/71 | LOSS: 4.529795588885983e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 35/71 | LOSS: 4.529778108361724e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 36/71 | LOSS: 4.530975035310336e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 37/71 | LOSS: 4.50645003231904e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 38/71 | LOSS: 4.494783728986453e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 39/71 | LOSS: 4.505525453168957e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 40/71 | LOSS: 4.528271857430454e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 41/71 | LOSS: 4.530857885194044e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 42/71 | LOSS: 4.55763626611537e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 43/71 | LOSS: 4.573300357564718e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 44/71 | LOSS: 4.611044803621351e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 45/71 | LOSS: 4.60236958093921e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 46/71 | LOSS: 4.616733688035698e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 47/71 | LOSS: 4.6392250681037694e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 48/71 | LOSS: 4.623739929526465e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 49/71 | LOSS: 4.6240676238085144e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 50/71 | LOSS: 4.62136500466252e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 51/71 | LOSS: 4.629507526050685e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 52/71 | LOSS: 4.622063820619896e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 53/71 | LOSS: 4.6089020694732126e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 54/71 | LOSS: 4.5931727196478855e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 55/71 | LOSS: 4.588311538457544e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 56/71 | LOSS: 4.5927981972725545e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 57/71 | LOSS: 4.58837261139079e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 58/71 | LOSS: 4.5846784591264885e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 59/71 | LOSS: 4.589225094756936e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 60/71 | LOSS: 4.585356769953772e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 61/71 | LOSS: 4.570682775803077e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 62/71 | LOSS: 4.5466291729618594e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 63/71 | LOSS: 4.582218785742498e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 64/71 | LOSS: 4.580336623733344e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 65/71 | LOSS: 4.602199486257494e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 66/71 | LOSS: 4.589683941904057e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 67/71 | LOSS: 4.585254410304676e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 68/71 | LOSS: 4.57188686424701e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 69/71 | LOSS: 4.574742388285813e-06\n",
      "TRAIN: EPOCH 613/1000 | BATCH 70/71 | LOSS: 4.573496310731058e-06\n",
      "VAL: EPOCH 613/1000 | BATCH 0/8 | LOSS: 4.798664122063201e-06\n",
      "VAL: EPOCH 613/1000 | BATCH 1/8 | LOSS: 4.969990186509676e-06\n",
      "VAL: EPOCH 613/1000 | BATCH 2/8 | LOSS: 5.093153352693965e-06\n",
      "VAL: EPOCH 613/1000 | BATCH 3/8 | LOSS: 5.0653177368076285e-06\n",
      "VAL: EPOCH 613/1000 | BATCH 4/8 | LOSS: 5.088565740152262e-06\n",
      "VAL: EPOCH 613/1000 | BATCH 5/8 | LOSS: 5.033516117691761e-06\n",
      "VAL: EPOCH 613/1000 | BATCH 6/8 | LOSS: 4.91523944999374e-06\n",
      "VAL: EPOCH 613/1000 | BATCH 7/8 | LOSS: 4.753025848458492e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 0/71 | LOSS: 4.672511749959085e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 1/71 | LOSS: 4.4336791233945405e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 2/71 | LOSS: 4.3108878647520514e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 3/71 | LOSS: 4.262890570316813e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 4/71 | LOSS: 4.179116149316542e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 5/71 | LOSS: 4.28312106729815e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 6/71 | LOSS: 4.364836513559567e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 7/71 | LOSS: 4.60165432514259e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 8/71 | LOSS: 4.725384668644337e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 9/71 | LOSS: 4.8059073378681205e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 10/71 | LOSS: 4.981622871789361e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 11/71 | LOSS: 4.9260592428860645e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 12/71 | LOSS: 5.116254704682013e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 13/71 | LOSS: 4.969619307563075e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 14/71 | LOSS: 5.195233097765595e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 15/71 | LOSS: 5.240171617515443e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 16/71 | LOSS: 5.305892366724199e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 17/71 | LOSS: 5.377872765974543e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 18/71 | LOSS: 5.313732967088551e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 19/71 | LOSS: 5.35147071332176e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 20/71 | LOSS: 5.274171317502069e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 21/71 | LOSS: 5.343063796978888e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 22/71 | LOSS: 5.323564753622201e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 23/71 | LOSS: 5.360521678691536e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 24/71 | LOSS: 5.369855971366633e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 25/71 | LOSS: 5.461391118352856e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 26/71 | LOSS: 5.449089072768672e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 27/71 | LOSS: 5.430345530450202e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 28/71 | LOSS: 5.422012989207548e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 29/71 | LOSS: 5.387836351170942e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 30/71 | LOSS: 5.3875033964070626e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 31/71 | LOSS: 5.345657044131258e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 32/71 | LOSS: 5.41497517284976e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 33/71 | LOSS: 5.3846384376169336e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 34/71 | LOSS: 5.416079264770295e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 35/71 | LOSS: 5.3510577547260455e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 36/71 | LOSS: 5.354771400487988e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 37/71 | LOSS: 5.3385401650685755e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 38/71 | LOSS: 5.39873218048552e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 39/71 | LOSS: 5.373594638058421e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 40/71 | LOSS: 5.4818837403528475e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 41/71 | LOSS: 5.4818783935063816e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 42/71 | LOSS: 5.4753715503889106e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 43/71 | LOSS: 5.5070445961808385e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 44/71 | LOSS: 5.487027253063085e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 45/71 | LOSS: 5.49957267930554e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 46/71 | LOSS: 5.4677199280357605e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 47/71 | LOSS: 5.491100656248212e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 48/71 | LOSS: 5.473641178877169e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 49/71 | LOSS: 5.4712021847080904e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 50/71 | LOSS: 5.438027305810429e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 51/71 | LOSS: 5.420779329002611e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 52/71 | LOSS: 5.412116959907405e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 53/71 | LOSS: 5.3941146234137705e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 54/71 | LOSS: 5.391349573487374e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 55/71 | LOSS: 5.359166103906838e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 56/71 | LOSS: 5.352477592966527e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 57/71 | LOSS: 5.330782861510043e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 58/71 | LOSS: 5.322584834406586e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 59/71 | LOSS: 5.310066687040186e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 60/71 | LOSS: 5.300555212819685e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 61/71 | LOSS: 5.292015731033791e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 62/71 | LOSS: 5.289557167189842e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 63/71 | LOSS: 5.292985669314021e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 64/71 | LOSS: 5.2691812785409495e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 65/71 | LOSS: 5.263784011979376e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 66/71 | LOSS: 5.242472457551352e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 67/71 | LOSS: 5.249578119968218e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 68/71 | LOSS: 5.248603890546635e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 69/71 | LOSS: 5.268377175785385e-06\n",
      "TRAIN: EPOCH 614/1000 | BATCH 70/71 | LOSS: 5.275250387826482e-06\n",
      "VAL: EPOCH 614/1000 | BATCH 0/8 | LOSS: 5.987210897728801e-06\n",
      "VAL: EPOCH 614/1000 | BATCH 1/8 | LOSS: 6.1399950936902314e-06\n",
      "VAL: EPOCH 614/1000 | BATCH 2/8 | LOSS: 6.270189411831477e-06\n",
      "VAL: EPOCH 614/1000 | BATCH 3/8 | LOSS: 6.2272898730952875e-06\n",
      "VAL: EPOCH 614/1000 | BATCH 4/8 | LOSS: 6.3513757595501374e-06\n",
      "VAL: EPOCH 614/1000 | BATCH 5/8 | LOSS: 6.364440575149881e-06\n",
      "VAL: EPOCH 614/1000 | BATCH 6/8 | LOSS: 6.312344404640109e-06\n",
      "VAL: EPOCH 614/1000 | BATCH 7/8 | LOSS: 6.179551178320253e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 0/71 | LOSS: 4.916039870295208e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 1/71 | LOSS: 5.357043164622155e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 2/71 | LOSS: 5.301211331243394e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 3/71 | LOSS: 5.165071911505947e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 4/71 | LOSS: 5.1492278544174045e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 5/71 | LOSS: 5.299551882368784e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 6/71 | LOSS: 5.502991825778736e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 7/71 | LOSS: 5.440847985482833e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 8/71 | LOSS: 5.367951871448895e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 9/71 | LOSS: 5.432831903817714e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 10/71 | LOSS: 5.4661962332664355e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 11/71 | LOSS: 5.586735596807557e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 12/71 | LOSS: 5.724838082152508e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 13/71 | LOSS: 5.671154765098306e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 14/71 | LOSS: 5.783524071982053e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 15/71 | LOSS: 5.689589613666612e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 16/71 | LOSS: 5.7048031093847174e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 17/71 | LOSS: 5.72228706611592e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 18/71 | LOSS: 5.751404746421797e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 19/71 | LOSS: 5.741596055486298e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 20/71 | LOSS: 5.736716589342991e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 21/71 | LOSS: 5.725488315213377e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 22/71 | LOSS: 5.652405730949755e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 23/71 | LOSS: 5.672018536036679e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 24/71 | LOSS: 5.636275709548499e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 25/71 | LOSS: 5.6252356793825475e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 26/71 | LOSS: 5.630413684061581e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 27/71 | LOSS: 5.60674137821999e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 28/71 | LOSS: 5.563782530049954e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 29/71 | LOSS: 5.543110576885131e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 30/71 | LOSS: 5.544949178123892e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 31/71 | LOSS: 5.4680920698046975e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 32/71 | LOSS: 5.538753005869997e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 33/71 | LOSS: 5.518746454125742e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 34/71 | LOSS: 5.601792651889679e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 35/71 | LOSS: 5.5698244144271785e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 36/71 | LOSS: 5.549958058435476e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 37/71 | LOSS: 5.60937694085863e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 38/71 | LOSS: 5.5687072022127e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 39/71 | LOSS: 5.628277506275481e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 40/71 | LOSS: 5.575284520579394e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 41/71 | LOSS: 5.616635798148872e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 42/71 | LOSS: 5.5492727221653064e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 43/71 | LOSS: 5.547728526587227e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 44/71 | LOSS: 5.510765954063067e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 45/71 | LOSS: 5.503666769705571e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 46/71 | LOSS: 5.4816472332644715e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 47/71 | LOSS: 5.438375524136063e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 48/71 | LOSS: 5.4278756282173515e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 49/71 | LOSS: 5.404560724855401e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 50/71 | LOSS: 5.386224529502215e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 51/71 | LOSS: 5.362738114364709e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 52/71 | LOSS: 5.326615255741997e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 53/71 | LOSS: 5.312107351461169e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 54/71 | LOSS: 5.313651114523633e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 55/71 | LOSS: 5.287413927135276e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 56/71 | LOSS: 5.286000267707941e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 57/71 | LOSS: 5.3211878688169485e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 58/71 | LOSS: 5.317646266614586e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 59/71 | LOSS: 5.3246644900658184e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 60/71 | LOSS: 5.329732291146848e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 61/71 | LOSS: 5.325430826756459e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 62/71 | LOSS: 5.307355455183174e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 63/71 | LOSS: 5.306036467089825e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 64/71 | LOSS: 5.297994186128983e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 65/71 | LOSS: 5.298999960332091e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 66/71 | LOSS: 5.28055932984749e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 67/71 | LOSS: 5.2755264886507255e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 68/71 | LOSS: 5.273441159432338e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 69/71 | LOSS: 5.271062130824638e-06\n",
      "TRAIN: EPOCH 615/1000 | BATCH 70/71 | LOSS: 5.293358385706981e-06\n",
      "VAL: EPOCH 615/1000 | BATCH 0/8 | LOSS: 3.897304395650281e-06\n",
      "VAL: EPOCH 615/1000 | BATCH 1/8 | LOSS: 4.144463673583232e-06\n",
      "VAL: EPOCH 615/1000 | BATCH 2/8 | LOSS: 4.195797979870501e-06\n",
      "VAL: EPOCH 615/1000 | BATCH 3/8 | LOSS: 4.193179506728484e-06\n",
      "VAL: EPOCH 615/1000 | BATCH 4/8 | LOSS: 4.204284141451353e-06\n",
      "VAL: EPOCH 615/1000 | BATCH 5/8 | LOSS: 4.205350857470573e-06\n",
      "VAL: EPOCH 615/1000 | BATCH 6/8 | LOSS: 4.054630575670412e-06\n",
      "VAL: EPOCH 615/1000 | BATCH 7/8 | LOSS: 3.889290439929027e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 0/71 | LOSS: 3.932394974981435e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 1/71 | LOSS: 4.250929805493797e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 2/71 | LOSS: 4.67434574602521e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 3/71 | LOSS: 4.3758053607234615e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 4/71 | LOSS: 4.772781539941206e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 5/71 | LOSS: 5.0673441667944035e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 6/71 | LOSS: 5.092787107839415e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 7/71 | LOSS: 5.2750048666894145e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 8/71 | LOSS: 5.460158970688806e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 9/71 | LOSS: 5.4706816172256366e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 10/71 | LOSS: 5.573791358156913e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 11/71 | LOSS: 5.39992976200665e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 12/71 | LOSS: 5.387221842913557e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 13/71 | LOSS: 5.380868953709848e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 14/71 | LOSS: 5.352709149519797e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 15/71 | LOSS: 5.344938657003695e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 16/71 | LOSS: 5.37213259121927e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 17/71 | LOSS: 5.3806866288949905e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 18/71 | LOSS: 5.439406167122223e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 19/71 | LOSS: 5.381570110785105e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 20/71 | LOSS: 5.3811492350741035e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 21/71 | LOSS: 5.352785189477965e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 22/71 | LOSS: 5.309131438480108e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 23/71 | LOSS: 5.3066440936315e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 24/71 | LOSS: 5.227635519986507e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 25/71 | LOSS: 5.179832512042664e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 26/71 | LOSS: 5.156124576320441e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 27/71 | LOSS: 5.103521224165368e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 28/71 | LOSS: 5.065917133018455e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 29/71 | LOSS: 5.02976048058675e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 30/71 | LOSS: 5.002132565418686e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 31/71 | LOSS: 4.969959846334859e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 32/71 | LOSS: 4.927824758135213e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 33/71 | LOSS: 4.884681268723559e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 34/71 | LOSS: 4.823802282771794e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 35/71 | LOSS: 4.791350407787023e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 36/71 | LOSS: 4.779055674392825e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 37/71 | LOSS: 4.765661419457358e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 38/71 | LOSS: 4.726978029513983e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 39/71 | LOSS: 4.691547212587466e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 40/71 | LOSS: 4.700561154405899e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 41/71 | LOSS: 4.702072579848824e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 42/71 | LOSS: 4.678211584981655e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 43/71 | LOSS: 4.698368322599642e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 44/71 | LOSS: 4.6884331722038846e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 45/71 | LOSS: 4.669833159546808e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 46/71 | LOSS: 4.656066702158027e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 47/71 | LOSS: 4.688660084184448e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 48/71 | LOSS: 4.658215064001543e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 49/71 | LOSS: 4.65477186935459e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 50/71 | LOSS: 4.630219751454527e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 51/71 | LOSS: 4.613590879041742e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 52/71 | LOSS: 4.603703808174512e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 53/71 | LOSS: 4.616614988764114e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 54/71 | LOSS: 4.617157678281232e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 55/71 | LOSS: 4.632825063513987e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 56/71 | LOSS: 4.613168636767609e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 57/71 | LOSS: 4.600862043530626e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 58/71 | LOSS: 4.621304658600928e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 59/71 | LOSS: 4.615409924705697e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 60/71 | LOSS: 4.6033381138721645e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 61/71 | LOSS: 4.592854056241894e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 62/71 | LOSS: 4.581738978000459e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 63/71 | LOSS: 4.574487970643304e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 64/71 | LOSS: 4.551484383997516e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 65/71 | LOSS: 4.550281590981643e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 66/71 | LOSS: 4.5522866396673476e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 67/71 | LOSS: 4.540708871178033e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 68/71 | LOSS: 4.569108058063463e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 69/71 | LOSS: 4.5726332408386434e-06\n",
      "TRAIN: EPOCH 616/1000 | BATCH 70/71 | LOSS: 4.574216300682911e-06\n",
      "VAL: EPOCH 616/1000 | BATCH 0/8 | LOSS: 5.067722668172792e-06\n",
      "VAL: EPOCH 616/1000 | BATCH 1/8 | LOSS: 5.52476012671832e-06\n",
      "VAL: EPOCH 616/1000 | BATCH 2/8 | LOSS: 5.766909453086555e-06\n",
      "VAL: EPOCH 616/1000 | BATCH 3/8 | LOSS: 5.718412467103917e-06\n",
      "VAL: EPOCH 616/1000 | BATCH 4/8 | LOSS: 5.865411549166311e-06\n",
      "VAL: EPOCH 616/1000 | BATCH 5/8 | LOSS: 5.8087103601186145e-06\n",
      "VAL: EPOCH 616/1000 | BATCH 6/8 | LOSS: 5.80777597341304e-06\n",
      "VAL: EPOCH 616/1000 | BATCH 7/8 | LOSS: 5.718812758459535e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 0/71 | LOSS: 4.497183908824809e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 1/71 | LOSS: 6.496611604234204e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 2/71 | LOSS: 5.678379390398429e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 3/71 | LOSS: 5.353487154025061e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 4/71 | LOSS: 5.0566882237035315e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 5/71 | LOSS: 4.783872820250205e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 6/71 | LOSS: 4.800389206138789e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 7/71 | LOSS: 4.781835144740398e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 8/71 | LOSS: 4.685330850406899e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 9/71 | LOSS: 4.632621562450368e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 10/71 | LOSS: 4.550873076037863e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 11/71 | LOSS: 4.464649009605637e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 12/71 | LOSS: 4.48348859768325e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 13/71 | LOSS: 4.513239478650835e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 14/71 | LOSS: 4.435626427342261e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 15/71 | LOSS: 4.450113522125321e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 16/71 | LOSS: 4.41575294269783e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 17/71 | LOSS: 4.439026320647423e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 18/71 | LOSS: 4.3577477802165574e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 19/71 | LOSS: 4.364851326954522e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 20/71 | LOSS: 4.31264846243028e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 21/71 | LOSS: 4.317875984518403e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 22/71 | LOSS: 4.286104760230165e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 23/71 | LOSS: 4.232650023065314e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 24/71 | LOSS: 4.229811456752941e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 25/71 | LOSS: 4.228271791362204e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 26/71 | LOSS: 4.248232572636981e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 27/71 | LOSS: 4.2850699628615985e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 28/71 | LOSS: 4.260042989448391e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 29/71 | LOSS: 4.3546838317827985e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 30/71 | LOSS: 4.353331970349011e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 31/71 | LOSS: 4.484792405889948e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 32/71 | LOSS: 4.452363699866427e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 33/71 | LOSS: 4.606440617916472e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 34/71 | LOSS: 4.578727430271101e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 35/71 | LOSS: 4.652126872568058e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 36/71 | LOSS: 4.671195483751944e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 37/71 | LOSS: 4.664815240798098e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 38/71 | LOSS: 4.691067068471546e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 39/71 | LOSS: 4.6703098689704344e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 40/71 | LOSS: 4.7177699864888135e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 41/71 | LOSS: 4.707227980188258e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 42/71 | LOSS: 4.730258661914187e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 43/71 | LOSS: 4.7188868899700704e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 44/71 | LOSS: 4.732428427208409e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 45/71 | LOSS: 4.710682703815742e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 46/71 | LOSS: 4.691602416255107e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 47/71 | LOSS: 4.666859216702808e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 48/71 | LOSS: 4.659965145260592e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 49/71 | LOSS: 4.643931674763735e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 50/71 | LOSS: 4.6319202657982445e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 51/71 | LOSS: 4.639572243657032e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 52/71 | LOSS: 4.632943921264224e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 53/71 | LOSS: 4.625490835981812e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 54/71 | LOSS: 4.612709649749873e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 55/71 | LOSS: 4.602781193138331e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 56/71 | LOSS: 4.596210701173843e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 57/71 | LOSS: 4.587552968493175e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 58/71 | LOSS: 4.591653574391523e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 59/71 | LOSS: 4.5712787558234895e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 60/71 | LOSS: 4.572775729744147e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 61/71 | LOSS: 4.560591741386405e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 62/71 | LOSS: 4.5588168819767095e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 63/71 | LOSS: 4.571995805946472e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 64/71 | LOSS: 4.5825975960794425e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 65/71 | LOSS: 4.573346734904397e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 66/71 | LOSS: 4.584749301930467e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 67/71 | LOSS: 4.604828254340485e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 68/71 | LOSS: 4.598989836017279e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 69/71 | LOSS: 4.609394938727616e-06\n",
      "TRAIN: EPOCH 617/1000 | BATCH 70/71 | LOSS: 4.588627311483872e-06\n",
      "VAL: EPOCH 617/1000 | BATCH 0/8 | LOSS: 4.956200427841395e-06\n",
      "VAL: EPOCH 617/1000 | BATCH 1/8 | LOSS: 5.16264026373392e-06\n",
      "VAL: EPOCH 617/1000 | BATCH 2/8 | LOSS: 5.31271583289102e-06\n",
      "VAL: EPOCH 617/1000 | BATCH 3/8 | LOSS: 5.241776761977235e-06\n",
      "VAL: EPOCH 617/1000 | BATCH 4/8 | LOSS: 5.306769526214339e-06\n",
      "VAL: EPOCH 617/1000 | BATCH 5/8 | LOSS: 5.2115096877969336e-06\n",
      "VAL: EPOCH 617/1000 | BATCH 6/8 | LOSS: 5.141197691825385e-06\n",
      "VAL: EPOCH 617/1000 | BATCH 7/8 | LOSS: 4.937516735026293e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 0/71 | LOSS: 5.121588401379995e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 1/71 | LOSS: 4.272051455700421e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 2/71 | LOSS: 4.484357001880805e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 3/71 | LOSS: 4.491624167712871e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 4/71 | LOSS: 4.524527412286261e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 5/71 | LOSS: 4.76600424311376e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 6/71 | LOSS: 4.7335640504440694e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 7/71 | LOSS: 4.620342792804877e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 8/71 | LOSS: 4.603292381943902e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 9/71 | LOSS: 4.627615953722852e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 10/71 | LOSS: 4.562359043285886e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 11/71 | LOSS: 4.483421397101968e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 12/71 | LOSS: 4.444631031219615e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 13/71 | LOSS: 4.355748582643823e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 14/71 | LOSS: 4.355485998530639e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 15/71 | LOSS: 4.32080284440417e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 16/71 | LOSS: 4.302250973149271e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 17/71 | LOSS: 4.296757020306864e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 18/71 | LOSS: 4.262732581513487e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 19/71 | LOSS: 4.332054720634914e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 20/71 | LOSS: 4.366150700921675e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 21/71 | LOSS: 4.3399192422426e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 22/71 | LOSS: 4.3482092006224376e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 23/71 | LOSS: 4.392130477981482e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 24/71 | LOSS: 4.345183833720512e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 25/71 | LOSS: 4.3528793779702164e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 26/71 | LOSS: 4.3482143947012785e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 27/71 | LOSS: 4.352942279898084e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 28/71 | LOSS: 4.375716320408111e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 29/71 | LOSS: 4.370135085688768e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 30/71 | LOSS: 4.385714367843672e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 31/71 | LOSS: 4.424125414459468e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 32/71 | LOSS: 4.455295111532023e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 33/71 | LOSS: 4.454332791764875e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 34/71 | LOSS: 4.452070262621938e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 35/71 | LOSS: 4.416760027147069e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 36/71 | LOSS: 4.48524544505769e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 37/71 | LOSS: 4.46640839120511e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 38/71 | LOSS: 4.470954548033217e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 39/71 | LOSS: 4.477778770706209e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 40/71 | LOSS: 4.45862657970127e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 41/71 | LOSS: 4.446278635008466e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 42/71 | LOSS: 4.4285411287239906e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 43/71 | LOSS: 4.41473727360889e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 44/71 | LOSS: 4.41424240812517e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 45/71 | LOSS: 4.4140391092577884e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 46/71 | LOSS: 4.411912994599218e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 47/71 | LOSS: 4.448840419020901e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 48/71 | LOSS: 4.416894356910809e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 49/71 | LOSS: 4.411979971337132e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 50/71 | LOSS: 4.419013713035668e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 51/71 | LOSS: 4.410196564864376e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 52/71 | LOSS: 4.408378117659595e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 53/71 | LOSS: 4.39413875702249e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 54/71 | LOSS: 4.392681074188493e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 55/71 | LOSS: 4.378855205037066e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 56/71 | LOSS: 4.3783022926634245e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 57/71 | LOSS: 4.349869429680784e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 58/71 | LOSS: 4.343833313476476e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 59/71 | LOSS: 4.327345239592735e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 60/71 | LOSS: 4.323624513044259e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 61/71 | LOSS: 4.31664903449165e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 62/71 | LOSS: 4.298902207925234e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 63/71 | LOSS: 4.3111414349539245e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 64/71 | LOSS: 4.316711689310838e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 65/71 | LOSS: 4.323709387074015e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 66/71 | LOSS: 4.355155590643851e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 67/71 | LOSS: 4.337202583696233e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 68/71 | LOSS: 4.365722087891629e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 69/71 | LOSS: 4.355400782125278e-06\n",
      "TRAIN: EPOCH 618/1000 | BATCH 70/71 | LOSS: 4.354465017071638e-06\n",
      "VAL: EPOCH 618/1000 | BATCH 0/8 | LOSS: 3.825948169833282e-06\n",
      "VAL: EPOCH 618/1000 | BATCH 1/8 | LOSS: 4.299125293982797e-06\n",
      "VAL: EPOCH 618/1000 | BATCH 2/8 | LOSS: 4.558832036612633e-06\n",
      "VAL: EPOCH 618/1000 | BATCH 3/8 | LOSS: 4.55924202924507e-06\n",
      "VAL: EPOCH 618/1000 | BATCH 4/8 | LOSS: 4.604071273206501e-06\n",
      "VAL: EPOCH 618/1000 | BATCH 5/8 | LOSS: 4.671648639487103e-06\n",
      "VAL: EPOCH 618/1000 | BATCH 6/8 | LOSS: 4.579469922256456e-06\n",
      "VAL: EPOCH 618/1000 | BATCH 7/8 | LOSS: 4.545264346234035e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 0/71 | LOSS: 3.904083769157296e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 1/71 | LOSS: 3.653957492133486e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 2/71 | LOSS: 5.1578643554724595e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 3/71 | LOSS: 5.128840598445095e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 4/71 | LOSS: 5.627387508866377e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 5/71 | LOSS: 5.373085286919377e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 6/71 | LOSS: 5.170439115837715e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 7/71 | LOSS: 5.032206729538302e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 8/71 | LOSS: 4.9401238963279566e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 9/71 | LOSS: 4.800933379556227e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 10/71 | LOSS: 4.8328251357981404e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 11/71 | LOSS: 4.680310723870207e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 12/71 | LOSS: 4.5977564800873206e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 13/71 | LOSS: 4.590548298568008e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 14/71 | LOSS: 4.660867746982452e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 15/71 | LOSS: 4.57980598866925e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 16/71 | LOSS: 4.517369555234088e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 17/71 | LOSS: 4.495222305599276e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 18/71 | LOSS: 4.4598692559367535e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 19/71 | LOSS: 4.408541508382769e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 20/71 | LOSS: 4.353691846994306e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 21/71 | LOSS: 4.327062284077825e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 22/71 | LOSS: 4.297155417431065e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 23/71 | LOSS: 4.287376403529682e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 24/71 | LOSS: 4.241249598635477e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 25/71 | LOSS: 4.274029877776159e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 26/71 | LOSS: 4.249630263883672e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 27/71 | LOSS: 4.295836220080673e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 28/71 | LOSS: 4.302154530736955e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 29/71 | LOSS: 4.316665551111025e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 30/71 | LOSS: 4.340677752489066e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 31/71 | LOSS: 4.375068670015025e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 32/71 | LOSS: 4.3572200190806836e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 33/71 | LOSS: 4.347894941127058e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 34/71 | LOSS: 4.320325336136323e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 35/71 | LOSS: 4.325268062327976e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 36/71 | LOSS: 4.368751912831637e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 37/71 | LOSS: 4.3574843259289326e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 38/71 | LOSS: 4.374102935309145e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 39/71 | LOSS: 4.371342436115811e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 40/71 | LOSS: 4.3622282380344654e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 41/71 | LOSS: 4.344926548496698e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 42/71 | LOSS: 4.363559741631075e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 43/71 | LOSS: 4.356265786885739e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 44/71 | LOSS: 4.3742361109859e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 45/71 | LOSS: 4.3471078519676505e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 46/71 | LOSS: 4.3450475246534045e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 47/71 | LOSS: 4.329901571509254e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 48/71 | LOSS: 4.341275431512741e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 49/71 | LOSS: 4.333621000114362e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 50/71 | LOSS: 4.3243024033561636e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 51/71 | LOSS: 4.330410083989241e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 52/71 | LOSS: 4.32018246873719e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 53/71 | LOSS: 4.328151884759858e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 54/71 | LOSS: 4.311595834720224e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 55/71 | LOSS: 4.295838461335474e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 56/71 | LOSS: 4.324085095600151e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 57/71 | LOSS: 4.3320561762184615e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 58/71 | LOSS: 4.332489147523351e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 59/71 | LOSS: 4.358896103440202e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 60/71 | LOSS: 4.350285530190525e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 61/71 | LOSS: 4.356719478716015e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 62/71 | LOSS: 4.353140584931853e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 63/71 | LOSS: 4.344222713115187e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 64/71 | LOSS: 4.342594432343434e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 65/71 | LOSS: 4.340268805153204e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 66/71 | LOSS: 4.350209188324603e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 67/71 | LOSS: 4.3413640101414996e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 68/71 | LOSS: 4.3482139655838106e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 69/71 | LOSS: 4.348365938052926e-06\n",
      "TRAIN: EPOCH 619/1000 | BATCH 70/71 | LOSS: 4.347666915659637e-06\n",
      "VAL: EPOCH 619/1000 | BATCH 0/8 | LOSS: 4.811018698092084e-06\n",
      "VAL: EPOCH 619/1000 | BATCH 1/8 | LOSS: 4.9823318022390595e-06\n",
      "VAL: EPOCH 619/1000 | BATCH 2/8 | LOSS: 5.118012116630173e-06\n",
      "VAL: EPOCH 619/1000 | BATCH 3/8 | LOSS: 5.060418516222853e-06\n",
      "VAL: EPOCH 619/1000 | BATCH 4/8 | LOSS: 5.231721661402844e-06\n",
      "VAL: EPOCH 619/1000 | BATCH 5/8 | LOSS: 5.197176960791694e-06\n",
      "VAL: EPOCH 619/1000 | BATCH 6/8 | LOSS: 5.140748076623172e-06\n",
      "VAL: EPOCH 619/1000 | BATCH 7/8 | LOSS: 5.042644318109524e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 0/71 | LOSS: 4.751869255414931e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 1/71 | LOSS: 5.606439799521468e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 2/71 | LOSS: 5.061309972613041e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 3/71 | LOSS: 5.091688649372372e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 4/71 | LOSS: 4.907968559564324e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 5/71 | LOSS: 5.007988950940974e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 6/71 | LOSS: 4.936565021905283e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 7/71 | LOSS: 4.871646581250388e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 8/71 | LOSS: 4.851800491047066e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 9/71 | LOSS: 4.7099814082685045e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 10/71 | LOSS: 4.792154682449605e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 11/71 | LOSS: 4.792025549704704e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 12/71 | LOSS: 4.750540081379138e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 13/71 | LOSS: 4.73373622103931e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 14/71 | LOSS: 4.653960271146692e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 15/71 | LOSS: 4.591667206454986e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 16/71 | LOSS: 4.550977618076585e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 17/71 | LOSS: 4.5078991964449715e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 18/71 | LOSS: 4.46454773258378e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 19/71 | LOSS: 4.417203729190078e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 20/71 | LOSS: 4.356734377454684e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 21/71 | LOSS: 4.309183075499525e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 22/71 | LOSS: 4.29720297807248e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 23/71 | LOSS: 4.267029311222359e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 24/71 | LOSS: 4.271729640095146e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 25/71 | LOSS: 4.2323694847604765e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 26/71 | LOSS: 4.209725779623517e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 27/71 | LOSS: 4.211937802470597e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 28/71 | LOSS: 4.220819313371161e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 29/71 | LOSS: 4.193348756113361e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 30/71 | LOSS: 4.2136320218794995e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 31/71 | LOSS: 4.225362204124394e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 32/71 | LOSS: 4.221720204937314e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 33/71 | LOSS: 4.24714764502663e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 34/71 | LOSS: 4.222504750422169e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 35/71 | LOSS: 4.206392923657e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 36/71 | LOSS: 4.1950140220669214e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 37/71 | LOSS: 4.242456073274968e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 38/71 | LOSS: 4.233601459465064e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 39/71 | LOSS: 4.212858419805343e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 40/71 | LOSS: 4.215174196981868e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 41/71 | LOSS: 4.238698200727716e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 42/71 | LOSS: 4.226326503813589e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 43/71 | LOSS: 4.213467780738773e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 44/71 | LOSS: 4.208353554228476e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 45/71 | LOSS: 4.215265077974713e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 46/71 | LOSS: 4.2405853433747515e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 47/71 | LOSS: 4.242805393535794e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 48/71 | LOSS: 4.254509163655137e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 49/71 | LOSS: 4.248197910783347e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 50/71 | LOSS: 4.263812196072917e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 51/71 | LOSS: 4.26808162501402e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 52/71 | LOSS: 4.268210415040881e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 53/71 | LOSS: 4.284382216442113e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 54/71 | LOSS: 4.292650580530542e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 55/71 | LOSS: 4.275735562941918e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 56/71 | LOSS: 4.281970652236691e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 57/71 | LOSS: 4.295997710273539e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 58/71 | LOSS: 4.296091427762403e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 59/71 | LOSS: 4.294000196599275e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 60/71 | LOSS: 4.327584575981209e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 61/71 | LOSS: 4.3440236771446856e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 62/71 | LOSS: 4.348797022300589e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 63/71 | LOSS: 4.349680427395697e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 64/71 | LOSS: 4.397676152970794e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 65/71 | LOSS: 4.382375367861117e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 66/71 | LOSS: 4.41183154370224e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 67/71 | LOSS: 4.435130309128213e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 68/71 | LOSS: 4.439707580509687e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 69/71 | LOSS: 4.426475275717426e-06\n",
      "TRAIN: EPOCH 620/1000 | BATCH 70/71 | LOSS: 4.44064241658542e-06\n",
      "VAL: EPOCH 620/1000 | BATCH 0/8 | LOSS: 6.613934601773508e-06\n",
      "VAL: EPOCH 620/1000 | BATCH 1/8 | LOSS: 7.663417818548623e-06\n",
      "VAL: EPOCH 620/1000 | BATCH 2/8 | LOSS: 8.078344156577563e-06\n",
      "VAL: EPOCH 620/1000 | BATCH 3/8 | LOSS: 8.060819482125225e-06\n",
      "VAL: EPOCH 620/1000 | BATCH 4/8 | LOSS: 8.175503717211541e-06\n",
      "VAL: EPOCH 620/1000 | BATCH 5/8 | LOSS: 8.299122176443538e-06\n",
      "VAL: EPOCH 620/1000 | BATCH 6/8 | LOSS: 8.18966119758053e-06\n",
      "VAL: EPOCH 620/1000 | BATCH 7/8 | LOSS: 8.304113293888804e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 0/71 | LOSS: 8.461995093966834e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 1/71 | LOSS: 6.083407583901135e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 2/71 | LOSS: 6.435659618849361e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 3/71 | LOSS: 6.422129388283793e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 4/71 | LOSS: 6.9134118803049205e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 5/71 | LOSS: 6.454902973018761e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 6/71 | LOSS: 6.386958018213461e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 7/71 | LOSS: 6.404050537867079e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 8/71 | LOSS: 6.247989025117325e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 9/71 | LOSS: 6.4676868760216165e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 10/71 | LOSS: 6.251423883441021e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 11/71 | LOSS: 6.351961606772723e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 12/71 | LOSS: 6.2299882826594585e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 13/71 | LOSS: 6.3162364735295085e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 14/71 | LOSS: 6.233346827381562e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 15/71 | LOSS: 6.210309848597717e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 16/71 | LOSS: 6.184284476309104e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 17/71 | LOSS: 6.079049537927656e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 18/71 | LOSS: 5.991389774575701e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 19/71 | LOSS: 5.9461151863615665e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 20/71 | LOSS: 5.878175746450627e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 21/71 | LOSS: 5.769865341872818e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 22/71 | LOSS: 5.689059089095479e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 23/71 | LOSS: 5.6269355184213055e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 24/71 | LOSS: 5.5551618152094304e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 25/71 | LOSS: 5.473095650533931e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 26/71 | LOSS: 5.419848464414288e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 27/71 | LOSS: 5.373744345758626e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 28/71 | LOSS: 5.351869856832341e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 29/71 | LOSS: 5.329902508795688e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 30/71 | LOSS: 5.275392536972387e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 31/71 | LOSS: 5.265300572432352e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 32/71 | LOSS: 5.220020649644677e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 33/71 | LOSS: 5.181672664173889e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 34/71 | LOSS: 5.137349424850461e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 35/71 | LOSS: 5.070314178586462e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 36/71 | LOSS: 5.04346822044927e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 37/71 | LOSS: 5.004355204475455e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 38/71 | LOSS: 4.982423480471017e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 39/71 | LOSS: 4.986832323083945e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 40/71 | LOSS: 4.9733493719164786e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 41/71 | LOSS: 4.963586975680387e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 42/71 | LOSS: 4.931448862661518e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 43/71 | LOSS: 4.935223971229292e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 44/71 | LOSS: 4.92313959309993e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 45/71 | LOSS: 4.913680151691116e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 46/71 | LOSS: 4.895402540530492e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 47/71 | LOSS: 4.889492378386724e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 48/71 | LOSS: 4.862118022192961e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 49/71 | LOSS: 4.877206279161328e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 50/71 | LOSS: 4.857172232768076e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 51/71 | LOSS: 4.85438053269368e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 52/71 | LOSS: 4.835789502736451e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 53/71 | LOSS: 4.822569638707608e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 54/71 | LOSS: 4.798478138664822e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 55/71 | LOSS: 4.791182610525928e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 56/71 | LOSS: 4.778839530971661e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 57/71 | LOSS: 4.754345001919287e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 58/71 | LOSS: 4.7392288822373884e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 59/71 | LOSS: 4.736916829036394e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 60/71 | LOSS: 4.717769525832111e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 61/71 | LOSS: 4.711434932628876e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 62/71 | LOSS: 4.7167932640900655e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 63/71 | LOSS: 4.715231234797557e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 64/71 | LOSS: 4.718673685252165e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 65/71 | LOSS: 4.732180197165827e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 66/71 | LOSS: 4.730604828114677e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 67/71 | LOSS: 4.735435254588083e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 68/71 | LOSS: 4.744918791455321e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 69/71 | LOSS: 4.736369088017715e-06\n",
      "TRAIN: EPOCH 621/1000 | BATCH 70/71 | LOSS: 4.702074929681798e-06\n",
      "VAL: EPOCH 621/1000 | BATCH 0/8 | LOSS: 5.420923571364256e-06\n",
      "VAL: EPOCH 621/1000 | BATCH 1/8 | LOSS: 5.6140063406928675e-06\n",
      "VAL: EPOCH 621/1000 | BATCH 2/8 | LOSS: 5.655830894587173e-06\n",
      "VAL: EPOCH 621/1000 | BATCH 3/8 | LOSS: 5.548666990762285e-06\n",
      "VAL: EPOCH 621/1000 | BATCH 4/8 | LOSS: 5.672970655723475e-06\n",
      "VAL: EPOCH 621/1000 | BATCH 5/8 | LOSS: 5.525504623922946e-06\n",
      "VAL: EPOCH 621/1000 | BATCH 6/8 | LOSS: 5.4182909090221595e-06\n",
      "VAL: EPOCH 621/1000 | BATCH 7/8 | LOSS: 5.261156047708937e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 0/71 | LOSS: 5.072874500910984e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 1/71 | LOSS: 4.8680658437660895e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 2/71 | LOSS: 5.444939385294371e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 3/71 | LOSS: 5.339006861504458e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 4/71 | LOSS: 5.1551906835811675e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 5/71 | LOSS: 5.590922607249619e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 6/71 | LOSS: 5.50942552633517e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 7/71 | LOSS: 5.792354500044894e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 8/71 | LOSS: 5.644158186947203e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 9/71 | LOSS: 5.830314012200688e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 10/71 | LOSS: 5.690082409935051e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 11/71 | LOSS: 5.778718294398762e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 12/71 | LOSS: 5.68692936352678e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 13/71 | LOSS: 5.750639404920678e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 14/71 | LOSS: 5.82849661441287e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 15/71 | LOSS: 5.792718013708509e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 16/71 | LOSS: 5.694525016759358e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 17/71 | LOSS: 5.704764943301497e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 18/71 | LOSS: 5.653900906939893e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 19/71 | LOSS: 5.579414050771447e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 20/71 | LOSS: 5.589532238878919e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 21/71 | LOSS: 5.52600528697605e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 22/71 | LOSS: 5.485584097186802e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 23/71 | LOSS: 5.462440337093237e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 24/71 | LOSS: 5.4252889458439315e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 25/71 | LOSS: 5.376315322372158e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 26/71 | LOSS: 5.309106039998436e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 27/71 | LOSS: 5.274716686796767e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 28/71 | LOSS: 5.225306432325381e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 29/71 | LOSS: 5.1710194914752115e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 30/71 | LOSS: 5.17791548396241e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 31/71 | LOSS: 5.133131544710068e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 32/71 | LOSS: 5.107863926517012e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 33/71 | LOSS: 5.098679346648329e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 34/71 | LOSS: 5.060999196107982e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 35/71 | LOSS: 5.052206012755454e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 36/71 | LOSS: 5.026732043080375e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 37/71 | LOSS: 5.005790955549042e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 38/71 | LOSS: 4.960810988800054e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 39/71 | LOSS: 4.922551897834637e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 40/71 | LOSS: 4.900790139711207e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 41/71 | LOSS: 4.867303513968918e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 42/71 | LOSS: 4.851256805924877e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 43/71 | LOSS: 4.848822858217648e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 44/71 | LOSS: 4.844212667699645e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 45/71 | LOSS: 4.814337992921418e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 46/71 | LOSS: 4.796532706716383e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 47/71 | LOSS: 4.782636149040324e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 48/71 | LOSS: 4.767304554494628e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 49/71 | LOSS: 4.771513499690627e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 50/71 | LOSS: 4.7656971994857646e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 51/71 | LOSS: 4.749417270816663e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 52/71 | LOSS: 4.742722660252322e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 53/71 | LOSS: 4.765972642383497e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 54/71 | LOSS: 4.733080798111835e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 55/71 | LOSS: 4.72832641662535e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 56/71 | LOSS: 4.7437972027278436e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 57/71 | LOSS: 4.721434054082583e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 58/71 | LOSS: 4.7023130125542256e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 59/71 | LOSS: 4.6959331067834375e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 60/71 | LOSS: 4.699913785368496e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 61/71 | LOSS: 4.682489086536084e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 62/71 | LOSS: 4.670598010696787e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 63/71 | LOSS: 4.660660845701159e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 64/71 | LOSS: 4.647960125881722e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 65/71 | LOSS: 4.632798284850044e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 66/71 | LOSS: 4.615090818930836e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 67/71 | LOSS: 4.6156548057083186e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 68/71 | LOSS: 4.596492185621595e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 69/71 | LOSS: 4.612093844765955e-06\n",
      "TRAIN: EPOCH 622/1000 | BATCH 70/71 | LOSS: 4.5884936766091875e-06\n",
      "VAL: EPOCH 622/1000 | BATCH 0/8 | LOSS: 3.997224666818511e-06\n",
      "VAL: EPOCH 622/1000 | BATCH 1/8 | LOSS: 4.122089649172267e-06\n",
      "VAL: EPOCH 622/1000 | BATCH 2/8 | LOSS: 4.327562540614356e-06\n",
      "VAL: EPOCH 622/1000 | BATCH 3/8 | LOSS: 4.381910912343301e-06\n",
      "VAL: EPOCH 622/1000 | BATCH 4/8 | LOSS: 4.391661786939949e-06\n",
      "VAL: EPOCH 622/1000 | BATCH 5/8 | LOSS: 4.449260662416539e-06\n",
      "VAL: EPOCH 622/1000 | BATCH 6/8 | LOSS: 4.3209821082460365e-06\n",
      "VAL: EPOCH 622/1000 | BATCH 7/8 | LOSS: 4.12947940731101e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 0/71 | LOSS: 5.423588845587801e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 1/71 | LOSS: 5.0706148613244295e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 2/71 | LOSS: 4.499892384046689e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 3/71 | LOSS: 4.413210035636439e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 4/71 | LOSS: 4.44403867732035e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 5/71 | LOSS: 4.409069636797843e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 6/71 | LOSS: 4.29697573573711e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 7/71 | LOSS: 4.341988699252397e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 8/71 | LOSS: 4.227609401318154e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 9/71 | LOSS: 4.484192709242052e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 10/71 | LOSS: 4.364279551406401e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 11/71 | LOSS: 4.358204080290307e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 12/71 | LOSS: 4.336337234725272e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 13/71 | LOSS: 4.33356157308091e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 14/71 | LOSS: 4.2902985114778854e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 15/71 | LOSS: 4.2592232887272985e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 16/71 | LOSS: 4.224207754412098e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 17/71 | LOSS: 4.2502007318034885e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 18/71 | LOSS: 4.249158301287312e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 19/71 | LOSS: 4.226485532399238e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 20/71 | LOSS: 4.221237683547467e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 21/71 | LOSS: 4.23541699877784e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 22/71 | LOSS: 4.2206297522447445e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 23/71 | LOSS: 4.233074709721525e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 24/71 | LOSS: 4.218055501041818e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 25/71 | LOSS: 4.219797635694424e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 26/71 | LOSS: 4.17569786906016e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 27/71 | LOSS: 4.186288127324847e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 28/71 | LOSS: 4.201127804380717e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 29/71 | LOSS: 4.188786783743126e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 30/71 | LOSS: 4.2023268780476726e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 31/71 | LOSS: 4.19432350184934e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 32/71 | LOSS: 4.174953415492906e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 33/71 | LOSS: 4.171167380959292e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 34/71 | LOSS: 4.169514360390686e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 35/71 | LOSS: 4.147507127072458e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 36/71 | LOSS: 4.133702655176544e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 37/71 | LOSS: 4.129507523861034e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 38/71 | LOSS: 4.126904817288353e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 39/71 | LOSS: 4.1221437413696546e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 40/71 | LOSS: 4.150450744108217e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 41/71 | LOSS: 4.182859822235298e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 42/71 | LOSS: 4.1631127884144475e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 43/71 | LOSS: 4.225202722161405e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 44/71 | LOSS: 4.2477365544376275e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 45/71 | LOSS: 4.272868191828425e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 46/71 | LOSS: 4.29379678222331e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 47/71 | LOSS: 4.310031139690788e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 48/71 | LOSS: 4.3328344297467024e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 49/71 | LOSS: 4.336444771979586e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 50/71 | LOSS: 4.377931810606742e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 51/71 | LOSS: 4.37637117136966e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 52/71 | LOSS: 4.419430769183168e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 53/71 | LOSS: 4.418847925922752e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 54/71 | LOSS: 4.453182530348634e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 55/71 | LOSS: 4.454407847528533e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 56/71 | LOSS: 4.455877555492886e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 57/71 | LOSS: 4.445322255033434e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 58/71 | LOSS: 4.462243796913723e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 59/71 | LOSS: 4.444693358133615e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 60/71 | LOSS: 4.4375967196383055e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 61/71 | LOSS: 4.474734451363357e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 62/71 | LOSS: 4.498337245552518e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 63/71 | LOSS: 4.507554304922223e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 64/71 | LOSS: 4.566443410042736e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 65/71 | LOSS: 4.5688195183229805e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 66/71 | LOSS: 4.6084445506521525e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 67/71 | LOSS: 4.601084713960838e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 68/71 | LOSS: 4.614969203180078e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 69/71 | LOSS: 4.6155552906514324e-06\n",
      "TRAIN: EPOCH 623/1000 | BATCH 70/71 | LOSS: 4.628220488446229e-06\n",
      "VAL: EPOCH 623/1000 | BATCH 0/8 | LOSS: 3.842502337647602e-06\n",
      "VAL: EPOCH 623/1000 | BATCH 1/8 | LOSS: 4.011698365502525e-06\n",
      "VAL: EPOCH 623/1000 | BATCH 2/8 | LOSS: 4.2256006054230966e-06\n",
      "VAL: EPOCH 623/1000 | BATCH 3/8 | LOSS: 4.228267584949208e-06\n",
      "VAL: EPOCH 623/1000 | BATCH 4/8 | LOSS: 4.35732790720067e-06\n",
      "VAL: EPOCH 623/1000 | BATCH 5/8 | LOSS: 4.381574020347519e-06\n",
      "VAL: EPOCH 623/1000 | BATCH 6/8 | LOSS: 4.327493050888214e-06\n",
      "VAL: EPOCH 623/1000 | BATCH 7/8 | LOSS: 4.251833075841205e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 0/71 | LOSS: 2.628949232530431e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 1/71 | LOSS: 3.4208347869935096e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 2/71 | LOSS: 4.812657683335904e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 3/71 | LOSS: 4.791716548879776e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 4/71 | LOSS: 5.6275057886523426e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 5/71 | LOSS: 5.5541462415931164e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 6/71 | LOSS: 5.557432392119413e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 7/71 | LOSS: 5.684887895540669e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 8/71 | LOSS: 5.667056055042647e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 9/71 | LOSS: 5.696107859876065e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 10/71 | LOSS: 5.669725913486962e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 11/71 | LOSS: 5.730421378302708e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 12/71 | LOSS: 5.6198461362118205e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 13/71 | LOSS: 5.691776696689235e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 14/71 | LOSS: 5.8047793118021215e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 15/71 | LOSS: 5.764306720834611e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 16/71 | LOSS: 5.803595968245645e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 17/71 | LOSS: 5.77538464262438e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 18/71 | LOSS: 5.934612150476344e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 19/71 | LOSS: 5.851916114352207e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 20/71 | LOSS: 5.893379726070894e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 21/71 | LOSS: 5.899875521787511e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 22/71 | LOSS: 5.9140028912699325e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 23/71 | LOSS: 5.953660140297264e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 24/71 | LOSS: 5.911955731789931e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 25/71 | LOSS: 5.931404049218914e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 26/71 | LOSS: 5.899071589333795e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 27/71 | LOSS: 5.885639268399245e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 28/71 | LOSS: 5.820377243159375e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 29/71 | LOSS: 5.853099999815944e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 30/71 | LOSS: 5.931697196795158e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 31/71 | LOSS: 5.918207357069605e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 32/71 | LOSS: 5.958760118918497e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 33/71 | LOSS: 5.9489924123066455e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 34/71 | LOSS: 5.985140504500513e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 35/71 | LOSS: 5.955017545349821e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 36/71 | LOSS: 5.940237493583432e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 37/71 | LOSS: 5.889398889130941e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 38/71 | LOSS: 5.971862527495921e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 39/71 | LOSS: 5.938057080356885e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 40/71 | LOSS: 5.957788272741997e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 41/71 | LOSS: 5.926949624847025e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 42/71 | LOSS: 5.9567067468336504e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 43/71 | LOSS: 5.932873834395154e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 44/71 | LOSS: 5.976174962698779e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 45/71 | LOSS: 5.98264550618573e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 46/71 | LOSS: 5.936301752800764e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 47/71 | LOSS: 5.95210513646786e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 48/71 | LOSS: 5.9889231003896325e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 49/71 | LOSS: 6.00510236381524e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 50/71 | LOSS: 6.007093098364124e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 51/71 | LOSS: 6.022900068868554e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 52/71 | LOSS: 6.065208515552375e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 53/71 | LOSS: 6.053453611693166e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 54/71 | LOSS: 6.087361018878503e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 55/71 | LOSS: 6.1304857784502275e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 56/71 | LOSS: 6.151900040270432e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 57/71 | LOSS: 6.154419573813984e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 58/71 | LOSS: 6.156975301965466e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 59/71 | LOSS: 6.1473761851023784e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 60/71 | LOSS: 6.126329022573954e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 61/71 | LOSS: 6.1329664049480925e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 62/71 | LOSS: 6.120307597527807e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 63/71 | LOSS: 6.1227753214154745e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 64/71 | LOSS: 6.107349337561083e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 65/71 | LOSS: 6.07995596882657e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 66/71 | LOSS: 6.074182424730571e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 67/71 | LOSS: 6.03267718321042e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 68/71 | LOSS: 6.014888828323582e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 69/71 | LOSS: 5.981741294038199e-06\n",
      "TRAIN: EPOCH 624/1000 | BATCH 70/71 | LOSS: 5.946975393335763e-06\n",
      "VAL: EPOCH 624/1000 | BATCH 0/8 | LOSS: 4.4023236114298925e-06\n",
      "VAL: EPOCH 624/1000 | BATCH 1/8 | LOSS: 4.618187176674837e-06\n",
      "VAL: EPOCH 624/1000 | BATCH 2/8 | LOSS: 4.676122595507574e-06\n",
      "VAL: EPOCH 624/1000 | BATCH 3/8 | LOSS: 4.675932132158778e-06\n",
      "VAL: EPOCH 624/1000 | BATCH 4/8 | LOSS: 4.649709444493055e-06\n",
      "VAL: EPOCH 624/1000 | BATCH 5/8 | LOSS: 4.632784111890942e-06\n",
      "VAL: EPOCH 624/1000 | BATCH 6/8 | LOSS: 4.458913865944071e-06\n",
      "VAL: EPOCH 624/1000 | BATCH 7/8 | LOSS: 4.285579166207754e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 0/71 | LOSS: 4.44876332039712e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 1/71 | LOSS: 4.53060056315735e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 2/71 | LOSS: 5.126072210259736e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 3/71 | LOSS: 4.6765006800342235e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 4/71 | LOSS: 4.615920352080138e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 5/71 | LOSS: 4.424135530219549e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 6/71 | LOSS: 4.52295064081097e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 7/71 | LOSS: 4.557729738507987e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 8/71 | LOSS: 4.4554302702535e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 9/71 | LOSS: 4.5890735691500595e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 10/71 | LOSS: 4.589961670056007e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 11/71 | LOSS: 4.723844578317464e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 12/71 | LOSS: 4.647147901638304e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 13/71 | LOSS: 4.8083781426352545e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 14/71 | LOSS: 4.724812303417517e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 15/71 | LOSS: 4.7004667464989325e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 16/71 | LOSS: 4.720829324736876e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 17/71 | LOSS: 4.696040377893951e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 18/71 | LOSS: 4.701121700858685e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 19/71 | LOSS: 4.691394042311003e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 20/71 | LOSS: 4.682259757481959e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 21/71 | LOSS: 4.675321633840213e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 22/71 | LOSS: 4.625761092661971e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 23/71 | LOSS: 4.6596133150463475e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 24/71 | LOSS: 4.648008562071482e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 25/71 | LOSS: 4.656669314219751e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 26/71 | LOSS: 4.687619248552774e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 27/71 | LOSS: 4.676821211952691e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 28/71 | LOSS: 4.643398361967511e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 29/71 | LOSS: 4.6174017597877535e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 30/71 | LOSS: 4.63108719281257e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 31/71 | LOSS: 4.643728182429641e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 32/71 | LOSS: 4.60188089439753e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 33/71 | LOSS: 4.5931990839295285e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 34/71 | LOSS: 4.6336830759433464e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 35/71 | LOSS: 4.6269762745194184e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 36/71 | LOSS: 4.601679853982893e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 37/71 | LOSS: 4.603901533970968e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 38/71 | LOSS: 4.633429917260238e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 39/71 | LOSS: 4.626078629144104e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 40/71 | LOSS: 4.627601306421594e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 41/71 | LOSS: 4.636613046689793e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 42/71 | LOSS: 4.642442221200665e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 43/71 | LOSS: 4.638086109439015e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 44/71 | LOSS: 4.6514881079120745e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 45/71 | LOSS: 4.6340877489732835e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 46/71 | LOSS: 4.61902503899337e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 47/71 | LOSS: 4.620772619053544e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 48/71 | LOSS: 4.605538374469651e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 49/71 | LOSS: 4.5970308974574435e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 50/71 | LOSS: 4.586190096447621e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 51/71 | LOSS: 4.588534977756353e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 52/71 | LOSS: 4.5751932814383975e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 53/71 | LOSS: 4.571651436014724e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 54/71 | LOSS: 4.561121010159364e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 55/71 | LOSS: 4.555482953654584e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 56/71 | LOSS: 4.5453787231708065e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 57/71 | LOSS: 4.533857969394815e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 58/71 | LOSS: 4.520925038655227e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 59/71 | LOSS: 4.512814996360248e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 60/71 | LOSS: 4.51150703022412e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 61/71 | LOSS: 4.526743297109563e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 62/71 | LOSS: 4.5281506912047805e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 63/71 | LOSS: 4.522579150290085e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 64/71 | LOSS: 4.5197173221822595e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 65/71 | LOSS: 4.5221979448659555e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 66/71 | LOSS: 4.519078402154264e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 67/71 | LOSS: 4.506423288282403e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 68/71 | LOSS: 4.495675996266705e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 69/71 | LOSS: 4.491585965687201e-06\n",
      "TRAIN: EPOCH 625/1000 | BATCH 70/71 | LOSS: 4.467480369090532e-06\n",
      "VAL: EPOCH 625/1000 | BATCH 0/8 | LOSS: 3.7180127492320025e-06\n",
      "VAL: EPOCH 625/1000 | BATCH 1/8 | LOSS: 3.952326210310275e-06\n",
      "VAL: EPOCH 625/1000 | BATCH 2/8 | LOSS: 4.129648080682576e-06\n",
      "VAL: EPOCH 625/1000 | BATCH 3/8 | LOSS: 4.140165458466072e-06\n",
      "VAL: EPOCH 625/1000 | BATCH 4/8 | LOSS: 4.209262669974123e-06\n",
      "VAL: EPOCH 625/1000 | BATCH 5/8 | LOSS: 4.327868850850791e-06\n",
      "VAL: EPOCH 625/1000 | BATCH 6/8 | LOSS: 4.234779843110508e-06\n",
      "VAL: EPOCH 625/1000 | BATCH 7/8 | LOSS: 4.1953785796522425e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 0/71 | LOSS: 4.170418833382428e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 1/71 | LOSS: 4.318029823480174e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 2/71 | LOSS: 4.095421824483007e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 3/71 | LOSS: 4.238635426645487e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 4/71 | LOSS: 4.187783042652882e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 5/71 | LOSS: 4.033036664926233e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 6/71 | LOSS: 4.026400184946917e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 7/71 | LOSS: 4.03654982505941e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 8/71 | LOSS: 4.017680491112212e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 9/71 | LOSS: 4.1339806330142895e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 10/71 | LOSS: 4.077806806890294e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 11/71 | LOSS: 4.097119699508767e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 12/71 | LOSS: 4.144289260483884e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 13/71 | LOSS: 4.081450794858808e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 14/71 | LOSS: 4.109287601750111e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 15/71 | LOSS: 4.131520313421788e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 16/71 | LOSS: 4.122755800541627e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 17/71 | LOSS: 4.12592508938461e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 18/71 | LOSS: 4.1818480224863285e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 19/71 | LOSS: 4.231653451824968e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 20/71 | LOSS: 4.2151145646043694e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 21/71 | LOSS: 4.271793345238124e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 22/71 | LOSS: 4.25030638641898e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 23/71 | LOSS: 4.249052219999309e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 24/71 | LOSS: 4.250463725838927e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 25/71 | LOSS: 4.244381924623807e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 26/71 | LOSS: 4.260772188731439e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 27/71 | LOSS: 4.249134828374476e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 28/71 | LOSS: 4.253392795550914e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 29/71 | LOSS: 4.252514077052183e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 30/71 | LOSS: 4.284755908473368e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 31/71 | LOSS: 4.266918317341606e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 32/71 | LOSS: 4.254696215976957e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 33/71 | LOSS: 4.241751144838738e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 34/71 | LOSS: 4.218900942630301e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 35/71 | LOSS: 4.194547513078659e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 36/71 | LOSS: 4.192298178708911e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 37/71 | LOSS: 4.198600412277492e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 38/71 | LOSS: 4.207128359923491e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 39/71 | LOSS: 4.2195300579805915e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 40/71 | LOSS: 4.2241767027362805e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 41/71 | LOSS: 4.257706710372336e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 42/71 | LOSS: 4.251668701198109e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 43/71 | LOSS: 4.281719870133119e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 44/71 | LOSS: 4.26853914885012e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 45/71 | LOSS: 4.266928385732172e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 46/71 | LOSS: 4.28045312586851e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 47/71 | LOSS: 4.294718266351083e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 48/71 | LOSS: 4.3076384613692536e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 49/71 | LOSS: 4.3294705437801895e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 50/71 | LOSS: 4.329170299873415e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 51/71 | LOSS: 4.327035714350831e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 52/71 | LOSS: 4.3381864097167605e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 53/71 | LOSS: 4.320271622654454e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 54/71 | LOSS: 4.322145845104718e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 55/71 | LOSS: 4.3048961434318955e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 56/71 | LOSS: 4.31654537654443e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 57/71 | LOSS: 4.335950938309844e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 58/71 | LOSS: 4.31816027045543e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 59/71 | LOSS: 4.313563465530024e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 60/71 | LOSS: 4.308050575509398e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 61/71 | LOSS: 4.297658957482547e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 62/71 | LOSS: 4.30005081311503e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 63/71 | LOSS: 4.299442121435959e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 64/71 | LOSS: 4.296193789671704e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 65/71 | LOSS: 4.2995232427005466e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 66/71 | LOSS: 4.295321081263498e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 67/71 | LOSS: 4.3050136803364745e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 68/71 | LOSS: 4.303394158948019e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 69/71 | LOSS: 4.296181730621486e-06\n",
      "TRAIN: EPOCH 626/1000 | BATCH 70/71 | LOSS: 4.27003302755905e-06\n",
      "VAL: EPOCH 626/1000 | BATCH 0/8 | LOSS: 4.134386472287588e-06\n",
      "VAL: EPOCH 626/1000 | BATCH 1/8 | LOSS: 4.484234978008317e-06\n",
      "VAL: EPOCH 626/1000 | BATCH 2/8 | LOSS: 4.574507026215239e-06\n",
      "VAL: EPOCH 626/1000 | BATCH 3/8 | LOSS: 4.511945917329285e-06\n",
      "VAL: EPOCH 626/1000 | BATCH 4/8 | LOSS: 4.587474450090667e-06\n",
      "VAL: EPOCH 626/1000 | BATCH 5/8 | LOSS: 4.49281348361789e-06\n",
      "VAL: EPOCH 626/1000 | BATCH 6/8 | LOSS: 4.354935949127789e-06\n",
      "VAL: EPOCH 626/1000 | BATCH 7/8 | LOSS: 4.1904880276888434e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 0/71 | LOSS: 4.961975719197653e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 1/71 | LOSS: 4.578015477818553e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 2/71 | LOSS: 4.3586113254908314e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 3/71 | LOSS: 4.425818247000279e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 4/71 | LOSS: 4.2771890093717955e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 5/71 | LOSS: 4.254005830262031e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 6/71 | LOSS: 4.239726195010007e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 7/71 | LOSS: 4.233974749467961e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 8/71 | LOSS: 4.211219245715054e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 9/71 | LOSS: 4.1068649807129985e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 10/71 | LOSS: 4.1611198288261555e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 11/71 | LOSS: 4.15327591933116e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 12/71 | LOSS: 4.1725229843555444e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 13/71 | LOSS: 4.206418517631911e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 14/71 | LOSS: 4.286387199196421e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 15/71 | LOSS: 4.344526331578891e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 16/71 | LOSS: 4.376400471194168e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 17/71 | LOSS: 4.395355037761166e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 18/71 | LOSS: 4.4085382365952495e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 19/71 | LOSS: 4.3934008772339436e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 20/71 | LOSS: 4.371832108604611e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 21/71 | LOSS: 4.415256750111109e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 22/71 | LOSS: 4.401983381324934e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 23/71 | LOSS: 4.371774158092497e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 24/71 | LOSS: 4.424071285029641e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 25/71 | LOSS: 4.434782445409487e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 26/71 | LOSS: 4.4799083096020495e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 27/71 | LOSS: 4.496098930368524e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 28/71 | LOSS: 4.5662676994056556e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 29/71 | LOSS: 4.595176293757201e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 30/71 | LOSS: 4.578195198372404e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 31/71 | LOSS: 4.601447287200244e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 32/71 | LOSS: 4.6747686472813115e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 33/71 | LOSS: 4.7150577114841915e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 34/71 | LOSS: 4.756713523030547e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 35/71 | LOSS: 4.821116400914131e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 36/71 | LOSS: 4.833876770331895e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 37/71 | LOSS: 4.840520707722662e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 38/71 | LOSS: 4.833003517071633e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 39/71 | LOSS: 4.8603662435198204e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 40/71 | LOSS: 4.878600898004285e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 41/71 | LOSS: 4.856484511470799e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 42/71 | LOSS: 4.876349489783278e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 43/71 | LOSS: 4.8828371125802566e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 44/71 | LOSS: 4.8637676425439875e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 45/71 | LOSS: 4.8588594045608975e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 46/71 | LOSS: 4.8809844806686325e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 47/71 | LOSS: 4.8924907218861335e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 48/71 | LOSS: 4.881146790522711e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 49/71 | LOSS: 4.896567106698057e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 50/71 | LOSS: 4.904819113161305e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 51/71 | LOSS: 4.8876922147218675e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 52/71 | LOSS: 4.92972079848427e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 53/71 | LOSS: 4.923826610239404e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 54/71 | LOSS: 4.9193239233731715e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 55/71 | LOSS: 4.914367683081114e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 56/71 | LOSS: 4.931568407316758e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 57/71 | LOSS: 4.943888794075284e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 58/71 | LOSS: 4.965275319730192e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 59/71 | LOSS: 4.945432340264233e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 60/71 | LOSS: 4.930034045352587e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 61/71 | LOSS: 4.933602561381591e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 62/71 | LOSS: 4.914127892276618e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 63/71 | LOSS: 4.895192148524075e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 64/71 | LOSS: 4.899809342672457e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 65/71 | LOSS: 4.872507719427989e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 66/71 | LOSS: 4.845487515623647e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 67/71 | LOSS: 4.847641568684433e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 68/71 | LOSS: 4.821770803771316e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 69/71 | LOSS: 4.821787095871904e-06\n",
      "TRAIN: EPOCH 627/1000 | BATCH 70/71 | LOSS: 4.813820390116004e-06\n",
      "VAL: EPOCH 627/1000 | BATCH 0/8 | LOSS: 6.366509296640288e-06\n",
      "VAL: EPOCH 627/1000 | BATCH 1/8 | LOSS: 6.434893521145568e-06\n",
      "VAL: EPOCH 627/1000 | BATCH 2/8 | LOSS: 6.63044875182095e-06\n",
      "VAL: EPOCH 627/1000 | BATCH 3/8 | LOSS: 6.598223990295082e-06\n",
      "VAL: EPOCH 627/1000 | BATCH 4/8 | LOSS: 6.659497194050345e-06\n",
      "VAL: EPOCH 627/1000 | BATCH 5/8 | LOSS: 6.476924227172276e-06\n",
      "VAL: EPOCH 627/1000 | BATCH 6/8 | LOSS: 6.326168854034872e-06\n",
      "VAL: EPOCH 627/1000 | BATCH 7/8 | LOSS: 6.102487247972022e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 0/71 | LOSS: 6.198562005010899e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 1/71 | LOSS: 4.745540650219482e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 2/71 | LOSS: 4.837667271810157e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 3/71 | LOSS: 4.491778270221403e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 4/71 | LOSS: 4.611072790794424e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 5/71 | LOSS: 4.716028456641652e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 6/71 | LOSS: 4.743082464691335e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 7/71 | LOSS: 4.768281456790646e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 8/71 | LOSS: 4.720468319242678e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 9/71 | LOSS: 4.539394603852998e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 10/71 | LOSS: 4.492525022214977e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 11/71 | LOSS: 4.5279029639762785e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 12/71 | LOSS: 4.483705407227927e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 13/71 | LOSS: 4.479558128878125e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 14/71 | LOSS: 4.455979978956748e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 15/71 | LOSS: 4.447892195003078e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 16/71 | LOSS: 4.409956652713899e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 17/71 | LOSS: 4.343997122507264e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 18/71 | LOSS: 4.322261474953848e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 19/71 | LOSS: 4.3464206669341365e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 20/71 | LOSS: 4.386099189581555e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 21/71 | LOSS: 4.3937333202848095e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 22/71 | LOSS: 4.421696273667571e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 23/71 | LOSS: 4.426662741252585e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 24/71 | LOSS: 4.470849771678331e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 25/71 | LOSS: 4.4503244593075715e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 26/71 | LOSS: 4.448392709727395e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 27/71 | LOSS: 4.4707459616282515e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 28/71 | LOSS: 4.441430495842661e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 29/71 | LOSS: 4.461689142469065e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 30/71 | LOSS: 4.443825035547977e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 31/71 | LOSS: 4.442356420497617e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 32/71 | LOSS: 4.438421738637826e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 33/71 | LOSS: 4.452749802861517e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 34/71 | LOSS: 4.4446298943512375e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 35/71 | LOSS: 4.4540246777715865e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 36/71 | LOSS: 4.452919827943677e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 37/71 | LOSS: 4.521476410589362e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 38/71 | LOSS: 4.519443832461361e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 39/71 | LOSS: 4.519856184970195e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 40/71 | LOSS: 4.532789367525455e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 41/71 | LOSS: 4.523351127058309e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 42/71 | LOSS: 4.5107181576472505e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 43/71 | LOSS: 4.525960998142347e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 44/71 | LOSS: 4.494278816006651e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 45/71 | LOSS: 4.472610187448204e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 46/71 | LOSS: 4.463027559548673e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 47/71 | LOSS: 4.473955693621671e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 48/71 | LOSS: 4.4714280072500455e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 49/71 | LOSS: 4.475920682125434e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 50/71 | LOSS: 4.472244810648251e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 51/71 | LOSS: 4.450590167310181e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 52/71 | LOSS: 4.462185381511655e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 53/71 | LOSS: 4.450794512599208e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 54/71 | LOSS: 4.430417919303926e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 55/71 | LOSS: 4.425677648863971e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 56/71 | LOSS: 4.415456076681598e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 57/71 | LOSS: 4.407292126102377e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 58/71 | LOSS: 4.416538204322471e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 59/71 | LOSS: 4.4148286406198166e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 60/71 | LOSS: 4.401404116013837e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 61/71 | LOSS: 4.3961063679929675e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 62/71 | LOSS: 4.386164330335016e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 63/71 | LOSS: 4.383548741770937e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 64/71 | LOSS: 4.3864417211107835e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 65/71 | LOSS: 4.3914445114117395e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 66/71 | LOSS: 4.38596347820165e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 67/71 | LOSS: 4.379647801964465e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 68/71 | LOSS: 4.39590705381293e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 69/71 | LOSS: 4.394154991262309e-06\n",
      "TRAIN: EPOCH 628/1000 | BATCH 70/71 | LOSS: 4.3985125532190965e-06\n",
      "VAL: EPOCH 628/1000 | BATCH 0/8 | LOSS: 3.6902815736539196e-06\n",
      "VAL: EPOCH 628/1000 | BATCH 1/8 | LOSS: 4.138936446906882e-06\n",
      "VAL: EPOCH 628/1000 | BATCH 2/8 | LOSS: 4.251356131135253e-06\n",
      "VAL: EPOCH 628/1000 | BATCH 3/8 | LOSS: 4.333964284342073e-06\n",
      "VAL: EPOCH 628/1000 | BATCH 4/8 | LOSS: 4.330544197728159e-06\n",
      "VAL: EPOCH 628/1000 | BATCH 5/8 | LOSS: 4.323940402173321e-06\n",
      "VAL: EPOCH 628/1000 | BATCH 6/8 | LOSS: 4.212793198478591e-06\n",
      "VAL: EPOCH 628/1000 | BATCH 7/8 | LOSS: 4.041825661715848e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 0/71 | LOSS: 3.657505885712453e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 1/71 | LOSS: 3.561913331395772e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 2/71 | LOSS: 3.817269392432839e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 3/71 | LOSS: 4.064975826167938e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 4/71 | LOSS: 4.08823611905973e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 5/71 | LOSS: 4.2853550515549914e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 6/71 | LOSS: 4.421421733370932e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 7/71 | LOSS: 4.351417061343454e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 8/71 | LOSS: 4.271174601752945e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 9/71 | LOSS: 4.35923000168259e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 10/71 | LOSS: 4.349027697323684e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 11/71 | LOSS: 4.402186296677731e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 12/71 | LOSS: 4.439171876723636e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 13/71 | LOSS: 4.457864877492414e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 14/71 | LOSS: 4.557337479127454e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 15/71 | LOSS: 4.586382672755462e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 16/71 | LOSS: 4.568363064048326e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 17/71 | LOSS: 4.532431312832665e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 18/71 | LOSS: 4.517611267529949e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 19/71 | LOSS: 4.5638613187293234e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 20/71 | LOSS: 4.583520757521564e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 21/71 | LOSS: 4.533735002289839e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 22/71 | LOSS: 4.495343714354575e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 23/71 | LOSS: 4.507069273055701e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 24/71 | LOSS: 4.503797617871896e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 25/71 | LOSS: 4.510205730394563e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 26/71 | LOSS: 4.522949060022559e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 27/71 | LOSS: 4.521882950874507e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 28/71 | LOSS: 4.522193343103121e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 29/71 | LOSS: 4.539665944018149e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 30/71 | LOSS: 4.552041365323314e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 31/71 | LOSS: 4.559722320607307e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 32/71 | LOSS: 4.549357864754821e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 33/71 | LOSS: 4.592224974979163e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 34/71 | LOSS: 4.551966432180571e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 35/71 | LOSS: 4.590739378171646e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 36/71 | LOSS: 4.597628251733998e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 37/71 | LOSS: 4.603896914695245e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 38/71 | LOSS: 4.627474137846431e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 39/71 | LOSS: 4.645660288815634e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 40/71 | LOSS: 4.699233885531146e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 41/71 | LOSS: 4.7187515054333965e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 42/71 | LOSS: 4.821430112251831e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 43/71 | LOSS: 4.87060724584477e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 44/71 | LOSS: 4.901833507473283e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 45/71 | LOSS: 4.90651200874234e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 46/71 | LOSS: 4.977168194447122e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 47/71 | LOSS: 4.984135619186721e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 48/71 | LOSS: 5.003713734980437e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 49/71 | LOSS: 5.00588241720834e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 50/71 | LOSS: 5.09306868497886e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 51/71 | LOSS: 5.073853628800862e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 52/71 | LOSS: 5.142573314206291e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 53/71 | LOSS: 5.162128643489807e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 54/71 | LOSS: 5.169631735539455e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 55/71 | LOSS: 5.190294347455295e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 56/71 | LOSS: 5.200700937629128e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 57/71 | LOSS: 5.177399202693778e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 58/71 | LOSS: 5.148571999289758e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 59/71 | LOSS: 5.189224119324839e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 60/71 | LOSS: 5.1890467043673106e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 61/71 | LOSS: 5.203689881689629e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 62/71 | LOSS: 5.193503757110438e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 63/71 | LOSS: 5.176393493400155e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 64/71 | LOSS: 5.173735434464806e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 65/71 | LOSS: 5.144425744440399e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 66/71 | LOSS: 5.165784692345274e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 67/71 | LOSS: 5.151968206091089e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 68/71 | LOSS: 5.140830356707e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 69/71 | LOSS: 5.161316520181052e-06\n",
      "TRAIN: EPOCH 629/1000 | BATCH 70/71 | LOSS: 5.156245286241141e-06\n",
      "VAL: EPOCH 629/1000 | BATCH 0/8 | LOSS: 3.6666967844212195e-06\n",
      "VAL: EPOCH 629/1000 | BATCH 1/8 | LOSS: 4.064846393703192e-06\n",
      "VAL: EPOCH 629/1000 | BATCH 2/8 | LOSS: 4.1424308771335445e-06\n",
      "VAL: EPOCH 629/1000 | BATCH 3/8 | LOSS: 4.208099369407137e-06\n",
      "VAL: EPOCH 629/1000 | BATCH 4/8 | LOSS: 4.182589555057347e-06\n",
      "VAL: EPOCH 629/1000 | BATCH 5/8 | LOSS: 4.2463321202982724e-06\n",
      "VAL: EPOCH 629/1000 | BATCH 6/8 | LOSS: 4.119166078453418e-06\n",
      "VAL: EPOCH 629/1000 | BATCH 7/8 | LOSS: 3.977675845590056e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 0/71 | LOSS: 4.9773152568377554e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 1/71 | LOSS: 4.543465365713928e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 2/71 | LOSS: 4.44247416453436e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 3/71 | LOSS: 4.50005995844549e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 4/71 | LOSS: 4.696911491919309e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 5/71 | LOSS: 4.797680579334458e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 6/71 | LOSS: 4.8259118037614305e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 7/71 | LOSS: 5.26010853718617e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 8/71 | LOSS: 5.1196332909360836e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 9/71 | LOSS: 5.1958392759843265e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 10/71 | LOSS: 5.210134986555204e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 11/71 | LOSS: 5.244850323530652e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 12/71 | LOSS: 5.206310211528594e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 13/71 | LOSS: 5.201718230247414e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 14/71 | LOSS: 5.346250964066712e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 15/71 | LOSS: 5.350862437580872e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 16/71 | LOSS: 5.487270407363266e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 17/71 | LOSS: 5.4806500379022e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 18/71 | LOSS: 5.4571703099339315e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 19/71 | LOSS: 5.371411612031807e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 20/71 | LOSS: 5.330302688629932e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 21/71 | LOSS: 5.26602343597915e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 22/71 | LOSS: 5.275408254775634e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 23/71 | LOSS: 5.205982034794943e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 24/71 | LOSS: 5.183050843697856e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 25/71 | LOSS: 5.133740361228634e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 26/71 | LOSS: 5.136621767734242e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 27/71 | LOSS: 5.158904928391296e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 28/71 | LOSS: 5.172410110703665e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 29/71 | LOSS: 5.127448753228237e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 30/71 | LOSS: 5.080184351872079e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 31/71 | LOSS: 5.060057382877403e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 32/71 | LOSS: 5.014594445597278e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 33/71 | LOSS: 4.977920017314746e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 34/71 | LOSS: 4.92267024258451e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 35/71 | LOSS: 4.900088747995647e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 36/71 | LOSS: 4.870123219105251e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 37/71 | LOSS: 4.854973906874779e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 38/71 | LOSS: 4.828873874756913e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 39/71 | LOSS: 4.810524524145876e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 40/71 | LOSS: 4.792243888049398e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 41/71 | LOSS: 4.782881487266249e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 42/71 | LOSS: 4.7476983973167584e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 43/71 | LOSS: 4.748361644405205e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 44/71 | LOSS: 4.7187079821419e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 45/71 | LOSS: 4.679261302914275e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 46/71 | LOSS: 4.681651876609898e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 47/71 | LOSS: 4.688580934460636e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 48/71 | LOSS: 4.691212459837683e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 49/71 | LOSS: 4.6764479748162556e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 50/71 | LOSS: 4.666776195496341e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 51/71 | LOSS: 4.66259780733498e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 52/71 | LOSS: 4.656995946239682e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 53/71 | LOSS: 4.65276030825879e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 54/71 | LOSS: 4.6509961851949235e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 55/71 | LOSS: 4.660539360656912e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 56/71 | LOSS: 4.649320827778419e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 57/71 | LOSS: 4.627062696335056e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 58/71 | LOSS: 4.619505724326315e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 59/71 | LOSS: 4.604039637949123e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 60/71 | LOSS: 4.594745465122988e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 61/71 | LOSS: 4.594378725091848e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 62/71 | LOSS: 4.61517701628581e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 63/71 | LOSS: 4.60640910660004e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 64/71 | LOSS: 4.60855924477353e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 65/71 | LOSS: 4.62216209570849e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 66/71 | LOSS: 4.628492905482487e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 67/71 | LOSS: 4.620145860452099e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 68/71 | LOSS: 4.622244043697509e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 69/71 | LOSS: 4.636300894032632e-06\n",
      "TRAIN: EPOCH 630/1000 | BATCH 70/71 | LOSS: 4.6176356514018116e-06\n",
      "VAL: EPOCH 630/1000 | BATCH 0/8 | LOSS: 3.973507773480378e-06\n",
      "VAL: EPOCH 630/1000 | BATCH 1/8 | LOSS: 4.371323029772611e-06\n",
      "VAL: EPOCH 630/1000 | BATCH 2/8 | LOSS: 4.703914783021901e-06\n",
      "VAL: EPOCH 630/1000 | BATCH 3/8 | LOSS: 4.768915232489235e-06\n",
      "VAL: EPOCH 630/1000 | BATCH 4/8 | LOSS: 4.810797508980613e-06\n",
      "VAL: EPOCH 630/1000 | BATCH 5/8 | LOSS: 4.875832170606979e-06\n",
      "VAL: EPOCH 630/1000 | BATCH 6/8 | LOSS: 4.769650851293201e-06\n",
      "VAL: EPOCH 630/1000 | BATCH 7/8 | LOSS: 4.659830779019103e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 0/71 | LOSS: 4.7006233216961846e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 1/71 | LOSS: 4.562238700600574e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 2/71 | LOSS: 4.3230326506697265e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 3/71 | LOSS: 4.207355686958181e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 4/71 | LOSS: 4.2084996493940706e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 5/71 | LOSS: 4.4090231009856024e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 6/71 | LOSS: 4.406998024413562e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 7/71 | LOSS: 4.382392603474727e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 8/71 | LOSS: 4.579118972792963e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 9/71 | LOSS: 4.607967775882571e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 10/71 | LOSS: 4.609530812806704e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 11/71 | LOSS: 4.630816950642232e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 12/71 | LOSS: 4.631856007528348e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 13/71 | LOSS: 4.59907057250218e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 14/71 | LOSS: 4.5952597550543334e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 15/71 | LOSS: 4.687017394644499e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 16/71 | LOSS: 4.707790969191324e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 17/71 | LOSS: 4.694783380424876e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 18/71 | LOSS: 4.8029629923755575e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 19/71 | LOSS: 4.759759644912265e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 20/71 | LOSS: 4.731805888912363e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 21/71 | LOSS: 4.7459222762633795e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 22/71 | LOSS: 4.802051001246108e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 23/71 | LOSS: 4.8453149474880775e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 24/71 | LOSS: 4.8004697418946305e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 25/71 | LOSS: 4.908211272217718e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 26/71 | LOSS: 4.914952670434205e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 27/71 | LOSS: 4.887430528859633e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 28/71 | LOSS: 4.834023151300338e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 29/71 | LOSS: 4.844196147738936e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 30/71 | LOSS: 4.809959972912873e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 31/71 | LOSS: 4.771276209680764e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 32/71 | LOSS: 4.777999837449787e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 33/71 | LOSS: 4.739076570438235e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 34/71 | LOSS: 4.6979804275386935e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 35/71 | LOSS: 4.66727950273101e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 36/71 | LOSS: 4.660810068221386e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 37/71 | LOSS: 4.657366293701527e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 38/71 | LOSS: 4.659172366071009e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 39/71 | LOSS: 4.674066281040723e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 40/71 | LOSS: 4.667934551501431e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 41/71 | LOSS: 4.64878119254579e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 42/71 | LOSS: 4.634045411833984e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 43/71 | LOSS: 4.624109868183785e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 44/71 | LOSS: 4.61098283671567e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 45/71 | LOSS: 4.599855698040261e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 46/71 | LOSS: 4.609646914994958e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 47/71 | LOSS: 4.6142718967227365e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 48/71 | LOSS: 4.622339419052433e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 49/71 | LOSS: 4.613321907527279e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 50/71 | LOSS: 4.651853950966981e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 51/71 | LOSS: 4.644065046914665e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 52/71 | LOSS: 4.6567931246311176e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 53/71 | LOSS: 4.644738375511089e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 54/71 | LOSS: 4.639671748340003e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 55/71 | LOSS: 4.645255744760236e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 56/71 | LOSS: 4.648810405789143e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 57/71 | LOSS: 4.6349759706008e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 58/71 | LOSS: 4.628716242685618e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 59/71 | LOSS: 4.6647776950218635e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 60/71 | LOSS: 4.66183759957947e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 61/71 | LOSS: 4.728333916289323e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 62/71 | LOSS: 4.728200608605264e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 63/71 | LOSS: 4.719882575443535e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 64/71 | LOSS: 4.699597429162867e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 65/71 | LOSS: 4.702884301409402e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 66/71 | LOSS: 4.693256153301779e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 67/71 | LOSS: 4.678318208227531e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 68/71 | LOSS: 4.660669746690605e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 69/71 | LOSS: 4.6486187524611265e-06\n",
      "TRAIN: EPOCH 631/1000 | BATCH 70/71 | LOSS: 4.6543726659339725e-06\n",
      "VAL: EPOCH 631/1000 | BATCH 0/8 | LOSS: 3.5593529901234433e-06\n",
      "VAL: EPOCH 631/1000 | BATCH 1/8 | LOSS: 3.895899453709717e-06\n",
      "VAL: EPOCH 631/1000 | BATCH 2/8 | LOSS: 4.049813924211776e-06\n",
      "VAL: EPOCH 631/1000 | BATCH 3/8 | LOSS: 4.055870135744044e-06\n",
      "VAL: EPOCH 631/1000 | BATCH 4/8 | LOSS: 4.087359957338776e-06\n",
      "VAL: EPOCH 631/1000 | BATCH 5/8 | LOSS: 4.125238092456129e-06\n",
      "VAL: EPOCH 631/1000 | BATCH 6/8 | LOSS: 4.027849107452701e-06\n",
      "VAL: EPOCH 631/1000 | BATCH 7/8 | LOSS: 3.905527563574651e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 0/71 | LOSS: 3.5095488328806823e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 1/71 | LOSS: 4.145117031839618e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 2/71 | LOSS: 3.9113613183872076e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 3/71 | LOSS: 3.865273129122215e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 4/71 | LOSS: 3.809618056038744e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 5/71 | LOSS: 3.7848134297746583e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 6/71 | LOSS: 3.8110341133038413e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 7/71 | LOSS: 3.8295664523957385e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 8/71 | LOSS: 3.8579426018259255e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 9/71 | LOSS: 3.86296803753794e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 10/71 | LOSS: 3.836082813987477e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 11/71 | LOSS: 3.848855556043418e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 12/71 | LOSS: 3.849912778045668e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 13/71 | LOSS: 3.862651559367285e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 14/71 | LOSS: 3.847435664283694e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 15/71 | LOSS: 3.880904515085604e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 16/71 | LOSS: 3.876653796824565e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 17/71 | LOSS: 3.9331978693250376e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 18/71 | LOSS: 3.906145653481393e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 19/71 | LOSS: 3.932430365694017e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 20/71 | LOSS: 3.93482801406381e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 21/71 | LOSS: 3.943730348510144e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 22/71 | LOSS: 3.956653027552934e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 23/71 | LOSS: 3.931950828928166e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 24/71 | LOSS: 3.9657795514358444e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 25/71 | LOSS: 4.013498579087564e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 26/71 | LOSS: 4.040683650061881e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 27/71 | LOSS: 4.04902564404048e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 28/71 | LOSS: 4.0410054445036905e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 29/71 | LOSS: 4.115868758465998e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 30/71 | LOSS: 4.1533307995128015e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 31/71 | LOSS: 4.16177748974178e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 32/71 | LOSS: 4.16488331101198e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 33/71 | LOSS: 4.2091932340661195e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 34/71 | LOSS: 4.205439518045751e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 35/71 | LOSS: 4.199713285086344e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 36/71 | LOSS: 4.223242132030538e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 37/71 | LOSS: 4.246614701897718e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 38/71 | LOSS: 4.2239247390031005e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 39/71 | LOSS: 4.212461385577626e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 40/71 | LOSS: 4.2326472040014515e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 41/71 | LOSS: 4.22291176012853e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 42/71 | LOSS: 4.235376245771984e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 43/71 | LOSS: 4.241711487586806e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 44/71 | LOSS: 4.240582580273945e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 45/71 | LOSS: 4.249629979391972e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 46/71 | LOSS: 4.270117439405789e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 47/71 | LOSS: 4.267324423305278e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 48/71 | LOSS: 4.28318102125076e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 49/71 | LOSS: 4.273721256140561e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 50/71 | LOSS: 4.276714007038904e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 51/71 | LOSS: 4.265690646521902e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 52/71 | LOSS: 4.260540547549235e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 53/71 | LOSS: 4.255937832803519e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 54/71 | LOSS: 4.272167598173837e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 55/71 | LOSS: 4.262846159365706e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 56/71 | LOSS: 4.259069164777828e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 57/71 | LOSS: 4.251241597526602e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 58/71 | LOSS: 4.252193174564173e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 59/71 | LOSS: 4.257369732840744e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 60/71 | LOSS: 4.273154929966845e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 61/71 | LOSS: 4.283789915080565e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 62/71 | LOSS: 4.291462349982115e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 63/71 | LOSS: 4.294768807255878e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 64/71 | LOSS: 4.303072245630364e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 65/71 | LOSS: 4.3142091326995855e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 66/71 | LOSS: 4.3272700453083265e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 67/71 | LOSS: 4.314148207092384e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 68/71 | LOSS: 4.306979719884744e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 69/71 | LOSS: 4.310305816293943e-06\n",
      "TRAIN: EPOCH 632/1000 | BATCH 70/71 | LOSS: 4.313142282216446e-06\n",
      "VAL: EPOCH 632/1000 | BATCH 0/8 | LOSS: 5.384566975408234e-06\n",
      "VAL: EPOCH 632/1000 | BATCH 1/8 | LOSS: 5.865443881702959e-06\n",
      "VAL: EPOCH 632/1000 | BATCH 2/8 | LOSS: 6.418372019349287e-06\n",
      "VAL: EPOCH 632/1000 | BATCH 3/8 | LOSS: 6.313213816611096e-06\n",
      "VAL: EPOCH 632/1000 | BATCH 4/8 | LOSS: 6.450402179325465e-06\n",
      "VAL: EPOCH 632/1000 | BATCH 5/8 | LOSS: 6.548825619271763e-06\n",
      "VAL: EPOCH 632/1000 | BATCH 6/8 | LOSS: 6.530413884320296e-06\n",
      "VAL: EPOCH 632/1000 | BATCH 7/8 | LOSS: 6.583535991921963e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 0/71 | LOSS: 6.610772288695443e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 1/71 | LOSS: 5.4684453516529175e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 2/71 | LOSS: 4.820450764479271e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 3/71 | LOSS: 4.465423501187615e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 4/71 | LOSS: 4.471185320653603e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 5/71 | LOSS: 4.46205585073282e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 6/71 | LOSS: 4.506524257002249e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 7/71 | LOSS: 4.503938129118978e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 8/71 | LOSS: 4.5614273555353675e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 9/71 | LOSS: 4.498326029533928e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 10/71 | LOSS: 4.485153257602096e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 11/71 | LOSS: 4.507027957364092e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 12/71 | LOSS: 4.496264392191034e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 13/71 | LOSS: 4.558272670302748e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 14/71 | LOSS: 4.57699805641217e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 15/71 | LOSS: 4.6143481569060896e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 16/71 | LOSS: 4.598121356192517e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 17/71 | LOSS: 4.649930019695603e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 18/71 | LOSS: 4.634783839432394e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 19/71 | LOSS: 4.613462795077794e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 20/71 | LOSS: 4.611879863399995e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 21/71 | LOSS: 4.579338885626137e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 22/71 | LOSS: 4.521156558004567e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 23/71 | LOSS: 4.538301662175097e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 24/71 | LOSS: 4.503450518313912e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 25/71 | LOSS: 4.4575270889114245e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 26/71 | LOSS: 4.452353213882479e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 27/71 | LOSS: 4.413779363078772e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 28/71 | LOSS: 4.365131895485382e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 29/71 | LOSS: 4.3580820298908895e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 30/71 | LOSS: 4.355023491136249e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 31/71 | LOSS: 4.342089553688311e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 32/71 | LOSS: 4.355659298611996e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 33/71 | LOSS: 4.331328508509186e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 34/71 | LOSS: 4.352237946087761e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 35/71 | LOSS: 4.385303580218331e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 36/71 | LOSS: 4.410142144086028e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 37/71 | LOSS: 4.424332796828055e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 38/71 | LOSS: 4.488839259955477e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 39/71 | LOSS: 4.4980371399105936e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 40/71 | LOSS: 4.542826811702093e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 41/71 | LOSS: 4.577318620812591e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 42/71 | LOSS: 4.5857025534767375e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 43/71 | LOSS: 4.58270886321605e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 44/71 | LOSS: 4.556281449670981e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 45/71 | LOSS: 4.546101971082743e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 46/71 | LOSS: 4.5558878055283245e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 47/71 | LOSS: 4.54957029205616e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 48/71 | LOSS: 4.541112651409373e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 49/71 | LOSS: 4.547702337731607e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 50/71 | LOSS: 4.5513074391968915e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 51/71 | LOSS: 4.5453909583906916e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 52/71 | LOSS: 4.564639020141188e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 53/71 | LOSS: 4.597723141665079e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 54/71 | LOSS: 4.601322391615461e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 55/71 | LOSS: 4.588252376639243e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 56/71 | LOSS: 4.582601018467501e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 57/71 | LOSS: 4.5678713620183205e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 58/71 | LOSS: 4.567196614423942e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 59/71 | LOSS: 4.546550511956108e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 60/71 | LOSS: 4.547926366404815e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 61/71 | LOSS: 4.521841791786081e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 62/71 | LOSS: 4.52901008790673e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 63/71 | LOSS: 4.51788417166199e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 64/71 | LOSS: 4.523375789106537e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 65/71 | LOSS: 4.518836125167912e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 66/71 | LOSS: 4.501636095795478e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 67/71 | LOSS: 4.503680462916878e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 68/71 | LOSS: 4.508332112627745e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 69/71 | LOSS: 4.5130248573903895e-06\n",
      "TRAIN: EPOCH 633/1000 | BATCH 70/71 | LOSS: 4.490500714382506e-06\n",
      "VAL: EPOCH 633/1000 | BATCH 0/8 | LOSS: 4.622401320375502e-06\n",
      "VAL: EPOCH 633/1000 | BATCH 1/8 | LOSS: 5.029166686654207e-06\n",
      "VAL: EPOCH 633/1000 | BATCH 2/8 | LOSS: 5.267333108349703e-06\n",
      "VAL: EPOCH 633/1000 | BATCH 3/8 | LOSS: 5.135370201969636e-06\n",
      "VAL: EPOCH 633/1000 | BATCH 4/8 | LOSS: 5.313448218657868e-06\n",
      "VAL: EPOCH 633/1000 | BATCH 5/8 | LOSS: 5.317569578740707e-06\n",
      "VAL: EPOCH 633/1000 | BATCH 6/8 | LOSS: 5.248523523602801e-06\n",
      "VAL: EPOCH 633/1000 | BATCH 7/8 | LOSS: 5.218124613293185e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 0/71 | LOSS: 6.002940153848613e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 1/71 | LOSS: 5.133601234774687e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 2/71 | LOSS: 4.958422474980277e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 3/71 | LOSS: 4.734674234896374e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 4/71 | LOSS: 4.5374606997938825e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 5/71 | LOSS: 4.586106570059201e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 6/71 | LOSS: 4.43824576125605e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 7/71 | LOSS: 4.429842107356308e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 8/71 | LOSS: 4.405571897021016e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 9/71 | LOSS: 4.536061146609427e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 10/71 | LOSS: 4.621018785077137e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 11/71 | LOSS: 4.5957808083585405e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 12/71 | LOSS: 4.7159541316852064e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 13/71 | LOSS: 4.685112685365311e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 14/71 | LOSS: 4.850148692033448e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 15/71 | LOSS: 4.780815530125437e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 16/71 | LOSS: 4.800005987871212e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 17/71 | LOSS: 4.884730957302155e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 18/71 | LOSS: 4.9237220728651954e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 19/71 | LOSS: 5.084102110686217e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 20/71 | LOSS: 5.123798549296528e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 21/71 | LOSS: 5.202096283276412e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 22/71 | LOSS: 5.1640809725422115e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 23/71 | LOSS: 5.194492842974796e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 24/71 | LOSS: 5.1648717544594545e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 25/71 | LOSS: 5.142444470388559e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 26/71 | LOSS: 5.159826548185299e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 27/71 | LOSS: 5.084346517898146e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 28/71 | LOSS: 5.007584518509624e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 29/71 | LOSS: 4.971038780846963e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 30/71 | LOSS: 4.937452844857152e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 31/71 | LOSS: 4.892367208242376e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 32/71 | LOSS: 4.848180451793296e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 33/71 | LOSS: 4.8559364011132e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 34/71 | LOSS: 4.81801612295385e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 35/71 | LOSS: 4.794893969675387e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 36/71 | LOSS: 4.784151862032878e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 37/71 | LOSS: 4.7366529064387766e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 38/71 | LOSS: 4.724503877439883e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 39/71 | LOSS: 4.706717913904868e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 40/71 | LOSS: 4.69505724739553e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 41/71 | LOSS: 4.683279171114832e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 42/71 | LOSS: 4.673532150136876e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 43/71 | LOSS: 4.679224471973694e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 44/71 | LOSS: 4.67076662163082e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 45/71 | LOSS: 4.6466083918146426e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 46/71 | LOSS: 4.645896355059255e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 47/71 | LOSS: 4.631086540030083e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 48/71 | LOSS: 4.607751365330507e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 49/71 | LOSS: 4.5860062436986485e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 50/71 | LOSS: 4.588090713000651e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 51/71 | LOSS: 4.5898664736271785e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 52/71 | LOSS: 4.571745524306195e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 53/71 | LOSS: 4.590204024747503e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 54/71 | LOSS: 4.577903130418483e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 55/71 | LOSS: 4.574645568514565e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 56/71 | LOSS: 4.568821188449715e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 57/71 | LOSS: 4.562831177595242e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 58/71 | LOSS: 4.547796252308664e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 59/71 | LOSS: 4.538751034033339e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 60/71 | LOSS: 4.530344551581113e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 61/71 | LOSS: 4.528379478078052e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 62/71 | LOSS: 4.517139539905382e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 63/71 | LOSS: 4.518118846164043e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 64/71 | LOSS: 4.51703524329507e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 65/71 | LOSS: 4.514554620293369e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 66/71 | LOSS: 4.49912312799811e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 67/71 | LOSS: 4.499327908835018e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 68/71 | LOSS: 4.509666855417469e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 69/71 | LOSS: 4.5527931563680926e-06\n",
      "TRAIN: EPOCH 634/1000 | BATCH 70/71 | LOSS: 4.568913174782056e-06\n",
      "VAL: EPOCH 634/1000 | BATCH 0/8 | LOSS: 7.3653818617458455e-06\n",
      "VAL: EPOCH 634/1000 | BATCH 1/8 | LOSS: 7.791305051796371e-06\n",
      "VAL: EPOCH 634/1000 | BATCH 2/8 | LOSS: 7.794348675815854e-06\n",
      "VAL: EPOCH 634/1000 | BATCH 3/8 | LOSS: 7.714151479376596e-06\n",
      "VAL: EPOCH 634/1000 | BATCH 4/8 | LOSS: 7.701938920945395e-06\n",
      "VAL: EPOCH 634/1000 | BATCH 5/8 | LOSS: 7.48551080202257e-06\n",
      "VAL: EPOCH 634/1000 | BATCH 6/8 | LOSS: 7.323055407531294e-06\n",
      "VAL: EPOCH 634/1000 | BATCH 7/8 | LOSS: 7.0834398115948716e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 0/71 | LOSS: 7.158891094150022e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 1/71 | LOSS: 6.650862815149594e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 2/71 | LOSS: 6.3741590565769e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 3/71 | LOSS: 6.149290129542351e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 4/71 | LOSS: 5.63749722459761e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 5/71 | LOSS: 5.936762249802996e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 6/71 | LOSS: 5.8023780898760635e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 7/71 | LOSS: 5.675031189866786e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 8/71 | LOSS: 5.652634652303985e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 9/71 | LOSS: 5.5387290331054825e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 10/71 | LOSS: 5.435527555164299e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 11/71 | LOSS: 5.287877608376827e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 12/71 | LOSS: 5.161702959482612e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 13/71 | LOSS: 5.056127650147703e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 14/71 | LOSS: 4.96477215771544e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 15/71 | LOSS: 4.960463328984588e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 16/71 | LOSS: 4.848309866728789e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 17/71 | LOSS: 4.8365841267796996e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 18/71 | LOSS: 4.736590043601034e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 19/71 | LOSS: 4.685022190642485e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 20/71 | LOSS: 4.586019085330745e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 21/71 | LOSS: 4.561756709749303e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 22/71 | LOSS: 4.541525611231286e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 23/71 | LOSS: 4.519443621120445e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 24/71 | LOSS: 4.4968693873670415e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 25/71 | LOSS: 4.464191673692907e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 26/71 | LOSS: 4.454662033552907e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 27/71 | LOSS: 4.4569298032521535e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 28/71 | LOSS: 4.46617863856553e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 29/71 | LOSS: 4.462657155575774e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 30/71 | LOSS: 4.502594610159811e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 31/71 | LOSS: 4.486629819666632e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 32/71 | LOSS: 4.496431729145997e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 33/71 | LOSS: 4.503014960231239e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 34/71 | LOSS: 4.472500222618692e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 35/71 | LOSS: 4.477250437654827e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 36/71 | LOSS: 4.465456481124715e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 37/71 | LOSS: 4.4843160666412014e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 38/71 | LOSS: 4.477203686129099e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 39/71 | LOSS: 4.4594712903744945e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 40/71 | LOSS: 4.465122479237701e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 41/71 | LOSS: 4.43979023484211e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 42/71 | LOSS: 4.423674163474569e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 43/71 | LOSS: 4.412040926705561e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 44/71 | LOSS: 4.410549116881965e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 45/71 | LOSS: 4.39237553366737e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 46/71 | LOSS: 4.37648059039233e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 47/71 | LOSS: 4.390124473729884e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 48/71 | LOSS: 4.4462740333983675e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 49/71 | LOSS: 4.422127008183452e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 50/71 | LOSS: 4.457982747814694e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 51/71 | LOSS: 4.448397235661944e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 52/71 | LOSS: 4.461926338544285e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 53/71 | LOSS: 4.481169345059176e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 54/71 | LOSS: 4.525603985298816e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 55/71 | LOSS: 4.543088822726661e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 56/71 | LOSS: 4.550094397353333e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 57/71 | LOSS: 4.577120962337709e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 58/71 | LOSS: 4.5688397245088465e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 59/71 | LOSS: 4.589648729809899e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 60/71 | LOSS: 4.631055702941281e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 61/71 | LOSS: 4.670061369530055e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 62/71 | LOSS: 4.692596370486648e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 63/71 | LOSS: 4.684571852209274e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 64/71 | LOSS: 4.738892623488657e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 65/71 | LOSS: 4.713817003049013e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 66/71 | LOSS: 4.757626757326314e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 67/71 | LOSS: 4.76282728895432e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 68/71 | LOSS: 4.7933823436218574e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 69/71 | LOSS: 4.8082772341980935e-06\n",
      "TRAIN: EPOCH 635/1000 | BATCH 70/71 | LOSS: 4.795580185651796e-06\n",
      "VAL: EPOCH 635/1000 | BATCH 0/8 | LOSS: 7.541208105976693e-06\n",
      "VAL: EPOCH 635/1000 | BATCH 1/8 | LOSS: 7.75929356677807e-06\n",
      "VAL: EPOCH 635/1000 | BATCH 2/8 | LOSS: 7.849242138036061e-06\n",
      "VAL: EPOCH 635/1000 | BATCH 3/8 | LOSS: 7.832126129869721e-06\n",
      "VAL: EPOCH 635/1000 | BATCH 4/8 | LOSS: 7.755935439490713e-06\n",
      "VAL: EPOCH 635/1000 | BATCH 5/8 | LOSS: 7.572706257027069e-06\n",
      "VAL: EPOCH 635/1000 | BATCH 6/8 | LOSS: 7.43428381611011e-06\n",
      "VAL: EPOCH 635/1000 | BATCH 7/8 | LOSS: 7.2243780095959664e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 0/71 | LOSS: 8.265146789199207e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 1/71 | LOSS: 6.404342457244638e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 2/71 | LOSS: 6.001192256614256e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 3/71 | LOSS: 5.427583801065339e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 4/71 | LOSS: 6.064045192033518e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 5/71 | LOSS: 5.6732330904196715e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 6/71 | LOSS: 5.630861258915891e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 7/71 | LOSS: 5.687081397809379e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 8/71 | LOSS: 5.502941955152589e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 9/71 | LOSS: 5.643993290505023e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 10/71 | LOSS: 5.492518580881138e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 11/71 | LOSS: 5.5113576233149315e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 12/71 | LOSS: 5.485046096090138e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 13/71 | LOSS: 5.452884189643166e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 14/71 | LOSS: 5.3761785238748415e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 15/71 | LOSS: 5.3552660403966e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 16/71 | LOSS: 5.3383042969097215e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 17/71 | LOSS: 5.228457009959837e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 18/71 | LOSS: 5.212622119487557e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 19/71 | LOSS: 5.126014889356156e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 20/71 | LOSS: 5.066316740946301e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 21/71 | LOSS: 5.007473553384691e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 22/71 | LOSS: 4.9874950629758965e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 23/71 | LOSS: 4.937074379540718e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 24/71 | LOSS: 4.901816646452062e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 25/71 | LOSS: 4.877602385442095e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 26/71 | LOSS: 4.902593024814484e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 27/71 | LOSS: 4.850736672779021e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 28/71 | LOSS: 4.838069971497291e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 29/71 | LOSS: 4.7955132383018885e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 30/71 | LOSS: 4.741812701115944e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 31/71 | LOSS: 4.735602836092312e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 32/71 | LOSS: 4.728761330790638e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 33/71 | LOSS: 4.751157181250592e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 34/71 | LOSS: 4.7309043306345955e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 35/71 | LOSS: 4.722124856471055e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 36/71 | LOSS: 4.712341810650164e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 37/71 | LOSS: 4.685588889610938e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 38/71 | LOSS: 4.721525882791283e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 39/71 | LOSS: 4.682374361664188e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 40/71 | LOSS: 4.6690340585017825e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 41/71 | LOSS: 4.659938348422224e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 42/71 | LOSS: 4.668077683036668e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 43/71 | LOSS: 4.660110639840489e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 44/71 | LOSS: 4.652122450150071e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 45/71 | LOSS: 4.646934302320955e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 46/71 | LOSS: 4.6279798173967955e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 47/71 | LOSS: 4.610488114546267e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 48/71 | LOSS: 4.609903007061506e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 49/71 | LOSS: 4.61131293377548e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 50/71 | LOSS: 4.58564165776753e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 51/71 | LOSS: 4.611437064410823e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 52/71 | LOSS: 4.606486604557777e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 53/71 | LOSS: 4.58917139877923e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 54/71 | LOSS: 4.609090675247981e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 55/71 | LOSS: 4.617620390945376e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 56/71 | LOSS: 4.614990238973916e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 57/71 | LOSS: 4.613868041219418e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 58/71 | LOSS: 4.6526244844599055e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 59/71 | LOSS: 4.653618119239885e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 60/71 | LOSS: 4.7076114018726335e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 61/71 | LOSS: 4.6855237402973395e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 62/71 | LOSS: 4.668883699802488e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 63/71 | LOSS: 4.690368424320468e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 64/71 | LOSS: 4.69363334769486e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 65/71 | LOSS: 4.692494764474924e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 66/71 | LOSS: 4.685585461412478e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 67/71 | LOSS: 4.68633938306298e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 68/71 | LOSS: 4.6934640924545406e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 69/71 | LOSS: 4.696900717655288e-06\n",
      "TRAIN: EPOCH 636/1000 | BATCH 70/71 | LOSS: 4.683937562167813e-06\n",
      "VAL: EPOCH 636/1000 | BATCH 0/8 | LOSS: 4.116076524951495e-06\n",
      "VAL: EPOCH 636/1000 | BATCH 1/8 | LOSS: 4.579459528031293e-06\n",
      "VAL: EPOCH 636/1000 | BATCH 2/8 | LOSS: 4.8924319647388375e-06\n",
      "VAL: EPOCH 636/1000 | BATCH 3/8 | LOSS: 4.872122985943861e-06\n",
      "VAL: EPOCH 636/1000 | BATCH 4/8 | LOSS: 4.950305810780264e-06\n",
      "VAL: EPOCH 636/1000 | BATCH 5/8 | LOSS: 5.033784646002459e-06\n",
      "VAL: EPOCH 636/1000 | BATCH 6/8 | LOSS: 4.9423126386370446e-06\n",
      "VAL: EPOCH 636/1000 | BATCH 7/8 | LOSS: 4.932678393743117e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 0/71 | LOSS: 4.393235030875076e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 1/71 | LOSS: 4.402706508699339e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 2/71 | LOSS: 4.20340529672103e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 3/71 | LOSS: 4.1973132738348795e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 4/71 | LOSS: 4.2313906305935236e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 5/71 | LOSS: 4.131271642412078e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 6/71 | LOSS: 4.369995199989976e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 7/71 | LOSS: 4.167382172681755e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 8/71 | LOSS: 4.482871822296551e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 9/71 | LOSS: 4.3844021547556625e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 10/71 | LOSS: 4.452654676159992e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 11/71 | LOSS: 4.370503669785346e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 12/71 | LOSS: 4.408897596546852e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 13/71 | LOSS: 4.428246110624709e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 14/71 | LOSS: 4.417736408868222e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 15/71 | LOSS: 4.3946785837079005e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 16/71 | LOSS: 4.3440548538723415e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 17/71 | LOSS: 4.392426970096292e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 18/71 | LOSS: 4.341316956637648e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 19/71 | LOSS: 4.415336729834962e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 20/71 | LOSS: 4.367181147591466e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 21/71 | LOSS: 4.483887799076249e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 22/71 | LOSS: 4.483760676521342e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 23/71 | LOSS: 4.4468953793360315e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 24/71 | LOSS: 4.472438895390951e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 25/71 | LOSS: 4.495873055860172e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 26/71 | LOSS: 4.504518756220932e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 27/71 | LOSS: 4.499688616631049e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 28/71 | LOSS: 4.524545578658882e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 29/71 | LOSS: 4.4659208318383514e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 30/71 | LOSS: 4.5124097596271895e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 31/71 | LOSS: 4.504937109572893e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 32/71 | LOSS: 4.510514262426913e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 33/71 | LOSS: 4.4788349401535335e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 34/71 | LOSS: 4.480144947852491e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 35/71 | LOSS: 4.472587906750252e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 36/71 | LOSS: 4.463092636797228e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 37/71 | LOSS: 4.450031760346687e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 38/71 | LOSS: 4.434815595325348e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 39/71 | LOSS: 4.4107111705216084e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 40/71 | LOSS: 4.405170546128719e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 41/71 | LOSS: 4.411069664369972e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 42/71 | LOSS: 4.387735670693368e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 43/71 | LOSS: 4.409516314105779e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 44/71 | LOSS: 4.392775488466012e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 45/71 | LOSS: 4.426015459063384e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 46/71 | LOSS: 4.424619588617327e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 47/71 | LOSS: 4.405674185174273e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 48/71 | LOSS: 4.388880630098616e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 49/71 | LOSS: 4.411310555951786e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 50/71 | LOSS: 4.421008154667124e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 51/71 | LOSS: 4.4420208164060914e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 52/71 | LOSS: 4.455145965451111e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 53/71 | LOSS: 4.462515149767839e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 54/71 | LOSS: 4.468087552760897e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 55/71 | LOSS: 4.4852032163232384e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 56/71 | LOSS: 4.485114164198055e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 57/71 | LOSS: 4.533097173156546e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 58/71 | LOSS: 4.530988007900305e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 59/71 | LOSS: 4.5288013855800575e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 60/71 | LOSS: 4.549892093832643e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 61/71 | LOSS: 4.575862410474069e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 62/71 | LOSS: 4.5631652916372764e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 63/71 | LOSS: 4.60307023431028e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 64/71 | LOSS: 4.609873461119535e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 65/71 | LOSS: 4.603137509465424e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 66/71 | LOSS: 4.619044338101252e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 67/71 | LOSS: 4.647177296887735e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 68/71 | LOSS: 4.651589756906395e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 69/71 | LOSS: 4.690140690399857e-06\n",
      "TRAIN: EPOCH 637/1000 | BATCH 70/71 | LOSS: 4.699853066149829e-06\n",
      "VAL: EPOCH 637/1000 | BATCH 0/8 | LOSS: 4.9906529966392554e-06\n",
      "VAL: EPOCH 637/1000 | BATCH 1/8 | LOSS: 5.126029236635077e-06\n",
      "VAL: EPOCH 637/1000 | BATCH 2/8 | LOSS: 5.279488808203799e-06\n",
      "VAL: EPOCH 637/1000 | BATCH 3/8 | LOSS: 5.2937502914574e-06\n",
      "VAL: EPOCH 637/1000 | BATCH 4/8 | LOSS: 5.316023180057527e-06\n",
      "VAL: EPOCH 637/1000 | BATCH 5/8 | LOSS: 5.211611475412307e-06\n",
      "VAL: EPOCH 637/1000 | BATCH 6/8 | LOSS: 5.036309955487793e-06\n",
      "VAL: EPOCH 637/1000 | BATCH 7/8 | LOSS: 4.828864348382922e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 0/71 | LOSS: 6.683731953671668e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 1/71 | LOSS: 5.181125175113266e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 2/71 | LOSS: 5.70994681462859e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 3/71 | LOSS: 5.303103478127014e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 4/71 | LOSS: 5.166072924112086e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 5/71 | LOSS: 5.187666564173317e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 6/71 | LOSS: 5.089558758949611e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 7/71 | LOSS: 5.062701603719688e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 8/71 | LOSS: 5.122436580980623e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 9/71 | LOSS: 5.148636478224944e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 10/71 | LOSS: 5.045222988800643e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 11/71 | LOSS: 5.010717491889712e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 12/71 | LOSS: 4.9702956193169275e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 13/71 | LOSS: 4.93343913733822e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 14/71 | LOSS: 4.827233215110027e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 15/71 | LOSS: 4.805567741072991e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 16/71 | LOSS: 4.756384909984156e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 17/71 | LOSS: 4.71747690274545e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 18/71 | LOSS: 4.692742059456864e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 19/71 | LOSS: 4.6868429421920155e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 20/71 | LOSS: 4.63582465012483e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 21/71 | LOSS: 4.615818697443914e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 22/71 | LOSS: 4.560605169029694e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 23/71 | LOSS: 4.5161697433589625e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 24/71 | LOSS: 4.479033459574566e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 25/71 | LOSS: 4.467486431936917e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 26/71 | LOSS: 4.456726931797069e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 27/71 | LOSS: 4.425019020410608e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 28/71 | LOSS: 4.402997750856167e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 29/71 | LOSS: 4.3716443466716255e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 30/71 | LOSS: 4.367309833058595e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 31/71 | LOSS: 4.354171842635424e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 32/71 | LOSS: 4.348549799198571e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 33/71 | LOSS: 4.421057764482291e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 34/71 | LOSS: 4.42170500199219e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 35/71 | LOSS: 4.429417300697322e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 36/71 | LOSS: 4.431584458772606e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 37/71 | LOSS: 4.442832068650061e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 38/71 | LOSS: 4.440334793961926e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 39/71 | LOSS: 4.4586546835034825e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 40/71 | LOSS: 4.437008739132136e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 41/71 | LOSS: 4.4146217067550645e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 42/71 | LOSS: 4.414277137309613e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 43/71 | LOSS: 4.4165657558633855e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 44/71 | LOSS: 4.388817044148002e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 45/71 | LOSS: 4.387261029755198e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 46/71 | LOSS: 4.383016514421275e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 47/71 | LOSS: 4.3815448265149826e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 48/71 | LOSS: 4.356622036297898e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 49/71 | LOSS: 4.388984484648972e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 50/71 | LOSS: 4.372868310312304e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 51/71 | LOSS: 4.385665100961398e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 52/71 | LOSS: 4.396679557585058e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 53/71 | LOSS: 4.379517337030039e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 54/71 | LOSS: 4.402045326721484e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 55/71 | LOSS: 4.404126778061774e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 56/71 | LOSS: 4.404438788952304e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 57/71 | LOSS: 4.386365001915567e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 58/71 | LOSS: 4.390808171438501e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 59/71 | LOSS: 4.3871963233262555e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 60/71 | LOSS: 4.385644712381124e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 61/71 | LOSS: 4.390446105712579e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 62/71 | LOSS: 4.379967809394916e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 63/71 | LOSS: 4.381722302326807e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 64/71 | LOSS: 4.3607586016710575e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 65/71 | LOSS: 4.362463721118922e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 66/71 | LOSS: 4.3441149629680734e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 67/71 | LOSS: 4.352211339450792e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 68/71 | LOSS: 4.340303852903004e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 69/71 | LOSS: 4.333874101446002e-06\n",
      "TRAIN: EPOCH 638/1000 | BATCH 70/71 | LOSS: 4.339530857872836e-06\n",
      "VAL: EPOCH 638/1000 | BATCH 0/8 | LOSS: 4.238891051500104e-06\n",
      "VAL: EPOCH 638/1000 | BATCH 1/8 | LOSS: 4.605948106473079e-06\n",
      "VAL: EPOCH 638/1000 | BATCH 2/8 | LOSS: 4.7445684382788995e-06\n",
      "VAL: EPOCH 638/1000 | BATCH 3/8 | LOSS: 4.683650672632211e-06\n",
      "VAL: EPOCH 638/1000 | BATCH 4/8 | LOSS: 4.807451568922261e-06\n",
      "VAL: EPOCH 638/1000 | BATCH 5/8 | LOSS: 4.867889401793946e-06\n",
      "VAL: EPOCH 638/1000 | BATCH 6/8 | LOSS: 4.77933632022801e-06\n",
      "VAL: EPOCH 638/1000 | BATCH 7/8 | LOSS: 4.756491705393273e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 0/71 | LOSS: 4.364082542451797e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 1/71 | LOSS: 4.59915145256673e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 2/71 | LOSS: 4.351242750999518e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 3/71 | LOSS: 4.3093082240375225e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 4/71 | LOSS: 4.446954608283704e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 5/71 | LOSS: 4.441525258395511e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 6/71 | LOSS: 4.266480833134015e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 7/71 | LOSS: 4.417865682171396e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 8/71 | LOSS: 4.272167087846255e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 9/71 | LOSS: 4.286495050109807e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 10/71 | LOSS: 4.366079379410208e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 11/71 | LOSS: 4.336288043305103e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 12/71 | LOSS: 4.3965878481125964e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 13/71 | LOSS: 4.4093918144686284e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 14/71 | LOSS: 4.398963816735583e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 15/71 | LOSS: 4.365591877331099e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 16/71 | LOSS: 4.516524554406608e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 17/71 | LOSS: 4.470637981689328e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 18/71 | LOSS: 4.4892905479125485e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 19/71 | LOSS: 4.535868129096343e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 20/71 | LOSS: 4.630356194129923e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 21/71 | LOSS: 4.583266466149573e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 22/71 | LOSS: 4.585215324566335e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 23/71 | LOSS: 4.6008223080207244e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 24/71 | LOSS: 4.579125779855531e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 25/71 | LOSS: 4.567817121334463e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 26/71 | LOSS: 4.625485635861642e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 27/71 | LOSS: 4.61854524149073e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 28/71 | LOSS: 4.64690948978512e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 29/71 | LOSS: 4.639897997549269e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 30/71 | LOSS: 4.598924973716719e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 31/71 | LOSS: 4.553256388817317e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 32/71 | LOSS: 4.561983160149821e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 33/71 | LOSS: 4.549207366889116e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 34/71 | LOSS: 4.524034816963e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 35/71 | LOSS: 4.5504451817477175e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 36/71 | LOSS: 4.572682735522756e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 37/71 | LOSS: 4.603111045454218e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 38/71 | LOSS: 4.62140604166705e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 39/71 | LOSS: 4.591521235397522e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 40/71 | LOSS: 4.583702465003248e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 41/71 | LOSS: 4.5735051203553755e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 42/71 | LOSS: 4.562816367217957e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 43/71 | LOSS: 4.559492646711243e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 44/71 | LOSS: 4.575977840835953e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 45/71 | LOSS: 4.543706742642035e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 46/71 | LOSS: 4.538413524177815e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 47/71 | LOSS: 4.526235694394624e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 48/71 | LOSS: 4.516586043290754e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 49/71 | LOSS: 4.512193040682177e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 50/71 | LOSS: 4.521255143973737e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 51/71 | LOSS: 4.5157094632486405e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 52/71 | LOSS: 4.508311735233566e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 53/71 | LOSS: 4.508911602998624e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 54/71 | LOSS: 4.496270920836568e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 55/71 | LOSS: 4.482019578842612e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 56/71 | LOSS: 4.503110248103774e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 57/71 | LOSS: 4.4986019678743045e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 58/71 | LOSS: 4.492034241917219e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 59/71 | LOSS: 4.491108560008191e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 60/71 | LOSS: 4.482377290350698e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 61/71 | LOSS: 4.49191755310054e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 62/71 | LOSS: 4.49713595102482e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 63/71 | LOSS: 4.489428015119756e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 64/71 | LOSS: 4.4896200401686096e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 65/71 | LOSS: 4.495315641344694e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 66/71 | LOSS: 4.505267898183327e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 67/71 | LOSS: 4.51326548045472e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 68/71 | LOSS: 4.501326060747395e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 69/71 | LOSS: 4.542582697857662e-06\n",
      "TRAIN: EPOCH 639/1000 | BATCH 70/71 | LOSS: 4.533097129316163e-06\n",
      "VAL: EPOCH 639/1000 | BATCH 0/8 | LOSS: 4.69168480776716e-06\n",
      "VAL: EPOCH 639/1000 | BATCH 1/8 | LOSS: 5.544708983507007e-06\n",
      "VAL: EPOCH 639/1000 | BATCH 2/8 | LOSS: 5.972515888667355e-06\n",
      "VAL: EPOCH 639/1000 | BATCH 3/8 | LOSS: 6.040860398570658e-06\n",
      "VAL: EPOCH 639/1000 | BATCH 4/8 | LOSS: 6.086158373364015e-06\n",
      "VAL: EPOCH 639/1000 | BATCH 5/8 | LOSS: 6.174070752725432e-06\n",
      "VAL: EPOCH 639/1000 | BATCH 6/8 | LOSS: 6.121664033084276e-06\n",
      "VAL: EPOCH 639/1000 | BATCH 7/8 | LOSS: 6.07540715691357e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 0/71 | LOSS: 6.141362064226996e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 1/71 | LOSS: 5.086620376459905e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 2/71 | LOSS: 5.264086667011725e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 3/71 | LOSS: 4.726831832613243e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 4/71 | LOSS: 4.615425359588698e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 5/71 | LOSS: 5.004517826516046e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 6/71 | LOSS: 4.990393953059018e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 7/71 | LOSS: 4.846760162990904e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 8/71 | LOSS: 4.77277719963038e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 9/71 | LOSS: 4.71256473701942e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 10/71 | LOSS: 4.54803799501943e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 11/71 | LOSS: 4.594876315877627e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 12/71 | LOSS: 4.604435879138271e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 13/71 | LOSS: 4.537475531053912e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 14/71 | LOSS: 4.496201806129345e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 15/71 | LOSS: 4.515491156098506e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 16/71 | LOSS: 4.530832939053653e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 17/71 | LOSS: 4.568399087171808e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 18/71 | LOSS: 4.5329560586986574e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 19/71 | LOSS: 4.610699579643551e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 20/71 | LOSS: 4.597532465379031e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 21/71 | LOSS: 4.568817179130168e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 22/71 | LOSS: 4.551038866496959e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 23/71 | LOSS: 4.599772978508554e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 24/71 | LOSS: 4.621712614607531e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 25/71 | LOSS: 4.591401018562745e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 26/71 | LOSS: 4.558306533942448e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 27/71 | LOSS: 4.518566186106909e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 28/71 | LOSS: 4.483720352976073e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 29/71 | LOSS: 4.476293793231889e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 30/71 | LOSS: 4.498996084352726e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 31/71 | LOSS: 4.462195029475424e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 32/71 | LOSS: 4.433645093134449e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 33/71 | LOSS: 4.416288314079044e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 34/71 | LOSS: 4.407930815172482e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 35/71 | LOSS: 4.4045288202849206e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 36/71 | LOSS: 4.37491442770241e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 37/71 | LOSS: 4.335599023282779e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 38/71 | LOSS: 4.334654943541421e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 39/71 | LOSS: 4.362867139207083e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 40/71 | LOSS: 4.383235169903278e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 41/71 | LOSS: 4.373082317345377e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 42/71 | LOSS: 4.404880687510486e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 43/71 | LOSS: 4.441306414677952e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 44/71 | LOSS: 4.450134333132559e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 45/71 | LOSS: 4.480011937076874e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 46/71 | LOSS: 4.496607966467173e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 47/71 | LOSS: 4.471101258711012e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 48/71 | LOSS: 4.472615635640235e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 49/71 | LOSS: 4.4699705722450745e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 50/71 | LOSS: 4.4871156063106585e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 51/71 | LOSS: 4.510363606431593e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 52/71 | LOSS: 4.489538546209279e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 53/71 | LOSS: 4.4827220462037885e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 54/71 | LOSS: 4.501878071426869e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 55/71 | LOSS: 4.522350621982696e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 56/71 | LOSS: 4.506537447524718e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 57/71 | LOSS: 4.524589642109089e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 58/71 | LOSS: 4.514556961004342e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 59/71 | LOSS: 4.515440915990136e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 60/71 | LOSS: 4.532710046659617e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 61/71 | LOSS: 4.555204463207291e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 62/71 | LOSS: 4.546088449488808e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 63/71 | LOSS: 4.5295076738227635e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 64/71 | LOSS: 4.526413036168938e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 65/71 | LOSS: 4.516750126723227e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 66/71 | LOSS: 4.506637451277808e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 67/71 | LOSS: 4.4903959051493185e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 68/71 | LOSS: 4.489054841885092e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 69/71 | LOSS: 4.480306353928297e-06\n",
      "TRAIN: EPOCH 640/1000 | BATCH 70/71 | LOSS: 4.488956091348959e-06\n",
      "VAL: EPOCH 640/1000 | BATCH 0/8 | LOSS: 3.801742650466622e-06\n",
      "VAL: EPOCH 640/1000 | BATCH 1/8 | LOSS: 4.369805651549541e-06\n",
      "VAL: EPOCH 640/1000 | BATCH 2/8 | LOSS: 4.6410001080706325e-06\n",
      "VAL: EPOCH 640/1000 | BATCH 3/8 | LOSS: 4.659031617393339e-06\n",
      "VAL: EPOCH 640/1000 | BATCH 4/8 | LOSS: 4.7108079797908434e-06\n",
      "VAL: EPOCH 640/1000 | BATCH 5/8 | LOSS: 4.768772707090345e-06\n",
      "VAL: EPOCH 640/1000 | BATCH 6/8 | LOSS: 4.66168333852173e-06\n",
      "VAL: EPOCH 640/1000 | BATCH 7/8 | LOSS: 4.5915747080016445e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 0/71 | LOSS: 4.915918907499872e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 1/71 | LOSS: 4.142547595620272e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 2/71 | LOSS: 4.083736409180953e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 3/71 | LOSS: 4.352237851890095e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 4/71 | LOSS: 4.144485774304485e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 5/71 | LOSS: 4.258879698681994e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 6/71 | LOSS: 4.283561338525033e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 7/71 | LOSS: 4.350781296125206e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 8/71 | LOSS: 4.434070534544945e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 9/71 | LOSS: 4.4906887978868324e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 10/71 | LOSS: 4.615432565267176e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 11/71 | LOSS: 4.592682140961794e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 12/71 | LOSS: 4.608476291860615e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 13/71 | LOSS: 4.636019281991009e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 14/71 | LOSS: 4.704172139706013e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 15/71 | LOSS: 4.62878239204656e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 16/71 | LOSS: 4.69527324586285e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 17/71 | LOSS: 4.695097888745497e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 18/71 | LOSS: 4.760088957386631e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 19/71 | LOSS: 4.779194659931818e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 20/71 | LOSS: 4.9227358734545606e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 21/71 | LOSS: 4.915896376835669e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 22/71 | LOSS: 4.905529709978272e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 23/71 | LOSS: 4.881252209543163e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 24/71 | LOSS: 4.866167510044761e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 25/71 | LOSS: 4.844265924055855e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 26/71 | LOSS: 4.856900162579862e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 27/71 | LOSS: 4.852124301838298e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 28/71 | LOSS: 4.819005363091931e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 29/71 | LOSS: 4.7997073276443794e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 30/71 | LOSS: 4.7951892948701914e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 31/71 | LOSS: 4.793965075577944e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 32/71 | LOSS: 4.739971493770347e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 33/71 | LOSS: 4.727272867325262e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 34/71 | LOSS: 4.698971952166175e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 35/71 | LOSS: 4.697088684224582e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 36/71 | LOSS: 4.6586831656994135e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 37/71 | LOSS: 4.658777925214963e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 38/71 | LOSS: 4.656698552142375e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 39/71 | LOSS: 4.664657240027736e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 40/71 | LOSS: 4.61662625084722e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 41/71 | LOSS: 4.639179375468909e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 42/71 | LOSS: 4.622774699677139e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 43/71 | LOSS: 4.6057431911156694e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 44/71 | LOSS: 4.573422231462448e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 45/71 | LOSS: 4.569417708056711e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 46/71 | LOSS: 4.582627791526821e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 47/71 | LOSS: 4.567589532674295e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 48/71 | LOSS: 4.541763459832398e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 49/71 | LOSS: 4.543245067907265e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 50/71 | LOSS: 4.546032583925362e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 51/71 | LOSS: 4.574424609087985e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 52/71 | LOSS: 4.571585483269169e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 53/71 | LOSS: 4.566014838389863e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 54/71 | LOSS: 4.555482271164444e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 55/71 | LOSS: 4.548041894330319e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 56/71 | LOSS: 4.5498838054507425e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 57/71 | LOSS: 4.545544776380572e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 58/71 | LOSS: 4.540752520292931e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 59/71 | LOSS: 4.5494822188629765e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 60/71 | LOSS: 4.584123148486499e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 61/71 | LOSS: 4.598930093291734e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 62/71 | LOSS: 4.585070337992557e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 63/71 | LOSS: 4.607210165374909e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 64/71 | LOSS: 4.59797713399614e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 65/71 | LOSS: 4.60004507245189e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 66/71 | LOSS: 4.589223822425884e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 67/71 | LOSS: 4.589909374718606e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 68/71 | LOSS: 4.6008580996029824e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 69/71 | LOSS: 4.584895818230247e-06\n",
      "TRAIN: EPOCH 641/1000 | BATCH 70/71 | LOSS: 4.562865512179741e-06\n",
      "VAL: EPOCH 641/1000 | BATCH 0/8 | LOSS: 3.856642251776066e-06\n",
      "VAL: EPOCH 641/1000 | BATCH 1/8 | LOSS: 4.418960315888398e-06\n",
      "VAL: EPOCH 641/1000 | BATCH 2/8 | LOSS: 4.577120762405684e-06\n",
      "VAL: EPOCH 641/1000 | BATCH 3/8 | LOSS: 4.691887170338305e-06\n",
      "VAL: EPOCH 641/1000 | BATCH 4/8 | LOSS: 4.681622885982506e-06\n",
      "VAL: EPOCH 641/1000 | BATCH 5/8 | LOSS: 4.742574371145262e-06\n",
      "VAL: EPOCH 641/1000 | BATCH 6/8 | LOSS: 4.639212680298702e-06\n",
      "VAL: EPOCH 641/1000 | BATCH 7/8 | LOSS: 4.454006898413354e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 0/71 | LOSS: 4.635299774236046e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 1/71 | LOSS: 5.098038172945962e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 2/71 | LOSS: 4.477980155570549e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 3/71 | LOSS: 4.4121067617197696e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 4/71 | LOSS: 4.417802983880392e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 5/71 | LOSS: 4.422033915337427e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 6/71 | LOSS: 4.32873330932385e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 7/71 | LOSS: 4.253548894439518e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 8/71 | LOSS: 4.221172402038873e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 9/71 | LOSS: 4.163388416600355e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 10/71 | LOSS: 4.101970737544682e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 11/71 | LOSS: 4.055837545517231e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 12/71 | LOSS: 4.058119289906104e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 13/71 | LOSS: 4.042091307253161e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 14/71 | LOSS: 4.0546538305837505e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 15/71 | LOSS: 4.049386177484848e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 16/71 | LOSS: 4.071447084938614e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 17/71 | LOSS: 4.138431753138623e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 18/71 | LOSS: 4.155654575075268e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 19/71 | LOSS: 4.186908063275041e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 20/71 | LOSS: 4.196058874922095e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 21/71 | LOSS: 4.176436980494393e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 22/71 | LOSS: 4.147769133218706e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 23/71 | LOSS: 4.174033241118498e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 24/71 | LOSS: 4.1596317896619435e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 25/71 | LOSS: 4.152483160606397e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 26/71 | LOSS: 4.129920735404421e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 27/71 | LOSS: 4.128445311835094e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 28/71 | LOSS: 4.127041377395127e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 29/71 | LOSS: 4.13629312182214e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 30/71 | LOSS: 4.10670597170059e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 31/71 | LOSS: 4.089362036552302e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 32/71 | LOSS: 4.085410334179566e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 33/71 | LOSS: 4.113542498838369e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 34/71 | LOSS: 4.124671041998746e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 35/71 | LOSS: 4.099824694650225e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 36/71 | LOSS: 4.083882569486033e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 37/71 | LOSS: 4.11048870319064e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 38/71 | LOSS: 4.099072734439128e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 39/71 | LOSS: 4.107798901031856e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 40/71 | LOSS: 4.132266112941491e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 41/71 | LOSS: 4.137987751900732e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 42/71 | LOSS: 4.1430873595747345e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 43/71 | LOSS: 4.156108859247483e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 44/71 | LOSS: 4.154947838388681e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 45/71 | LOSS: 4.152553564697724e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 46/71 | LOSS: 4.1716845355543404e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 47/71 | LOSS: 4.19828318835395e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 48/71 | LOSS: 4.200363282878391e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 49/71 | LOSS: 4.196244940430916e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 50/71 | LOSS: 4.217722709440482e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 51/71 | LOSS: 4.236125361190856e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 52/71 | LOSS: 4.226047175901806e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 53/71 | LOSS: 4.221867601262164e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 54/71 | LOSS: 4.201975203117896e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 55/71 | LOSS: 4.186093929904798e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 56/71 | LOSS: 4.173181539220407e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 57/71 | LOSS: 4.152106381015669e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 58/71 | LOSS: 4.145501121857413e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 59/71 | LOSS: 4.145642916834428e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 60/71 | LOSS: 4.138326653073967e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 61/71 | LOSS: 4.125337737135734e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 62/71 | LOSS: 4.129261660925713e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 63/71 | LOSS: 4.1319006562900995e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 64/71 | LOSS: 4.134374722315652e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 65/71 | LOSS: 4.128867872030682e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 66/71 | LOSS: 4.123331991785793e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 67/71 | LOSS: 4.125652507729469e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 68/71 | LOSS: 4.130921844488762e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 69/71 | LOSS: 4.131865164578942e-06\n",
      "TRAIN: EPOCH 642/1000 | BATCH 70/71 | LOSS: 4.137788968825628e-06\n",
      "VAL: EPOCH 642/1000 | BATCH 0/8 | LOSS: 3.856645889754873e-06\n",
      "VAL: EPOCH 642/1000 | BATCH 1/8 | LOSS: 4.107184849999612e-06\n",
      "VAL: EPOCH 642/1000 | BATCH 2/8 | LOSS: 4.276494109944906e-06\n",
      "VAL: EPOCH 642/1000 | BATCH 3/8 | LOSS: 4.206115590932313e-06\n",
      "VAL: EPOCH 642/1000 | BATCH 4/8 | LOSS: 4.3068632294307465e-06\n",
      "VAL: EPOCH 642/1000 | BATCH 5/8 | LOSS: 4.315951173339272e-06\n",
      "VAL: EPOCH 642/1000 | BATCH 6/8 | LOSS: 4.211127523896201e-06\n",
      "VAL: EPOCH 642/1000 | BATCH 7/8 | LOSS: 4.088669214752372e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 0/71 | LOSS: 3.782701014642953e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 1/71 | LOSS: 4.0043481703833095e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 2/71 | LOSS: 4.002036424329465e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 3/71 | LOSS: 3.989709114193829e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 4/71 | LOSS: 3.851721476166858e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 5/71 | LOSS: 4.092586967393193e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 6/71 | LOSS: 4.073236563531932e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 7/71 | LOSS: 4.171343022107976e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 8/71 | LOSS: 4.140381735042966e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 9/71 | LOSS: 4.232335800224973e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 10/71 | LOSS: 4.476201638681232e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 11/71 | LOSS: 4.407088492068094e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 12/71 | LOSS: 4.600877113821648e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 13/71 | LOSS: 4.581876022971951e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 14/71 | LOSS: 4.6787271761180214e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 15/71 | LOSS: 4.703279984141773e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 16/71 | LOSS: 4.862018854242599e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 17/71 | LOSS: 4.819651697592538e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 18/71 | LOSS: 4.988820675958107e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 19/71 | LOSS: 5.0787947884600724e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 20/71 | LOSS: 5.224782810165336e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 21/71 | LOSS: 5.137228620035537e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 22/71 | LOSS: 5.130628076822967e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 23/71 | LOSS: 5.172248895253991e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 24/71 | LOSS: 5.153836473255069e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 25/71 | LOSS: 5.17642135596711e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 26/71 | LOSS: 5.190692019351883e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 27/71 | LOSS: 5.288656024861536e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 28/71 | LOSS: 5.234128342761261e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 29/71 | LOSS: 5.260302509668691e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 30/71 | LOSS: 5.23300705466564e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 31/71 | LOSS: 5.226453104967277e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 32/71 | LOSS: 5.194303584534227e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 33/71 | LOSS: 5.159705250769227e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 34/71 | LOSS: 5.148056301810097e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 35/71 | LOSS: 5.090716696385142e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 36/71 | LOSS: 5.103392441998113e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 37/71 | LOSS: 5.0828458597607096e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 38/71 | LOSS: 5.139428180765931e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 39/71 | LOSS: 5.138510704227883e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 40/71 | LOSS: 5.128375344766693e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 41/71 | LOSS: 5.153821310462566e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 42/71 | LOSS: 5.163191084394321e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 43/71 | LOSS: 5.1806882610931515e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 44/71 | LOSS: 5.210948362825244e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 45/71 | LOSS: 5.2064714677528725e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 46/71 | LOSS: 5.196895999494913e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 47/71 | LOSS: 5.200780876180033e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 48/71 | LOSS: 5.194486346004035e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 49/71 | LOSS: 5.2340859519972584e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 50/71 | LOSS: 5.220365504582707e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 51/71 | LOSS: 5.2335793244450406e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 52/71 | LOSS: 5.217085186333587e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 53/71 | LOSS: 5.2148772814083104e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 54/71 | LOSS: 5.183748813082771e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 55/71 | LOSS: 5.180272909553553e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 56/71 | LOSS: 5.146392005260343e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 57/71 | LOSS: 5.157232235746803e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 58/71 | LOSS: 5.122042983017864e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 59/71 | LOSS: 5.109362174001338e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 60/71 | LOSS: 5.083175915015239e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 61/71 | LOSS: 5.06132108336216e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 62/71 | LOSS: 5.052461726033168e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 63/71 | LOSS: 5.045754296162386e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 64/71 | LOSS: 5.037838179934904e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 65/71 | LOSS: 5.013071100537755e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 66/71 | LOSS: 5.023691229909483e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 67/71 | LOSS: 4.999360391914652e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 68/71 | LOSS: 5.010252765635454e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 69/71 | LOSS: 4.9907086642113944e-06\n",
      "TRAIN: EPOCH 643/1000 | BATCH 70/71 | LOSS: 4.955165896832805e-06\n",
      "VAL: EPOCH 643/1000 | BATCH 0/8 | LOSS: 4.756753241963452e-06\n",
      "VAL: EPOCH 643/1000 | BATCH 1/8 | LOSS: 4.838418362851371e-06\n",
      "VAL: EPOCH 643/1000 | BATCH 2/8 | LOSS: 4.8408578550152015e-06\n",
      "VAL: EPOCH 643/1000 | BATCH 3/8 | LOSS: 4.7251691057681455e-06\n",
      "VAL: EPOCH 643/1000 | BATCH 4/8 | LOSS: 4.786150202562567e-06\n",
      "VAL: EPOCH 643/1000 | BATCH 5/8 | LOSS: 4.714675166421027e-06\n",
      "VAL: EPOCH 643/1000 | BATCH 6/8 | LOSS: 4.598780898439665e-06\n",
      "VAL: EPOCH 643/1000 | BATCH 7/8 | LOSS: 4.491314456345208e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 0/71 | LOSS: 4.562937647278886e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 1/71 | LOSS: 3.8813408309579245e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 2/71 | LOSS: 4.168851091890247e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 3/71 | LOSS: 4.307968254124717e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 4/71 | LOSS: 4.243158900862909e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 5/71 | LOSS: 4.0739269403881435e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 6/71 | LOSS: 4.2243977661980485e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 7/71 | LOSS: 4.220802594545603e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 8/71 | LOSS: 4.1443955877993076e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 9/71 | LOSS: 4.175741310064041e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 10/71 | LOSS: 4.179174972953413e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 11/71 | LOSS: 4.116336810966459e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 12/71 | LOSS: 4.121574927641912e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 13/71 | LOSS: 4.122186396671168e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 14/71 | LOSS: 4.063799694146534e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 15/71 | LOSS: 4.127386475261119e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 16/71 | LOSS: 4.177031033738304e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 17/71 | LOSS: 4.139849314318174e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 18/71 | LOSS: 4.157016435687853e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 19/71 | LOSS: 4.128344005494e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 20/71 | LOSS: 4.2783960690853526e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 21/71 | LOSS: 4.255444216803219e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 22/71 | LOSS: 4.247406442790753e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 23/71 | LOSS: 4.257711538002695e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 24/71 | LOSS: 4.264160297680064e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 25/71 | LOSS: 4.229118810774078e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 26/71 | LOSS: 4.228855282740875e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 27/71 | LOSS: 4.296039053640145e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 28/71 | LOSS: 4.300356522435322e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 29/71 | LOSS: 4.333138591997946e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 30/71 | LOSS: 4.317890876160574e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 31/71 | LOSS: 4.287168053451751e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 32/71 | LOSS: 4.237032926263937e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 33/71 | LOSS: 4.275869313834632e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 34/71 | LOSS: 4.2661931209815e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 35/71 | LOSS: 4.2574408780637896e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 36/71 | LOSS: 4.2758646658684655e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 37/71 | LOSS: 4.300341348234245e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 38/71 | LOSS: 4.277550254837246e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 39/71 | LOSS: 4.262348471684163e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 40/71 | LOSS: 4.260353390658033e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 41/71 | LOSS: 4.246564100997473e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 42/71 | LOSS: 4.255939349901001e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 43/71 | LOSS: 4.269172222848283e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 44/71 | LOSS: 4.264880145557173e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 45/71 | LOSS: 4.2680682346816985e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 46/71 | LOSS: 4.275540195134879e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 47/71 | LOSS: 4.277786151381709e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 48/71 | LOSS: 4.291413971463489e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 49/71 | LOSS: 4.301043509258306e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 50/71 | LOSS: 4.295645348796754e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 51/71 | LOSS: 4.3296038344264025e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 52/71 | LOSS: 4.3290270816283255e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 53/71 | LOSS: 4.34239247489906e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 54/71 | LOSS: 4.338444052171491e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 55/71 | LOSS: 4.329073034016671e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 56/71 | LOSS: 4.3304301734200345e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 57/71 | LOSS: 4.336580610501626e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 58/71 | LOSS: 4.330927957476055e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 59/71 | LOSS: 4.342366541247126e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 60/71 | LOSS: 4.327604182301419e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 61/71 | LOSS: 4.3182015896227015e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 62/71 | LOSS: 4.301160829743908e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 63/71 | LOSS: 4.292761698110326e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 64/71 | LOSS: 4.278538209781311e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 65/71 | LOSS: 4.303748562161865e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 66/71 | LOSS: 4.294799282433814e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 67/71 | LOSS: 4.305478077693645e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 68/71 | LOSS: 4.310569670121987e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 69/71 | LOSS: 4.326644482586874e-06\n",
      "TRAIN: EPOCH 644/1000 | BATCH 70/71 | LOSS: 4.315022880278699e-06\n",
      "VAL: EPOCH 644/1000 | BATCH 0/8 | LOSS: 4.166196958976798e-06\n",
      "VAL: EPOCH 644/1000 | BATCH 1/8 | LOSS: 4.4093767428421415e-06\n",
      "VAL: EPOCH 644/1000 | BATCH 2/8 | LOSS: 4.531925696937833e-06\n",
      "VAL: EPOCH 644/1000 | BATCH 3/8 | LOSS: 4.502206820689025e-06\n",
      "VAL: EPOCH 644/1000 | BATCH 4/8 | LOSS: 4.499276474234648e-06\n",
      "VAL: EPOCH 644/1000 | BATCH 5/8 | LOSS: 4.477524877681087e-06\n",
      "VAL: EPOCH 644/1000 | BATCH 6/8 | LOSS: 4.3261945523097116e-06\n",
      "VAL: EPOCH 644/1000 | BATCH 7/8 | LOSS: 4.1394004881567525e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 0/71 | LOSS: 4.674541742133442e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 1/71 | LOSS: 4.125863370063598e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 2/71 | LOSS: 3.908703168538826e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 3/71 | LOSS: 4.267101019195252e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 4/71 | LOSS: 4.596370990839204e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 5/71 | LOSS: 4.409076533799332e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 6/71 | LOSS: 4.457765304063546e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 7/71 | LOSS: 4.614601436969679e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 8/71 | LOSS: 4.55366489404696e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 9/71 | LOSS: 4.561545210890472e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 10/71 | LOSS: 4.603831216214034e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 11/71 | LOSS: 4.636456310436188e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 12/71 | LOSS: 4.651585676093908e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 13/71 | LOSS: 4.677784415564799e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 14/71 | LOSS: 4.740597493461488e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 15/71 | LOSS: 4.861253472654425e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 16/71 | LOSS: 4.821280286063591e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 17/71 | LOSS: 4.8587298048611656e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 18/71 | LOSS: 4.882459530850419e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 19/71 | LOSS: 4.911347650704556e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 20/71 | LOSS: 4.830697987147557e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 21/71 | LOSS: 4.835670458967136e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 22/71 | LOSS: 4.836860536828602e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 23/71 | LOSS: 4.888101149920961e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 24/71 | LOSS: 4.8749301004136215e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 25/71 | LOSS: 4.925024847160295e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 26/71 | LOSS: 4.935280491769879e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 27/71 | LOSS: 4.8929527695561e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 28/71 | LOSS: 4.897292550093221e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 29/71 | LOSS: 4.913885201555483e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 30/71 | LOSS: 4.893270303704588e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 31/71 | LOSS: 4.871554494911834e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 32/71 | LOSS: 4.927039988229588e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 33/71 | LOSS: 4.895869484672028e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 34/71 | LOSS: 4.88883473995624e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 35/71 | LOSS: 4.894994301846762e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 36/71 | LOSS: 4.925995088686594e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 37/71 | LOSS: 4.927211696332578e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 38/71 | LOSS: 4.92457344919855e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 39/71 | LOSS: 5.000776695851528e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 40/71 | LOSS: 4.993895522713457e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 41/71 | LOSS: 5.021700742093214e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 42/71 | LOSS: 5.0069809223587495e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 43/71 | LOSS: 5.022896991911032e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 44/71 | LOSS: 5.034341827720507e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 45/71 | LOSS: 5.020986009185435e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 46/71 | LOSS: 5.004090132601089e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 47/71 | LOSS: 4.99437232785264e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 48/71 | LOSS: 4.979634839549604e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 49/71 | LOSS: 4.943480234942399e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 50/71 | LOSS: 4.9252997390188584e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 51/71 | LOSS: 4.924968406017937e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 52/71 | LOSS: 4.880780305132437e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 53/71 | LOSS: 4.887788295322766e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 54/71 | LOSS: 4.873860581772698e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 55/71 | LOSS: 4.87453329307235e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 56/71 | LOSS: 4.857609734212849e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 57/71 | LOSS: 4.86480088323427e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 58/71 | LOSS: 4.84962401969824e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 59/71 | LOSS: 4.832838286953726e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 60/71 | LOSS: 4.818251089419078e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 61/71 | LOSS: 4.829682174820318e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 62/71 | LOSS: 4.8144580028343225e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 63/71 | LOSS: 4.810642458608072e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 64/71 | LOSS: 4.812416150693585e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 65/71 | LOSS: 4.814344217238338e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 66/71 | LOSS: 4.8270147890977585e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 67/71 | LOSS: 4.816780005967303e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 68/71 | LOSS: 4.808374487238589e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 69/71 | LOSS: 4.799092682463067e-06\n",
      "TRAIN: EPOCH 645/1000 | BATCH 70/71 | LOSS: 4.785223910390337e-06\n",
      "VAL: EPOCH 645/1000 | BATCH 0/8 | LOSS: 3.474326149444096e-06\n",
      "VAL: EPOCH 645/1000 | BATCH 1/8 | LOSS: 4.071423290952225e-06\n",
      "VAL: EPOCH 645/1000 | BATCH 2/8 | LOSS: 4.411427350229739e-06\n",
      "VAL: EPOCH 645/1000 | BATCH 3/8 | LOSS: 4.494569566304563e-06\n",
      "VAL: EPOCH 645/1000 | BATCH 4/8 | LOSS: 4.521702157944673e-06\n",
      "VAL: EPOCH 645/1000 | BATCH 5/8 | LOSS: 4.574806856301923e-06\n",
      "VAL: EPOCH 645/1000 | BATCH 6/8 | LOSS: 4.4742992031387985e-06\n",
      "VAL: EPOCH 645/1000 | BATCH 7/8 | LOSS: 4.3638748934426985e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 0/71 | LOSS: 4.65662196802441e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 1/71 | LOSS: 5.124234348841128e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 2/71 | LOSS: 5.184481324249646e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 3/71 | LOSS: 5.332247383194044e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 4/71 | LOSS: 5.01232648275618e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 5/71 | LOSS: 5.2119793281235616e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 6/71 | LOSS: 4.898023852157556e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 7/71 | LOSS: 4.944254868632925e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 8/71 | LOSS: 5.02250741697531e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 9/71 | LOSS: 5.348737408894521e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 10/71 | LOSS: 5.382737630167288e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 11/71 | LOSS: 5.2537694159582315e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 12/71 | LOSS: 5.520291202069179e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 13/71 | LOSS: 5.3944532315394355e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 14/71 | LOSS: 5.6466519254172455e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 15/71 | LOSS: 5.518801657444783e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 16/71 | LOSS: 5.714248934108421e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 17/71 | LOSS: 5.649184585207776e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 18/71 | LOSS: 5.6253027794714155e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 19/71 | LOSS: 5.741136033066141e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 20/71 | LOSS: 5.7044297439764096e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 21/71 | LOSS: 5.764658701743676e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 22/71 | LOSS: 5.715459673924868e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 23/71 | LOSS: 5.785473206287861e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 24/71 | LOSS: 5.713539994758321e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 25/71 | LOSS: 5.664858917953097e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 26/71 | LOSS: 5.650438332075417e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 27/71 | LOSS: 5.637315163247487e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 28/71 | LOSS: 5.644071154228154e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 29/71 | LOSS: 5.566805126970091e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 30/71 | LOSS: 5.557462211600655e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 31/71 | LOSS: 5.5298732490882685e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 32/71 | LOSS: 5.510641985341808e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 33/71 | LOSS: 5.51191327330718e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 34/71 | LOSS: 5.5119050167767064e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 35/71 | LOSS: 5.446074333627217e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 36/71 | LOSS: 5.41961683641578e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 37/71 | LOSS: 5.394529843646237e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 38/71 | LOSS: 5.363735232467894e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 39/71 | LOSS: 5.371220140659716e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 40/71 | LOSS: 5.381457812313229e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 41/71 | LOSS: 5.3700039513517235e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 42/71 | LOSS: 5.356922450351534e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 43/71 | LOSS: 5.320609323627485e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 44/71 | LOSS: 5.3007814939418395e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 45/71 | LOSS: 5.316338580957128e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 46/71 | LOSS: 5.334434292230884e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 47/71 | LOSS: 5.302750873662869e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 48/71 | LOSS: 5.274029685356904e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 49/71 | LOSS: 5.247822214187181e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 50/71 | LOSS: 5.203739678394163e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 51/71 | LOSS: 5.174798113435268e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 52/71 | LOSS: 5.153897132433211e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 53/71 | LOSS: 5.139894091775608e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 54/71 | LOSS: 5.118163331148522e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 55/71 | LOSS: 5.113373971101152e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 56/71 | LOSS: 5.103840996462008e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 57/71 | LOSS: 5.077582217977736e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 58/71 | LOSS: 5.064467740786791e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 59/71 | LOSS: 5.041464544319752e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 60/71 | LOSS: 5.024823752539872e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 61/71 | LOSS: 5.022224253216043e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 62/71 | LOSS: 5.001012336184171e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 63/71 | LOSS: 4.9977505369724895e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 64/71 | LOSS: 4.99262005178025e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 65/71 | LOSS: 5.000261167383129e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 66/71 | LOSS: 4.979498149324756e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 67/71 | LOSS: 4.987307778157949e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 68/71 | LOSS: 4.9644502203169605e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 69/71 | LOSS: 4.967198196936806e-06\n",
      "TRAIN: EPOCH 646/1000 | BATCH 70/71 | LOSS: 4.959123053838635e-06\n",
      "VAL: EPOCH 646/1000 | BATCH 0/8 | LOSS: 1.0460711564519443e-05\n",
      "VAL: EPOCH 646/1000 | BATCH 1/8 | LOSS: 1.0881194612011313e-05\n",
      "VAL: EPOCH 646/1000 | BATCH 2/8 | LOSS: 1.0964934820852553e-05\n",
      "VAL: EPOCH 646/1000 | BATCH 3/8 | LOSS: 1.0769467280624667e-05\n",
      "VAL: EPOCH 646/1000 | BATCH 4/8 | LOSS: 1.0904231930908282e-05\n",
      "VAL: EPOCH 646/1000 | BATCH 5/8 | LOSS: 1.0616621845353317e-05\n",
      "VAL: EPOCH 646/1000 | BATCH 6/8 | LOSS: 1.0486102058036653e-05\n",
      "VAL: EPOCH 646/1000 | BATCH 7/8 | LOSS: 1.0221163051937765e-05\n",
      "TRAIN: EPOCH 647/1000 | BATCH 0/71 | LOSS: 1.0880419722525403e-05\n",
      "TRAIN: EPOCH 647/1000 | BATCH 1/71 | LOSS: 7.845657364669023e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 2/71 | LOSS: 8.414314834226388e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 3/71 | LOSS: 7.849009989513434e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 4/71 | LOSS: 7.492551139876013e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 5/71 | LOSS: 7.424790055665653e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 6/71 | LOSS: 6.9926728818765175e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 7/71 | LOSS: 6.984001515775162e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 8/71 | LOSS: 6.7490342795887654e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 9/71 | LOSS: 6.55302355880849e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 10/71 | LOSS: 6.584420258480929e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 11/71 | LOSS: 6.406686149299882e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 12/71 | LOSS: 6.366631029008064e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 13/71 | LOSS: 6.191437705638236e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 14/71 | LOSS: 6.139554989204043e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 15/71 | LOSS: 6.1451349893104634e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 16/71 | LOSS: 6.0796006865155715e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 17/71 | LOSS: 6.143914409525071e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 18/71 | LOSS: 6.059960277956347e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 19/71 | LOSS: 6.099003394410829e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 20/71 | LOSS: 6.032425149203932e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 21/71 | LOSS: 6.009569047323153e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 22/71 | LOSS: 5.949746042935421e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 23/71 | LOSS: 5.954803990941097e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 24/71 | LOSS: 5.976936427032342e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 25/71 | LOSS: 6.0559894581484305e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 26/71 | LOSS: 6.030638596038679e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 27/71 | LOSS: 5.986963904953362e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 28/71 | LOSS: 6.038184092803617e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 29/71 | LOSS: 5.987438938367025e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 30/71 | LOSS: 5.986636493147554e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 31/71 | LOSS: 5.960148101280538e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 32/71 | LOSS: 5.928799606463696e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 33/71 | LOSS: 5.930357086553533e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 34/71 | LOSS: 5.8708403880051e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 35/71 | LOSS: 5.884411077911662e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 36/71 | LOSS: 5.8469157846883646e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 37/71 | LOSS: 5.831923030391887e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 38/71 | LOSS: 5.831802529708231e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 39/71 | LOSS: 5.807797469969955e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 40/71 | LOSS: 5.826889452980206e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 41/71 | LOSS: 5.803652616772784e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 42/71 | LOSS: 5.80556518819554e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 43/71 | LOSS: 5.805683434284054e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 44/71 | LOSS: 5.786526172515652e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 45/71 | LOSS: 5.80212867648895e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 46/71 | LOSS: 5.756722188418783e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 47/71 | LOSS: 5.764680589474362e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 48/71 | LOSS: 5.759528202899525e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 49/71 | LOSS: 5.7393920769754915e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 50/71 | LOSS: 5.777170733434853e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 51/71 | LOSS: 5.7442586668142985e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 52/71 | LOSS: 5.764683254590568e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 53/71 | LOSS: 5.73706036330232e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 54/71 | LOSS: 5.70404568779023e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 55/71 | LOSS: 5.675623538893758e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 56/71 | LOSS: 5.675595806072427e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 57/71 | LOSS: 5.654424014743387e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 58/71 | LOSS: 5.61373583538733e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 59/71 | LOSS: 5.596965885009316e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 60/71 | LOSS: 5.568865181423283e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 61/71 | LOSS: 5.58090633354192e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 62/71 | LOSS: 5.553831117919929e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 63/71 | LOSS: 5.528771144014399e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 64/71 | LOSS: 5.496065093141694e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 65/71 | LOSS: 5.478352966637238e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 66/71 | LOSS: 5.474535508038747e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 67/71 | LOSS: 5.455341492426455e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 68/71 | LOSS: 5.4493371348855915e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 69/71 | LOSS: 5.443517024364805e-06\n",
      "TRAIN: EPOCH 647/1000 | BATCH 70/71 | LOSS: 5.4517479480304065e-06\n",
      "VAL: EPOCH 647/1000 | BATCH 0/8 | LOSS: 3.871614353556652e-06\n",
      "VAL: EPOCH 647/1000 | BATCH 1/8 | LOSS: 3.952186489186715e-06\n",
      "VAL: EPOCH 647/1000 | BATCH 2/8 | LOSS: 4.12316361083261e-06\n",
      "VAL: EPOCH 647/1000 | BATCH 3/8 | LOSS: 4.15355168570386e-06\n",
      "VAL: EPOCH 647/1000 | BATCH 4/8 | LOSS: 4.228365287417546e-06\n",
      "VAL: EPOCH 647/1000 | BATCH 5/8 | LOSS: 4.283176470683732e-06\n",
      "VAL: EPOCH 647/1000 | BATCH 6/8 | LOSS: 4.210699736566832e-06\n",
      "VAL: EPOCH 647/1000 | BATCH 7/8 | LOSS: 4.097965558003125e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 0/71 | LOSS: 5.322709967003902e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 1/71 | LOSS: 5.483710310727474e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 2/71 | LOSS: 5.442642304842593e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 3/71 | LOSS: 5.0047061677105376e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 4/71 | LOSS: 4.933311811328167e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 5/71 | LOSS: 5.006259925721679e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 6/71 | LOSS: 5.3093263626838705e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 7/71 | LOSS: 5.070492221648237e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 8/71 | LOSS: 5.145481458789112e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 9/71 | LOSS: 5.072100407232938e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 10/71 | LOSS: 5.117317062864789e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 11/71 | LOSS: 5.302456296855477e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 12/71 | LOSS: 5.3031616551799325e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 13/71 | LOSS: 5.450528192341153e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 14/71 | LOSS: 5.53562790628348e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 15/71 | LOSS: 5.580311650987824e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 16/71 | LOSS: 5.7101975765844515e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 17/71 | LOSS: 5.672831914833094e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 18/71 | LOSS: 5.8510068260817975e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 19/71 | LOSS: 5.788282840057945e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 20/71 | LOSS: 5.848251465154351e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 21/71 | LOSS: 5.734231535825529e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 22/71 | LOSS: 5.7248211424334645e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 23/71 | LOSS: 5.679983037983523e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 24/71 | LOSS: 5.637953072437085e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 25/71 | LOSS: 5.6506215654595644e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 26/71 | LOSS: 5.6003534693984505e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 27/71 | LOSS: 5.632493915592411e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 28/71 | LOSS: 5.5686952754481826e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 29/71 | LOSS: 5.53862359993218e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 30/71 | LOSS: 5.471040704803947e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 31/71 | LOSS: 5.482201856921165e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 32/71 | LOSS: 5.397814179146239e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 33/71 | LOSS: 5.346420099318921e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 34/71 | LOSS: 5.342078442741435e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 35/71 | LOSS: 5.299131588445663e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 36/71 | LOSS: 5.266336185397105e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 37/71 | LOSS: 5.247157936968219e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 38/71 | LOSS: 5.211761915164131e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 39/71 | LOSS: 5.178718203069365e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 40/71 | LOSS: 5.1527767659646955e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 41/71 | LOSS: 5.1297927202973546e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 42/71 | LOSS: 5.1109460178680775e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 43/71 | LOSS: 5.101184295984819e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 44/71 | LOSS: 5.102314889882109e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 45/71 | LOSS: 5.0631782642085454e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 46/71 | LOSS: 5.037674748351453e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 47/71 | LOSS: 5.016574471975825e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 48/71 | LOSS: 4.993239205147375e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 49/71 | LOSS: 4.960294099873863e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 50/71 | LOSS: 4.936664628771455e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 51/71 | LOSS: 4.905235952124853e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 52/71 | LOSS: 4.8830557965252325e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 53/71 | LOSS: 4.877395276202555e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 54/71 | LOSS: 4.871402982396169e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 55/71 | LOSS: 4.842878191928841e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 56/71 | LOSS: 4.824272365479837e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 57/71 | LOSS: 4.804096706407265e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 58/71 | LOSS: 4.796732779233657e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 59/71 | LOSS: 4.774734293278016e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 60/71 | LOSS: 4.757335311117712e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 61/71 | LOSS: 4.744776648998771e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 62/71 | LOSS: 4.74464168147524e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 63/71 | LOSS: 4.733922530419932e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 64/71 | LOSS: 4.716393672984291e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 65/71 | LOSS: 4.709459345218475e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 66/71 | LOSS: 4.729160584464631e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 67/71 | LOSS: 4.72928008869477e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 68/71 | LOSS: 4.745925977751039e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 69/71 | LOSS: 4.768044657664307e-06\n",
      "TRAIN: EPOCH 648/1000 | BATCH 70/71 | LOSS: 4.735080742199937e-06\n",
      "VAL: EPOCH 648/1000 | BATCH 0/8 | LOSS: 3.3552851164131425e-06\n",
      "VAL: EPOCH 648/1000 | BATCH 1/8 | LOSS: 3.775829554797383e-06\n",
      "VAL: EPOCH 648/1000 | BATCH 2/8 | LOSS: 3.932198220960951e-06\n",
      "VAL: EPOCH 648/1000 | BATCH 3/8 | LOSS: 3.95364327232528e-06\n",
      "VAL: EPOCH 648/1000 | BATCH 4/8 | LOSS: 3.962891423725523e-06\n",
      "VAL: EPOCH 648/1000 | BATCH 5/8 | LOSS: 4.0267029817186994e-06\n",
      "VAL: EPOCH 648/1000 | BATCH 6/8 | LOSS: 3.894004781354202e-06\n",
      "VAL: EPOCH 648/1000 | BATCH 7/8 | LOSS: 3.7919280657661147e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 0/71 | LOSS: 3.899898274539737e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 1/71 | LOSS: 3.881574457409442e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 2/71 | LOSS: 3.6510779561164477e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 3/71 | LOSS: 3.714267222676426e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 4/71 | LOSS: 3.6088039450987707e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 5/71 | LOSS: 3.6735092029023995e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 6/71 | LOSS: 3.707483366659809e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 7/71 | LOSS: 3.7907241789980617e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 8/71 | LOSS: 3.8099141824608928e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 9/71 | LOSS: 3.790568894146418e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 10/71 | LOSS: 3.893955243422004e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 11/71 | LOSS: 3.894011247969805e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 12/71 | LOSS: 4.011673931577557e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 13/71 | LOSS: 3.943078825484138e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 14/71 | LOSS: 4.081513316123164e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 15/71 | LOSS: 4.068881551688719e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 16/71 | LOSS: 4.034955469144145e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 17/71 | LOSS: 4.089881902776445e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 18/71 | LOSS: 4.084132378442232e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 19/71 | LOSS: 4.111352734526008e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 20/71 | LOSS: 4.071580460298546e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 21/71 | LOSS: 4.0849887661600155e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 22/71 | LOSS: 4.110726452025752e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 23/71 | LOSS: 4.1137014648029435e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 24/71 | LOSS: 4.083828862349037e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 25/71 | LOSS: 4.062466980660527e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 26/71 | LOSS: 4.108473056610954e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 27/71 | LOSS: 4.076584023745714e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 28/71 | LOSS: 4.064081159758016e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 29/71 | LOSS: 4.088959462933417e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 30/71 | LOSS: 4.097368578333126e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 31/71 | LOSS: 4.090671517076316e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 32/71 | LOSS: 4.070864006831526e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 33/71 | LOSS: 4.065452799295599e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 34/71 | LOSS: 4.0901359105711374e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 35/71 | LOSS: 4.107244996652702e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 36/71 | LOSS: 4.110056204661157e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 37/71 | LOSS: 4.160684122660696e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 38/71 | LOSS: 4.12670694971987e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 39/71 | LOSS: 4.114348081429853e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 40/71 | LOSS: 4.089020832197559e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 41/71 | LOSS: 4.082269475431877e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 42/71 | LOSS: 4.075282065461452e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 43/71 | LOSS: 4.089516566678371e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 44/71 | LOSS: 4.0923936012404735e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 45/71 | LOSS: 4.0809213813823275e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 46/71 | LOSS: 4.06888253526128e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 47/71 | LOSS: 4.05716619411578e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 48/71 | LOSS: 4.03865193239115e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 49/71 | LOSS: 4.03122323405114e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 50/71 | LOSS: 4.031550815852825e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 51/71 | LOSS: 4.022950459050033e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 52/71 | LOSS: 4.035183801019195e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 53/71 | LOSS: 4.032832765477468e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 54/71 | LOSS: 4.044708244195631e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 55/71 | LOSS: 4.056025464446325e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 56/71 | LOSS: 4.0738614667421455e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 57/71 | LOSS: 4.076945930952708e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 58/71 | LOSS: 4.0796715639170525e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 59/71 | LOSS: 4.085818144024719e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 60/71 | LOSS: 4.07566719823975e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 61/71 | LOSS: 4.070926930551415e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 62/71 | LOSS: 4.071386146033691e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 63/71 | LOSS: 4.061694571788621e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 64/71 | LOSS: 4.0688914815781085e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 65/71 | LOSS: 4.058915385829812e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 66/71 | LOSS: 4.052019418320843e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 67/71 | LOSS: 4.083222182436978e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 68/71 | LOSS: 4.068057396052873e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 69/71 | LOSS: 4.0664022273111705e-06\n",
      "TRAIN: EPOCH 649/1000 | BATCH 70/71 | LOSS: 4.068842766263144e-06\n",
      "VAL: EPOCH 649/1000 | BATCH 0/8 | LOSS: 4.5588476496050134e-06\n",
      "VAL: EPOCH 649/1000 | BATCH 1/8 | LOSS: 4.671285296353744e-06\n",
      "VAL: EPOCH 649/1000 | BATCH 2/8 | LOSS: 4.712125701189507e-06\n",
      "VAL: EPOCH 649/1000 | BATCH 3/8 | LOSS: 4.654435997508699e-06\n",
      "VAL: EPOCH 649/1000 | BATCH 4/8 | LOSS: 4.713456291938201e-06\n",
      "VAL: EPOCH 649/1000 | BATCH 5/8 | LOSS: 4.6847585508658085e-06\n",
      "VAL: EPOCH 649/1000 | BATCH 6/8 | LOSS: 4.546476800117359e-06\n",
      "VAL: EPOCH 649/1000 | BATCH 7/8 | LOSS: 4.411746033383679e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 0/71 | LOSS: 5.33265574631514e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 1/71 | LOSS: 4.15855845403712e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 2/71 | LOSS: 4.254672679356493e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 3/71 | LOSS: 4.1000368469212845e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 4/71 | LOSS: 4.197811585981981e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 5/71 | LOSS: 4.0340407849726034e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 6/71 | LOSS: 4.088675723323831e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 7/71 | LOSS: 4.033183415685926e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 8/71 | LOSS: 4.1309831431135535e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 9/71 | LOSS: 4.0808086396282304e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 10/71 | LOSS: 4.079613848840712e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 11/71 | LOSS: 4.168613600086246e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 12/71 | LOSS: 4.110076973190119e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 13/71 | LOSS: 4.104974615464536e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 14/71 | LOSS: 4.080422362070142e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 15/71 | LOSS: 4.072461564419427e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 16/71 | LOSS: 4.0332465954741245e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 17/71 | LOSS: 4.033503602032498e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 18/71 | LOSS: 4.004296550575466e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 19/71 | LOSS: 4.0230896161119745e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 20/71 | LOSS: 4.005560781262743e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 21/71 | LOSS: 3.971725401573746e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 22/71 | LOSS: 3.9918316301539205e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 23/71 | LOSS: 4.008957669536055e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 24/71 | LOSS: 3.976362440880621e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 25/71 | LOSS: 3.998792320859278e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 26/71 | LOSS: 4.028005133915468e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 27/71 | LOSS: 4.005532332485018e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 28/71 | LOSS: 4.014761120272632e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 29/71 | LOSS: 4.023082086253756e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 30/71 | LOSS: 4.021499916557185e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 31/71 | LOSS: 4.006365749376073e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 32/71 | LOSS: 4.0111377102496615e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 33/71 | LOSS: 4.012383703197616e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 34/71 | LOSS: 4.04254080552034e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 35/71 | LOSS: 4.047051049838046e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 36/71 | LOSS: 4.066503870981742e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 37/71 | LOSS: 4.090087683930745e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 38/71 | LOSS: 4.07371047190398e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 39/71 | LOSS: 4.077376939903843e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 40/71 | LOSS: 4.095261479827658e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 41/71 | LOSS: 4.100028453043099e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 42/71 | LOSS: 4.120991058888923e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 43/71 | LOSS: 4.125131769424942e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 44/71 | LOSS: 4.126122040462279e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 45/71 | LOSS: 4.1381491358982245e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 46/71 | LOSS: 4.131043653537648e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 47/71 | LOSS: 4.145069776010739e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 48/71 | LOSS: 4.1416363329721655e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 49/71 | LOSS: 4.1355245866725455e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 50/71 | LOSS: 4.154644387859198e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 51/71 | LOSS: 4.159543524250554e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 52/71 | LOSS: 4.152749865307618e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 53/71 | LOSS: 4.156144086664426e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 54/71 | LOSS: 4.159212643240939e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 55/71 | LOSS: 4.165724419635808e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 56/71 | LOSS: 4.192387207200354e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 57/71 | LOSS: 4.177315962931908e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 58/71 | LOSS: 4.1927858130632446e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 59/71 | LOSS: 4.185500180634941e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 60/71 | LOSS: 4.207122587939041e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 61/71 | LOSS: 4.201549700155738e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 62/71 | LOSS: 4.210181833470761e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 63/71 | LOSS: 4.201406042625422e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 64/71 | LOSS: 4.209516093676659e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 65/71 | LOSS: 4.2470832423449876e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 66/71 | LOSS: 4.234062055444672e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 67/71 | LOSS: 4.254190598123173e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 68/71 | LOSS: 4.272936377568892e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 69/71 | LOSS: 4.283598965620123e-06\n",
      "TRAIN: EPOCH 650/1000 | BATCH 70/71 | LOSS: 4.289417562099964e-06\n",
      "VAL: EPOCH 650/1000 | BATCH 0/8 | LOSS: 4.838785571337212e-06\n",
      "VAL: EPOCH 650/1000 | BATCH 1/8 | LOSS: 5.079651600681245e-06\n",
      "VAL: EPOCH 650/1000 | BATCH 2/8 | LOSS: 5.044787940278184e-06\n",
      "VAL: EPOCH 650/1000 | BATCH 3/8 | LOSS: 4.929496412842127e-06\n",
      "VAL: EPOCH 650/1000 | BATCH 4/8 | LOSS: 5.003830847272184e-06\n",
      "VAL: EPOCH 650/1000 | BATCH 5/8 | LOSS: 4.844126427390923e-06\n",
      "VAL: EPOCH 650/1000 | BATCH 6/8 | LOSS: 4.718594482255867e-06\n",
      "VAL: EPOCH 650/1000 | BATCH 7/8 | LOSS: 4.56957889127807e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 0/71 | LOSS: 4.925120265397709e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 1/71 | LOSS: 5.219640115683433e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 2/71 | LOSS: 5.054391597999104e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 3/71 | LOSS: 5.399072165346297e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 4/71 | LOSS: 4.995588642486837e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 5/71 | LOSS: 4.656732168465775e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 6/71 | LOSS: 4.572785331090147e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 7/71 | LOSS: 4.607181665505777e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 8/71 | LOSS: 4.703642743051104e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 9/71 | LOSS: 4.751955384563189e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 10/71 | LOSS: 4.627041252288936e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 11/71 | LOSS: 4.614227104108674e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 12/71 | LOSS: 4.515546519402191e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 13/71 | LOSS: 4.477720543556123e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 14/71 | LOSS: 4.476106172054036e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 15/71 | LOSS: 4.505969357637696e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 16/71 | LOSS: 4.506692100810196e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 17/71 | LOSS: 4.5217060965620076e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 18/71 | LOSS: 4.548068617575489e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 19/71 | LOSS: 4.511466534040664e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 20/71 | LOSS: 4.491081729914488e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 21/71 | LOSS: 4.519354113406073e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 22/71 | LOSS: 4.507337576876085e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 23/71 | LOSS: 4.472013500844696e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 24/71 | LOSS: 4.485737017603242e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 25/71 | LOSS: 4.4867866790809785e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 26/71 | LOSS: 4.482282553942275e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 27/71 | LOSS: 4.439285825290946e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 28/71 | LOSS: 4.438898305783494e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 29/71 | LOSS: 4.4552812672312335e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 30/71 | LOSS: 4.418231033161001e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 31/71 | LOSS: 4.432066027959536e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 32/71 | LOSS: 4.425079575384823e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 33/71 | LOSS: 4.405015428186316e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 34/71 | LOSS: 4.397498384735497e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 35/71 | LOSS: 4.3672734351376776e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 36/71 | LOSS: 4.351027206900432e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 37/71 | LOSS: 4.3488347740238075e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 38/71 | LOSS: 4.328152791015619e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 39/71 | LOSS: 4.324732481109095e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 40/71 | LOSS: 4.333873664841011e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 41/71 | LOSS: 4.306443191212782e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 42/71 | LOSS: 4.299731472201529e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 43/71 | LOSS: 4.294279614731005e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 44/71 | LOSS: 4.29115969685275e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 45/71 | LOSS: 4.2940078193841265e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 46/71 | LOSS: 4.305620324614326e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 47/71 | LOSS: 4.292637735640407e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 48/71 | LOSS: 4.289513563723018e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 49/71 | LOSS: 4.275294149920228e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 50/71 | LOSS: 4.272398218388701e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 51/71 | LOSS: 4.2743325995895875e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 52/71 | LOSS: 4.263341460895356e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 53/71 | LOSS: 4.2878290750431245e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 54/71 | LOSS: 4.2848826110871e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 55/71 | LOSS: 4.284139095034334e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 56/71 | LOSS: 4.283395690818954e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 57/71 | LOSS: 4.292217344582555e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 58/71 | LOSS: 4.312644921163279e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 59/71 | LOSS: 4.3254441607132325e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 60/71 | LOSS: 4.354971090341693e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 61/71 | LOSS: 4.3366603676986415e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 62/71 | LOSS: 4.330990801492359e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 63/71 | LOSS: 4.314899182844556e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 64/71 | LOSS: 4.296548160290023e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 65/71 | LOSS: 4.30732698629127e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 66/71 | LOSS: 4.301980235181669e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 67/71 | LOSS: 4.306805247806016e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 68/71 | LOSS: 4.294770603693273e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 69/71 | LOSS: 4.301913272034394e-06\n",
      "TRAIN: EPOCH 651/1000 | BATCH 70/71 | LOSS: 4.308727729436778e-06\n",
      "VAL: EPOCH 651/1000 | BATCH 0/8 | LOSS: 3.7263707781676203e-06\n",
      "VAL: EPOCH 651/1000 | BATCH 1/8 | LOSS: 4.324775090935873e-06\n",
      "VAL: EPOCH 651/1000 | BATCH 2/8 | LOSS: 4.61760904120941e-06\n",
      "VAL: EPOCH 651/1000 | BATCH 3/8 | LOSS: 4.711590349870676e-06\n",
      "VAL: EPOCH 651/1000 | BATCH 4/8 | LOSS: 4.75294118587044e-06\n",
      "VAL: EPOCH 651/1000 | BATCH 5/8 | LOSS: 4.9235860994182685e-06\n",
      "VAL: EPOCH 651/1000 | BATCH 6/8 | LOSS: 4.833172429893498e-06\n",
      "VAL: EPOCH 651/1000 | BATCH 7/8 | LOSS: 4.78809079140774e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 0/71 | LOSS: 4.584309863275848e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 1/71 | LOSS: 4.866975814366015e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 2/71 | LOSS: 4.715999542289258e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 3/71 | LOSS: 4.737106792163104e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 4/71 | LOSS: 4.431789238878991e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 5/71 | LOSS: 4.420461740058575e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 6/71 | LOSS: 4.342526218741634e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 7/71 | LOSS: 4.50990233957782e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 8/71 | LOSS: 4.5530110684113524e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 9/71 | LOSS: 4.5664794470212655e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 10/71 | LOSS: 4.593614076756322e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 11/71 | LOSS: 4.787432430930494e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 12/71 | LOSS: 4.783012277934736e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 13/71 | LOSS: 4.770335246056285e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 14/71 | LOSS: 4.871008604823146e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 15/71 | LOSS: 4.8554231852904195e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 16/71 | LOSS: 4.896149488665876e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 17/71 | LOSS: 4.8423566012287565e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 18/71 | LOSS: 4.848318583658272e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 19/71 | LOSS: 4.777986612225505e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 20/71 | LOSS: 4.803425359308936e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 21/71 | LOSS: 4.760479720806126e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 22/71 | LOSS: 4.693523499593974e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 23/71 | LOSS: 4.653437571278118e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 24/71 | LOSS: 4.599957828759216e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 25/71 | LOSS: 4.594723892436237e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 26/71 | LOSS: 4.542670947355671e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 27/71 | LOSS: 4.516109852050119e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 28/71 | LOSS: 4.501060684874417e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 29/71 | LOSS: 4.482217059376126e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 30/71 | LOSS: 4.4667058685468695e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 31/71 | LOSS: 4.487623350257763e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 32/71 | LOSS: 4.468100686691204e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 33/71 | LOSS: 4.480407964739938e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 34/71 | LOSS: 4.460093902837668e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 35/71 | LOSS: 4.445787447416984e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 36/71 | LOSS: 4.430853790393952e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 37/71 | LOSS: 4.399182982620244e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 38/71 | LOSS: 4.371286031994551e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 39/71 | LOSS: 4.39111372543266e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 40/71 | LOSS: 4.389686293462746e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 41/71 | LOSS: 4.384924024981003e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 42/71 | LOSS: 4.377838068162524e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 43/71 | LOSS: 4.387990140292624e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 44/71 | LOSS: 4.383178010256314e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 45/71 | LOSS: 4.367005638651671e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 46/71 | LOSS: 4.402029399569344e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 47/71 | LOSS: 4.389263546992576e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 48/71 | LOSS: 4.380291487422608e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 49/71 | LOSS: 4.414440440996259e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 50/71 | LOSS: 4.42326268949175e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 51/71 | LOSS: 4.408860622089168e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 52/71 | LOSS: 4.401384154883793e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 53/71 | LOSS: 4.44123182003548e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 54/71 | LOSS: 4.453900212617802e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 55/71 | LOSS: 4.4487312607088825e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 56/71 | LOSS: 4.452515384943118e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 57/71 | LOSS: 4.451087659555046e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 58/71 | LOSS: 4.475859896047041e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 59/71 | LOSS: 4.462777368037981e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 60/71 | LOSS: 4.471576817120906e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 61/71 | LOSS: 4.481632590942618e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 62/71 | LOSS: 4.459900689350131e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 63/71 | LOSS: 4.44492678042252e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 64/71 | LOSS: 4.450428178433168e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 65/71 | LOSS: 4.455298032939247e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 66/71 | LOSS: 4.443241459060784e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 67/71 | LOSS: 4.428908382311313e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 68/71 | LOSS: 4.414508135016439e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 69/71 | LOSS: 4.410415772326814e-06\n",
      "TRAIN: EPOCH 652/1000 | BATCH 70/71 | LOSS: 4.416682268641471e-06\n",
      "VAL: EPOCH 652/1000 | BATCH 0/8 | LOSS: 3.7409931792353746e-06\n",
      "VAL: EPOCH 652/1000 | BATCH 1/8 | LOSS: 3.985360535807558e-06\n",
      "VAL: EPOCH 652/1000 | BATCH 2/8 | LOSS: 4.107456637332992e-06\n",
      "VAL: EPOCH 652/1000 | BATCH 3/8 | LOSS: 4.0414381601294735e-06\n",
      "VAL: EPOCH 652/1000 | BATCH 4/8 | LOSS: 4.112038095627213e-06\n",
      "VAL: EPOCH 652/1000 | BATCH 5/8 | LOSS: 4.075284778082278e-06\n",
      "VAL: EPOCH 652/1000 | BATCH 6/8 | LOSS: 3.954571768969929e-06\n",
      "VAL: EPOCH 652/1000 | BATCH 7/8 | LOSS: 3.80961765245047e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 0/71 | LOSS: 3.1856325222179294e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 1/71 | LOSS: 3.7969075492583215e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 2/71 | LOSS: 3.7623545570871406e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 3/71 | LOSS: 3.893358950790571e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 4/71 | LOSS: 3.833507389572333e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 5/71 | LOSS: 3.756915968248601e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 6/71 | LOSS: 3.7352369547859e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 7/71 | LOSS: 3.877089426396196e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 8/71 | LOSS: 4.030106614057634e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 9/71 | LOSS: 4.029021670248767e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 10/71 | LOSS: 3.987713088546033e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 11/71 | LOSS: 4.074104860289178e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 12/71 | LOSS: 4.0376993148618985e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 13/71 | LOSS: 4.152106742237395e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 14/71 | LOSS: 4.137651891748343e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 15/71 | LOSS: 4.100495488046363e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 16/71 | LOSS: 4.214488800404497e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 17/71 | LOSS: 4.191275492202193e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 18/71 | LOSS: 4.2281840251234826e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 19/71 | LOSS: 4.257711179889157e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 20/71 | LOSS: 4.360150991747755e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 21/71 | LOSS: 4.369920836986487e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 22/71 | LOSS: 4.378701345083992e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 23/71 | LOSS: 4.559780639586582e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 24/71 | LOSS: 4.6027067764953245e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 25/71 | LOSS: 4.688159204026586e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 26/71 | LOSS: 4.743332232869256e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 27/71 | LOSS: 4.8081438665446645e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 28/71 | LOSS: 4.813871422309854e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 29/71 | LOSS: 4.829577649919277e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 30/71 | LOSS: 4.820001469206838e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 31/71 | LOSS: 4.841947188083395e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 32/71 | LOSS: 4.823389851728941e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 33/71 | LOSS: 4.800066001146607e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 34/71 | LOSS: 4.827399113440021e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 35/71 | LOSS: 4.8098719389599864e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 36/71 | LOSS: 4.7952310868257315e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 37/71 | LOSS: 4.786946934281762e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 38/71 | LOSS: 4.804707795977802e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 39/71 | LOSS: 4.771434043959744e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 40/71 | LOSS: 4.755246142694221e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 41/71 | LOSS: 4.740328633834863e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 42/71 | LOSS: 4.758919954457629e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 43/71 | LOSS: 4.733424759706395e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 44/71 | LOSS: 4.722951371149975e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 45/71 | LOSS: 4.745675996871243e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 46/71 | LOSS: 4.7472587413451155e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 47/71 | LOSS: 4.726134321231257e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 48/71 | LOSS: 4.731707618628541e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 49/71 | LOSS: 4.723430515696237e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 50/71 | LOSS: 4.702672939339548e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 51/71 | LOSS: 4.694877650133723e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 52/71 | LOSS: 4.67844023969176e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 53/71 | LOSS: 4.653604441450119e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 54/71 | LOSS: 4.655171911012572e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 55/71 | LOSS: 4.6462247139191144e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 56/71 | LOSS: 4.65635871717639e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 57/71 | LOSS: 4.65456959130373e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 58/71 | LOSS: 4.637413285771321e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 59/71 | LOSS: 4.6332935047151596e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 60/71 | LOSS: 4.6187544329932575e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 61/71 | LOSS: 4.6063465678373415e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 62/71 | LOSS: 4.598597859021827e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 63/71 | LOSS: 4.613610975923166e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 64/71 | LOSS: 4.604270634445129e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 65/71 | LOSS: 4.589152205991206e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 66/71 | LOSS: 4.596327709072311e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 67/71 | LOSS: 4.579033817636253e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 68/71 | LOSS: 4.571193170230127e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 69/71 | LOSS: 4.564668987378744e-06\n",
      "TRAIN: EPOCH 653/1000 | BATCH 70/71 | LOSS: 4.566899854542268e-06\n",
      "VAL: EPOCH 653/1000 | BATCH 0/8 | LOSS: 3.801774937528535e-06\n",
      "VAL: EPOCH 653/1000 | BATCH 1/8 | LOSS: 4.0306430264536175e-06\n",
      "VAL: EPOCH 653/1000 | BATCH 2/8 | LOSS: 4.1264007298498955e-06\n",
      "VAL: EPOCH 653/1000 | BATCH 3/8 | LOSS: 4.069481576607359e-06\n",
      "VAL: EPOCH 653/1000 | BATCH 4/8 | LOSS: 4.143851174376323e-06\n",
      "VAL: EPOCH 653/1000 | BATCH 5/8 | LOSS: 4.155613131236653e-06\n",
      "VAL: EPOCH 653/1000 | BATCH 6/8 | LOSS: 4.048832417018795e-06\n",
      "VAL: EPOCH 653/1000 | BATCH 7/8 | LOSS: 3.942241221466247e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 0/71 | LOSS: 3.330278559587896e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 1/71 | LOSS: 3.560884124453878e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 2/71 | LOSS: 3.6338058180263033e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 3/71 | LOSS: 3.7442142115651222e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 4/71 | LOSS: 3.6779496895178456e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 5/71 | LOSS: 3.710391752065334e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 6/71 | LOSS: 3.861094065931476e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 7/71 | LOSS: 3.930638996507696e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 8/71 | LOSS: 3.814252699562025e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 9/71 | LOSS: 3.979522875852126e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 10/71 | LOSS: 3.968595034497081e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 11/71 | LOSS: 3.974862977429439e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 12/71 | LOSS: 3.9074325111589414e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 13/71 | LOSS: 4.002547890585058e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 14/71 | LOSS: 4.0676088701729896e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 15/71 | LOSS: 4.059968475189635e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 16/71 | LOSS: 4.07591162055563e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 17/71 | LOSS: 4.072599002332329e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 18/71 | LOSS: 4.060285786372812e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 19/71 | LOSS: 4.069636213444028e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 20/71 | LOSS: 4.106636911923747e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 21/71 | LOSS: 4.110909778567227e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 22/71 | LOSS: 4.102226769290333e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 23/71 | LOSS: 4.157292162669061e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 24/71 | LOSS: 4.17317011851992e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 25/71 | LOSS: 4.197700639871562e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 26/71 | LOSS: 4.205504890503815e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 27/71 | LOSS: 4.1923984700328476e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 28/71 | LOSS: 4.252107240470901e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 29/71 | LOSS: 4.267450723697645e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 30/71 | LOSS: 4.258211058714228e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 31/71 | LOSS: 4.256249418688185e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 32/71 | LOSS: 4.269861640502561e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 33/71 | LOSS: 4.2830671150916125e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 34/71 | LOSS: 4.284343471486604e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 35/71 | LOSS: 4.356635795627679e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 36/71 | LOSS: 4.355678092414982e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 37/71 | LOSS: 4.391874432784741e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 38/71 | LOSS: 4.39251553311456e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 39/71 | LOSS: 4.40033507516091e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 40/71 | LOSS: 4.417495552488395e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 41/71 | LOSS: 4.397418365442718e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 42/71 | LOSS: 4.4013451273247825e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 43/71 | LOSS: 4.397659903464625e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 44/71 | LOSS: 4.3981658412424926e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 45/71 | LOSS: 4.404327500117728e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 46/71 | LOSS: 4.426591382806246e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 47/71 | LOSS: 4.403526001321249e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 48/71 | LOSS: 4.394723261185038e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 49/71 | LOSS: 4.404216833790997e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 50/71 | LOSS: 4.413155078081592e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 51/71 | LOSS: 4.383284546677784e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 52/71 | LOSS: 4.377682740115868e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 53/71 | LOSS: 4.35592289283738e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 54/71 | LOSS: 4.344233056227412e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 55/71 | LOSS: 4.354829396139134e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 56/71 | LOSS: 4.348974746455114e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 57/71 | LOSS: 4.356733421477186e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 58/71 | LOSS: 4.352932503518217e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 59/71 | LOSS: 4.371255662742138e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 60/71 | LOSS: 4.356340919822869e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 61/71 | LOSS: 4.386633936666195e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 62/71 | LOSS: 4.375909727045043e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 63/71 | LOSS: 4.385902407477715e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 64/71 | LOSS: 4.373907268018229e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 65/71 | LOSS: 4.366876847791101e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 66/71 | LOSS: 4.3599611807852605e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 67/71 | LOSS: 4.358068816359532e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 68/71 | LOSS: 4.363225541411616e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 69/71 | LOSS: 4.362897470855387e-06\n",
      "TRAIN: EPOCH 654/1000 | BATCH 70/71 | LOSS: 4.414710186300537e-06\n",
      "VAL: EPOCH 654/1000 | BATCH 0/8 | LOSS: 4.432215973793063e-06\n",
      "VAL: EPOCH 654/1000 | BATCH 1/8 | LOSS: 4.8248448365484364e-06\n",
      "VAL: EPOCH 654/1000 | BATCH 2/8 | LOSS: 4.93174896594913e-06\n",
      "VAL: EPOCH 654/1000 | BATCH 3/8 | LOSS: 4.937272478855448e-06\n",
      "VAL: EPOCH 654/1000 | BATCH 4/8 | LOSS: 4.888940929959063e-06\n",
      "VAL: EPOCH 654/1000 | BATCH 5/8 | LOSS: 4.8190286179305986e-06\n",
      "VAL: EPOCH 654/1000 | BATCH 6/8 | LOSS: 4.695991654963498e-06\n",
      "VAL: EPOCH 654/1000 | BATCH 7/8 | LOSS: 4.4789508422127255e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 0/71 | LOSS: 4.311433258408215e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 1/71 | LOSS: 5.91494563195738e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 2/71 | LOSS: 5.86751457376522e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 3/71 | LOSS: 5.965192599433067e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 4/71 | LOSS: 6.423009563150117e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 5/71 | LOSS: 6.227391850188724e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 6/71 | LOSS: 6.163293944285085e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 7/71 | LOSS: 6.091433363053511e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 8/71 | LOSS: 6.269658671145508e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 9/71 | LOSS: 5.951166804152308e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 10/71 | LOSS: 5.923611504179214e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 11/71 | LOSS: 5.8798189760030555e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 12/71 | LOSS: 5.84795830386261e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 13/71 | LOSS: 5.696466977107255e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 14/71 | LOSS: 5.736742029209078e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 15/71 | LOSS: 5.735361099823422e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 16/71 | LOSS: 5.72705651874832e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 17/71 | LOSS: 5.6173293311682455e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 18/71 | LOSS: 5.666950384789073e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 19/71 | LOSS: 5.6357187531830276e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 20/71 | LOSS: 5.672201734255179e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 21/71 | LOSS: 5.655540317920068e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 22/71 | LOSS: 5.611637057565689e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 23/71 | LOSS: 5.609836970658459e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 24/71 | LOSS: 5.646976587740937e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 25/71 | LOSS: 5.762968769956317e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 26/71 | LOSS: 5.7123452447908846e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 27/71 | LOSS: 5.749963033200142e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 28/71 | LOSS: 5.7235965241802505e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 29/71 | LOSS: 5.730913380830316e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 30/71 | LOSS: 5.821796138104307e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 31/71 | LOSS: 5.803405159099384e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 32/71 | LOSS: 5.8362816437720815e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 33/71 | LOSS: 5.794434561240076e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 34/71 | LOSS: 5.794344698577853e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 35/71 | LOSS: 5.79263536969342e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 36/71 | LOSS: 5.752144912745476e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 37/71 | LOSS: 5.776237314517187e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 38/71 | LOSS: 5.751628356311136e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 39/71 | LOSS: 5.7914493140742705e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 40/71 | LOSS: 5.748626324752766e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 41/71 | LOSS: 5.777232282915585e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 42/71 | LOSS: 5.7524811339579515e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 43/71 | LOSS: 5.7888030162509745e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 44/71 | LOSS: 5.784242875961354e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 45/71 | LOSS: 5.72549358524987e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 46/71 | LOSS: 5.713771083429356e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 47/71 | LOSS: 5.719328489324956e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 48/71 | LOSS: 5.765212772159282e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 49/71 | LOSS: 5.780043784398003e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 50/71 | LOSS: 5.7927975688596464e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 51/71 | LOSS: 5.866127218303938e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 52/71 | LOSS: 5.853750546932888e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 53/71 | LOSS: 5.884649664477803e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 54/71 | LOSS: 5.891412034874189e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 55/71 | LOSS: 5.9263546907979096e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 56/71 | LOSS: 5.933708180555984e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 57/71 | LOSS: 5.9080973799076485e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 58/71 | LOSS: 5.953772731534288e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 59/71 | LOSS: 5.948757628478537e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 60/71 | LOSS: 5.966866746214199e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 61/71 | LOSS: 5.933263177975013e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 62/71 | LOSS: 5.92568981768896e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 63/71 | LOSS: 5.9041443662977144e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 64/71 | LOSS: 5.8790050038973835e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 65/71 | LOSS: 5.853794074496501e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 66/71 | LOSS: 5.838958306629871e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 67/71 | LOSS: 5.832914974448881e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 68/71 | LOSS: 5.797164262269465e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 69/71 | LOSS: 5.784440177844122e-06\n",
      "TRAIN: EPOCH 655/1000 | BATCH 70/71 | LOSS: 5.783408647710556e-06\n",
      "VAL: EPOCH 655/1000 | BATCH 0/8 | LOSS: 3.564817234291695e-06\n",
      "VAL: EPOCH 655/1000 | BATCH 1/8 | LOSS: 4.000523176728166e-06\n",
      "VAL: EPOCH 655/1000 | BATCH 2/8 | LOSS: 4.0594800339022186e-06\n",
      "VAL: EPOCH 655/1000 | BATCH 3/8 | LOSS: 4.0505324250261765e-06\n",
      "VAL: EPOCH 655/1000 | BATCH 4/8 | LOSS: 4.051871837873478e-06\n",
      "VAL: EPOCH 655/1000 | BATCH 5/8 | LOSS: 4.108092525711982e-06\n",
      "VAL: EPOCH 655/1000 | BATCH 6/8 | LOSS: 3.984936126601367e-06\n",
      "VAL: EPOCH 655/1000 | BATCH 7/8 | LOSS: 3.872985786301797e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 0/71 | LOSS: 4.1724224502104335e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 1/71 | LOSS: 4.3515863126231125e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 2/71 | LOSS: 4.568581516650738e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 3/71 | LOSS: 4.288582772460359e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 4/71 | LOSS: 4.7227974391717e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 5/71 | LOSS: 4.727415443994687e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 6/71 | LOSS: 4.774581092143697e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 7/71 | LOSS: 4.668360134019167e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 8/71 | LOSS: 4.53353062665782e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 9/71 | LOSS: 4.463208347260661e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 10/71 | LOSS: 4.527366544690184e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 11/71 | LOSS: 4.5492269578062405e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 12/71 | LOSS: 4.464780151745519e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 13/71 | LOSS: 4.511437834610531e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 14/71 | LOSS: 4.439801008023399e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 15/71 | LOSS: 4.436372734062388e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 16/71 | LOSS: 4.4053571775079675e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 17/71 | LOSS: 4.305620740928538e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 18/71 | LOSS: 4.320299886354692e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 19/71 | LOSS: 4.288734749025025e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 20/71 | LOSS: 4.272993096708974e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 21/71 | LOSS: 4.251992364019812e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 22/71 | LOSS: 4.317796617175665e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 23/71 | LOSS: 4.321191037585474e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 24/71 | LOSS: 4.3171800462005196e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 25/71 | LOSS: 4.347416366866897e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 26/71 | LOSS: 4.364509617584257e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 27/71 | LOSS: 4.3591702641216604e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 28/71 | LOSS: 4.3813655605029865e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 29/71 | LOSS: 4.3710885014055146e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 30/71 | LOSS: 4.3304549146869835e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 31/71 | LOSS: 4.386992699778602e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 32/71 | LOSS: 4.393564722704468e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 33/71 | LOSS: 4.378407792875591e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 34/71 | LOSS: 4.363100522043948e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 35/71 | LOSS: 4.367072847344389e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 36/71 | LOSS: 4.390882082048306e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 37/71 | LOSS: 4.366790413338381e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 38/71 | LOSS: 4.381941899294198e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 39/71 | LOSS: 4.3667763009125334e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 40/71 | LOSS: 4.349555793607312e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 41/71 | LOSS: 4.3732383606506375e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 42/71 | LOSS: 4.402722625791496e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 43/71 | LOSS: 4.399700794072652e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 44/71 | LOSS: 4.432280618655366e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 45/71 | LOSS: 4.451734540729692e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 46/71 | LOSS: 4.457048546216956e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 47/71 | LOSS: 4.445545793411536e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 48/71 | LOSS: 4.443281109068229e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 49/71 | LOSS: 4.4644284753303505e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 50/71 | LOSS: 4.443492843183419e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 51/71 | LOSS: 4.431848774052364e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 52/71 | LOSS: 4.427565598680818e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 53/71 | LOSS: 4.423633072292432e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 54/71 | LOSS: 4.401560214649759e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 55/71 | LOSS: 4.418308078259413e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 56/71 | LOSS: 4.4140543139834785e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 57/71 | LOSS: 4.415239463296844e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 58/71 | LOSS: 4.412668666622791e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 59/71 | LOSS: 4.405653241216593e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 60/71 | LOSS: 4.388305502497595e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 61/71 | LOSS: 4.414253218724738e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 62/71 | LOSS: 4.416431166314324e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 63/71 | LOSS: 4.410891897066449e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 64/71 | LOSS: 4.404877801369786e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 65/71 | LOSS: 4.43474536124003e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 66/71 | LOSS: 4.425902376957012e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 67/71 | LOSS: 4.438183108344246e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 68/71 | LOSS: 4.428174792258727e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 69/71 | LOSS: 4.427859403222101e-06\n",
      "TRAIN: EPOCH 656/1000 | BATCH 70/71 | LOSS: 4.4028330341436074e-06\n",
      "VAL: EPOCH 656/1000 | BATCH 0/8 | LOSS: 3.4383449474262306e-06\n",
      "VAL: EPOCH 656/1000 | BATCH 1/8 | LOSS: 3.793433620558062e-06\n",
      "VAL: EPOCH 656/1000 | BATCH 2/8 | LOSS: 4.152605394362278e-06\n",
      "VAL: EPOCH 656/1000 | BATCH 3/8 | LOSS: 4.193140682673402e-06\n",
      "VAL: EPOCH 656/1000 | BATCH 4/8 | LOSS: 4.270221370461513e-06\n",
      "VAL: EPOCH 656/1000 | BATCH 5/8 | LOSS: 4.335079400637672e-06\n",
      "VAL: EPOCH 656/1000 | BATCH 6/8 | LOSS: 4.2864437546086265e-06\n",
      "VAL: EPOCH 656/1000 | BATCH 7/8 | LOSS: 4.215575074795197e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 0/71 | LOSS: 4.948117293679388e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 1/71 | LOSS: 4.229576688885572e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 2/71 | LOSS: 4.230443664710037e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 3/71 | LOSS: 4.111079419999442e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 4/71 | LOSS: 4.097245619050227e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 5/71 | LOSS: 4.169986368651735e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 6/71 | LOSS: 4.073507755362828e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 7/71 | LOSS: 4.06923902573908e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 8/71 | LOSS: 4.076244800267483e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 9/71 | LOSS: 4.02196565119084e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 10/71 | LOSS: 3.938123303338546e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 11/71 | LOSS: 3.946651531805401e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 12/71 | LOSS: 3.967097374977759e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 13/71 | LOSS: 3.999550569590481e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 14/71 | LOSS: 4.017089789461655e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 15/71 | LOSS: 4.092025051249948e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 16/71 | LOSS: 4.039847654519904e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 17/71 | LOSS: 4.007222918921292e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 18/71 | LOSS: 4.0174868924001015e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 19/71 | LOSS: 3.9568247984789195e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 20/71 | LOSS: 3.9806618154952525e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 21/71 | LOSS: 4.0190908831308745e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 22/71 | LOSS: 3.992570910945094e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 23/71 | LOSS: 3.963437308129869e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 24/71 | LOSS: 3.97689479541441e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 25/71 | LOSS: 3.976320552003297e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 26/71 | LOSS: 4.010016987700106e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 27/71 | LOSS: 4.036792810373819e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 28/71 | LOSS: 4.0275309119947606e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 29/71 | LOSS: 4.029073352285195e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 30/71 | LOSS: 4.058535758362737e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 31/71 | LOSS: 4.0376070202796654e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 32/71 | LOSS: 4.055663442737899e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 33/71 | LOSS: 4.071719177666048e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 34/71 | LOSS: 4.0750684807530235e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 35/71 | LOSS: 4.049207828100205e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 36/71 | LOSS: 4.092271170963146e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 37/71 | LOSS: 4.108644176133232e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 38/71 | LOSS: 4.1071090995851245e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 39/71 | LOSS: 4.090038748927327e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 40/71 | LOSS: 4.0971566061382805e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 41/71 | LOSS: 4.097869599545015e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 42/71 | LOSS: 4.100152584087905e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 43/71 | LOSS: 4.090275353973993e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 44/71 | LOSS: 4.0701898797124155e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 45/71 | LOSS: 4.054254186089763e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 46/71 | LOSS: 4.06527356352989e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 47/71 | LOSS: 4.065961263677309e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 48/71 | LOSS: 4.0761426543194455e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 49/71 | LOSS: 4.068011462550203e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 50/71 | LOSS: 4.0733020573408755e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 51/71 | LOSS: 4.05797203484326e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 52/71 | LOSS: 4.049905298398819e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 53/71 | LOSS: 4.056126756411147e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 54/71 | LOSS: 4.06034509978781e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 55/71 | LOSS: 4.067727774424514e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 56/71 | LOSS: 4.069184513897645e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 57/71 | LOSS: 4.1052411696565e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 58/71 | LOSS: 4.105915268617968e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 59/71 | LOSS: 4.105273603727255e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 60/71 | LOSS: 4.111723687036247e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 61/71 | LOSS: 4.126703915531952e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 62/71 | LOSS: 4.115418931409266e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 63/71 | LOSS: 4.117976544648627e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 64/71 | LOSS: 4.103079810682595e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 65/71 | LOSS: 4.100421006803641e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 66/71 | LOSS: 4.0866249145804415e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 67/71 | LOSS: 4.084062030575724e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 68/71 | LOSS: 4.082696144958364e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 69/71 | LOSS: 4.0864749150517e-06\n",
      "TRAIN: EPOCH 657/1000 | BATCH 70/71 | LOSS: 4.072847975973199e-06\n",
      "VAL: EPOCH 657/1000 | BATCH 0/8 | LOSS: 4.906639787805034e-06\n",
      "VAL: EPOCH 657/1000 | BATCH 1/8 | LOSS: 5.051233756603324e-06\n",
      "VAL: EPOCH 657/1000 | BATCH 2/8 | LOSS: 5.005613729736069e-06\n",
      "VAL: EPOCH 657/1000 | BATCH 3/8 | LOSS: 4.91033244998107e-06\n",
      "VAL: EPOCH 657/1000 | BATCH 4/8 | LOSS: 4.965197513229214e-06\n",
      "VAL: EPOCH 657/1000 | BATCH 5/8 | LOSS: 4.821238690055907e-06\n",
      "VAL: EPOCH 657/1000 | BATCH 6/8 | LOSS: 4.68631950362968e-06\n",
      "VAL: EPOCH 657/1000 | BATCH 7/8 | LOSS: 4.564358590641859e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 0/71 | LOSS: 4.794905180460773e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 1/71 | LOSS: 4.493359028856503e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 2/71 | LOSS: 4.233075893959419e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 3/71 | LOSS: 4.518567834566056e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 4/71 | LOSS: 4.880713913735235e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 5/71 | LOSS: 5.0151794918444166e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 6/71 | LOSS: 5.042258180765202e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 7/71 | LOSS: 5.024636777761771e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 8/71 | LOSS: 5.028143126158587e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 9/71 | LOSS: 4.863751792072435e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 10/71 | LOSS: 4.93004373228442e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 11/71 | LOSS: 4.844190508871786e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 12/71 | LOSS: 4.7777380831785895e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 13/71 | LOSS: 4.826238149949599e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 14/71 | LOSS: 4.837405322177802e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 15/71 | LOSS: 4.820921986947724e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 16/71 | LOSS: 4.864998947382178e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 17/71 | LOSS: 4.840878394437216e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 18/71 | LOSS: 4.756453922470284e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 19/71 | LOSS: 4.728478711513162e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 20/71 | LOSS: 4.678949716478764e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 21/71 | LOSS: 4.666709566498122e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 22/71 | LOSS: 4.629692640541496e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 23/71 | LOSS: 4.587488547258545e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 24/71 | LOSS: 4.593143166857772e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 25/71 | LOSS: 4.575195152238638e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 26/71 | LOSS: 4.593666946877622e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 27/71 | LOSS: 4.570357889731115e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 28/71 | LOSS: 4.608290902101074e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 29/71 | LOSS: 4.605440320422834e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 30/71 | LOSS: 4.619084613237177e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 31/71 | LOSS: 4.598004593958649e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 32/71 | LOSS: 4.632632736143988e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 33/71 | LOSS: 4.62027025391495e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 34/71 | LOSS: 4.608408133727997e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 35/71 | LOSS: 4.59973993353439e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 36/71 | LOSS: 4.57463701226319e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 37/71 | LOSS: 4.559393723387697e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 38/71 | LOSS: 4.536259742356681e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 39/71 | LOSS: 4.528854651653091e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 40/71 | LOSS: 4.503693972077511e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 41/71 | LOSS: 4.490477717159333e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 42/71 | LOSS: 4.446972038855419e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 43/71 | LOSS: 4.430770972744953e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 44/71 | LOSS: 4.407932642101918e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 45/71 | LOSS: 4.388657084350796e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 46/71 | LOSS: 4.37865630006622e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 47/71 | LOSS: 4.373781228158198e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 48/71 | LOSS: 4.359378496645855e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 49/71 | LOSS: 4.405307054184959e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 50/71 | LOSS: 4.422015749964082e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 51/71 | LOSS: 4.422554175457312e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 52/71 | LOSS: 4.411767684291161e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 53/71 | LOSS: 4.420417907466698e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 54/71 | LOSS: 4.429137499590235e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 55/71 | LOSS: 4.420749259484832e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 56/71 | LOSS: 4.419864860226466e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 57/71 | LOSS: 4.409700538636916e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 58/71 | LOSS: 4.417154386982922e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 59/71 | LOSS: 4.408390683844724e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 60/71 | LOSS: 4.422134284662907e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 61/71 | LOSS: 4.41020122546052e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 62/71 | LOSS: 4.402052831037076e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 63/71 | LOSS: 4.4005159942628325e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 64/71 | LOSS: 4.385937689641231e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 65/71 | LOSS: 4.387117917648346e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 66/71 | LOSS: 4.381920764071358e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 67/71 | LOSS: 4.361432137797879e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 68/71 | LOSS: 4.356716047276269e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 69/71 | LOSS: 4.344432811974132e-06\n",
      "TRAIN: EPOCH 658/1000 | BATCH 70/71 | LOSS: 4.3657900098712075e-06\n",
      "VAL: EPOCH 658/1000 | BATCH 0/8 | LOSS: 3.4727131605905015e-06\n",
      "VAL: EPOCH 658/1000 | BATCH 1/8 | LOSS: 4.020391770609422e-06\n",
      "VAL: EPOCH 658/1000 | BATCH 2/8 | LOSS: 4.245528089086292e-06\n",
      "VAL: EPOCH 658/1000 | BATCH 3/8 | LOSS: 4.259889806235151e-06\n",
      "VAL: EPOCH 658/1000 | BATCH 4/8 | LOSS: 4.273219474271173e-06\n",
      "VAL: EPOCH 658/1000 | BATCH 5/8 | LOSS: 4.2708088585641235e-06\n",
      "VAL: EPOCH 658/1000 | BATCH 6/8 | LOSS: 4.2013209297562885e-06\n",
      "VAL: EPOCH 658/1000 | BATCH 7/8 | LOSS: 4.1194043092218635e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 0/71 | LOSS: 4.458274815988261e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 1/71 | LOSS: 3.7648042052751407e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 2/71 | LOSS: 4.129613595675134e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 3/71 | LOSS: 4.019324478576891e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 4/71 | LOSS: 4.1226346183975695e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 5/71 | LOSS: 4.271809909065875e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 6/71 | LOSS: 4.150333162605031e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 7/71 | LOSS: 4.043955101451502e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 8/71 | LOSS: 3.998765704535698e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 9/71 | LOSS: 4.023373321615509e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 10/71 | LOSS: 3.998980756395295e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 11/71 | LOSS: 4.118081070222009e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 12/71 | LOSS: 4.12053187714124e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 13/71 | LOSS: 4.118206350410349e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 14/71 | LOSS: 4.105915695618023e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 15/71 | LOSS: 4.091245202175742e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 16/71 | LOSS: 4.083984349028255e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 17/71 | LOSS: 4.0842639540035934e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 18/71 | LOSS: 4.014279930312401e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 19/71 | LOSS: 3.969085162225383e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 20/71 | LOSS: 3.923083037686108e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 21/71 | LOSS: 3.92472486510087e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 22/71 | LOSS: 3.9868318214322915e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 23/71 | LOSS: 3.964775629583528e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 24/71 | LOSS: 3.971014912167448e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 25/71 | LOSS: 3.985348826063273e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 26/71 | LOSS: 3.97256212636421e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 27/71 | LOSS: 3.9608980760671586e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 28/71 | LOSS: 4.0281522458450315e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 29/71 | LOSS: 4.030222218413352e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 30/71 | LOSS: 4.023230868340526e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 31/71 | LOSS: 4.011865485153976e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 32/71 | LOSS: 3.990342155234292e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 33/71 | LOSS: 3.981365680641936e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 34/71 | LOSS: 3.976290996173962e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 35/71 | LOSS: 4.000528848438181e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 36/71 | LOSS: 3.990170595185387e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 37/71 | LOSS: 4.073144946472128e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 38/71 | LOSS: 4.059647363437062e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 39/71 | LOSS: 4.0723573363266045e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 40/71 | LOSS: 4.082016630491125e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 41/71 | LOSS: 4.076212424782473e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 42/71 | LOSS: 4.089601795152872e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 43/71 | LOSS: 4.094952182433851e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 44/71 | LOSS: 4.119461315591858e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 45/71 | LOSS: 4.095511208699621e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 46/71 | LOSS: 4.128163322519036e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 47/71 | LOSS: 4.1433342516938865e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 48/71 | LOSS: 4.168125078463998e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 49/71 | LOSS: 4.17141414800426e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 50/71 | LOSS: 4.182528443255019e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 51/71 | LOSS: 4.196452603257338e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 52/71 | LOSS: 4.191602037504056e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 53/71 | LOSS: 4.208132740704632e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 54/71 | LOSS: 4.197694943286479e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 55/71 | LOSS: 4.231495276079451e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 56/71 | LOSS: 4.22103755881197e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 57/71 | LOSS: 4.2182700310839775e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 58/71 | LOSS: 4.2030125722842555e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 59/71 | LOSS: 4.221485498116332e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 60/71 | LOSS: 4.207307058811134e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 61/71 | LOSS: 4.217120594177921e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 62/71 | LOSS: 4.218666126978369e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 63/71 | LOSS: 4.211723613423146e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 64/71 | LOSS: 4.214505392715532e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 65/71 | LOSS: 4.1929787151027336e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 66/71 | LOSS: 4.1861996490283745e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 67/71 | LOSS: 4.1920611124623974e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 68/71 | LOSS: 4.188204803026367e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 69/71 | LOSS: 4.181395089679946e-06\n",
      "TRAIN: EPOCH 659/1000 | BATCH 70/71 | LOSS: 4.169717042931949e-06\n",
      "VAL: EPOCH 659/1000 | BATCH 0/8 | LOSS: 3.343375738040777e-06\n",
      "VAL: EPOCH 659/1000 | BATCH 1/8 | LOSS: 3.6876381273032166e-06\n",
      "VAL: EPOCH 659/1000 | BATCH 2/8 | LOSS: 3.882667442667298e-06\n",
      "VAL: EPOCH 659/1000 | BATCH 3/8 | LOSS: 3.859036610265321e-06\n",
      "VAL: EPOCH 659/1000 | BATCH 4/8 | LOSS: 3.893367374985246e-06\n",
      "VAL: EPOCH 659/1000 | BATCH 5/8 | LOSS: 3.867751274810871e-06\n",
      "VAL: EPOCH 659/1000 | BATCH 6/8 | LOSS: 3.7543685331391837e-06\n",
      "VAL: EPOCH 659/1000 | BATCH 7/8 | LOSS: 3.6326841552636324e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 0/71 | LOSS: 3.7283452911651693e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 1/71 | LOSS: 3.772021045733709e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 2/71 | LOSS: 4.015021052813002e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 3/71 | LOSS: 4.1922056652765605e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 4/71 | LOSS: 4.166101734881522e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 5/71 | LOSS: 4.068059486902105e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 6/71 | LOSS: 3.982475781023302e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 7/71 | LOSS: 3.9714547028779634e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 8/71 | LOSS: 4.012636408232437e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 9/71 | LOSS: 4.081228371433099e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 10/71 | LOSS: 4.139030806982191e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 11/71 | LOSS: 4.177463741446748e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 12/71 | LOSS: 4.133395682751702e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 13/71 | LOSS: 4.233616469459126e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 14/71 | LOSS: 4.2092032951283425e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 15/71 | LOSS: 4.22672312083705e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 16/71 | LOSS: 4.212762579460532e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 17/71 | LOSS: 4.2232507490148746e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 18/71 | LOSS: 4.189579158264678e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 19/71 | LOSS: 4.163035532656067e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 20/71 | LOSS: 4.159521354484327e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 21/71 | LOSS: 4.173712423752559e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 22/71 | LOSS: 4.160641706023497e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 23/71 | LOSS: 4.122855794245576e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 24/71 | LOSS: 4.073673580933246e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 25/71 | LOSS: 4.059595461023071e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 26/71 | LOSS: 4.036036856363083e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 27/71 | LOSS: 4.026631237203608e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 28/71 | LOSS: 4.044767036185508e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 29/71 | LOSS: 4.044413155194586e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 30/71 | LOSS: 4.045042810356526e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 31/71 | LOSS: 4.053562712158509e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 32/71 | LOSS: 4.078794566367199e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 33/71 | LOSS: 4.095789128822456e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 34/71 | LOSS: 4.07898853284548e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 35/71 | LOSS: 4.06279313993865e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 36/71 | LOSS: 4.1166774612852225e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 37/71 | LOSS: 4.112042395383086e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 38/71 | LOSS: 4.190164692119698e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 39/71 | LOSS: 4.176129533561834e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 40/71 | LOSS: 4.256415356239148e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 41/71 | LOSS: 4.272852801737567e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 42/71 | LOSS: 4.312605877330074e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 43/71 | LOSS: 4.341831015608477e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 44/71 | LOSS: 4.375786279019343e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 45/71 | LOSS: 4.417440940731309e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 46/71 | LOSS: 4.469974144624157e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 47/71 | LOSS: 4.501436511835284e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 48/71 | LOSS: 4.5257770812078925e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 49/71 | LOSS: 4.628957026397984e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 50/71 | LOSS: 4.6527674180206125e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 51/71 | LOSS: 4.756078042716511e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 52/71 | LOSS: 4.754799857557441e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 53/71 | LOSS: 4.7987262245509625e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 54/71 | LOSS: 4.788731377464666e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 55/71 | LOSS: 4.819990550686271e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 56/71 | LOSS: 4.8232972714781924e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 57/71 | LOSS: 4.87350298734437e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 58/71 | LOSS: 4.880895665460769e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 59/71 | LOSS: 4.896151354690422e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 60/71 | LOSS: 4.906700504031251e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 61/71 | LOSS: 4.911069309386104e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 62/71 | LOSS: 4.92535690364815e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 63/71 | LOSS: 4.903533934452753e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 64/71 | LOSS: 4.931457267677895e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 65/71 | LOSS: 4.914515364251918e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 66/71 | LOSS: 4.9299081712969885e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 67/71 | LOSS: 4.915285578189308e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 68/71 | LOSS: 4.921822463971245e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 69/71 | LOSS: 4.923566185815226e-06\n",
      "TRAIN: EPOCH 660/1000 | BATCH 70/71 | LOSS: 4.908047922584392e-06\n",
      "VAL: EPOCH 660/1000 | BATCH 0/8 | LOSS: 4.205114692013012e-06\n",
      "VAL: EPOCH 660/1000 | BATCH 1/8 | LOSS: 4.716826197181945e-06\n",
      "VAL: EPOCH 660/1000 | BATCH 2/8 | LOSS: 4.9082408016450545e-06\n",
      "VAL: EPOCH 660/1000 | BATCH 3/8 | LOSS: 4.879305947724788e-06\n",
      "VAL: EPOCH 660/1000 | BATCH 4/8 | LOSS: 5.013968711864436e-06\n",
      "VAL: EPOCH 660/1000 | BATCH 5/8 | LOSS: 5.025235168432118e-06\n",
      "VAL: EPOCH 660/1000 | BATCH 6/8 | LOSS: 4.961208430488893e-06\n",
      "VAL: EPOCH 660/1000 | BATCH 7/8 | LOSS: 4.927569023038814e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 0/71 | LOSS: 4.7077569433895405e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 1/71 | LOSS: 4.514077772910241e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 2/71 | LOSS: 4.349623850430362e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 3/71 | LOSS: 4.494013523981266e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 4/71 | LOSS: 4.459040064830333e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 5/71 | LOSS: 4.3296978446960566e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 6/71 | LOSS: 4.3887086381541196e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 7/71 | LOSS: 4.5406368656131235e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 8/71 | LOSS: 4.529065765584366e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 9/71 | LOSS: 4.488657828005671e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 10/71 | LOSS: 4.4599073796482776e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 11/71 | LOSS: 4.493144558637141e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 12/71 | LOSS: 4.601996474425855e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 13/71 | LOSS: 4.566202684535321e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 14/71 | LOSS: 4.609621055351454e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 15/71 | LOSS: 4.526020617845461e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 16/71 | LOSS: 4.545047017125077e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 17/71 | LOSS: 4.628580667384894e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 18/71 | LOSS: 4.603608443319804e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 19/71 | LOSS: 4.629229135844071e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 20/71 | LOSS: 4.5843075462298224e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 21/71 | LOSS: 4.632564861656812e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 22/71 | LOSS: 4.649188054906477e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 23/71 | LOSS: 4.7306924291964e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 24/71 | LOSS: 4.655619686673163e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 25/71 | LOSS: 4.601827203469628e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 26/71 | LOSS: 4.55420325596168e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 27/71 | LOSS: 4.6268698708575225e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 28/71 | LOSS: 4.575929363548719e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 29/71 | LOSS: 4.5796840746940385e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 30/71 | LOSS: 4.53423227085178e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 31/71 | LOSS: 4.510492935594357e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 32/71 | LOSS: 4.495877922663842e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 33/71 | LOSS: 4.497166969485988e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 34/71 | LOSS: 4.485571616896778e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 35/71 | LOSS: 4.438435237514366e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 36/71 | LOSS: 4.434667148047624e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 37/71 | LOSS: 4.429978868134449e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 38/71 | LOSS: 4.391990282434003e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 39/71 | LOSS: 4.357949535460648e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 40/71 | LOSS: 4.330829586074177e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 41/71 | LOSS: 4.376491709817423e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 42/71 | LOSS: 4.374646566649845e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 43/71 | LOSS: 4.390546151129953e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 44/71 | LOSS: 4.388214013791488e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 45/71 | LOSS: 4.381184107446841e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 46/71 | LOSS: 4.3925862515374935e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 47/71 | LOSS: 4.412892167238169e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 48/71 | LOSS: 4.431256475443809e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 49/71 | LOSS: 4.45517839580134e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 50/71 | LOSS: 4.447382448858002e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 51/71 | LOSS: 4.451477751060268e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 52/71 | LOSS: 4.459351777837102e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 53/71 | LOSS: 4.455536436917437e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 54/71 | LOSS: 4.460273496740476e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 55/71 | LOSS: 4.444923385043305e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 56/71 | LOSS: 4.441425293758552e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 57/71 | LOSS: 4.429818283103392e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 58/71 | LOSS: 4.4268562097555735e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 59/71 | LOSS: 4.4211867248122875e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 60/71 | LOSS: 4.420719196638007e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 61/71 | LOSS: 4.4230987771609966e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 62/71 | LOSS: 4.433149357167182e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 63/71 | LOSS: 4.41221871838593e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 64/71 | LOSS: 4.399312204511191e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 65/71 | LOSS: 4.397959097938535e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 66/71 | LOSS: 4.3725154723484705e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 67/71 | LOSS: 4.365247237354411e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 68/71 | LOSS: 4.353395343718728e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 69/71 | LOSS: 4.347415282219507e-06\n",
      "TRAIN: EPOCH 661/1000 | BATCH 70/71 | LOSS: 4.342974742824699e-06\n",
      "VAL: EPOCH 661/1000 | BATCH 0/8 | LOSS: 3.3199853533005808e-06\n",
      "VAL: EPOCH 661/1000 | BATCH 1/8 | LOSS: 3.734765186891309e-06\n",
      "VAL: EPOCH 661/1000 | BATCH 2/8 | LOSS: 3.858097291716452e-06\n",
      "VAL: EPOCH 661/1000 | BATCH 3/8 | LOSS: 3.8486045923491474e-06\n",
      "VAL: EPOCH 661/1000 | BATCH 4/8 | LOSS: 3.956715227104724e-06\n",
      "VAL: EPOCH 661/1000 | BATCH 5/8 | LOSS: 3.978822557352639e-06\n",
      "VAL: EPOCH 661/1000 | BATCH 6/8 | LOSS: 3.904118979595036e-06\n",
      "VAL: EPOCH 661/1000 | BATCH 7/8 | LOSS: 3.7958902794343885e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 0/71 | LOSS: 2.904824668803485e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 1/71 | LOSS: 3.353291617713694e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 2/71 | LOSS: 3.41832772695246e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 3/71 | LOSS: 3.4724933470897668e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 4/71 | LOSS: 3.6796241602132794e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 5/71 | LOSS: 3.842569147612569e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 6/71 | LOSS: 4.039457800380271e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 7/71 | LOSS: 4.023975662903467e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 8/71 | LOSS: 4.516282615441014e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 9/71 | LOSS: 4.383084069559118e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 10/71 | LOSS: 4.7648683738026936e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 11/71 | LOSS: 4.851887448846052e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 12/71 | LOSS: 5.013750803930005e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 13/71 | LOSS: 5.0855684483914435e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 14/71 | LOSS: 5.042055454396177e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 15/71 | LOSS: 5.2879738063893456e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 16/71 | LOSS: 5.273986784338965e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 17/71 | LOSS: 5.247975928392003e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 18/71 | LOSS: 5.356766785936136e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 19/71 | LOSS: 5.366380719351582e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 20/71 | LOSS: 5.3375854891090145e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 21/71 | LOSS: 5.4794492867089995e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 22/71 | LOSS: 5.634885640218904e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 23/71 | LOSS: 5.556244957460876e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 24/71 | LOSS: 5.652655727317324e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 25/71 | LOSS: 5.659782223469497e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 26/71 | LOSS: 5.660091742525024e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 27/71 | LOSS: 5.616382671697855e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 28/71 | LOSS: 5.587274292087511e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 29/71 | LOSS: 5.683418339685886e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 30/71 | LOSS: 5.617991819822384e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 31/71 | LOSS: 5.6412651545656445e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 32/71 | LOSS: 5.669101655615977e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 33/71 | LOSS: 5.715567667988687e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 34/71 | LOSS: 5.687320929480068e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 35/71 | LOSS: 5.7406523726260475e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 36/71 | LOSS: 5.717104718191754e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 37/71 | LOSS: 5.702142510400383e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 38/71 | LOSS: 5.7217321169352944e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 39/71 | LOSS: 5.707026667778337e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 40/71 | LOSS: 5.730996463171323e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 41/71 | LOSS: 5.685885228212144e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 42/71 | LOSS: 5.71588749301364e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 43/71 | LOSS: 5.702680766717094e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 44/71 | LOSS: 5.689119638595406e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 45/71 | LOSS: 5.6860177239840315e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 46/71 | LOSS: 5.669376629699167e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 47/71 | LOSS: 5.705498547096492e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 48/71 | LOSS: 5.6689843395175485e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 49/71 | LOSS: 5.654304800373211e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 50/71 | LOSS: 5.673122943477995e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 51/71 | LOSS: 5.648548140540627e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 52/71 | LOSS: 5.606442854050843e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 53/71 | LOSS: 5.579220210502508e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 54/71 | LOSS: 5.552647131811351e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 55/71 | LOSS: 5.5506221404552236e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 56/71 | LOSS: 5.5166733027203515e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 57/71 | LOSS: 5.493708374223356e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 58/71 | LOSS: 5.48204840184954e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 59/71 | LOSS: 5.463731201871269e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 60/71 | LOSS: 5.4535820761318106e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 61/71 | LOSS: 5.444617217577733e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 62/71 | LOSS: 5.422209578827033e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 63/71 | LOSS: 5.392898327016837e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 64/71 | LOSS: 5.3710961520454684e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 65/71 | LOSS: 5.35317163515477e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 66/71 | LOSS: 5.323714510689285e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 67/71 | LOSS: 5.30658829735758e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 68/71 | LOSS: 5.29293788201977e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 69/71 | LOSS: 5.284398548610625e-06\n",
      "TRAIN: EPOCH 662/1000 | BATCH 70/71 | LOSS: 5.269459852826631e-06\n",
      "VAL: EPOCH 662/1000 | BATCH 0/8 | LOSS: 5.024047823098954e-06\n",
      "VAL: EPOCH 662/1000 | BATCH 1/8 | LOSS: 5.350633728085086e-06\n",
      "VAL: EPOCH 662/1000 | BATCH 2/8 | LOSS: 5.440610342096382e-06\n",
      "VAL: EPOCH 662/1000 | BATCH 3/8 | LOSS: 5.346355692381621e-06\n",
      "VAL: EPOCH 662/1000 | BATCH 4/8 | LOSS: 5.4930434089328625e-06\n",
      "VAL: EPOCH 662/1000 | BATCH 5/8 | LOSS: 5.378466160739966e-06\n",
      "VAL: EPOCH 662/1000 | BATCH 6/8 | LOSS: 5.2813990935516945e-06\n",
      "VAL: EPOCH 662/1000 | BATCH 7/8 | LOSS: 5.113666304623621e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 0/71 | LOSS: 4.775912657351e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 1/71 | LOSS: 4.924608902001637e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 2/71 | LOSS: 5.168209706122677e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 3/71 | LOSS: 5.165494485481759e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 4/71 | LOSS: 5.2158153266645965e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 5/71 | LOSS: 4.989912971116913e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 6/71 | LOSS: 4.9696632881282965e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 7/71 | LOSS: 4.8975231266013e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 8/71 | LOSS: 4.7468382086258825e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 9/71 | LOSS: 4.735441893899406e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 10/71 | LOSS: 4.7133747888107216e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 11/71 | LOSS: 4.7836856727675086e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 12/71 | LOSS: 4.710264192908653e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 13/71 | LOSS: 4.6724511548745795e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 14/71 | LOSS: 4.748850263543621e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 15/71 | LOSS: 4.713662733024648e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 16/71 | LOSS: 4.807637236915659e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 17/71 | LOSS: 4.7553558412497905e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 18/71 | LOSS: 4.740159283755929e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 19/71 | LOSS: 4.694615370226529e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 20/71 | LOSS: 4.747109187519527e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 21/71 | LOSS: 4.782323881871972e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 22/71 | LOSS: 4.765339791328702e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 23/71 | LOSS: 4.755942777971238e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 24/71 | LOSS: 4.790458888237481e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 25/71 | LOSS: 4.8032509224727756e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 26/71 | LOSS: 4.838038160381041e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 27/71 | LOSS: 4.822650542532106e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 28/71 | LOSS: 4.862981148985925e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 29/71 | LOSS: 4.798171138039227e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 30/71 | LOSS: 4.77486783130453e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 31/71 | LOSS: 4.7604960755620596e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 32/71 | LOSS: 4.786052000561154e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 33/71 | LOSS: 4.765964343492028e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 34/71 | LOSS: 4.763048952684455e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 35/71 | LOSS: 4.732399664438465e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 36/71 | LOSS: 4.730432170322888e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 37/71 | LOSS: 4.749275879140968e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 38/71 | LOSS: 4.747585721932596e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 39/71 | LOSS: 4.737568735890818e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 40/71 | LOSS: 4.745957750589984e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 41/71 | LOSS: 4.767064529871623e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 42/71 | LOSS: 4.7814860091569055e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 43/71 | LOSS: 4.77697033767816e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 44/71 | LOSS: 4.7533086621519435e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 45/71 | LOSS: 4.7425309098205455e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 46/71 | LOSS: 4.722868275263302e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 47/71 | LOSS: 4.728994213110127e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 48/71 | LOSS: 4.699278336801752e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 49/71 | LOSS: 4.679147755268787e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 50/71 | LOSS: 4.672613742656597e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 51/71 | LOSS: 4.653369129615525e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 52/71 | LOSS: 4.648612961970667e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 53/71 | LOSS: 4.63589293741717e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 54/71 | LOSS: 4.609361791484513e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 55/71 | LOSS: 4.596120029581081e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 56/71 | LOSS: 4.5715811259134696e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 57/71 | LOSS: 4.56570283263161e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 58/71 | LOSS: 4.555196483124746e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 59/71 | LOSS: 4.5554613355610245e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 60/71 | LOSS: 4.543838608920761e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 61/71 | LOSS: 4.556638215592923e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 62/71 | LOSS: 4.559484461915129e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 63/71 | LOSS: 4.54288956319715e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 64/71 | LOSS: 4.520321989240564e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 65/71 | LOSS: 4.549085260943244e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 66/71 | LOSS: 4.5349470485180335e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 67/71 | LOSS: 4.537237196152145e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 68/71 | LOSS: 4.528494394211178e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 69/71 | LOSS: 4.516685966596664e-06\n",
      "TRAIN: EPOCH 663/1000 | BATCH 70/71 | LOSS: 4.509603203087594e-06\n",
      "VAL: EPOCH 663/1000 | BATCH 0/8 | LOSS: 4.371976501715835e-06\n",
      "VAL: EPOCH 663/1000 | BATCH 1/8 | LOSS: 4.798412874151836e-06\n",
      "VAL: EPOCH 663/1000 | BATCH 2/8 | LOSS: 5.162883553566644e-06\n",
      "VAL: EPOCH 663/1000 | BATCH 3/8 | LOSS: 5.173725639906479e-06\n",
      "VAL: EPOCH 663/1000 | BATCH 4/8 | LOSS: 5.251811853668187e-06\n",
      "VAL: EPOCH 663/1000 | BATCH 5/8 | LOSS: 5.371895667849458e-06\n",
      "VAL: EPOCH 663/1000 | BATCH 6/8 | LOSS: 5.3249321224159625e-06\n",
      "VAL: EPOCH 663/1000 | BATCH 7/8 | LOSS: 5.31485613919358e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 0/71 | LOSS: 5.743958354287315e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 1/71 | LOSS: 4.986913154425565e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 2/71 | LOSS: 4.775706353636148e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 3/71 | LOSS: 4.904120714854798e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 4/71 | LOSS: 4.696018822869519e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 5/71 | LOSS: 4.6535198331791134e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 6/71 | LOSS: 4.738971256301738e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 7/71 | LOSS: 4.710933808382833e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 8/71 | LOSS: 4.8275699858398484e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 9/71 | LOSS: 4.744582565763267e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 10/71 | LOSS: 4.763047234684398e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 11/71 | LOSS: 4.8539697369657615e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 12/71 | LOSS: 4.767333638693134e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 13/71 | LOSS: 4.734089803345601e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 14/71 | LOSS: 4.786607011434777e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 15/71 | LOSS: 4.737352526262839e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 16/71 | LOSS: 4.723365183214328e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 17/71 | LOSS: 4.714692446820361e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 18/71 | LOSS: 4.69088248983577e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 19/71 | LOSS: 4.6506492708431326e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 20/71 | LOSS: 4.724828613689169e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 21/71 | LOSS: 4.688608751993425e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 22/71 | LOSS: 4.680706059962572e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 23/71 | LOSS: 4.615504129408994e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 24/71 | LOSS: 4.5684636552323356e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 25/71 | LOSS: 4.5595273657174684e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 26/71 | LOSS: 4.5445095077378e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 27/71 | LOSS: 4.533388448635378e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 28/71 | LOSS: 4.527402301517151e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 29/71 | LOSS: 4.512603989799876e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 30/71 | LOSS: 4.517207630229795e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 31/71 | LOSS: 4.507052516089516e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 32/71 | LOSS: 4.537697949754974e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 33/71 | LOSS: 4.526792569028176e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 34/71 | LOSS: 4.485680743268209e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 35/71 | LOSS: 4.490612575914889e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 36/71 | LOSS: 4.46091877853866e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 37/71 | LOSS: 4.522015974304706e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 38/71 | LOSS: 4.507140555431714e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 39/71 | LOSS: 4.538531516118382e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 40/71 | LOSS: 4.552571592684239e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 41/71 | LOSS: 4.5673100322303416e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 42/71 | LOSS: 4.57110776509115e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 43/71 | LOSS: 4.541619437697426e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 44/71 | LOSS: 4.5491897910526155e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 45/71 | LOSS: 4.547788604292651e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 46/71 | LOSS: 4.537500778516257e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 47/71 | LOSS: 4.599392236552073e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 48/71 | LOSS: 4.586215110827411e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 49/71 | LOSS: 4.611562458194385e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 50/71 | LOSS: 4.618515883218303e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 51/71 | LOSS: 4.633118320585849e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 52/71 | LOSS: 4.612974755287243e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 53/71 | LOSS: 4.638297582459927e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 54/71 | LOSS: 4.618863803526355e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 55/71 | LOSS: 4.614201433891399e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 56/71 | LOSS: 4.611028699448645e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 57/71 | LOSS: 4.621984712998553e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 58/71 | LOSS: 4.5968769103699555e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 59/71 | LOSS: 4.588145062219458e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 60/71 | LOSS: 4.588495279617224e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 61/71 | LOSS: 4.600664371331908e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 62/71 | LOSS: 4.612155043292977e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 63/71 | LOSS: 4.608982028031505e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 64/71 | LOSS: 4.624428083843668e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 65/71 | LOSS: 4.6480532911587176e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 66/71 | LOSS: 4.648220724515911e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 67/71 | LOSS: 4.658116124039011e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 68/71 | LOSS: 4.644696373625134e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 69/71 | LOSS: 4.651489731648845e-06\n",
      "TRAIN: EPOCH 664/1000 | BATCH 70/71 | LOSS: 4.640632766149537e-06\n",
      "VAL: EPOCH 664/1000 | BATCH 0/8 | LOSS: 3.4108024919987656e-06\n",
      "VAL: EPOCH 664/1000 | BATCH 1/8 | LOSS: 3.967536031268537e-06\n",
      "VAL: EPOCH 664/1000 | BATCH 2/8 | LOSS: 4.273830048380963e-06\n",
      "VAL: EPOCH 664/1000 | BATCH 3/8 | LOSS: 4.290737138035183e-06\n",
      "VAL: EPOCH 664/1000 | BATCH 4/8 | LOSS: 4.356471890787361e-06\n",
      "VAL: EPOCH 664/1000 | BATCH 5/8 | LOSS: 4.473039401394392e-06\n",
      "VAL: EPOCH 664/1000 | BATCH 6/8 | LOSS: 4.403980970632152e-06\n",
      "VAL: EPOCH 664/1000 | BATCH 7/8 | LOSS: 4.3439948740342516e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 0/71 | LOSS: 3.240291334805079e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 1/71 | LOSS: 3.951416374547989e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 2/71 | LOSS: 3.912044273117014e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 3/71 | LOSS: 4.128573777961719e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 4/71 | LOSS: 4.1084609620156695e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 5/71 | LOSS: 4.3689898726976635e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 6/71 | LOSS: 4.533806337089377e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 7/71 | LOSS: 4.455867383512668e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 8/71 | LOSS: 4.4330188049773115e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 9/71 | LOSS: 4.4782359964301575e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 10/71 | LOSS: 4.393130645687713e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 11/71 | LOSS: 4.421536004883819e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 12/71 | LOSS: 4.483544636549106e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 13/71 | LOSS: 4.520138580638948e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 14/71 | LOSS: 4.472485337222073e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 15/71 | LOSS: 4.520017398590426e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 16/71 | LOSS: 4.427596115205875e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 17/71 | LOSS: 4.394542480036358e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 18/71 | LOSS: 4.482485852158309e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 19/71 | LOSS: 4.447826154319046e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 20/71 | LOSS: 4.3763333602934535e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 21/71 | LOSS: 4.363582351371323e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 22/71 | LOSS: 4.32787492568225e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 23/71 | LOSS: 4.401666672038118e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 24/71 | LOSS: 4.3586234460235575e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 25/71 | LOSS: 4.335028576791116e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 26/71 | LOSS: 4.328154604822716e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 27/71 | LOSS: 4.307333085762366e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 28/71 | LOSS: 4.309327958504457e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 29/71 | LOSS: 4.323858161114913e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 30/71 | LOSS: 4.3377631520284454e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 31/71 | LOSS: 4.3443972330692304e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 32/71 | LOSS: 4.374750687929918e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 33/71 | LOSS: 4.363896236474616e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 34/71 | LOSS: 4.366725683472136e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 35/71 | LOSS: 4.424859582741192e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 36/71 | LOSS: 4.446875522803198e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 37/71 | LOSS: 4.45254957675819e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 38/71 | LOSS: 4.4666659132184204e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 39/71 | LOSS: 4.487130917141258e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 40/71 | LOSS: 4.507123526864012e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 41/71 | LOSS: 4.481527926578738e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 42/71 | LOSS: 4.491859404287277e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 43/71 | LOSS: 4.501647889686534e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 44/71 | LOSS: 4.533137441992747e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 45/71 | LOSS: 4.537038940408978e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 46/71 | LOSS: 4.557876217346502e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 47/71 | LOSS: 4.587922778872174e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 48/71 | LOSS: 4.5840635619019945e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 49/71 | LOSS: 4.619713549800508e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 50/71 | LOSS: 4.600613790756267e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 51/71 | LOSS: 4.621968387779746e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 52/71 | LOSS: 4.6429428030538505e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 53/71 | LOSS: 4.652136184362192e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 54/71 | LOSS: 4.670120080680301e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 55/71 | LOSS: 4.691789890947413e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 56/71 | LOSS: 4.7269801151534855e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 57/71 | LOSS: 4.731326173206416e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 58/71 | LOSS: 4.759468010426213e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 59/71 | LOSS: 4.759572914281307e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 60/71 | LOSS: 4.753748223012109e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 61/71 | LOSS: 4.7518537536645105e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 62/71 | LOSS: 4.7695480603463705e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 63/71 | LOSS: 4.782749545739762e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 64/71 | LOSS: 4.797436041861567e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 65/71 | LOSS: 4.828907029177847e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 66/71 | LOSS: 4.831710771553057e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 67/71 | LOSS: 4.899760527036158e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 68/71 | LOSS: 4.88927374109166e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 69/71 | LOSS: 4.954080210544427e-06\n",
      "TRAIN: EPOCH 665/1000 | BATCH 70/71 | LOSS: 4.921860114387835e-06\n",
      "VAL: EPOCH 665/1000 | BATCH 0/8 | LOSS: 4.946785793435993e-06\n",
      "VAL: EPOCH 665/1000 | BATCH 1/8 | LOSS: 5.726000154027133e-06\n",
      "VAL: EPOCH 665/1000 | BATCH 2/8 | LOSS: 6.0445824298464386e-06\n",
      "VAL: EPOCH 665/1000 | BATCH 3/8 | LOSS: 6.128660970716737e-06\n",
      "VAL: EPOCH 665/1000 | BATCH 4/8 | LOSS: 6.142679103504633e-06\n",
      "VAL: EPOCH 665/1000 | BATCH 5/8 | LOSS: 6.321127254219998e-06\n",
      "VAL: EPOCH 665/1000 | BATCH 6/8 | LOSS: 6.274179863144777e-06\n",
      "VAL: EPOCH 665/1000 | BATCH 7/8 | LOSS: 6.241148639674066e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 0/71 | LOSS: 5.853055881743785e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 1/71 | LOSS: 6.0692887018376496e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 2/71 | LOSS: 6.356052229724203e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 3/71 | LOSS: 6.425126002795878e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 4/71 | LOSS: 6.039163690729765e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 5/71 | LOSS: 6.226201321624103e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 6/71 | LOSS: 5.864336019710338e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 7/71 | LOSS: 5.806925287288323e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 8/71 | LOSS: 5.7135909325249186e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 9/71 | LOSS: 5.647406169373426e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 10/71 | LOSS: 5.648738021012502e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 11/71 | LOSS: 5.553516340720914e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 12/71 | LOSS: 5.6600655485593825e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 13/71 | LOSS: 5.5050374580137685e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 14/71 | LOSS: 5.504162724416044e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 15/71 | LOSS: 5.419700570996611e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 16/71 | LOSS: 5.415082167768132e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 17/71 | LOSS: 5.41793968346206e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 18/71 | LOSS: 5.361481522700878e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 19/71 | LOSS: 5.415949055986858e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 20/71 | LOSS: 5.332408742712384e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 21/71 | LOSS: 5.372780120441431e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 22/71 | LOSS: 5.394889295411818e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 23/71 | LOSS: 5.407346056548097e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 24/71 | LOSS: 5.361305256883497e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 25/71 | LOSS: 5.3112433988644625e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 26/71 | LOSS: 5.2848316769356e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 27/71 | LOSS: 5.278158036097531e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 28/71 | LOSS: 5.290746693045266e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 29/71 | LOSS: 5.234548833262428e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 30/71 | LOSS: 5.2458629676449725e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 31/71 | LOSS: 5.20864211495109e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 32/71 | LOSS: 5.1859024166112775e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 33/71 | LOSS: 5.214550884268058e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 34/71 | LOSS: 5.166629216546426e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 35/71 | LOSS: 5.20071370778573e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 36/71 | LOSS: 5.144589510097363e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 37/71 | LOSS: 5.160143042590598e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 38/71 | LOSS: 5.11284413453093e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 39/71 | LOSS: 5.077608790315935e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 40/71 | LOSS: 5.083199356504781e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 41/71 | LOSS: 5.0735444249637244e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 42/71 | LOSS: 5.065379688202547e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 43/71 | LOSS: 5.031194224226386e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 44/71 | LOSS: 4.999249757828592e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 45/71 | LOSS: 4.983655191173729e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 46/71 | LOSS: 4.960820259394166e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 47/71 | LOSS: 4.944236569789003e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 48/71 | LOSS: 4.939296285537777e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 49/71 | LOSS: 4.915715094284679e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 50/71 | LOSS: 4.902253477648985e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 51/71 | LOSS: 4.872385204635066e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 52/71 | LOSS: 4.855193083130871e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 53/71 | LOSS: 4.853589875555944e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 54/71 | LOSS: 4.863104934636133e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 55/71 | LOSS: 4.86583432137065e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 56/71 | LOSS: 4.8652887136612615e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 57/71 | LOSS: 4.862687378277017e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 58/71 | LOSS: 4.869250225533891e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 59/71 | LOSS: 4.844136518992551e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 60/71 | LOSS: 4.839700206175025e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 61/71 | LOSS: 4.846527252583098e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 62/71 | LOSS: 4.837578920896271e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 63/71 | LOSS: 4.82710199278813e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 64/71 | LOSS: 4.818937126261657e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 65/71 | LOSS: 4.81261515412343e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 66/71 | LOSS: 4.815465838724614e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 67/71 | LOSS: 4.808354410269042e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 68/71 | LOSS: 4.795302646121802e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 69/71 | LOSS: 4.778560472524467e-06\n",
      "TRAIN: EPOCH 666/1000 | BATCH 70/71 | LOSS: 4.774250373244081e-06\n",
      "VAL: EPOCH 666/1000 | BATCH 0/8 | LOSS: 4.220220034767408e-06\n",
      "VAL: EPOCH 666/1000 | BATCH 1/8 | LOSS: 4.639690132535179e-06\n",
      "VAL: EPOCH 666/1000 | BATCH 2/8 | LOSS: 4.767220464903706e-06\n",
      "VAL: EPOCH 666/1000 | BATCH 3/8 | LOSS: 4.798884901902056e-06\n",
      "VAL: EPOCH 666/1000 | BATCH 4/8 | LOSS: 4.7401983465533705e-06\n",
      "VAL: EPOCH 666/1000 | BATCH 5/8 | LOSS: 4.7671061717361835e-06\n",
      "VAL: EPOCH 666/1000 | BATCH 6/8 | LOSS: 4.643918535813489e-06\n",
      "VAL: EPOCH 666/1000 | BATCH 7/8 | LOSS: 4.4821607332323765e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 0/71 | LOSS: 5.538397090276703e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 1/71 | LOSS: 5.143955604580697e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 2/71 | LOSS: 5.166665990448867e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 3/71 | LOSS: 5.057927864982048e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 4/71 | LOSS: 5.254529878584435e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 5/71 | LOSS: 5.481086494304084e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 6/71 | LOSS: 5.109933161812867e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 7/71 | LOSS: 5.185267184515396e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 8/71 | LOSS: 5.372435427691218e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 9/71 | LOSS: 5.553184382733889e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 10/71 | LOSS: 5.400895877921192e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 11/71 | LOSS: 5.454773713609029e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 12/71 | LOSS: 5.454904812023205e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 13/71 | LOSS: 5.288354438042526e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 14/71 | LOSS: 5.301861604796917e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 15/71 | LOSS: 5.317546353467151e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 16/71 | LOSS: 5.201354711329366e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 17/71 | LOSS: 5.142214263287315e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 18/71 | LOSS: 5.163166514122215e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 19/71 | LOSS: 5.113677104873204e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 20/71 | LOSS: 5.0673080576892795e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 21/71 | LOSS: 5.060217342195921e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 22/71 | LOSS: 5.038037937518548e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 23/71 | LOSS: 4.957477149976815e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 24/71 | LOSS: 4.944630945828976e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 25/71 | LOSS: 4.942347949019257e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 26/71 | LOSS: 4.894264327082999e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 27/71 | LOSS: 4.917028182457475e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 28/71 | LOSS: 4.906757269430994e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 29/71 | LOSS: 4.87132856505923e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 30/71 | LOSS: 4.856331777832125e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 31/71 | LOSS: 4.862427744001252e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 32/71 | LOSS: 4.836783263630368e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 33/71 | LOSS: 4.847952542310368e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 34/71 | LOSS: 4.836302949635345e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 35/71 | LOSS: 4.802192633077438e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 36/71 | LOSS: 4.8359166686242206e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 37/71 | LOSS: 4.849235557757263e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 38/71 | LOSS: 4.833299044548767e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 39/71 | LOSS: 4.8236947804980446e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 40/71 | LOSS: 4.796979549321738e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 41/71 | LOSS: 4.793942447836993e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 42/71 | LOSS: 4.80535501019615e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 43/71 | LOSS: 4.808170005658625e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 44/71 | LOSS: 4.7916393264636605e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 45/71 | LOSS: 4.808530093214358e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 46/71 | LOSS: 4.79353111780043e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 47/71 | LOSS: 4.898562044293915e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 48/71 | LOSS: 4.870188325983282e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 49/71 | LOSS: 4.884107274847338e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 50/71 | LOSS: 4.898434710020072e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 51/71 | LOSS: 4.912537310002125e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 52/71 | LOSS: 4.906797096067886e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 53/71 | LOSS: 4.932275807597627e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 54/71 | LOSS: 4.935142640699103e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 55/71 | LOSS: 4.925961572358314e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 56/71 | LOSS: 4.918305613069893e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 57/71 | LOSS: 4.9123433858892835e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 58/71 | LOSS: 4.940772121976883e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 59/71 | LOSS: 4.926854042726821e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 60/71 | LOSS: 4.944444768983378e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 61/71 | LOSS: 4.913458992045742e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 62/71 | LOSS: 4.90921886575095e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 63/71 | LOSS: 4.905713769431941e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 64/71 | LOSS: 4.937666888228985e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 65/71 | LOSS: 4.943782259529557e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 66/71 | LOSS: 4.948824442778202e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 67/71 | LOSS: 4.92676530725208e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 68/71 | LOSS: 4.923566011448333e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 69/71 | LOSS: 4.919616172368738e-06\n",
      "TRAIN: EPOCH 667/1000 | BATCH 70/71 | LOSS: 4.9147050523176186e-06\n",
      "VAL: EPOCH 667/1000 | BATCH 0/8 | LOSS: 3.8185280573088676e-06\n",
      "VAL: EPOCH 667/1000 | BATCH 1/8 | LOSS: 4.369128419057233e-06\n",
      "VAL: EPOCH 667/1000 | BATCH 2/8 | LOSS: 4.5625659671107615e-06\n",
      "VAL: EPOCH 667/1000 | BATCH 3/8 | LOSS: 4.499348051467678e-06\n",
      "VAL: EPOCH 667/1000 | BATCH 4/8 | LOSS: 4.569421889755176e-06\n",
      "VAL: EPOCH 667/1000 | BATCH 5/8 | LOSS: 4.527720193436835e-06\n",
      "VAL: EPOCH 667/1000 | BATCH 6/8 | LOSS: 4.438271519445282e-06\n",
      "VAL: EPOCH 667/1000 | BATCH 7/8 | LOSS: 4.361518904261175e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 0/71 | LOSS: 4.374299351184163e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 1/71 | LOSS: 4.2585488699842244e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 2/71 | LOSS: 4.017579385617864e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 3/71 | LOSS: 3.846625816095184e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 4/71 | LOSS: 3.914770832125214e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 5/71 | LOSS: 3.974000605921901e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 6/71 | LOSS: 3.98490773737389e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 7/71 | LOSS: 4.054110974038849e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 8/71 | LOSS: 3.965435679573501e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 9/71 | LOSS: 3.8659338542856855e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 10/71 | LOSS: 3.856022534488478e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 11/71 | LOSS: 3.979504356266261e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 12/71 | LOSS: 3.903158673123331e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 13/71 | LOSS: 3.924566239480295e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 14/71 | LOSS: 4.026139261744296e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 15/71 | LOSS: 4.067569392418591e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 16/71 | LOSS: 4.104336205594282e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 17/71 | LOSS: 4.140430140372094e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 18/71 | LOSS: 4.208066433133545e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 19/71 | LOSS: 4.201813931103971e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 20/71 | LOSS: 4.2404787044298634e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 21/71 | LOSS: 4.256157519028585e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 22/71 | LOSS: 4.250095512163724e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 23/71 | LOSS: 4.250786787451943e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 24/71 | LOSS: 4.2533157829893754e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 25/71 | LOSS: 4.304588156418714e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 26/71 | LOSS: 4.247957164167289e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 27/71 | LOSS: 4.236382600148707e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 28/71 | LOSS: 4.227155433658607e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 29/71 | LOSS: 4.239090549162938e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 30/71 | LOSS: 4.218662935830164e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 31/71 | LOSS: 4.2082023696821125e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 32/71 | LOSS: 4.216188975106254e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 33/71 | LOSS: 4.205632047374178e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 34/71 | LOSS: 4.1923907052218315e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 35/71 | LOSS: 4.174473272339836e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 36/71 | LOSS: 4.183848469020214e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 37/71 | LOSS: 4.1778342189278e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 38/71 | LOSS: 4.17967869044589e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 39/71 | LOSS: 4.193221980131057e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 40/71 | LOSS: 4.1703394577869e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 41/71 | LOSS: 4.166660416904917e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 42/71 | LOSS: 4.190459935142467e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 43/71 | LOSS: 4.1894475967555165e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 44/71 | LOSS: 4.2073533576411945e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 45/71 | LOSS: 4.192425061101739e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 46/71 | LOSS: 4.182632674781687e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 47/71 | LOSS: 4.169839968426459e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 48/71 | LOSS: 4.170557308591052e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 49/71 | LOSS: 4.163139283264172e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 50/71 | LOSS: 4.158201243305538e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 51/71 | LOSS: 4.147173696695133e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 52/71 | LOSS: 4.141197180461121e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 53/71 | LOSS: 4.153083224351846e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 54/71 | LOSS: 4.16544199569299e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 55/71 | LOSS: 4.189476677700961e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 56/71 | LOSS: 4.181362815040956e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 57/71 | LOSS: 4.185865017116784e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 58/71 | LOSS: 4.201281349119431e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 59/71 | LOSS: 4.240349441412642e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 60/71 | LOSS: 4.248559651118665e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 61/71 | LOSS: 4.2220531103368796e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 62/71 | LOSS: 4.230290584480769e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 63/71 | LOSS: 4.2306291199167845e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 64/71 | LOSS: 4.231403429982422e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 65/71 | LOSS: 4.258753117000831e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 66/71 | LOSS: 4.291556302605522e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 67/71 | LOSS: 4.3072013093425245e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 68/71 | LOSS: 4.3137397798104296e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 69/71 | LOSS: 4.3392288099702065e-06\n",
      "TRAIN: EPOCH 668/1000 | BATCH 70/71 | LOSS: 4.394234876189599e-06\n",
      "VAL: EPOCH 668/1000 | BATCH 0/8 | LOSS: 4.696325049735606e-06\n",
      "VAL: EPOCH 668/1000 | BATCH 1/8 | LOSS: 4.986167596143787e-06\n",
      "VAL: EPOCH 668/1000 | BATCH 2/8 | LOSS: 5.1484703362802975e-06\n",
      "VAL: EPOCH 668/1000 | BATCH 3/8 | LOSS: 5.034552486904431e-06\n",
      "VAL: EPOCH 668/1000 | BATCH 4/8 | LOSS: 5.262112608761527e-06\n",
      "VAL: EPOCH 668/1000 | BATCH 5/8 | LOSS: 5.228466003851888e-06\n",
      "VAL: EPOCH 668/1000 | BATCH 6/8 | LOSS: 5.175833653733467e-06\n",
      "VAL: EPOCH 668/1000 | BATCH 7/8 | LOSS: 5.135324897764804e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 0/71 | LOSS: 4.009383701486513e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 1/71 | LOSS: 4.07471929975145e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 2/71 | LOSS: 4.577445148849317e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 3/71 | LOSS: 4.377764184937405e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 4/71 | LOSS: 4.724068185169017e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 5/71 | LOSS: 4.669829422709881e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 6/71 | LOSS: 4.933187158582898e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 7/71 | LOSS: 4.853611471844488e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 8/71 | LOSS: 4.6815506872614305e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 9/71 | LOSS: 4.6806735326754275e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 10/71 | LOSS: 4.632512276326782e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 11/71 | LOSS: 4.50893384140727e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 12/71 | LOSS: 4.483843108223832e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 13/71 | LOSS: 4.521036008294946e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 14/71 | LOSS: 4.536064549635436e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 15/71 | LOSS: 4.556597886562486e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 16/71 | LOSS: 4.560676282327383e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 17/71 | LOSS: 4.539721114977308e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 18/71 | LOSS: 4.606450159515493e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 19/71 | LOSS: 4.583350289522059e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 20/71 | LOSS: 4.556605539446104e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 21/71 | LOSS: 4.558064502320618e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 22/71 | LOSS: 4.537539839673074e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 23/71 | LOSS: 4.489021222298106e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 24/71 | LOSS: 4.4704065567202635e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 25/71 | LOSS: 4.494734246061573e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 26/71 | LOSS: 4.485711871085287e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 27/71 | LOSS: 4.533632388107175e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 28/71 | LOSS: 4.533654450354114e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 29/71 | LOSS: 4.582482544416659e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 30/71 | LOSS: 4.54347531147857e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 31/71 | LOSS: 4.574155639147648e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 32/71 | LOSS: 4.593699817302298e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 33/71 | LOSS: 4.57908345197211e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 34/71 | LOSS: 4.638601421902422e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 35/71 | LOSS: 4.658987778485526e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 36/71 | LOSS: 4.693664765442451e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 37/71 | LOSS: 4.728493506359212e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 38/71 | LOSS: 4.83176631277251e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 39/71 | LOSS: 4.8084809691317785e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 40/71 | LOSS: 4.795017236862871e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 41/71 | LOSS: 4.82812626095048e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 42/71 | LOSS: 4.823622020921903e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 43/71 | LOSS: 4.823684869072829e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 44/71 | LOSS: 4.846876527153654e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 45/71 | LOSS: 4.843616589705166e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 46/71 | LOSS: 4.8246939765336484e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 47/71 | LOSS: 4.836424352561153e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 48/71 | LOSS: 4.840933463722705e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 49/71 | LOSS: 4.826380491067539e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 50/71 | LOSS: 4.827534640379086e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 51/71 | LOSS: 4.833149152825801e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 52/71 | LOSS: 4.800195402255993e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 53/71 | LOSS: 4.774021977869712e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 54/71 | LOSS: 4.75255368325055e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 55/71 | LOSS: 4.7474225734081954e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 56/71 | LOSS: 4.754513084710772e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 57/71 | LOSS: 4.725363294647682e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 58/71 | LOSS: 4.757050307597314e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 59/71 | LOSS: 4.7453313072765015e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 60/71 | LOSS: 4.74271334988756e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 61/71 | LOSS: 4.740005043722997e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 62/71 | LOSS: 4.735479229739645e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 63/71 | LOSS: 4.730580712930532e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 64/71 | LOSS: 4.709084071062254e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 65/71 | LOSS: 4.706461726472656e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 66/71 | LOSS: 4.7311405408558425e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 67/71 | LOSS: 4.742209665999005e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 68/71 | LOSS: 4.730711007932373e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 69/71 | LOSS: 4.732778396438724e-06\n",
      "TRAIN: EPOCH 669/1000 | BATCH 70/71 | LOSS: 4.725894541479647e-06\n",
      "VAL: EPOCH 669/1000 | BATCH 0/8 | LOSS: 3.7827096548426198e-06\n",
      "VAL: EPOCH 669/1000 | BATCH 1/8 | LOSS: 4.1566303252693615e-06\n",
      "VAL: EPOCH 669/1000 | BATCH 2/8 | LOSS: 4.356805144804336e-06\n",
      "VAL: EPOCH 669/1000 | BATCH 3/8 | LOSS: 4.374941397600196e-06\n",
      "VAL: EPOCH 669/1000 | BATCH 4/8 | LOSS: 4.513103931458318e-06\n",
      "VAL: EPOCH 669/1000 | BATCH 5/8 | LOSS: 4.543268043259256e-06\n",
      "VAL: EPOCH 669/1000 | BATCH 6/8 | LOSS: 4.47764442209778e-06\n",
      "VAL: EPOCH 669/1000 | BATCH 7/8 | LOSS: 4.3958052344805765e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 0/71 | LOSS: 3.839952114503831e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 1/71 | LOSS: 3.6453650409384863e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 2/71 | LOSS: 3.6463089448564765e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 3/71 | LOSS: 3.995466329342889e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 4/71 | LOSS: 3.996771329184412e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 5/71 | LOSS: 4.097862642993277e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 6/71 | LOSS: 4.1514905270949905e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 7/71 | LOSS: 4.03984759600462e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 8/71 | LOSS: 3.997251143219829e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 9/71 | LOSS: 4.167590577708324e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 10/71 | LOSS: 4.1734095000058664e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 11/71 | LOSS: 4.161029664828675e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 12/71 | LOSS: 4.19486185329823e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 13/71 | LOSS: 4.272447638088904e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 14/71 | LOSS: 4.384657919824045e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 15/71 | LOSS: 4.391563521721764e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 16/71 | LOSS: 4.3767884241841624e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 17/71 | LOSS: 4.436122960921946e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 18/71 | LOSS: 4.485271490825414e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 19/71 | LOSS: 4.568347321765032e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 20/71 | LOSS: 4.547780138506953e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 21/71 | LOSS: 4.543571569890694e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 22/71 | LOSS: 4.5610332049166216e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 23/71 | LOSS: 4.5500102790659485e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 24/71 | LOSS: 4.582197398121935e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 25/71 | LOSS: 4.5545842007973424e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 26/71 | LOSS: 4.5799843570017e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 27/71 | LOSS: 4.553488695689469e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 28/71 | LOSS: 4.563722384397085e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 29/71 | LOSS: 4.545938478865234e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 30/71 | LOSS: 4.538931828010977e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 31/71 | LOSS: 4.574795610778892e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 32/71 | LOSS: 4.549756120082527e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 33/71 | LOSS: 4.573664562164649e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 34/71 | LOSS: 4.537022959993919e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 35/71 | LOSS: 4.537443032859301e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 36/71 | LOSS: 4.508981407058387e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 37/71 | LOSS: 4.497107654874287e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 38/71 | LOSS: 4.49894508701385e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 39/71 | LOSS: 4.494907625485212e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 40/71 | LOSS: 4.450750000659155e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 41/71 | LOSS: 4.436544269906119e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 42/71 | LOSS: 4.449309958439445e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 43/71 | LOSS: 4.443627124766698e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 44/71 | LOSS: 4.4320827429247505e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 45/71 | LOSS: 4.453428197809009e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 46/71 | LOSS: 4.440482206654393e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 47/71 | LOSS: 4.41812512500898e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 48/71 | LOSS: 4.423172199773146e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 49/71 | LOSS: 4.410328356243553e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 50/71 | LOSS: 4.414199855661946e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 51/71 | LOSS: 4.424644456636228e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 52/71 | LOSS: 4.426424577516473e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 53/71 | LOSS: 4.435940203014373e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 54/71 | LOSS: 4.404602503365657e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 55/71 | LOSS: 4.400467612900424e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 56/71 | LOSS: 4.393627872751064e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 57/71 | LOSS: 4.383671368706363e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 58/71 | LOSS: 4.367479747951759e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 59/71 | LOSS: 4.3815186283306195e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 60/71 | LOSS: 4.378497515361113e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 61/71 | LOSS: 4.385003322685436e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 62/71 | LOSS: 4.388285059042299e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 63/71 | LOSS: 4.412350673277388e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 64/71 | LOSS: 4.397142993184388e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 65/71 | LOSS: 4.378057593440286e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 66/71 | LOSS: 4.3932179507003216e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 67/71 | LOSS: 4.386634516857371e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 68/71 | LOSS: 4.3722753695834875e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 69/71 | LOSS: 4.358927688891916e-06\n",
      "TRAIN: EPOCH 670/1000 | BATCH 70/71 | LOSS: 4.357214454782005e-06\n",
      "VAL: EPOCH 670/1000 | BATCH 0/8 | LOSS: 3.8002553992555477e-06\n",
      "VAL: EPOCH 670/1000 | BATCH 1/8 | LOSS: 4.224472377245547e-06\n",
      "VAL: EPOCH 670/1000 | BATCH 2/8 | LOSS: 4.400339093990624e-06\n",
      "VAL: EPOCH 670/1000 | BATCH 3/8 | LOSS: 4.314535317462287e-06\n",
      "VAL: EPOCH 670/1000 | BATCH 4/8 | LOSS: 4.425526367413113e-06\n",
      "VAL: EPOCH 670/1000 | BATCH 5/8 | LOSS: 4.4221202794384835e-06\n",
      "VAL: EPOCH 670/1000 | BATCH 6/8 | LOSS: 4.332723879737646e-06\n",
      "VAL: EPOCH 670/1000 | BATCH 7/8 | LOSS: 4.283508246771817e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 0/71 | LOSS: 4.393326889839955e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 1/71 | LOSS: 4.282627742213663e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 2/71 | LOSS: 3.982097041443922e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 3/71 | LOSS: 3.834100255062367e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 4/71 | LOSS: 3.884515672325506e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 5/71 | LOSS: 3.815007592796367e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 6/71 | LOSS: 4.244716982222079e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 7/71 | LOSS: 4.219721489562289e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 8/71 | LOSS: 4.250488953200855e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 9/71 | LOSS: 4.254612235854438e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 10/71 | LOSS: 4.23117318281253e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 11/71 | LOSS: 4.328744864778855e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 12/71 | LOSS: 4.414495991183615e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 13/71 | LOSS: 4.461406222487442e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 14/71 | LOSS: 4.435886982416074e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 15/71 | LOSS: 4.4623588593140084e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 16/71 | LOSS: 4.446825468061006e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 17/71 | LOSS: 4.470051926040873e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 18/71 | LOSS: 4.458548298651677e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 19/71 | LOSS: 4.4558926333593265e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 20/71 | LOSS: 4.4706925910716e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 21/71 | LOSS: 4.4899746233733655e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 22/71 | LOSS: 4.485228788800431e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 23/71 | LOSS: 4.463570509945687e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 24/71 | LOSS: 4.502589263211121e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 25/71 | LOSS: 4.472226936951091e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 26/71 | LOSS: 4.451471004021759e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 27/71 | LOSS: 4.4824399602865535e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 28/71 | LOSS: 4.503832848246835e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 29/71 | LOSS: 4.4878479760276e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 30/71 | LOSS: 4.475082571462412e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 31/71 | LOSS: 4.482777967496077e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 32/71 | LOSS: 4.488640625464596e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 33/71 | LOSS: 4.513967055305222e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 34/71 | LOSS: 4.5532153827869995e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 35/71 | LOSS: 4.52639090377084e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 36/71 | LOSS: 4.533591178384282e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 37/71 | LOSS: 4.606952595503045e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 38/71 | LOSS: 4.624233415830904e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 39/71 | LOSS: 4.68103577304646e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 40/71 | LOSS: 4.7127730284977945e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 41/71 | LOSS: 4.748389571995219e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 42/71 | LOSS: 4.729683369500995e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 43/71 | LOSS: 4.731106090813145e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 44/71 | LOSS: 4.7329213783895186e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 45/71 | LOSS: 4.734321187836189e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 46/71 | LOSS: 4.763649398460365e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 47/71 | LOSS: 4.753787010258748e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 48/71 | LOSS: 4.795517391042323e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 49/71 | LOSS: 4.810766467926442e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 50/71 | LOSS: 4.829680450462083e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 51/71 | LOSS: 4.860654380186484e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 52/71 | LOSS: 4.894387624311625e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 53/71 | LOSS: 4.933629725201172e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 54/71 | LOSS: 4.911069097281804e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 55/71 | LOSS: 4.996256762979101e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 56/71 | LOSS: 4.994873550681963e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 57/71 | LOSS: 5.035711720226792e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 58/71 | LOSS: 5.0451802986048034e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 59/71 | LOSS: 5.0233259571541565e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 60/71 | LOSS: 5.091647731429386e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 61/71 | LOSS: 5.080107858966878e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 62/71 | LOSS: 5.151682411711331e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 63/71 | LOSS: 5.135836502745406e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 64/71 | LOSS: 5.144551806342161e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 65/71 | LOSS: 5.1564393357473435e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 66/71 | LOSS: 5.144481251794678e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 67/71 | LOSS: 5.178369332604823e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 68/71 | LOSS: 5.161515877841959e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 69/71 | LOSS: 5.193993488969032e-06\n",
      "TRAIN: EPOCH 671/1000 | BATCH 70/71 | LOSS: 5.161670860872337e-06\n",
      "VAL: EPOCH 671/1000 | BATCH 0/8 | LOSS: 4.973573595634662e-06\n",
      "VAL: EPOCH 671/1000 | BATCH 1/8 | LOSS: 5.284369763103314e-06\n",
      "VAL: EPOCH 671/1000 | BATCH 2/8 | LOSS: 5.694095307262614e-06\n",
      "VAL: EPOCH 671/1000 | BATCH 3/8 | LOSS: 5.717808107874589e-06\n",
      "VAL: EPOCH 671/1000 | BATCH 4/8 | LOSS: 5.759458599641221e-06\n",
      "VAL: EPOCH 671/1000 | BATCH 5/8 | LOSS: 5.9236125859267e-06\n",
      "VAL: EPOCH 671/1000 | BATCH 6/8 | LOSS: 5.857972545137662e-06\n",
      "VAL: EPOCH 671/1000 | BATCH 7/8 | LOSS: 5.914024939102092e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 0/71 | LOSS: 5.047390914114658e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 1/71 | LOSS: 4.97228279527917e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 2/71 | LOSS: 5.285135406059756e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 3/71 | LOSS: 5.418837531578902e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 4/71 | LOSS: 5.243482064543059e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 5/71 | LOSS: 5.169213864064659e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 6/71 | LOSS: 5.160940223244584e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 7/71 | LOSS: 4.9843608564970054e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 8/71 | LOSS: 4.907217695896786e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 9/71 | LOSS: 4.832359559259203e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 10/71 | LOSS: 4.798904207994135e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 11/71 | LOSS: 4.6949018989532005e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 12/71 | LOSS: 4.624933650960044e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 13/71 | LOSS: 4.571466352639878e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 14/71 | LOSS: 4.5399346314904205e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 15/71 | LOSS: 4.467173411626391e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 16/71 | LOSS: 4.434665925145964e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 17/71 | LOSS: 4.444795637128765e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 18/71 | LOSS: 4.4348303522144165e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 19/71 | LOSS: 4.395721043692902e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 20/71 | LOSS: 4.359454188878382e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 21/71 | LOSS: 4.337408840497532e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 22/71 | LOSS: 4.308613617230668e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 23/71 | LOSS: 4.3186084610624675e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 24/71 | LOSS: 4.372841694930685e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 25/71 | LOSS: 4.302088314034336e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 26/71 | LOSS: 4.3038716605089445e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 27/71 | LOSS: 4.3286103732498304e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 28/71 | LOSS: 4.3352693347546375e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 29/71 | LOSS: 4.3108375772741665e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 30/71 | LOSS: 4.361917644341555e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 31/71 | LOSS: 4.405186452061116e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 32/71 | LOSS: 4.395733598853853e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 33/71 | LOSS: 4.4500754203160315e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 34/71 | LOSS: 4.461345081706116e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 35/71 | LOSS: 4.467555748356568e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 36/71 | LOSS: 4.480645503536908e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 37/71 | LOSS: 4.487192050443187e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 38/71 | LOSS: 4.508272322476469e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 39/71 | LOSS: 4.495005185845002e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 40/71 | LOSS: 4.51543859326637e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 41/71 | LOSS: 4.486515422286175e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 42/71 | LOSS: 4.468830210562162e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 43/71 | LOSS: 4.4589270477114375e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 44/71 | LOSS: 4.451557176960503e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 45/71 | LOSS: 4.4372350400764215e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 46/71 | LOSS: 4.420077475321917e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 47/71 | LOSS: 4.414025165525952e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 48/71 | LOSS: 4.402533551576792e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 49/71 | LOSS: 4.375856010483404e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 50/71 | LOSS: 4.3629225577509105e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 51/71 | LOSS: 4.383364053130488e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 52/71 | LOSS: 4.370670347202147e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 53/71 | LOSS: 4.3576570592281144e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 54/71 | LOSS: 4.34756764413826e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 55/71 | LOSS: 4.3204751989378565e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 56/71 | LOSS: 4.313105200867938e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 57/71 | LOSS: 4.294647918209978e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 58/71 | LOSS: 4.2995361543605325e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 59/71 | LOSS: 4.310931850189567e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 60/71 | LOSS: 4.310577703451644e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 61/71 | LOSS: 4.3146922016310255e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 62/71 | LOSS: 4.314521776096727e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 63/71 | LOSS: 4.326739716731254e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 64/71 | LOSS: 4.3263313448103596e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 65/71 | LOSS: 4.349396542210977e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 66/71 | LOSS: 4.347438711355703e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 67/71 | LOSS: 4.35572028369707e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 68/71 | LOSS: 4.36831458935795e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 69/71 | LOSS: 4.361326633833024e-06\n",
      "TRAIN: EPOCH 672/1000 | BATCH 70/71 | LOSS: 4.3799935918312414e-06\n",
      "VAL: EPOCH 672/1000 | BATCH 0/8 | LOSS: 3.673967739814543e-06\n",
      "VAL: EPOCH 672/1000 | BATCH 1/8 | LOSS: 3.946977244595473e-06\n",
      "VAL: EPOCH 672/1000 | BATCH 2/8 | LOSS: 4.070965663534783e-06\n",
      "VAL: EPOCH 672/1000 | BATCH 3/8 | LOSS: 4.055776400946343e-06\n",
      "VAL: EPOCH 672/1000 | BATCH 4/8 | LOSS: 4.082934401594684e-06\n",
      "VAL: EPOCH 672/1000 | BATCH 5/8 | LOSS: 4.049663440734245e-06\n",
      "VAL: EPOCH 672/1000 | BATCH 6/8 | LOSS: 3.917794564196291e-06\n",
      "VAL: EPOCH 672/1000 | BATCH 7/8 | LOSS: 3.765960684631864e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 0/71 | LOSS: 3.3582455216674134e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 1/71 | LOSS: 3.2143645967153134e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 2/71 | LOSS: 3.092578936048085e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 3/71 | LOSS: 3.2170827921618184e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 4/71 | LOSS: 3.2539639050810364e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 5/71 | LOSS: 3.1846993806539103e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 6/71 | LOSS: 3.486966566664965e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 7/71 | LOSS: 3.6108624499320285e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 8/71 | LOSS: 3.662549513844877e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 9/71 | LOSS: 3.7098823213455034e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 10/71 | LOSS: 3.8237937835881235e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 11/71 | LOSS: 3.839231074683387e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 12/71 | LOSS: 3.880994916891303e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 13/71 | LOSS: 3.865892040266772e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 14/71 | LOSS: 3.973758672752107e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 15/71 | LOSS: 4.00340007900013e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 16/71 | LOSS: 3.993658377593794e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 17/71 | LOSS: 3.967850893281542e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 18/71 | LOSS: 4.054004780061261e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 19/71 | LOSS: 4.064699237460445e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 20/71 | LOSS: 4.140548060691106e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 21/71 | LOSS: 4.150190947753303e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 22/71 | LOSS: 4.1234562045049286e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 23/71 | LOSS: 4.0991763701943755e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 24/71 | LOSS: 4.068060588906519e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 25/71 | LOSS: 4.023848042869269e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 26/71 | LOSS: 4.075579134358257e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 27/71 | LOSS: 4.074113202737603e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 28/71 | LOSS: 4.119868569024879e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 29/71 | LOSS: 4.13009115618479e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 30/71 | LOSS: 4.17480625049305e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 31/71 | LOSS: 4.160745625370055e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 32/71 | LOSS: 4.261394024345491e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 33/71 | LOSS: 4.260598048484312e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 34/71 | LOSS: 4.2533217635666785e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 35/71 | LOSS: 4.2960022849709076e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 36/71 | LOSS: 4.297469067259462e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 37/71 | LOSS: 4.319718964564488e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 38/71 | LOSS: 4.3077390949829114e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 39/71 | LOSS: 4.351668616209281e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 40/71 | LOSS: 4.342034444340331e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 41/71 | LOSS: 4.331451491236207e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 42/71 | LOSS: 4.3483933305376625e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 43/71 | LOSS: 4.331578421360146e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 44/71 | LOSS: 4.342148036408212e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 45/71 | LOSS: 4.329363669650933e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 46/71 | LOSS: 4.314777363996403e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 47/71 | LOSS: 4.29104484093538e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 48/71 | LOSS: 4.311139538022333e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 49/71 | LOSS: 4.299794891267084e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 50/71 | LOSS: 4.283236666749729e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 51/71 | LOSS: 4.289575993145543e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 52/71 | LOSS: 4.282908261268189e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 53/71 | LOSS: 4.274949114241517e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 54/71 | LOSS: 4.280171875011133e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 55/71 | LOSS: 4.286350425835346e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 56/71 | LOSS: 4.275516694338365e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 57/71 | LOSS: 4.261527131912489e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 58/71 | LOSS: 4.265568369361463e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 59/71 | LOSS: 4.266754729087552e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 60/71 | LOSS: 4.26139570395133e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 61/71 | LOSS: 4.277914040211462e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 62/71 | LOSS: 4.2717825700882325e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 63/71 | LOSS: 4.268982639388241e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 64/71 | LOSS: 4.275925287472022e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 65/71 | LOSS: 4.286369840938829e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 66/71 | LOSS: 4.3005254542889026e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 67/71 | LOSS: 4.306545630537157e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 68/71 | LOSS: 4.290378808656132e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 69/71 | LOSS: 4.287439706526389e-06\n",
      "TRAIN: EPOCH 673/1000 | BATCH 70/71 | LOSS: 4.272675506771021e-06\n",
      "VAL: EPOCH 673/1000 | BATCH 0/8 | LOSS: 4.366665962152183e-06\n",
      "VAL: EPOCH 673/1000 | BATCH 1/8 | LOSS: 4.50958350484143e-06\n",
      "VAL: EPOCH 673/1000 | BATCH 2/8 | LOSS: 4.481087671592832e-06\n",
      "VAL: EPOCH 673/1000 | BATCH 3/8 | LOSS: 4.40117310063215e-06\n",
      "VAL: EPOCH 673/1000 | BATCH 4/8 | LOSS: 4.515331602306105e-06\n",
      "VAL: EPOCH 673/1000 | BATCH 5/8 | LOSS: 4.5199693280058755e-06\n",
      "VAL: EPOCH 673/1000 | BATCH 6/8 | LOSS: 4.43106403378936e-06\n",
      "VAL: EPOCH 673/1000 | BATCH 7/8 | LOSS: 4.365519828297693e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 0/71 | LOSS: 3.4408276405883953e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 1/71 | LOSS: 3.5836764027408208e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 2/71 | LOSS: 3.674569218977316e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 3/71 | LOSS: 3.7520130149459874e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 4/71 | LOSS: 4.017210312667885e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 5/71 | LOSS: 3.992585069075479e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 6/71 | LOSS: 4.010115422456043e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 7/71 | LOSS: 3.985844529097449e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 8/71 | LOSS: 4.104525866447754e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 9/71 | LOSS: 4.1990461113528e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 10/71 | LOSS: 4.268289744273219e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 11/71 | LOSS: 4.2756587959047465e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 12/71 | LOSS: 4.382334509500652e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 13/71 | LOSS: 4.469801410778018e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 14/71 | LOSS: 4.408035692904377e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 15/71 | LOSS: 4.555708187581331e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 16/71 | LOSS: 4.542526767181698e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 17/71 | LOSS: 4.641112708567461e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 18/71 | LOSS: 4.689686073489659e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 19/71 | LOSS: 4.63593667063833e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 20/71 | LOSS: 4.6843226430480304e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 21/71 | LOSS: 4.7850254428670844e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 22/71 | LOSS: 4.826340876246357e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 23/71 | LOSS: 4.977634716851753e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 24/71 | LOSS: 5.023802332289051e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 25/71 | LOSS: 5.1212421987237085e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 26/71 | LOSS: 5.090721717553808e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 27/71 | LOSS: 5.16316474983926e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 28/71 | LOSS: 5.119574309681976e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 29/71 | LOSS: 5.237650566414232e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 30/71 | LOSS: 5.198548977557127e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 31/71 | LOSS: 5.205995321944101e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 32/71 | LOSS: 5.206003786015296e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 33/71 | LOSS: 5.237677789730643e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 34/71 | LOSS: 5.216397130425321e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 35/71 | LOSS: 5.160038262551501e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 36/71 | LOSS: 5.11260949482479e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 37/71 | LOSS: 5.095691957840724e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 38/71 | LOSS: 5.098986694298592e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 39/71 | LOSS: 5.07542660557192e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 40/71 | LOSS: 5.051561607260515e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 41/71 | LOSS: 5.043463147558214e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 42/71 | LOSS: 5.01499000144344e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 43/71 | LOSS: 4.9985604484638975e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 44/71 | LOSS: 4.9997921602577355e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 45/71 | LOSS: 4.968689925432124e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 46/71 | LOSS: 4.939804386324001e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 47/71 | LOSS: 4.941577856243384e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 48/71 | LOSS: 4.948265773329732e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 49/71 | LOSS: 4.938186002618749e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 50/71 | LOSS: 4.93537153684598e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 51/71 | LOSS: 4.916655965504246e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 52/71 | LOSS: 4.898954943681294e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 53/71 | LOSS: 4.90115868429981e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 54/71 | LOSS: 4.899237914783457e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 55/71 | LOSS: 4.874942565688148e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 56/71 | LOSS: 4.864176860377356e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 57/71 | LOSS: 4.839739043244004e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 58/71 | LOSS: 4.868294165474146e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 59/71 | LOSS: 4.8605761473178666e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 60/71 | LOSS: 4.861937735559415e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 61/71 | LOSS: 4.848046772519499e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 62/71 | LOSS: 4.849219460574825e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 63/71 | LOSS: 4.81725385625964e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 64/71 | LOSS: 4.821392146298492e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 65/71 | LOSS: 4.817821777027293e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 66/71 | LOSS: 4.814934754904243e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 67/71 | LOSS: 4.805766503282297e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 68/71 | LOSS: 4.807458787542165e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 69/71 | LOSS: 4.805820308969747e-06\n",
      "TRAIN: EPOCH 674/1000 | BATCH 70/71 | LOSS: 4.796900877639492e-06\n",
      "VAL: EPOCH 674/1000 | BATCH 0/8 | LOSS: 3.8005657643225277e-06\n",
      "VAL: EPOCH 674/1000 | BATCH 1/8 | LOSS: 4.423885570759012e-06\n",
      "VAL: EPOCH 674/1000 | BATCH 2/8 | LOSS: 4.52962353847397e-06\n",
      "VAL: EPOCH 674/1000 | BATCH 3/8 | LOSS: 4.590848618590826e-06\n",
      "VAL: EPOCH 674/1000 | BATCH 4/8 | LOSS: 4.55599606539181e-06\n",
      "VAL: EPOCH 674/1000 | BATCH 5/8 | LOSS: 4.586057798405818e-06\n",
      "VAL: EPOCH 674/1000 | BATCH 6/8 | LOSS: 4.521820853499646e-06\n",
      "VAL: EPOCH 674/1000 | BATCH 7/8 | LOSS: 4.383110592698358e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 0/71 | LOSS: 4.3897630348510575e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 1/71 | LOSS: 4.480660891204025e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 2/71 | LOSS: 4.641704890673282e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 3/71 | LOSS: 4.4219729034011834e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 4/71 | LOSS: 4.297057694202522e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 5/71 | LOSS: 4.374912577986834e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 6/71 | LOSS: 4.220851581391928e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 7/71 | LOSS: 4.181031926009382e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 8/71 | LOSS: 4.076664203643708e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 9/71 | LOSS: 4.070267937095195e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 10/71 | LOSS: 4.020721214394805e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 11/71 | LOSS: 4.041239359745911e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 12/71 | LOSS: 4.0590676829966606e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 13/71 | LOSS: 4.0513509052938645e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 14/71 | LOSS: 4.026109218102647e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 15/71 | LOSS: 3.9883275633201265e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 16/71 | LOSS: 3.97347404192688e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 17/71 | LOSS: 3.974162938094297e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 18/71 | LOSS: 3.947970550430718e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 19/71 | LOSS: 3.939083558179845e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 20/71 | LOSS: 3.977097657341455e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 21/71 | LOSS: 3.989094362954017e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 22/71 | LOSS: 4.00659825533259e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 23/71 | LOSS: 3.978598632177939e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 24/71 | LOSS: 3.981822792411549e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 25/71 | LOSS: 3.980416487628156e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 26/71 | LOSS: 3.97916356733832e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 27/71 | LOSS: 4.033975772342403e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 28/71 | LOSS: 4.020875237607658e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 29/71 | LOSS: 3.999562515370295e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 30/71 | LOSS: 3.995559095968812e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 31/71 | LOSS: 4.0284228859377436e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 32/71 | LOSS: 4.064381583344639e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 33/71 | LOSS: 4.041315117533754e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 34/71 | LOSS: 4.061264079220044e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 35/71 | LOSS: 4.0394795822370115e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 36/71 | LOSS: 4.032289178065037e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 37/71 | LOSS: 4.0708711438990004e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 38/71 | LOSS: 4.058792461737772e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 39/71 | LOSS: 4.064168189188422e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 40/71 | LOSS: 4.0617724984835285e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 41/71 | LOSS: 4.105700181103852e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 42/71 | LOSS: 4.106853355044096e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 43/71 | LOSS: 4.1101509137578516e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 44/71 | LOSS: 4.12171233518974e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 45/71 | LOSS: 4.121207661742317e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 46/71 | LOSS: 4.117317254721011e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 47/71 | LOSS: 4.11394171824971e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 48/71 | LOSS: 4.113783071651207e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 49/71 | LOSS: 4.11175434692268e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 50/71 | LOSS: 4.107431595020542e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 51/71 | LOSS: 4.1108492172707674e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 52/71 | LOSS: 4.11310259310857e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 53/71 | LOSS: 4.124753209671999e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 54/71 | LOSS: 4.11235667095091e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 55/71 | LOSS: 4.108012575443354e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 56/71 | LOSS: 4.1014137431167845e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 57/71 | LOSS: 4.105846242368741e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 58/71 | LOSS: 4.100269383713366e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 59/71 | LOSS: 4.1219734763823604e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 60/71 | LOSS: 4.112366136947368e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 61/71 | LOSS: 4.1282647001231275e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 62/71 | LOSS: 4.1329825661604345e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 63/71 | LOSS: 4.120656086570307e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 64/71 | LOSS: 4.1156652304590145e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 65/71 | LOSS: 4.1142067760974346e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 66/71 | LOSS: 4.104763737773486e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 67/71 | LOSS: 4.1123187971232466e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 68/71 | LOSS: 4.106008138504863e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 69/71 | LOSS: 4.117418331718779e-06\n",
      "TRAIN: EPOCH 675/1000 | BATCH 70/71 | LOSS: 4.131437035411322e-06\n",
      "VAL: EPOCH 675/1000 | BATCH 0/8 | LOSS: 4.16162220062688e-06\n",
      "VAL: EPOCH 675/1000 | BATCH 1/8 | LOSS: 4.632556056094472e-06\n",
      "VAL: EPOCH 675/1000 | BATCH 2/8 | LOSS: 4.7780769515763195e-06\n",
      "VAL: EPOCH 675/1000 | BATCH 3/8 | LOSS: 4.915971999253088e-06\n",
      "VAL: EPOCH 675/1000 | BATCH 4/8 | LOSS: 4.864297898166114e-06\n",
      "VAL: EPOCH 675/1000 | BATCH 5/8 | LOSS: 4.995631191680634e-06\n",
      "VAL: EPOCH 675/1000 | BATCH 6/8 | LOSS: 4.965474090568023e-06\n",
      "VAL: EPOCH 675/1000 | BATCH 7/8 | LOSS: 4.8460443053954805e-06\n",
      "TRAIN: EPOCH 676/1000 | BATCH 0/71 | LOSS: 5.033479283156339e-06\n",
      "TRAIN: EPOCH 676/1000 | BATCH 1/71 | LOSS: 4.651923518395051e-06\n",
      "TRAIN: EPOCH 676/1000 | BATCH 2/71 | LOSS: 4.290299784770468e-06\n",
      "TRAIN: EPOCH 676/1000 | BATCH 3/71 | LOSS: 4.455561224858684e-06\n",
      "TRAIN: EPOCH 676/1000 | BATCH 4/71 | LOSS: 4.438941505213734e-06\n",
      "TRAIN: EPOCH 676/1000 | BATCH 5/71 | LOSS: 4.489657309629062e-06\n",
      "TRAIN: EPOCH 676/1000 | BATCH 6/71 | LOSS: 4.417887079658353e-06\n",
      "TRAIN: EPOCH 676/1000 | BATCH 7/71 | LOSS: 4.460843229026068e-06\n",
      "TRAIN: EPOCH 676/1000 | BATCH 8/71 | LOSS: 4.339975096930478e-06\n",
      "TRAIN: EPOCH 676/1000 | BATCH 9/71 | LOSS: 4.3322955889379955e-06\n",
      "TRAIN: EPOCH 676/1000 | BATCH 10/71 | LOSS: 4.328213042919577e-06\n",
      "TRAIN: EPOCH 676/1000 | BATCH 11/71 | LOSS: 4.332441619681049e-06\n",
      "TRAIN: EPOCH 676/1000 | BATCH 12/71 | LOSS: 4.269489741440898e-06\n",
      "TRAIN: EPOCH 676/1000 | BATCH 13/71 | LOSS: 4.234300896704164e-06\n",
      "TRAIN: EPOCH 676/1000 | BATCH 14/71 | LOSS: 4.46959566033911e-06\n",
      "TRAIN: EPOCH 676/1000 | BATCH 15/71 | LOSS: 4.420359715595623e-06\n",
      "TRAIN: EPOCH 676/1000 | BATCH 16/71 | LOSS: 4.441744268286343e-06\n",
      "TRAIN: EPOCH 676/1000 | BATCH 17/71 | LOSS: 4.498290208074549e-06\n",
      "TRAIN: EPOCH 676/1000 | BATCH 18/71 | LOSS: 4.464897995747283e-06\n",
      "TRAIN: EPOCH 676/1000 | BATCH 19/71 | LOSS: 4.4646830247074835e-06\n",
      "TRAIN: EPOCH 676/1000 | BATCH 20/71 | LOSS: 4.504566320628371e-06\n",
      "TRAIN: EPOCH 676/1000 | BATCH 21/71 | LOSS: 4.569344334661516e-06\n",
      "TRAIN: EPOCH 676/1000 | BATCH 22/71 | LOSS: 4.567362101743524e-06\n",
      "TRAIN: EPOCH 676/1000 | BATCH 23/71 | LOSS: 4.629299799792837e-06\n",
      "TRAIN: EPOCH 676/1000 | BATCH 24/71 | LOSS: 4.630896255548578e-06\n"
     ]
    }
   ],
   "source": [
    "trained_state = trainer(\n",
    "    state, train_loader, val_loader, l2_loss_fn, \n",
    "    num_epochs=1000, exp_str=hyperparams.to_str())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
