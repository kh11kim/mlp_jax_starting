{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jax\n",
    "from jax import random\n",
    "import jax.numpy as jnp\n",
    "from network import *\n",
    "from train import *\n",
    "from dataset import *\n",
    "from flax.training.train_state import TrainState\n",
    "import torch.utils.data as data\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [2, 20, 20, 1]\n",
    "lr = 0.001\n",
    "batch_size = 128\n",
    "hyperparams = dict(lr=0.001, layers=layers, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"circle.csv\")\n",
    "dataset = NumpyDataset(df[[\"x\", \"y\"]].to_numpy(), df[\"d\"].to_numpy())\n",
    "train_dataset, val_dataset = train_test_split(dataset, train_size=0.9, shuffle=True)\n",
    "train_loader = data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=numpy_collate)\n",
    "val_loader = data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, collate_fn=numpy_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(layers[0], layers[1:], 0)\n",
    "\n",
    "key1, key2 = random.split(random.PRNGKey(0))\n",
    "x = random.normal(key1, (2,)) # Dummy input data\n",
    "params = model.init(key2, x) # Initialization call\n",
    "tx = optax.adam(learning_rate=lr)\n",
    "state = TrainState.create(apply_fn=model.apply, params=params, tx=tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: EPOCH 1/100 | BATCH 0/71 | LOSS: 0.24438031017780304\n",
      "TRAIN: EPOCH 1/100 | BATCH 1/71 | LOSS: 0.24572228640317917\n",
      "TRAIN: EPOCH 1/100 | BATCH 2/71 | LOSS: 0.24942639966805777\n",
      "TRAIN: EPOCH 1/100 | BATCH 3/71 | LOSS: 0.24645480886101723\n",
      "TRAIN: EPOCH 1/100 | BATCH 4/71 | LOSS: 0.2503469377756119\n",
      "TRAIN: EPOCH 1/100 | BATCH 5/71 | LOSS: 0.24746694415807724\n",
      "TRAIN: EPOCH 1/100 | BATCH 6/71 | LOSS: 0.24702578570161546\n",
      "TRAIN: EPOCH 1/100 | BATCH 7/71 | LOSS: 0.2452220842242241\n",
      "TRAIN: EPOCH 1/100 | BATCH 8/71 | LOSS: 0.238811817434099\n",
      "TRAIN: EPOCH 1/100 | BATCH 9/71 | LOSS: 0.23679004609584808\n",
      "TRAIN: EPOCH 1/100 | BATCH 10/71 | LOSS: 0.23462197184562683\n",
      "TRAIN: EPOCH 1/100 | BATCH 11/71 | LOSS: 0.23097097997864088\n",
      "TRAIN: EPOCH 1/100 | BATCH 12/71 | LOSS: 0.23170654475688934\n",
      "TRAIN: EPOCH 1/100 | BATCH 13/71 | LOSS: 0.22949908993073873\n",
      "TRAIN: EPOCH 1/100 | BATCH 14/71 | LOSS: 0.2254416137933731\n",
      "TRAIN: EPOCH 1/100 | BATCH 15/71 | LOSS: 0.22129459120333195\n",
      "TRAIN: EPOCH 1/100 | BATCH 16/71 | LOSS: 0.2179059666745803\n",
      "TRAIN: EPOCH 1/100 | BATCH 17/71 | LOSS: 0.21496720612049103\n",
      "TRAIN: EPOCH 1/100 | BATCH 18/71 | LOSS: 0.21046431284201772\n",
      "TRAIN: EPOCH 1/100 | BATCH 19/71 | LOSS: 0.20778029337525367\n",
      "TRAIN: EPOCH 1/100 | BATCH 20/71 | LOSS: 0.20404411497570218\n",
      "TRAIN: EPOCH 1/100 | BATCH 21/71 | LOSS: 0.20128471539779144\n",
      "TRAIN: EPOCH 1/100 | BATCH 22/71 | LOSS: 0.1986758300791616\n",
      "TRAIN: EPOCH 1/100 | BATCH 23/71 | LOSS: 0.19627352183063826\n",
      "TRAIN: EPOCH 1/100 | BATCH 24/71 | LOSS: 0.19375263810157775\n",
      "TRAIN: EPOCH 1/100 | BATCH 25/71 | LOSS: 0.19137915567709848\n",
      "TRAIN: EPOCH 1/100 | BATCH 26/71 | LOSS: 0.1891230515859745\n",
      "TRAIN: EPOCH 1/100 | BATCH 27/71 | LOSS: 0.1869246789387294\n",
      "TRAIN: EPOCH 1/100 | BATCH 28/71 | LOSS: 0.18423933612889257\n",
      "TRAIN: EPOCH 1/100 | BATCH 29/71 | LOSS: 0.1813571368654569\n",
      "TRAIN: EPOCH 1/100 | BATCH 30/71 | LOSS: 0.17913428958385222\n",
      "TRAIN: EPOCH 1/100 | BATCH 31/71 | LOSS: 0.17710109520703554\n",
      "TRAIN: EPOCH 1/100 | BATCH 32/71 | LOSS: 0.17523517504786001\n",
      "TRAIN: EPOCH 1/100 | BATCH 33/71 | LOSS: 0.17268610373139381\n",
      "TRAIN: EPOCH 1/100 | BATCH 34/71 | LOSS: 0.17071345916816166\n",
      "TRAIN: EPOCH 1/100 | BATCH 35/71 | LOSS: 0.16840615102814305\n",
      "TRAIN: EPOCH 1/100 | BATCH 36/71 | LOSS: 0.1662258846131531\n",
      "TRAIN: EPOCH 1/100 | BATCH 37/71 | LOSS: 0.16430126601143888\n",
      "TRAIN: EPOCH 1/100 | BATCH 38/71 | LOSS: 0.16179969333685362\n",
      "TRAIN: EPOCH 1/100 | BATCH 39/71 | LOSS: 0.15994369611144066\n",
      "TRAIN: EPOCH 1/100 | BATCH 40/71 | LOSS: 0.15836082289858563\n",
      "TRAIN: EPOCH 1/100 | BATCH 41/71 | LOSS: 0.1565464185107322\n",
      "TRAIN: EPOCH 1/100 | BATCH 42/71 | LOSS: 0.15485844050728997\n",
      "TRAIN: EPOCH 1/100 | BATCH 43/71 | LOSS: 0.1532152516936714\n",
      "TRAIN: EPOCH 1/100 | BATCH 44/71 | LOSS: 0.15128420193990072\n",
      "TRAIN: EPOCH 1/100 | BATCH 45/71 | LOSS: 0.1494211313193259\n",
      "TRAIN: EPOCH 1/100 | BATCH 46/71 | LOSS: 0.1477492221809448\n",
      "TRAIN: EPOCH 1/100 | BATCH 47/71 | LOSS: 0.14597048393140236\n",
      "TRAIN: EPOCH 1/100 | BATCH 48/71 | LOSS: 0.14458950472121335\n",
      "TRAIN: EPOCH 1/100 | BATCH 49/71 | LOSS: 0.14318416982889176\n",
      "TRAIN: EPOCH 1/100 | BATCH 50/71 | LOSS: 0.14160642290816589\n",
      "TRAIN: EPOCH 1/100 | BATCH 51/71 | LOSS: 0.14036355927013433\n",
      "TRAIN: EPOCH 1/100 | BATCH 52/71 | LOSS: 0.13890041748307785\n",
      "TRAIN: EPOCH 1/100 | BATCH 53/71 | LOSS: 0.13757225098433318\n",
      "TRAIN: EPOCH 1/100 | BATCH 54/71 | LOSS: 0.1361940688707612\n",
      "TRAIN: EPOCH 1/100 | BATCH 55/71 | LOSS: 0.1350295011486326\n",
      "TRAIN: EPOCH 1/100 | BATCH 56/71 | LOSS: 0.13360022041097022\n",
      "TRAIN: EPOCH 1/100 | BATCH 57/71 | LOSS: 0.13235765528576127\n",
      "TRAIN: EPOCH 1/100 | BATCH 58/71 | LOSS: 0.13087753282260087\n",
      "TRAIN: EPOCH 1/100 | BATCH 59/71 | LOSS: 0.12981980989376704\n",
      "TRAIN: EPOCH 1/100 | BATCH 60/71 | LOSS: 0.12869216611639397\n",
      "TRAIN: EPOCH 1/100 | BATCH 61/71 | LOSS: 0.12762532491357095\n",
      "TRAIN: EPOCH 1/100 | BATCH 62/71 | LOSS: 0.12658987146994424\n",
      "TRAIN: EPOCH 1/100 | BATCH 63/71 | LOSS: 0.12552263477118686\n",
      "TRAIN: EPOCH 1/100 | BATCH 64/71 | LOSS: 0.12428527216498668\n",
      "TRAIN: EPOCH 1/100 | BATCH 65/71 | LOSS: 0.12333912803142359\n",
      "TRAIN: EPOCH 1/100 | BATCH 66/71 | LOSS: 0.12226788049091154\n",
      "TRAIN: EPOCH 1/100 | BATCH 67/71 | LOSS: 0.12133592931444154\n",
      "TRAIN: EPOCH 1/100 | BATCH 68/71 | LOSS: 0.12065101593084958\n",
      "TRAIN: EPOCH 1/100 | BATCH 69/71 | LOSS: 0.1197330227387803\n",
      "TRAIN: EPOCH 1/100 | BATCH 70/71 | LOSS: 0.11867833378869043\n",
      "VAL: EPOCH 1/100 | BATCH 0/8 | LOSS: 0.05813115835189819\n",
      "VAL: EPOCH 1/100 | BATCH 1/8 | LOSS: 0.056852586567401886\n",
      "VAL: EPOCH 1/100 | BATCH 2/8 | LOSS: 0.05792065213123957\n",
      "VAL: EPOCH 1/100 | BATCH 3/8 | LOSS: 0.05707158800214529\n",
      "VAL: EPOCH 1/100 | BATCH 4/8 | LOSS: 0.05584411695599556\n",
      "VAL: EPOCH 1/100 | BATCH 5/8 | LOSS: 0.055036863312125206\n",
      "VAL: EPOCH 1/100 | BATCH 6/8 | LOSS: 0.055734965418066294\n",
      "VAL: EPOCH 1/100 | BATCH 7/8 | LOSS: 0.0549308555200696\n",
      "TRAIN: EPOCH 2/100 | BATCH 0/71 | LOSS: 0.06443404406309128\n",
      "TRAIN: EPOCH 2/100 | BATCH 1/71 | LOSS: 0.05849965289235115\n",
      "TRAIN: EPOCH 2/100 | BATCH 2/71 | LOSS: 0.056509206692377724\n",
      "TRAIN: EPOCH 2/100 | BATCH 3/71 | LOSS: 0.05412032827734947\n",
      "TRAIN: EPOCH 2/100 | BATCH 4/71 | LOSS: 0.05564012080430984\n",
      "TRAIN: EPOCH 2/100 | BATCH 5/71 | LOSS: 0.055054868261019387\n",
      "TRAIN: EPOCH 2/100 | BATCH 6/71 | LOSS: 0.053657806877579005\n",
      "TRAIN: EPOCH 2/100 | BATCH 7/71 | LOSS: 0.053355376701802015\n",
      "TRAIN: EPOCH 2/100 | BATCH 8/71 | LOSS: 0.05342127589715852\n",
      "TRAIN: EPOCH 2/100 | BATCH 9/71 | LOSS: 0.05321037657558918\n",
      "TRAIN: EPOCH 2/100 | BATCH 10/71 | LOSS: 0.05274552411653779\n",
      "TRAIN: EPOCH 2/100 | BATCH 11/71 | LOSS: 0.052813829543689884\n",
      "TRAIN: EPOCH 2/100 | BATCH 12/71 | LOSS: 0.05224584902708347\n",
      "TRAIN: EPOCH 2/100 | BATCH 13/71 | LOSS: 0.052412537059613636\n",
      "TRAIN: EPOCH 2/100 | BATCH 14/71 | LOSS: 0.051958793153365455\n",
      "TRAIN: EPOCH 2/100 | BATCH 15/71 | LOSS: 0.05203688680194318\n",
      "TRAIN: EPOCH 2/100 | BATCH 16/71 | LOSS: 0.05152411430197604\n",
      "TRAIN: EPOCH 2/100 | BATCH 17/71 | LOSS: 0.05136081845396095\n",
      "TRAIN: EPOCH 2/100 | BATCH 18/71 | LOSS: 0.05171624041701618\n",
      "TRAIN: EPOCH 2/100 | BATCH 19/71 | LOSS: 0.051324117369949816\n",
      "TRAIN: EPOCH 2/100 | BATCH 20/71 | LOSS: 0.05136637691231001\n",
      "TRAIN: EPOCH 2/100 | BATCH 21/71 | LOSS: 0.05102598023685542\n",
      "TRAIN: EPOCH 2/100 | BATCH 22/71 | LOSS: 0.0508525928725367\n",
      "TRAIN: EPOCH 2/100 | BATCH 23/71 | LOSS: 0.05055317593117555\n",
      "TRAIN: EPOCH 2/100 | BATCH 24/71 | LOSS: 0.05044169902801514\n",
      "TRAIN: EPOCH 2/100 | BATCH 25/71 | LOSS: 0.0504447532674441\n",
      "TRAIN: EPOCH 2/100 | BATCH 26/71 | LOSS: 0.05023607708237789\n",
      "TRAIN: EPOCH 2/100 | BATCH 27/71 | LOSS: 0.0501125053103481\n",
      "TRAIN: EPOCH 2/100 | BATCH 28/71 | LOSS: 0.04972593383542422\n",
      "TRAIN: EPOCH 2/100 | BATCH 29/71 | LOSS: 0.04943224166830381\n",
      "TRAIN: EPOCH 2/100 | BATCH 30/71 | LOSS: 0.04947619500660127\n",
      "TRAIN: EPOCH 2/100 | BATCH 31/71 | LOSS: 0.049234103644266725\n",
      "TRAIN: EPOCH 2/100 | BATCH 32/71 | LOSS: 0.04915067892182957\n",
      "TRAIN: EPOCH 2/100 | BATCH 33/71 | LOSS: 0.04916393789736664\n",
      "TRAIN: EPOCH 2/100 | BATCH 34/71 | LOSS: 0.04901871457695961\n",
      "TRAIN: EPOCH 2/100 | BATCH 35/71 | LOSS: 0.04887915557871262\n",
      "TRAIN: EPOCH 2/100 | BATCH 36/71 | LOSS: 0.04881902492126903\n",
      "TRAIN: EPOCH 2/100 | BATCH 37/71 | LOSS: 0.04865469881578496\n",
      "TRAIN: EPOCH 2/100 | BATCH 38/71 | LOSS: 0.048335729501186274\n",
      "TRAIN: EPOCH 2/100 | BATCH 39/71 | LOSS: 0.04796618577092886\n",
      "TRAIN: EPOCH 2/100 | BATCH 40/71 | LOSS: 0.04780648566964196\n",
      "TRAIN: EPOCH 2/100 | BATCH 41/71 | LOSS: 0.04765485528679121\n",
      "TRAIN: EPOCH 2/100 | BATCH 42/71 | LOSS: 0.04762306410905927\n",
      "TRAIN: EPOCH 2/100 | BATCH 43/71 | LOSS: 0.04764565664597533\n",
      "TRAIN: EPOCH 2/100 | BATCH 44/71 | LOSS: 0.04739966400795513\n",
      "TRAIN: EPOCH 2/100 | BATCH 45/71 | LOSS: 0.04731513187289238\n",
      "TRAIN: EPOCH 2/100 | BATCH 46/71 | LOSS: 0.04726645366308537\n",
      "TRAIN: EPOCH 2/100 | BATCH 47/71 | LOSS: 0.04712189004446069\n",
      "TRAIN: EPOCH 2/100 | BATCH 48/71 | LOSS: 0.046921313873359134\n",
      "TRAIN: EPOCH 2/100 | BATCH 49/71 | LOSS: 0.04689234666526318\n",
      "TRAIN: EPOCH 2/100 | BATCH 50/71 | LOSS: 0.046782388333596436\n",
      "TRAIN: EPOCH 2/100 | BATCH 51/71 | LOSS: 0.046647920846365966\n",
      "TRAIN: EPOCH 2/100 | BATCH 52/71 | LOSS: 0.046543866114796335\n",
      "TRAIN: EPOCH 2/100 | BATCH 53/71 | LOSS: 0.04638379350028656\n",
      "TRAIN: EPOCH 2/100 | BATCH 54/71 | LOSS: 0.04627762802622535\n",
      "TRAIN: EPOCH 2/100 | BATCH 55/71 | LOSS: 0.046181872220976014\n",
      "TRAIN: EPOCH 2/100 | BATCH 56/71 | LOSS: 0.04601640960103587\n",
      "TRAIN: EPOCH 2/100 | BATCH 57/71 | LOSS: 0.04591988746462197\n",
      "TRAIN: EPOCH 2/100 | BATCH 58/71 | LOSS: 0.04582777063725358\n",
      "TRAIN: EPOCH 2/100 | BATCH 59/71 | LOSS: 0.04571614017089208\n",
      "TRAIN: EPOCH 2/100 | BATCH 60/71 | LOSS: 0.04556598709743531\n",
      "TRAIN: EPOCH 2/100 | BATCH 61/71 | LOSS: 0.045381743100381663\n",
      "TRAIN: EPOCH 2/100 | BATCH 62/71 | LOSS: 0.045127354857940526\n",
      "TRAIN: EPOCH 2/100 | BATCH 63/71 | LOSS: 0.04504703241400421\n",
      "TRAIN: EPOCH 2/100 | BATCH 64/71 | LOSS: 0.04488633538667972\n",
      "TRAIN: EPOCH 2/100 | BATCH 65/71 | LOSS: 0.044666350593395306\n",
      "TRAIN: EPOCH 2/100 | BATCH 66/71 | LOSS: 0.044425052187558434\n",
      "TRAIN: EPOCH 2/100 | BATCH 67/71 | LOSS: 0.04411670173901845\n",
      "TRAIN: EPOCH 2/100 | BATCH 68/71 | LOSS: 0.04383049843211969\n",
      "TRAIN: EPOCH 2/100 | BATCH 69/71 | LOSS: 0.0437122305855155\n",
      "TRAIN: EPOCH 2/100 | BATCH 70/71 | LOSS: 0.043552283958440095\n",
      "VAL: EPOCH 2/100 | BATCH 0/8 | LOSS: 0.032731689512729645\n",
      "VAL: EPOCH 2/100 | BATCH 1/8 | LOSS: 0.03083557542413473\n",
      "VAL: EPOCH 2/100 | BATCH 2/8 | LOSS: 0.031455002104242645\n",
      "VAL: EPOCH 2/100 | BATCH 3/8 | LOSS: 0.030920876655727625\n",
      "VAL: EPOCH 2/100 | BATCH 4/8 | LOSS: 0.030240009352564812\n",
      "VAL: EPOCH 2/100 | BATCH 5/8 | LOSS: 0.029508091198901337\n",
      "VAL: EPOCH 2/100 | BATCH 6/8 | LOSS: 0.029769978086863245\n",
      "VAL: EPOCH 2/100 | BATCH 7/8 | LOSS: 0.02929056854918599\n",
      "TRAIN: EPOCH 3/100 | BATCH 0/71 | LOSS: 0.032987192273139954\n",
      "TRAIN: EPOCH 3/100 | BATCH 1/71 | LOSS: 0.03365972638130188\n",
      "TRAIN: EPOCH 3/100 | BATCH 2/71 | LOSS: 0.032064722230037056\n",
      "TRAIN: EPOCH 3/100 | BATCH 3/71 | LOSS: 0.03059465205296874\n",
      "TRAIN: EPOCH 3/100 | BATCH 4/71 | LOSS: 0.028899280354380608\n",
      "TRAIN: EPOCH 3/100 | BATCH 5/71 | LOSS: 0.02824695936093728\n",
      "TRAIN: EPOCH 3/100 | BATCH 6/71 | LOSS: 0.02859246278447764\n",
      "TRAIN: EPOCH 3/100 | BATCH 7/71 | LOSS: 0.027484327321872115\n",
      "TRAIN: EPOCH 3/100 | BATCH 8/71 | LOSS: 0.027254290671812162\n",
      "TRAIN: EPOCH 3/100 | BATCH 9/71 | LOSS: 0.026505112275481225\n",
      "TRAIN: EPOCH 3/100 | BATCH 10/71 | LOSS: 0.026674938134171745\n",
      "TRAIN: EPOCH 3/100 | BATCH 11/71 | LOSS: 0.026290233557422955\n",
      "TRAIN: EPOCH 3/100 | BATCH 12/71 | LOSS: 0.025923241789524373\n",
      "TRAIN: EPOCH 3/100 | BATCH 13/71 | LOSS: 0.02566630087260689\n",
      "TRAIN: EPOCH 3/100 | BATCH 14/71 | LOSS: 0.025468303139011064\n",
      "TRAIN: EPOCH 3/100 | BATCH 15/71 | LOSS: 0.02533983951434493\n",
      "TRAIN: EPOCH 3/100 | BATCH 16/71 | LOSS: 0.025358225383302745\n",
      "TRAIN: EPOCH 3/100 | BATCH 17/71 | LOSS: 0.02514355681422684\n",
      "TRAIN: EPOCH 3/100 | BATCH 18/71 | LOSS: 0.024890312336777385\n",
      "TRAIN: EPOCH 3/100 | BATCH 19/71 | LOSS: 0.02459545321762562\n",
      "TRAIN: EPOCH 3/100 | BATCH 20/71 | LOSS: 0.024212724484858058\n",
      "TRAIN: EPOCH 3/100 | BATCH 21/71 | LOSS: 0.02412906407632611\n",
      "TRAIN: EPOCH 3/100 | BATCH 22/71 | LOSS: 0.023961534075762913\n",
      "TRAIN: EPOCH 3/100 | BATCH 23/71 | LOSS: 0.02366518966543178\n",
      "TRAIN: EPOCH 3/100 | BATCH 24/71 | LOSS: 0.02350289262831211\n",
      "TRAIN: EPOCH 3/100 | BATCH 25/71 | LOSS: 0.023368830219484292\n",
      "TRAIN: EPOCH 3/100 | BATCH 26/71 | LOSS: 0.023053199156290956\n",
      "TRAIN: EPOCH 3/100 | BATCH 27/71 | LOSS: 0.022752952728686587\n",
      "TRAIN: EPOCH 3/100 | BATCH 28/71 | LOSS: 0.022518479393730903\n",
      "TRAIN: EPOCH 3/100 | BATCH 29/71 | LOSS: 0.02231497662141919\n",
      "TRAIN: EPOCH 3/100 | BATCH 30/71 | LOSS: 0.022058977174662774\n",
      "TRAIN: EPOCH 3/100 | BATCH 31/71 | LOSS: 0.021863532776478678\n",
      "TRAIN: EPOCH 3/100 | BATCH 32/71 | LOSS: 0.021593198801080387\n",
      "TRAIN: EPOCH 3/100 | BATCH 33/71 | LOSS: 0.02143497332273161\n",
      "TRAIN: EPOCH 3/100 | BATCH 34/71 | LOSS: 0.021157543414405414\n",
      "TRAIN: EPOCH 3/100 | BATCH 35/71 | LOSS: 0.02090636511436767\n",
      "TRAIN: EPOCH 3/100 | BATCH 36/71 | LOSS: 0.02072298446217099\n",
      "TRAIN: EPOCH 3/100 | BATCH 37/71 | LOSS: 0.020524947168795687\n",
      "TRAIN: EPOCH 3/100 | BATCH 38/71 | LOSS: 0.02030448381526348\n",
      "TRAIN: EPOCH 3/100 | BATCH 39/71 | LOSS: 0.02015959252603352\n",
      "TRAIN: EPOCH 3/100 | BATCH 40/71 | LOSS: 0.019959254190325737\n",
      "TRAIN: EPOCH 3/100 | BATCH 41/71 | LOSS: 0.01968669430131004\n",
      "TRAIN: EPOCH 3/100 | BATCH 42/71 | LOSS: 0.019457974211247855\n",
      "TRAIN: EPOCH 3/100 | BATCH 43/71 | LOSS: 0.01923040023327551\n",
      "TRAIN: EPOCH 3/100 | BATCH 44/71 | LOSS: 0.019000713299545977\n",
      "TRAIN: EPOCH 3/100 | BATCH 45/71 | LOSS: 0.018813606189644855\n",
      "TRAIN: EPOCH 3/100 | BATCH 46/71 | LOSS: 0.018608222220172273\n",
      "TRAIN: EPOCH 3/100 | BATCH 47/71 | LOSS: 0.018424831815840054\n",
      "TRAIN: EPOCH 3/100 | BATCH 48/71 | LOSS: 0.018200703782542627\n",
      "TRAIN: EPOCH 3/100 | BATCH 49/71 | LOSS: 0.01800946220755577\n",
      "TRAIN: EPOCH 3/100 | BATCH 50/71 | LOSS: 0.017836895625234817\n",
      "TRAIN: EPOCH 3/100 | BATCH 51/71 | LOSS: 0.017666563731976427\n",
      "TRAIN: EPOCH 3/100 | BATCH 52/71 | LOSS: 0.017489996492722124\n",
      "TRAIN: EPOCH 3/100 | BATCH 53/71 | LOSS: 0.017350099256469145\n",
      "TRAIN: EPOCH 3/100 | BATCH 54/71 | LOSS: 0.017197613926096394\n",
      "TRAIN: EPOCH 3/100 | BATCH 55/71 | LOSS: 0.017041713631312762\n",
      "TRAIN: EPOCH 3/100 | BATCH 56/71 | LOSS: 0.0168605179101098\n",
      "TRAIN: EPOCH 3/100 | BATCH 57/71 | LOSS: 0.016720377575156504\n",
      "TRAIN: EPOCH 3/100 | BATCH 58/71 | LOSS: 0.016574619266883295\n",
      "TRAIN: EPOCH 3/100 | BATCH 59/71 | LOSS: 0.016427605901844798\n",
      "TRAIN: EPOCH 3/100 | BATCH 60/71 | LOSS: 0.016252345061998387\n",
      "TRAIN: EPOCH 3/100 | BATCH 61/71 | LOSS: 0.01610760555242098\n",
      "TRAIN: EPOCH 3/100 | BATCH 62/71 | LOSS: 0.015934574077763254\n",
      "TRAIN: EPOCH 3/100 | BATCH 63/71 | LOSS: 0.01580071794160176\n",
      "TRAIN: EPOCH 3/100 | BATCH 64/71 | LOSS: 0.015670776553452014\n",
      "TRAIN: EPOCH 3/100 | BATCH 65/71 | LOSS: 0.015530789052277352\n",
      "TRAIN: EPOCH 3/100 | BATCH 66/71 | LOSS: 0.01537520803316538\n",
      "TRAIN: EPOCH 3/100 | BATCH 67/71 | LOSS: 0.01522001126260661\n",
      "TRAIN: EPOCH 3/100 | BATCH 68/71 | LOSS: 0.015074494908955217\n",
      "TRAIN: EPOCH 3/100 | BATCH 69/71 | LOSS: 0.014932651478531105\n",
      "TRAIN: EPOCH 3/100 | BATCH 70/71 | LOSS: 0.014809069210584735\n",
      "VAL: EPOCH 3/100 | BATCH 0/8 | LOSS: 0.0058680386282503605\n",
      "VAL: EPOCH 3/100 | BATCH 1/8 | LOSS: 0.00529504707083106\n",
      "VAL: EPOCH 3/100 | BATCH 2/8 | LOSS: 0.0055230942865212755\n",
      "VAL: EPOCH 3/100 | BATCH 3/8 | LOSS: 0.005317940725944936\n",
      "VAL: EPOCH 3/100 | BATCH 4/8 | LOSS: 0.005284110363572836\n",
      "VAL: EPOCH 3/100 | BATCH 5/8 | LOSS: 0.005177283116305868\n",
      "VAL: EPOCH 3/100 | BATCH 6/8 | LOSS: 0.005203956205930028\n",
      "VAL: EPOCH 3/100 | BATCH 7/8 | LOSS: 0.005182084511034191\n",
      "TRAIN: EPOCH 4/100 | BATCH 0/71 | LOSS: 0.005597303621470928\n",
      "TRAIN: EPOCH 4/100 | BATCH 1/71 | LOSS: 0.005241926526650786\n",
      "TRAIN: EPOCH 4/100 | BATCH 2/71 | LOSS: 0.004752698043982188\n",
      "TRAIN: EPOCH 4/100 | BATCH 3/71 | LOSS: 0.004907898372039199\n",
      "TRAIN: EPOCH 4/100 | BATCH 4/71 | LOSS: 0.004721770063042641\n",
      "TRAIN: EPOCH 4/100 | BATCH 5/71 | LOSS: 0.004654792137444019\n",
      "TRAIN: EPOCH 4/100 | BATCH 6/71 | LOSS: 0.0045074855110475\n",
      "TRAIN: EPOCH 4/100 | BATCH 7/71 | LOSS: 0.004357352736406028\n",
      "TRAIN: EPOCH 4/100 | BATCH 8/71 | LOSS: 0.0042913317318177885\n",
      "TRAIN: EPOCH 4/100 | BATCH 9/71 | LOSS: 0.004261917038820684\n",
      "TRAIN: EPOCH 4/100 | BATCH 10/71 | LOSS: 0.004220671803605827\n",
      "TRAIN: EPOCH 4/100 | BATCH 11/71 | LOSS: 0.004208349021306883\n",
      "TRAIN: EPOCH 4/100 | BATCH 12/71 | LOSS: 0.004091464860651355\n",
      "TRAIN: EPOCH 4/100 | BATCH 13/71 | LOSS: 0.004058703763543495\n",
      "TRAIN: EPOCH 4/100 | BATCH 14/71 | LOSS: 0.003996533481404186\n",
      "TRAIN: EPOCH 4/100 | BATCH 15/71 | LOSS: 0.0039026918821036816\n",
      "TRAIN: EPOCH 4/100 | BATCH 16/71 | LOSS: 0.003855852218454375\n",
      "TRAIN: EPOCH 4/100 | BATCH 17/71 | LOSS: 0.003795408747262425\n",
      "TRAIN: EPOCH 4/100 | BATCH 18/71 | LOSS: 0.0037454430406030858\n",
      "TRAIN: EPOCH 4/100 | BATCH 19/71 | LOSS: 0.0037086666678078474\n",
      "TRAIN: EPOCH 4/100 | BATCH 20/71 | LOSS: 0.003640771621749515\n",
      "TRAIN: EPOCH 4/100 | BATCH 21/71 | LOSS: 0.0036013754804364658\n",
      "TRAIN: EPOCH 4/100 | BATCH 22/71 | LOSS: 0.003550893991537716\n",
      "TRAIN: EPOCH 4/100 | BATCH 23/71 | LOSS: 0.003521458313722784\n",
      "TRAIN: EPOCH 4/100 | BATCH 24/71 | LOSS: 0.00345734438393265\n",
      "TRAIN: EPOCH 4/100 | BATCH 25/71 | LOSS: 0.0034101993391791787\n",
      "TRAIN: EPOCH 4/100 | BATCH 26/71 | LOSS: 0.0033612707845383774\n",
      "TRAIN: EPOCH 4/100 | BATCH 27/71 | LOSS: 0.0033282698672597428\n",
      "TRAIN: EPOCH 4/100 | BATCH 28/71 | LOSS: 0.0032756959674237617\n",
      "TRAIN: EPOCH 4/100 | BATCH 29/71 | LOSS: 0.0032297006032119195\n",
      "TRAIN: EPOCH 4/100 | BATCH 30/71 | LOSS: 0.0031869113107302017\n",
      "TRAIN: EPOCH 4/100 | BATCH 31/71 | LOSS: 0.0031482060257985722\n",
      "TRAIN: EPOCH 4/100 | BATCH 32/71 | LOSS: 0.0031127442189520507\n",
      "TRAIN: EPOCH 4/100 | BATCH 33/71 | LOSS: 0.0030770375194740206\n",
      "TRAIN: EPOCH 4/100 | BATCH 34/71 | LOSS: 0.0030334831647840996\n",
      "TRAIN: EPOCH 4/100 | BATCH 35/71 | LOSS: 0.003010349112123044\n",
      "TRAIN: EPOCH 4/100 | BATCH 36/71 | LOSS: 0.0029861115995240776\n",
      "TRAIN: EPOCH 4/100 | BATCH 37/71 | LOSS: 0.00293781626724491\n",
      "TRAIN: EPOCH 4/100 | BATCH 38/71 | LOSS: 0.0029058390153715243\n",
      "TRAIN: EPOCH 4/100 | BATCH 39/71 | LOSS: 0.002883953007403761\n",
      "TRAIN: EPOCH 4/100 | BATCH 40/71 | LOSS: 0.0028536009698788205\n",
      "TRAIN: EPOCH 4/100 | BATCH 41/71 | LOSS: 0.0028283290504034432\n",
      "TRAIN: EPOCH 4/100 | BATCH 42/71 | LOSS: 0.002798338166199798\n",
      "TRAIN: EPOCH 4/100 | BATCH 43/71 | LOSS: 0.002773202148752964\n",
      "TRAIN: EPOCH 4/100 | BATCH 44/71 | LOSS: 0.0027382148584971827\n",
      "TRAIN: EPOCH 4/100 | BATCH 45/71 | LOSS: 0.002706230777498011\n",
      "TRAIN: EPOCH 4/100 | BATCH 46/71 | LOSS: 0.002675066464283365\n",
      "TRAIN: EPOCH 4/100 | BATCH 47/71 | LOSS: 0.0026482619238474094\n",
      "TRAIN: EPOCH 4/100 | BATCH 48/71 | LOSS: 0.002624361725923206\n",
      "TRAIN: EPOCH 4/100 | BATCH 49/71 | LOSS: 0.002611300649587065\n",
      "TRAIN: EPOCH 4/100 | BATCH 50/71 | LOSS: 0.0025860933911091848\n",
      "TRAIN: EPOCH 4/100 | BATCH 51/71 | LOSS: 0.0025613387809314122\n",
      "TRAIN: EPOCH 4/100 | BATCH 52/71 | LOSS: 0.0025458576109365755\n",
      "TRAIN: EPOCH 4/100 | BATCH 53/71 | LOSS: 0.002526214296705331\n",
      "TRAIN: EPOCH 4/100 | BATCH 54/71 | LOSS: 0.0025055983717637985\n",
      "TRAIN: EPOCH 4/100 | BATCH 55/71 | LOSS: 0.0024830294818717186\n",
      "TRAIN: EPOCH 4/100 | BATCH 56/71 | LOSS: 0.0024638001075864098\n",
      "TRAIN: EPOCH 4/100 | BATCH 57/71 | LOSS: 0.002445480160029798\n",
      "TRAIN: EPOCH 4/100 | BATCH 58/71 | LOSS: 0.0024223443262784156\n",
      "TRAIN: EPOCH 4/100 | BATCH 59/71 | LOSS: 0.002404596458654851\n",
      "TRAIN: EPOCH 4/100 | BATCH 60/71 | LOSS: 0.0023822623427163384\n",
      "TRAIN: EPOCH 4/100 | BATCH 61/71 | LOSS: 0.0023629870415935593\n",
      "TRAIN: EPOCH 4/100 | BATCH 62/71 | LOSS: 0.0023413238481485416\n",
      "TRAIN: EPOCH 4/100 | BATCH 63/71 | LOSS: 0.0023226381108543137\n",
      "TRAIN: EPOCH 4/100 | BATCH 64/71 | LOSS: 0.0023055534433716763\n",
      "TRAIN: EPOCH 4/100 | BATCH 65/71 | LOSS: 0.002287718197542497\n",
      "TRAIN: EPOCH 4/100 | BATCH 66/71 | LOSS: 0.0022658509977245287\n",
      "TRAIN: EPOCH 4/100 | BATCH 67/71 | LOSS: 0.002249581556123517\n",
      "TRAIN: EPOCH 4/100 | BATCH 68/71 | LOSS: 0.002232131306959343\n",
      "TRAIN: EPOCH 4/100 | BATCH 69/71 | LOSS: 0.002213995151188491\n",
      "TRAIN: EPOCH 4/100 | BATCH 70/71 | LOSS: 0.0021935100175282905\n",
      "VAL: EPOCH 4/100 | BATCH 0/8 | LOSS: 0.0012478101998567581\n",
      "VAL: EPOCH 4/100 | BATCH 1/8 | LOSS: 0.0011083697900176048\n",
      "VAL: EPOCH 4/100 | BATCH 2/8 | LOSS: 0.0011331949693461258\n",
      "VAL: EPOCH 4/100 | BATCH 3/8 | LOSS: 0.0010547780257184058\n",
      "VAL: EPOCH 4/100 | BATCH 4/8 | LOSS: 0.0011232458986341954\n",
      "VAL: EPOCH 4/100 | BATCH 5/8 | LOSS: 0.00113179845114549\n",
      "VAL: EPOCH 4/100 | BATCH 6/8 | LOSS: 0.0010974134118961437\n",
      "VAL: EPOCH 4/100 | BATCH 7/8 | LOSS: 0.0011170993093401194\n",
      "TRAIN: EPOCH 5/100 | BATCH 0/71 | LOSS: 0.0007759070140309632\n",
      "TRAIN: EPOCH 5/100 | BATCH 1/71 | LOSS: 0.0007735273393336684\n",
      "TRAIN: EPOCH 5/100 | BATCH 2/71 | LOSS: 0.0009000088127019504\n",
      "TRAIN: EPOCH 5/100 | BATCH 3/71 | LOSS: 0.0009173140133498237\n",
      "TRAIN: EPOCH 5/100 | BATCH 4/71 | LOSS: 0.0009446940268389881\n",
      "TRAIN: EPOCH 5/100 | BATCH 5/71 | LOSS: 0.0009527079237159342\n",
      "TRAIN: EPOCH 5/100 | BATCH 6/71 | LOSS: 0.000944299906093095\n",
      "TRAIN: EPOCH 5/100 | BATCH 7/71 | LOSS: 0.0009544794884277508\n",
      "TRAIN: EPOCH 5/100 | BATCH 8/71 | LOSS: 0.0009718903521489766\n",
      "TRAIN: EPOCH 5/100 | BATCH 9/71 | LOSS: 0.0009585487190634012\n",
      "TRAIN: EPOCH 5/100 | BATCH 10/71 | LOSS: 0.0009574076816947622\n",
      "TRAIN: EPOCH 5/100 | BATCH 11/71 | LOSS: 0.0009785669099073857\n",
      "TRAIN: EPOCH 5/100 | BATCH 12/71 | LOSS: 0.0009624562763537352\n",
      "TRAIN: EPOCH 5/100 | BATCH 13/71 | LOSS: 0.0009599767633647259\n",
      "TRAIN: EPOCH 5/100 | BATCH 14/71 | LOSS: 0.0009524538298137486\n",
      "TRAIN: EPOCH 5/100 | BATCH 15/71 | LOSS: 0.0009532844596833456\n",
      "TRAIN: EPOCH 5/100 | BATCH 16/71 | LOSS: 0.0009435796459643718\n",
      "TRAIN: EPOCH 5/100 | BATCH 17/71 | LOSS: 0.0009441591973882169\n",
      "TRAIN: EPOCH 5/100 | BATCH 18/71 | LOSS: 0.0009430841034229257\n",
      "TRAIN: EPOCH 5/100 | BATCH 19/71 | LOSS: 0.000930378126213327\n",
      "TRAIN: EPOCH 5/100 | BATCH 20/71 | LOSS: 0.0009233260172463599\n",
      "TRAIN: EPOCH 5/100 | BATCH 21/71 | LOSS: 0.0009153123616918244\n",
      "TRAIN: EPOCH 5/100 | BATCH 22/71 | LOSS: 0.0009087263657104062\n",
      "TRAIN: EPOCH 5/100 | BATCH 23/71 | LOSS: 0.0009059519967801558\n",
      "TRAIN: EPOCH 5/100 | BATCH 24/71 | LOSS: 0.0009019298921339214\n",
      "TRAIN: EPOCH 5/100 | BATCH 25/71 | LOSS: 0.0008967706973915204\n",
      "TRAIN: EPOCH 5/100 | BATCH 26/71 | LOSS: 0.0009086556512758964\n",
      "TRAIN: EPOCH 5/100 | BATCH 27/71 | LOSS: 0.0009074632918262589\n",
      "TRAIN: EPOCH 5/100 | BATCH 28/71 | LOSS: 0.0009079504485145725\n",
      "TRAIN: EPOCH 5/100 | BATCH 29/71 | LOSS: 0.0009031692325758438\n",
      "TRAIN: EPOCH 5/100 | BATCH 30/71 | LOSS: 0.0008956240058215635\n",
      "TRAIN: EPOCH 5/100 | BATCH 31/71 | LOSS: 0.0008916745136957616\n",
      "TRAIN: EPOCH 5/100 | BATCH 32/71 | LOSS: 0.0008900261512308409\n",
      "TRAIN: EPOCH 5/100 | BATCH 33/71 | LOSS: 0.000886401058832074\n",
      "TRAIN: EPOCH 5/100 | BATCH 34/71 | LOSS: 0.0008811887686273882\n",
      "TRAIN: EPOCH 5/100 | BATCH 35/71 | LOSS: 0.0008859529625624418\n",
      "TRAIN: EPOCH 5/100 | BATCH 36/71 | LOSS: 0.0008852989947725389\n",
      "TRAIN: EPOCH 5/100 | BATCH 37/71 | LOSS: 0.00088088371864471\n",
      "TRAIN: EPOCH 5/100 | BATCH 38/71 | LOSS: 0.0008779716534683338\n",
      "TRAIN: EPOCH 5/100 | BATCH 39/71 | LOSS: 0.0008797720729489811\n",
      "TRAIN: EPOCH 5/100 | BATCH 40/71 | LOSS: 0.0008742337813600898\n",
      "TRAIN: EPOCH 5/100 | BATCH 41/71 | LOSS: 0.0008723495203802096\n",
      "TRAIN: EPOCH 5/100 | BATCH 42/71 | LOSS: 0.0008687917427376432\n",
      "TRAIN: EPOCH 5/100 | BATCH 43/71 | LOSS: 0.0008660981799370016\n",
      "TRAIN: EPOCH 5/100 | BATCH 44/71 | LOSS: 0.0008614167014861273\n",
      "TRAIN: EPOCH 5/100 | BATCH 45/71 | LOSS: 0.0008555817071085229\n",
      "TRAIN: EPOCH 5/100 | BATCH 46/71 | LOSS: 0.0008517781314638226\n",
      "TRAIN: EPOCH 5/100 | BATCH 47/71 | LOSS: 0.0008445264696395801\n",
      "TRAIN: EPOCH 5/100 | BATCH 48/71 | LOSS: 0.0008402468781557162\n",
      "TRAIN: EPOCH 5/100 | BATCH 49/71 | LOSS: 0.0008403450518380851\n",
      "TRAIN: EPOCH 5/100 | BATCH 50/71 | LOSS: 0.0008380222898981004\n",
      "TRAIN: EPOCH 5/100 | BATCH 51/71 | LOSS: 0.0008354094651832173\n",
      "TRAIN: EPOCH 5/100 | BATCH 52/71 | LOSS: 0.0008343604320818382\n",
      "TRAIN: EPOCH 5/100 | BATCH 53/71 | LOSS: 0.0008306538899584363\n",
      "TRAIN: EPOCH 5/100 | BATCH 54/71 | LOSS: 0.0008281432917680253\n",
      "TRAIN: EPOCH 5/100 | BATCH 55/71 | LOSS: 0.0008241550229805787\n",
      "TRAIN: EPOCH 5/100 | BATCH 56/71 | LOSS: 0.0008215222358621919\n",
      "TRAIN: EPOCH 5/100 | BATCH 57/71 | LOSS: 0.0008165244472710865\n",
      "TRAIN: EPOCH 5/100 | BATCH 58/71 | LOSS: 0.0008144819333565311\n",
      "TRAIN: EPOCH 5/100 | BATCH 59/71 | LOSS: 0.0008124679569543029\n",
      "TRAIN: EPOCH 5/100 | BATCH 60/71 | LOSS: 0.0008092290124489513\n",
      "TRAIN: EPOCH 5/100 | BATCH 61/71 | LOSS: 0.0008041242563781599\n",
      "TRAIN: EPOCH 5/100 | BATCH 62/71 | LOSS: 0.0008000149659531576\n",
      "TRAIN: EPOCH 5/100 | BATCH 63/71 | LOSS: 0.0007976968636285164\n",
      "TRAIN: EPOCH 5/100 | BATCH 64/71 | LOSS: 0.0007963694381312682\n",
      "TRAIN: EPOCH 5/100 | BATCH 65/71 | LOSS: 0.0007926313035458213\n",
      "TRAIN: EPOCH 5/100 | BATCH 66/71 | LOSS: 0.0007913939423734017\n",
      "TRAIN: EPOCH 5/100 | BATCH 67/71 | LOSS: 0.0007881767763619256\n",
      "TRAIN: EPOCH 5/100 | BATCH 68/71 | LOSS: 0.0007851820614566838\n",
      "TRAIN: EPOCH 5/100 | BATCH 69/71 | LOSS: 0.0007839642997298922\n",
      "TRAIN: EPOCH 5/100 | BATCH 70/71 | LOSS: 0.0007798589133119709\n",
      "VAL: EPOCH 5/100 | BATCH 0/8 | LOSS: 0.0006299949018284678\n",
      "VAL: EPOCH 5/100 | BATCH 1/8 | LOSS: 0.0005498816608451307\n",
      "VAL: EPOCH 5/100 | BATCH 2/8 | LOSS: 0.0005816800985485315\n",
      "VAL: EPOCH 5/100 | BATCH 3/8 | LOSS: 0.0005449198870337568\n",
      "VAL: EPOCH 5/100 | BATCH 4/8 | LOSS: 0.0005858750722836703\n",
      "VAL: EPOCH 5/100 | BATCH 5/8 | LOSS: 0.0005882220139028504\n",
      "VAL: EPOCH 5/100 | BATCH 6/8 | LOSS: 0.000567445286183751\n",
      "VAL: EPOCH 5/100 | BATCH 7/8 | LOSS: 0.0005815565928060096\n",
      "TRAIN: EPOCH 6/100 | BATCH 0/71 | LOSS: 0.0004706868785433471\n",
      "TRAIN: EPOCH 6/100 | BATCH 1/71 | LOSS: 0.000485995173221454\n",
      "TRAIN: EPOCH 6/100 | BATCH 2/71 | LOSS: 0.0004848970177893837\n",
      "TRAIN: EPOCH 6/100 | BATCH 3/71 | LOSS: 0.000544549198821187\n",
      "TRAIN: EPOCH 6/100 | BATCH 4/71 | LOSS: 0.0005211251904256642\n",
      "TRAIN: EPOCH 6/100 | BATCH 5/71 | LOSS: 0.0005356132266266892\n",
      "TRAIN: EPOCH 6/100 | BATCH 6/71 | LOSS: 0.0005357556593870478\n",
      "TRAIN: EPOCH 6/100 | BATCH 7/71 | LOSS: 0.0005180465959711\n",
      "TRAIN: EPOCH 6/100 | BATCH 8/71 | LOSS: 0.0005232701983509792\n",
      "TRAIN: EPOCH 6/100 | BATCH 9/71 | LOSS: 0.0005432085075881332\n",
      "TRAIN: EPOCH 6/100 | BATCH 10/71 | LOSS: 0.0005554541765542871\n",
      "TRAIN: EPOCH 6/100 | BATCH 11/71 | LOSS: 0.0005454202473629266\n",
      "TRAIN: EPOCH 6/100 | BATCH 12/71 | LOSS: 0.0005320328099724765\n",
      "TRAIN: EPOCH 6/100 | BATCH 13/71 | LOSS: 0.0005316108872648329\n",
      "TRAIN: EPOCH 6/100 | BATCH 14/71 | LOSS: 0.0005373799513715009\n",
      "TRAIN: EPOCH 6/100 | BATCH 15/71 | LOSS: 0.0005340924853953766\n",
      "TRAIN: EPOCH 6/100 | BATCH 16/71 | LOSS: 0.0005409740654559916\n",
      "TRAIN: EPOCH 6/100 | BATCH 17/71 | LOSS: 0.000537672194898025\n",
      "TRAIN: EPOCH 6/100 | BATCH 18/71 | LOSS: 0.0005382529603881075\n",
      "TRAIN: EPOCH 6/100 | BATCH 19/71 | LOSS: 0.0005349068887881003\n",
      "TRAIN: EPOCH 6/100 | BATCH 20/71 | LOSS: 0.0005356330172313998\n",
      "TRAIN: EPOCH 6/100 | BATCH 21/71 | LOSS: 0.0005369430198863319\n",
      "TRAIN: EPOCH 6/100 | BATCH 22/71 | LOSS: 0.0005347029591971279\n",
      "TRAIN: EPOCH 6/100 | BATCH 23/71 | LOSS: 0.0005330744230983934\n",
      "TRAIN: EPOCH 6/100 | BATCH 24/71 | LOSS: 0.0005337832972873003\n",
      "TRAIN: EPOCH 6/100 | BATCH 25/71 | LOSS: 0.0005313900888163167\n",
      "TRAIN: EPOCH 6/100 | BATCH 26/71 | LOSS: 0.0005302008627310257\n",
      "TRAIN: EPOCH 6/100 | BATCH 27/71 | LOSS: 0.000526100026036147\n",
      "TRAIN: EPOCH 6/100 | BATCH 28/71 | LOSS: 0.0005261941024118328\n",
      "TRAIN: EPOCH 6/100 | BATCH 29/71 | LOSS: 0.000527331026387401\n",
      "TRAIN: EPOCH 6/100 | BATCH 30/71 | LOSS: 0.0005262367879866713\n",
      "TRAIN: EPOCH 6/100 | BATCH 31/71 | LOSS: 0.0005256477970760898\n",
      "TRAIN: EPOCH 6/100 | BATCH 32/71 | LOSS: 0.0005278017356018113\n",
      "TRAIN: EPOCH 6/100 | BATCH 33/71 | LOSS: 0.0005305925536530969\n",
      "TRAIN: EPOCH 6/100 | BATCH 34/71 | LOSS: 0.0005297005717043898\n",
      "TRAIN: EPOCH 6/100 | BATCH 35/71 | LOSS: 0.000526850504684262\n",
      "TRAIN: EPOCH 6/100 | BATCH 36/71 | LOSS: 0.0005242244216198152\n",
      "TRAIN: EPOCH 6/100 | BATCH 37/71 | LOSS: 0.0005233451137380479\n",
      "TRAIN: EPOCH 6/100 | BATCH 38/71 | LOSS: 0.0005239513277326926\n",
      "TRAIN: EPOCH 6/100 | BATCH 39/71 | LOSS: 0.0005234645323071163\n",
      "TRAIN: EPOCH 6/100 | BATCH 40/71 | LOSS: 0.0005223706475368179\n",
      "TRAIN: EPOCH 6/100 | BATCH 41/71 | LOSS: 0.0005206884580686511\n",
      "TRAIN: EPOCH 6/100 | BATCH 42/71 | LOSS: 0.0005181565763006463\n",
      "TRAIN: EPOCH 6/100 | BATCH 43/71 | LOSS: 0.000515579046887896\n",
      "TRAIN: EPOCH 6/100 | BATCH 44/71 | LOSS: 0.0005161852310670333\n",
      "TRAIN: EPOCH 6/100 | BATCH 45/71 | LOSS: 0.0005182847311042007\n",
      "TRAIN: EPOCH 6/100 | BATCH 46/71 | LOSS: 0.0005179962218848077\n",
      "TRAIN: EPOCH 6/100 | BATCH 47/71 | LOSS: 0.0005157613541086903\n",
      "TRAIN: EPOCH 6/100 | BATCH 48/71 | LOSS: 0.0005146412693239672\n",
      "TRAIN: EPOCH 6/100 | BATCH 49/71 | LOSS: 0.0005137891910271719\n",
      "TRAIN: EPOCH 6/100 | BATCH 50/71 | LOSS: 0.0005125107418280095\n",
      "TRAIN: EPOCH 6/100 | BATCH 51/71 | LOSS: 0.0005129369914916774\n",
      "TRAIN: EPOCH 6/100 | BATCH 52/71 | LOSS: 0.0005106191542922115\n",
      "TRAIN: EPOCH 6/100 | BATCH 53/71 | LOSS: 0.0005084969150764799\n",
      "TRAIN: EPOCH 6/100 | BATCH 54/71 | LOSS: 0.0005089721166190098\n",
      "TRAIN: EPOCH 6/100 | BATCH 55/71 | LOSS: 0.0005085208363847674\n",
      "TRAIN: EPOCH 6/100 | BATCH 56/71 | LOSS: 0.0005075537978483593\n",
      "TRAIN: EPOCH 6/100 | BATCH 57/71 | LOSS: 0.0005051874418156864\n",
      "TRAIN: EPOCH 6/100 | BATCH 58/71 | LOSS: 0.0005045056987309166\n",
      "TRAIN: EPOCH 6/100 | BATCH 59/71 | LOSS: 0.0005036504041830388\n",
      "TRAIN: EPOCH 6/100 | BATCH 60/71 | LOSS: 0.0005017308156639643\n",
      "TRAIN: EPOCH 6/100 | BATCH 61/71 | LOSS: 0.0005023000830401396\n",
      "TRAIN: EPOCH 6/100 | BATCH 62/71 | LOSS: 0.0005014397777683501\n",
      "TRAIN: EPOCH 6/100 | BATCH 63/71 | LOSS: 0.0004998324270673038\n",
      "TRAIN: EPOCH 6/100 | BATCH 64/71 | LOSS: 0.0005014988786290185\n",
      "TRAIN: EPOCH 6/100 | BATCH 65/71 | LOSS: 0.0005008679612673762\n",
      "TRAIN: EPOCH 6/100 | BATCH 66/71 | LOSS: 0.0004988939283283622\n",
      "TRAIN: EPOCH 6/100 | BATCH 67/71 | LOSS: 0.000498641660241821\n",
      "TRAIN: EPOCH 6/100 | BATCH 68/71 | LOSS: 0.0004970928433897865\n",
      "TRAIN: EPOCH 6/100 | BATCH 69/71 | LOSS: 0.0004963573837553017\n",
      "TRAIN: EPOCH 6/100 | BATCH 70/71 | LOSS: 0.0004956464817695005\n",
      "VAL: EPOCH 6/100 | BATCH 0/8 | LOSS: 0.00046489309170283377\n",
      "VAL: EPOCH 6/100 | BATCH 1/8 | LOSS: 0.00039224280044436455\n",
      "VAL: EPOCH 6/100 | BATCH 2/8 | LOSS: 0.00041679715892920893\n",
      "VAL: EPOCH 6/100 | BATCH 3/8 | LOSS: 0.00039656084845773876\n",
      "VAL: EPOCH 6/100 | BATCH 4/8 | LOSS: 0.000423001020681113\n",
      "VAL: EPOCH 6/100 | BATCH 5/8 | LOSS: 0.00042334647150710225\n",
      "VAL: EPOCH 6/100 | BATCH 6/8 | LOSS: 0.0004098780087328383\n",
      "VAL: EPOCH 6/100 | BATCH 7/8 | LOSS: 0.0004200485855108127\n",
      "TRAIN: EPOCH 7/100 | BATCH 0/71 | LOSS: 0.00046705635031685233\n",
      "TRAIN: EPOCH 7/100 | BATCH 1/71 | LOSS: 0.00042586890049278736\n",
      "TRAIN: EPOCH 7/100 | BATCH 2/71 | LOSS: 0.00039843433963445324\n",
      "TRAIN: EPOCH 7/100 | BATCH 3/71 | LOSS: 0.0004229533005855046\n",
      "TRAIN: EPOCH 7/100 | BATCH 4/71 | LOSS: 0.0004183643439318985\n",
      "TRAIN: EPOCH 7/100 | BATCH 5/71 | LOSS: 0.00042383572629963356\n",
      "TRAIN: EPOCH 7/100 | BATCH 6/71 | LOSS: 0.0004219533293507993\n",
      "TRAIN: EPOCH 7/100 | BATCH 7/71 | LOSS: 0.0004195567307760939\n",
      "TRAIN: EPOCH 7/100 | BATCH 8/71 | LOSS: 0.00041592840959007543\n",
      "TRAIN: EPOCH 7/100 | BATCH 9/71 | LOSS: 0.0004124225379200652\n",
      "TRAIN: EPOCH 7/100 | BATCH 10/71 | LOSS: 0.0004064951816954735\n",
      "TRAIN: EPOCH 7/100 | BATCH 11/71 | LOSS: 0.00040759222853618365\n",
      "TRAIN: EPOCH 7/100 | BATCH 12/71 | LOSS: 0.000409589549795223\n",
      "TRAIN: EPOCH 7/100 | BATCH 13/71 | LOSS: 0.0004082472317220111\n",
      "TRAIN: EPOCH 7/100 | BATCH 14/71 | LOSS: 0.0004099028049192081\n",
      "TRAIN: EPOCH 7/100 | BATCH 15/71 | LOSS: 0.0004193289587419713\n",
      "TRAIN: EPOCH 7/100 | BATCH 16/71 | LOSS: 0.00042079516638563396\n",
      "TRAIN: EPOCH 7/100 | BATCH 17/71 | LOSS: 0.00041908555107915565\n",
      "TRAIN: EPOCH 7/100 | BATCH 18/71 | LOSS: 0.0004191588040588326\n",
      "TRAIN: EPOCH 7/100 | BATCH 19/71 | LOSS: 0.0004149628250161186\n",
      "TRAIN: EPOCH 7/100 | BATCH 20/71 | LOSS: 0.0004106540152514797\n",
      "TRAIN: EPOCH 7/100 | BATCH 21/71 | LOSS: 0.00040721703209617937\n",
      "TRAIN: EPOCH 7/100 | BATCH 22/71 | LOSS: 0.00040685942614167607\n",
      "TRAIN: EPOCH 7/100 | BATCH 23/71 | LOSS: 0.00040525519580114633\n",
      "TRAIN: EPOCH 7/100 | BATCH 24/71 | LOSS: 0.00039925370598211887\n",
      "TRAIN: EPOCH 7/100 | BATCH 25/71 | LOSS: 0.0003986093238031921\n",
      "TRAIN: EPOCH 7/100 | BATCH 26/71 | LOSS: 0.00039883459383552826\n",
      "TRAIN: EPOCH 7/100 | BATCH 27/71 | LOSS: 0.0003995053328773273\n",
      "TRAIN: EPOCH 7/100 | BATCH 28/71 | LOSS: 0.0003999892766748009\n",
      "TRAIN: EPOCH 7/100 | BATCH 29/71 | LOSS: 0.0004028414861143877\n",
      "TRAIN: EPOCH 7/100 | BATCH 30/71 | LOSS: 0.0004044686492923046\n",
      "TRAIN: EPOCH 7/100 | BATCH 31/71 | LOSS: 0.00040477827860740945\n",
      "TRAIN: EPOCH 7/100 | BATCH 32/71 | LOSS: 0.00040450428654863077\n",
      "TRAIN: EPOCH 7/100 | BATCH 33/71 | LOSS: 0.00040772685657857973\n",
      "TRAIN: EPOCH 7/100 | BATCH 34/71 | LOSS: 0.0004075617013898279\n",
      "TRAIN: EPOCH 7/100 | BATCH 35/71 | LOSS: 0.00040700234724984813\n",
      "TRAIN: EPOCH 7/100 | BATCH 36/71 | LOSS: 0.0004068219364066986\n",
      "TRAIN: EPOCH 7/100 | BATCH 37/71 | LOSS: 0.00040759849071037024\n",
      "TRAIN: EPOCH 7/100 | BATCH 38/71 | LOSS: 0.0004090868102279134\n",
      "TRAIN: EPOCH 7/100 | BATCH 39/71 | LOSS: 0.00040836341577232815\n",
      "TRAIN: EPOCH 7/100 | BATCH 40/71 | LOSS: 0.00040945888244228935\n",
      "TRAIN: EPOCH 7/100 | BATCH 41/71 | LOSS: 0.0004102774089135762\n",
      "TRAIN: EPOCH 7/100 | BATCH 42/71 | LOSS: 0.0004085264612682337\n",
      "TRAIN: EPOCH 7/100 | BATCH 43/71 | LOSS: 0.0004062570091760294\n",
      "TRAIN: EPOCH 7/100 | BATCH 44/71 | LOSS: 0.00040569433541451063\n",
      "TRAIN: EPOCH 7/100 | BATCH 45/71 | LOSS: 0.0004060541383375454\n",
      "TRAIN: EPOCH 7/100 | BATCH 46/71 | LOSS: 0.0004058963739303278\n",
      "TRAIN: EPOCH 7/100 | BATCH 47/71 | LOSS: 0.00040346076093555894\n",
      "TRAIN: EPOCH 7/100 | BATCH 48/71 | LOSS: 0.0004025676806827969\n",
      "TRAIN: EPOCH 7/100 | BATCH 49/71 | LOSS: 0.00040039986604824664\n",
      "TRAIN: EPOCH 7/100 | BATCH 50/71 | LOSS: 0.0003982339266632848\n",
      "TRAIN: EPOCH 7/100 | BATCH 51/71 | LOSS: 0.00039822017648615514\n",
      "TRAIN: EPOCH 7/100 | BATCH 52/71 | LOSS: 0.0003982223740576784\n",
      "TRAIN: EPOCH 7/100 | BATCH 53/71 | LOSS: 0.00039785243818726113\n",
      "TRAIN: EPOCH 7/100 | BATCH 54/71 | LOSS: 0.00039574803826822476\n",
      "TRAIN: EPOCH 7/100 | BATCH 55/71 | LOSS: 0.00039518291824996207\n",
      "TRAIN: EPOCH 7/100 | BATCH 56/71 | LOSS: 0.0003936857295533021\n",
      "TRAIN: EPOCH 7/100 | BATCH 57/71 | LOSS: 0.00039184129052265577\n",
      "TRAIN: EPOCH 7/100 | BATCH 58/71 | LOSS: 0.00039136384059359336\n",
      "TRAIN: EPOCH 7/100 | BATCH 59/71 | LOSS: 0.00039060835018365955\n",
      "TRAIN: EPOCH 7/100 | BATCH 60/71 | LOSS: 0.0003900069998172646\n",
      "TRAIN: EPOCH 7/100 | BATCH 61/71 | LOSS: 0.0003890906354495054\n",
      "TRAIN: EPOCH 7/100 | BATCH 62/71 | LOSS: 0.00039008371799903376\n",
      "TRAIN: EPOCH 7/100 | BATCH 63/71 | LOSS: 0.0003913023433597118\n",
      "TRAIN: EPOCH 7/100 | BATCH 64/71 | LOSS: 0.00039292080372643583\n",
      "TRAIN: EPOCH 7/100 | BATCH 65/71 | LOSS: 0.0003911564774832434\n",
      "TRAIN: EPOCH 7/100 | BATCH 66/71 | LOSS: 0.00038953669110326956\n",
      "TRAIN: EPOCH 7/100 | BATCH 67/71 | LOSS: 0.0003898945643039256\n",
      "TRAIN: EPOCH 7/100 | BATCH 68/71 | LOSS: 0.00038846187711949796\n",
      "TRAIN: EPOCH 7/100 | BATCH 69/71 | LOSS: 0.0003873318433761597\n",
      "TRAIN: EPOCH 7/100 | BATCH 70/71 | LOSS: 0.0003872117975359442\n",
      "VAL: EPOCH 7/100 | BATCH 0/8 | LOSS: 0.0003977488959208131\n",
      "VAL: EPOCH 7/100 | BATCH 1/8 | LOSS: 0.0003237850614823401\n",
      "VAL: EPOCH 7/100 | BATCH 2/8 | LOSS: 0.00034238142931523424\n",
      "VAL: EPOCH 7/100 | BATCH 3/8 | LOSS: 0.0003285640414105728\n",
      "VAL: EPOCH 7/100 | BATCH 4/8 | LOSS: 0.00035177790559828284\n",
      "VAL: EPOCH 7/100 | BATCH 5/8 | LOSS: 0.00034851165643582743\n",
      "VAL: EPOCH 7/100 | BATCH 6/8 | LOSS: 0.00033850816544145346\n",
      "VAL: EPOCH 7/100 | BATCH 7/8 | LOSS: 0.00034222370959469117\n",
      "TRAIN: EPOCH 8/100 | BATCH 0/71 | LOSS: 0.0002795846958179027\n",
      "TRAIN: EPOCH 8/100 | BATCH 1/71 | LOSS: 0.0003004994650837034\n",
      "TRAIN: EPOCH 8/100 | BATCH 2/71 | LOSS: 0.00034870570137475926\n",
      "TRAIN: EPOCH 8/100 | BATCH 3/71 | LOSS: 0.00037182008964009583\n",
      "TRAIN: EPOCH 8/100 | BATCH 4/71 | LOSS: 0.0003713898709975183\n",
      "TRAIN: EPOCH 8/100 | BATCH 5/71 | LOSS: 0.0003673759735344599\n",
      "TRAIN: EPOCH 8/100 | BATCH 6/71 | LOSS: 0.000361292327787461\n",
      "TRAIN: EPOCH 8/100 | BATCH 7/71 | LOSS: 0.0003686684212880209\n",
      "TRAIN: EPOCH 8/100 | BATCH 8/71 | LOSS: 0.0003642259107436985\n",
      "TRAIN: EPOCH 8/100 | BATCH 9/71 | LOSS: 0.0003648603189503774\n",
      "TRAIN: EPOCH 8/100 | BATCH 10/71 | LOSS: 0.00035834773602387446\n",
      "TRAIN: EPOCH 8/100 | BATCH 11/71 | LOSS: 0.00035346761433174834\n",
      "TRAIN: EPOCH 8/100 | BATCH 12/71 | LOSS: 0.00035392478009900794\n",
      "TRAIN: EPOCH 8/100 | BATCH 13/71 | LOSS: 0.0003472298787008705\n",
      "TRAIN: EPOCH 8/100 | BATCH 14/71 | LOSS: 0.0003441009340652575\n",
      "TRAIN: EPOCH 8/100 | BATCH 15/71 | LOSS: 0.00034144898563681636\n",
      "TRAIN: EPOCH 8/100 | BATCH 16/71 | LOSS: 0.00033965147860512575\n",
      "TRAIN: EPOCH 8/100 | BATCH 17/71 | LOSS: 0.0003388215263839811\n",
      "TRAIN: EPOCH 8/100 | BATCH 18/71 | LOSS: 0.0003385228812572007\n",
      "TRAIN: EPOCH 8/100 | BATCH 19/71 | LOSS: 0.0003411463912925683\n",
      "TRAIN: EPOCH 8/100 | BATCH 20/71 | LOSS: 0.000342093286148849\n",
      "TRAIN: EPOCH 8/100 | BATCH 21/71 | LOSS: 0.00034078757611992347\n",
      "TRAIN: EPOCH 8/100 | BATCH 22/71 | LOSS: 0.0003414806826343841\n",
      "TRAIN: EPOCH 8/100 | BATCH 23/71 | LOSS: 0.0003422613311461949\n",
      "TRAIN: EPOCH 8/100 | BATCH 24/71 | LOSS: 0.0003379327565198764\n",
      "TRAIN: EPOCH 8/100 | BATCH 25/71 | LOSS: 0.00033815734283192654\n",
      "TRAIN: EPOCH 8/100 | BATCH 26/71 | LOSS: 0.00034041482151089304\n",
      "TRAIN: EPOCH 8/100 | BATCH 27/71 | LOSS: 0.0003403740696999843\n",
      "TRAIN: EPOCH 8/100 | BATCH 28/71 | LOSS: 0.0003442069538668248\n",
      "TRAIN: EPOCH 8/100 | BATCH 29/71 | LOSS: 0.0003456952685761886\n",
      "TRAIN: EPOCH 8/100 | BATCH 30/71 | LOSS: 0.0003440126052854823\n",
      "TRAIN: EPOCH 8/100 | BATCH 31/71 | LOSS: 0.00034428922162987874\n",
      "TRAIN: EPOCH 8/100 | BATCH 32/71 | LOSS: 0.00034481907840183175\n",
      "TRAIN: EPOCH 8/100 | BATCH 33/71 | LOSS: 0.00034572889382937267\n",
      "TRAIN: EPOCH 8/100 | BATCH 34/71 | LOSS: 0.0003468224059491019\n",
      "TRAIN: EPOCH 8/100 | BATCH 35/71 | LOSS: 0.0003450367459865649\n",
      "TRAIN: EPOCH 8/100 | BATCH 36/71 | LOSS: 0.00034510093462666045\n",
      "TRAIN: EPOCH 8/100 | BATCH 37/71 | LOSS: 0.0003432457432561358\n",
      "TRAIN: EPOCH 8/100 | BATCH 38/71 | LOSS: 0.0003441741092291732\n",
      "TRAIN: EPOCH 8/100 | BATCH 39/71 | LOSS: 0.00034292923337488903\n",
      "TRAIN: EPOCH 8/100 | BATCH 40/71 | LOSS: 0.0003418151680351712\n",
      "TRAIN: EPOCH 8/100 | BATCH 41/71 | LOSS: 0.00034105884281286435\n",
      "TRAIN: EPOCH 8/100 | BATCH 42/71 | LOSS: 0.0003400649574211647\n",
      "TRAIN: EPOCH 8/100 | BATCH 43/71 | LOSS: 0.00033908440111025067\n",
      "TRAIN: EPOCH 8/100 | BATCH 44/71 | LOSS: 0.00033903797124771196\n",
      "TRAIN: EPOCH 8/100 | BATCH 45/71 | LOSS: 0.00033888722353604743\n",
      "TRAIN: EPOCH 8/100 | BATCH 46/71 | LOSS: 0.0003402724368572037\n",
      "TRAIN: EPOCH 8/100 | BATCH 47/71 | LOSS: 0.00033839242587418994\n",
      "TRAIN: EPOCH 8/100 | BATCH 48/71 | LOSS: 0.0003404169826297926\n",
      "TRAIN: EPOCH 8/100 | BATCH 49/71 | LOSS: 0.0003395669130259193\n",
      "TRAIN: EPOCH 8/100 | BATCH 50/71 | LOSS: 0.000341131643211816\n",
      "TRAIN: EPOCH 8/100 | BATCH 51/71 | LOSS: 0.00034032242015550414\n",
      "TRAIN: EPOCH 8/100 | BATCH 52/71 | LOSS: 0.00033931364332773846\n",
      "TRAIN: EPOCH 8/100 | BATCH 53/71 | LOSS: 0.00033709504323606207\n",
      "TRAIN: EPOCH 8/100 | BATCH 54/71 | LOSS: 0.0003364682665877891\n",
      "TRAIN: EPOCH 8/100 | BATCH 55/71 | LOSS: 0.0003353749346258285\n",
      "TRAIN: EPOCH 8/100 | BATCH 56/71 | LOSS: 0.00033600583673544683\n",
      "TRAIN: EPOCH 8/100 | BATCH 57/71 | LOSS: 0.000336179091516834\n",
      "TRAIN: EPOCH 8/100 | BATCH 58/71 | LOSS: 0.000335050924134567\n",
      "TRAIN: EPOCH 8/100 | BATCH 59/71 | LOSS: 0.00033362973457163513\n",
      "TRAIN: EPOCH 8/100 | BATCH 60/71 | LOSS: 0.00033460113770504037\n",
      "TRAIN: EPOCH 8/100 | BATCH 61/71 | LOSS: 0.0003333741499978538\n",
      "TRAIN: EPOCH 8/100 | BATCH 62/71 | LOSS: 0.00033312443332443576\n",
      "TRAIN: EPOCH 8/100 | BATCH 63/71 | LOSS: 0.00033171791255881544\n",
      "TRAIN: EPOCH 8/100 | BATCH 64/71 | LOSS: 0.0003300774112666169\n",
      "TRAIN: EPOCH 8/100 | BATCH 65/71 | LOSS: 0.00032897490283093333\n",
      "TRAIN: EPOCH 8/100 | BATCH 66/71 | LOSS: 0.00032857942235058366\n",
      "TRAIN: EPOCH 8/100 | BATCH 67/71 | LOSS: 0.0003289856266586439\n",
      "TRAIN: EPOCH 8/100 | BATCH 68/71 | LOSS: 0.00032809034472439384\n",
      "TRAIN: EPOCH 8/100 | BATCH 69/71 | LOSS: 0.0003271681555945958\n",
      "TRAIN: EPOCH 8/100 | BATCH 70/71 | LOSS: 0.00032746515599158134\n",
      "VAL: EPOCH 8/100 | BATCH 0/8 | LOSS: 0.0003685543197207153\n",
      "VAL: EPOCH 8/100 | BATCH 1/8 | LOSS: 0.00029093037301208824\n",
      "VAL: EPOCH 8/100 | BATCH 2/8 | LOSS: 0.0003040982992388308\n",
      "VAL: EPOCH 8/100 | BATCH 3/8 | LOSS: 0.00029089810414006934\n",
      "VAL: EPOCH 8/100 | BATCH 4/8 | LOSS: 0.0003148950811009854\n",
      "VAL: EPOCH 8/100 | BATCH 5/8 | LOSS: 0.00030943204668195296\n",
      "VAL: EPOCH 8/100 | BATCH 6/8 | LOSS: 0.000301048147126234\n",
      "VAL: EPOCH 8/100 | BATCH 7/8 | LOSS: 0.0003016818700416479\n",
      "TRAIN: EPOCH 9/100 | BATCH 0/71 | LOSS: 0.0002800259389914572\n",
      "TRAIN: EPOCH 9/100 | BATCH 1/71 | LOSS: 0.0003059309965465218\n",
      "TRAIN: EPOCH 9/100 | BATCH 2/71 | LOSS: 0.00030040063817674917\n",
      "TRAIN: EPOCH 9/100 | BATCH 3/71 | LOSS: 0.0002778166890493594\n",
      "TRAIN: EPOCH 9/100 | BATCH 4/71 | LOSS: 0.00028724290314130485\n",
      "TRAIN: EPOCH 9/100 | BATCH 5/71 | LOSS: 0.00029236176730288815\n",
      "TRAIN: EPOCH 9/100 | BATCH 6/71 | LOSS: 0.0002913746076436447\n",
      "TRAIN: EPOCH 9/100 | BATCH 7/71 | LOSS: 0.0002787225184874842\n",
      "TRAIN: EPOCH 9/100 | BATCH 8/71 | LOSS: 0.00027947287081689056\n",
      "TRAIN: EPOCH 9/100 | BATCH 9/71 | LOSS: 0.0002827342614182271\n",
      "TRAIN: EPOCH 9/100 | BATCH 10/71 | LOSS: 0.0002844587262106043\n",
      "TRAIN: EPOCH 9/100 | BATCH 11/71 | LOSS: 0.0002835322120517958\n",
      "TRAIN: EPOCH 9/100 | BATCH 12/71 | LOSS: 0.00028820272396282794\n",
      "TRAIN: EPOCH 9/100 | BATCH 13/71 | LOSS: 0.0002871262123725111\n",
      "TRAIN: EPOCH 9/100 | BATCH 14/71 | LOSS: 0.00028607518470380455\n",
      "TRAIN: EPOCH 9/100 | BATCH 15/71 | LOSS: 0.0002836152643794776\n",
      "TRAIN: EPOCH 9/100 | BATCH 16/71 | LOSS: 0.0002818610082613304\n",
      "TRAIN: EPOCH 9/100 | BATCH 17/71 | LOSS: 0.00027888814171698567\n",
      "TRAIN: EPOCH 9/100 | BATCH 18/71 | LOSS: 0.00028090524612803405\n",
      "TRAIN: EPOCH 9/100 | BATCH 19/71 | LOSS: 0.0002800132882839534\n",
      "TRAIN: EPOCH 9/100 | BATCH 20/71 | LOSS: 0.00028216775709075765\n",
      "TRAIN: EPOCH 9/100 | BATCH 21/71 | LOSS: 0.0002825137020078149\n",
      "TRAIN: EPOCH 9/100 | BATCH 22/71 | LOSS: 0.00028351929896693355\n",
      "TRAIN: EPOCH 9/100 | BATCH 23/71 | LOSS: 0.00028407055712402024\n",
      "TRAIN: EPOCH 9/100 | BATCH 24/71 | LOSS: 0.0002827816765056923\n",
      "TRAIN: EPOCH 9/100 | BATCH 25/71 | LOSS: 0.0002834195581202109\n",
      "TRAIN: EPOCH 9/100 | BATCH 26/71 | LOSS: 0.0002838477941824951\n",
      "TRAIN: EPOCH 9/100 | BATCH 27/71 | LOSS: 0.00028534422796967974\n",
      "TRAIN: EPOCH 9/100 | BATCH 28/71 | LOSS: 0.00028567563828707125\n",
      "TRAIN: EPOCH 9/100 | BATCH 29/71 | LOSS: 0.00028444676766715325\n",
      "TRAIN: EPOCH 9/100 | BATCH 30/71 | LOSS: 0.0002863942697125998\n",
      "TRAIN: EPOCH 9/100 | BATCH 31/71 | LOSS: 0.00028676107785940985\n",
      "TRAIN: EPOCH 9/100 | BATCH 32/71 | LOSS: 0.00028773049151905895\n",
      "TRAIN: EPOCH 9/100 | BATCH 33/71 | LOSS: 0.00028945590662153657\n",
      "TRAIN: EPOCH 9/100 | BATCH 34/71 | LOSS: 0.000291902174025641\n",
      "TRAIN: EPOCH 9/100 | BATCH 35/71 | LOSS: 0.0002931147331158475\n",
      "TRAIN: EPOCH 9/100 | BATCH 36/71 | LOSS: 0.00029403757503747036\n",
      "TRAIN: EPOCH 9/100 | BATCH 37/71 | LOSS: 0.0002942277133115567\n",
      "TRAIN: EPOCH 9/100 | BATCH 38/71 | LOSS: 0.00029364816257909226\n",
      "TRAIN: EPOCH 9/100 | BATCH 39/71 | LOSS: 0.00029469437249645124\n",
      "TRAIN: EPOCH 9/100 | BATCH 40/71 | LOSS: 0.00029377108858614343\n",
      "TRAIN: EPOCH 9/100 | BATCH 41/71 | LOSS: 0.00029218703344841267\n",
      "TRAIN: EPOCH 9/100 | BATCH 42/71 | LOSS: 0.0002915797826866511\n",
      "TRAIN: EPOCH 9/100 | BATCH 43/71 | LOSS: 0.00029280526492088524\n",
      "TRAIN: EPOCH 9/100 | BATCH 44/71 | LOSS: 0.000292694978027915\n",
      "TRAIN: EPOCH 9/100 | BATCH 45/71 | LOSS: 0.00029360409509694284\n",
      "TRAIN: EPOCH 9/100 | BATCH 46/71 | LOSS: 0.0002934192431139502\n",
      "TRAIN: EPOCH 9/100 | BATCH 47/71 | LOSS: 0.0002935172245391489\n",
      "TRAIN: EPOCH 9/100 | BATCH 48/71 | LOSS: 0.000293884830245254\n",
      "TRAIN: EPOCH 9/100 | BATCH 49/71 | LOSS: 0.0002946797228651121\n",
      "TRAIN: EPOCH 9/100 | BATCH 50/71 | LOSS: 0.0002948229495600304\n",
      "TRAIN: EPOCH 9/100 | BATCH 51/71 | LOSS: 0.00029469787505849335\n",
      "TRAIN: EPOCH 9/100 | BATCH 52/71 | LOSS: 0.000295970235832633\n",
      "TRAIN: EPOCH 9/100 | BATCH 53/71 | LOSS: 0.00029551614931767325\n",
      "TRAIN: EPOCH 9/100 | BATCH 54/71 | LOSS: 0.0002954374919433824\n",
      "TRAIN: EPOCH 9/100 | BATCH 55/71 | LOSS: 0.00029568075582834093\n",
      "TRAIN: EPOCH 9/100 | BATCH 56/71 | LOSS: 0.0002962593647871951\n",
      "TRAIN: EPOCH 9/100 | BATCH 57/71 | LOSS: 0.0002969498871544633\n",
      "TRAIN: EPOCH 9/100 | BATCH 58/71 | LOSS: 0.00029726565265002014\n",
      "TRAIN: EPOCH 9/100 | BATCH 59/71 | LOSS: 0.0002968192891178963\n",
      "TRAIN: EPOCH 9/100 | BATCH 60/71 | LOSS: 0.00029666334739886224\n",
      "TRAIN: EPOCH 9/100 | BATCH 61/71 | LOSS: 0.00029590767348230246\n",
      "TRAIN: EPOCH 9/100 | BATCH 62/71 | LOSS: 0.0002948643754251183\n",
      "TRAIN: EPOCH 9/100 | BATCH 63/71 | LOSS: 0.0002941939501397428\n",
      "TRAIN: EPOCH 9/100 | BATCH 64/71 | LOSS: 0.00029421613838237064\n",
      "TRAIN: EPOCH 9/100 | BATCH 65/71 | LOSS: 0.0002936193266192056\n",
      "TRAIN: EPOCH 9/100 | BATCH 66/71 | LOSS: 0.00029367580836110594\n",
      "TRAIN: EPOCH 9/100 | BATCH 67/71 | LOSS: 0.0002926648782757933\n",
      "TRAIN: EPOCH 9/100 | BATCH 68/71 | LOSS: 0.000292209926503373\n",
      "TRAIN: EPOCH 9/100 | BATCH 69/71 | LOSS: 0.0002922502816155819\n",
      "TRAIN: EPOCH 9/100 | BATCH 70/71 | LOSS: 0.0002948443403690529\n",
      "VAL: EPOCH 9/100 | BATCH 0/8 | LOSS: 0.00033988599898293614\n",
      "VAL: EPOCH 9/100 | BATCH 1/8 | LOSS: 0.0002644726919243112\n",
      "VAL: EPOCH 9/100 | BATCH 2/8 | LOSS: 0.00027753714433250326\n",
      "VAL: EPOCH 9/100 | BATCH 3/8 | LOSS: 0.00026578012329991907\n",
      "VAL: EPOCH 9/100 | BATCH 4/8 | LOSS: 0.0002849909069482237\n",
      "VAL: EPOCH 9/100 | BATCH 5/8 | LOSS: 0.00027848922763951123\n",
      "VAL: EPOCH 9/100 | BATCH 6/8 | LOSS: 0.0002718949877557212\n",
      "VAL: EPOCH 9/100 | BATCH 7/8 | LOSS: 0.00027038523148803506\n",
      "TRAIN: EPOCH 10/100 | BATCH 0/71 | LOSS: 0.0002099187986459583\n",
      "TRAIN: EPOCH 10/100 | BATCH 1/71 | LOSS: 0.0002330953866476193\n",
      "TRAIN: EPOCH 10/100 | BATCH 2/71 | LOSS: 0.00024726762785576284\n",
      "TRAIN: EPOCH 10/100 | BATCH 3/71 | LOSS: 0.00024451944773318246\n",
      "TRAIN: EPOCH 10/100 | BATCH 4/71 | LOSS: 0.0002732039720285684\n",
      "TRAIN: EPOCH 10/100 | BATCH 5/71 | LOSS: 0.0002688862635598828\n",
      "TRAIN: EPOCH 10/100 | BATCH 6/71 | LOSS: 0.0002705302571744791\n",
      "TRAIN: EPOCH 10/100 | BATCH 7/71 | LOSS: 0.0002641204100655159\n",
      "TRAIN: EPOCH 10/100 | BATCH 8/71 | LOSS: 0.0002617436855669237\n",
      "TRAIN: EPOCH 10/100 | BATCH 9/71 | LOSS: 0.0002674488234333694\n",
      "TRAIN: EPOCH 10/100 | BATCH 10/71 | LOSS: 0.0002726668740665032\n",
      "TRAIN: EPOCH 10/100 | BATCH 11/71 | LOSS: 0.00026546245013984543\n",
      "TRAIN: EPOCH 10/100 | BATCH 12/71 | LOSS: 0.00027167114715736645\n",
      "TRAIN: EPOCH 10/100 | BATCH 13/71 | LOSS: 0.00027063961065973023\n",
      "TRAIN: EPOCH 10/100 | BATCH 14/71 | LOSS: 0.0002721763332374394\n",
      "TRAIN: EPOCH 10/100 | BATCH 15/71 | LOSS: 0.0002759933195193298\n",
      "TRAIN: EPOCH 10/100 | BATCH 16/71 | LOSS: 0.00027578717837219727\n",
      "TRAIN: EPOCH 10/100 | BATCH 17/71 | LOSS: 0.0002756485385665049\n",
      "TRAIN: EPOCH 10/100 | BATCH 18/71 | LOSS: 0.0002746566437723997\n",
      "TRAIN: EPOCH 10/100 | BATCH 19/71 | LOSS: 0.00027289391291560603\n",
      "TRAIN: EPOCH 10/100 | BATCH 20/71 | LOSS: 0.0002706216612187702\n",
      "TRAIN: EPOCH 10/100 | BATCH 21/71 | LOSS: 0.00026970665864858097\n",
      "TRAIN: EPOCH 10/100 | BATCH 22/71 | LOSS: 0.00027018569981800795\n",
      "TRAIN: EPOCH 10/100 | BATCH 23/71 | LOSS: 0.0002682275062397821\n",
      "TRAIN: EPOCH 10/100 | BATCH 24/71 | LOSS: 0.00026466124516446145\n",
      "TRAIN: EPOCH 10/100 | BATCH 25/71 | LOSS: 0.0002656741444997561\n",
      "TRAIN: EPOCH 10/100 | BATCH 26/71 | LOSS: 0.0002668587123345653\n",
      "TRAIN: EPOCH 10/100 | BATCH 27/71 | LOSS: 0.0002662792804975262\n",
      "TRAIN: EPOCH 10/100 | BATCH 28/71 | LOSS: 0.00026516026740990066\n",
      "TRAIN: EPOCH 10/100 | BATCH 29/71 | LOSS: 0.00026424726529512555\n",
      "TRAIN: EPOCH 10/100 | BATCH 30/71 | LOSS: 0.0002647025234651782\n",
      "TRAIN: EPOCH 10/100 | BATCH 31/71 | LOSS: 0.00026371122567070415\n",
      "TRAIN: EPOCH 10/100 | BATCH 32/71 | LOSS: 0.00026314753322918534\n",
      "TRAIN: EPOCH 10/100 | BATCH 33/71 | LOSS: 0.00026084732663173997\n",
      "TRAIN: EPOCH 10/100 | BATCH 34/71 | LOSS: 0.00026072500747561985\n",
      "TRAIN: EPOCH 10/100 | BATCH 35/71 | LOSS: 0.000259874952462269\n",
      "TRAIN: EPOCH 10/100 | BATCH 36/71 | LOSS: 0.0002584690475880445\n",
      "TRAIN: EPOCH 10/100 | BATCH 37/71 | LOSS: 0.0002598867236213808\n",
      "TRAIN: EPOCH 10/100 | BATCH 38/71 | LOSS: 0.00026032615278381854\n",
      "TRAIN: EPOCH 10/100 | BATCH 39/71 | LOSS: 0.00026170516248384955\n",
      "TRAIN: EPOCH 10/100 | BATCH 40/71 | LOSS: 0.000260929699501636\n",
      "TRAIN: EPOCH 10/100 | BATCH 41/71 | LOSS: 0.0002592360736647\n",
      "TRAIN: EPOCH 10/100 | BATCH 42/71 | LOSS: 0.00025922683917722385\n",
      "TRAIN: EPOCH 10/100 | BATCH 43/71 | LOSS: 0.0002584490006980063\n",
      "TRAIN: EPOCH 10/100 | BATCH 44/71 | LOSS: 0.00025808231310091084\n",
      "TRAIN: EPOCH 10/100 | BATCH 45/71 | LOSS: 0.0002594984480706246\n",
      "TRAIN: EPOCH 10/100 | BATCH 46/71 | LOSS: 0.0002610174644282682\n",
      "TRAIN: EPOCH 10/100 | BATCH 47/71 | LOSS: 0.00026008822533185594\n",
      "TRAIN: EPOCH 10/100 | BATCH 48/71 | LOSS: 0.0002599245992166047\n",
      "TRAIN: EPOCH 10/100 | BATCH 49/71 | LOSS: 0.0002606631373055279\n",
      "TRAIN: EPOCH 10/100 | BATCH 50/71 | LOSS: 0.00026034129333828447\n",
      "TRAIN: EPOCH 10/100 | BATCH 51/71 | LOSS: 0.0002594813063646703\n",
      "TRAIN: EPOCH 10/100 | BATCH 52/71 | LOSS: 0.0002593733169872068\n",
      "TRAIN: EPOCH 10/100 | BATCH 53/71 | LOSS: 0.00025920106418197975\n",
      "TRAIN: EPOCH 10/100 | BATCH 54/71 | LOSS: 0.00025860837803603234\n",
      "TRAIN: EPOCH 10/100 | BATCH 55/71 | LOSS: 0.00025887617734302433\n",
      "TRAIN: EPOCH 10/100 | BATCH 56/71 | LOSS: 0.00025858416854150656\n",
      "TRAIN: EPOCH 10/100 | BATCH 57/71 | LOSS: 0.0002587342241895385\n",
      "TRAIN: EPOCH 10/100 | BATCH 58/71 | LOSS: 0.0002575022369717895\n",
      "TRAIN: EPOCH 10/100 | BATCH 59/71 | LOSS: 0.00025852135634825874\n",
      "TRAIN: EPOCH 10/100 | BATCH 60/71 | LOSS: 0.00025799922305388284\n",
      "TRAIN: EPOCH 10/100 | BATCH 61/71 | LOSS: 0.0002578981840352137\n",
      "TRAIN: EPOCH 10/100 | BATCH 62/71 | LOSS: 0.00025914963436431235\n",
      "TRAIN: EPOCH 10/100 | BATCH 63/71 | LOSS: 0.0002590198037069058\n",
      "TRAIN: EPOCH 10/100 | BATCH 64/71 | LOSS: 0.00025905443785282284\n",
      "TRAIN: EPOCH 10/100 | BATCH 65/71 | LOSS: 0.0002595324851537236\n",
      "TRAIN: EPOCH 10/100 | BATCH 66/71 | LOSS: 0.0002598303440548202\n",
      "TRAIN: EPOCH 10/100 | BATCH 67/71 | LOSS: 0.00026037007174797025\n",
      "TRAIN: EPOCH 10/100 | BATCH 68/71 | LOSS: 0.00026005901209841335\n",
      "TRAIN: EPOCH 10/100 | BATCH 69/71 | LOSS: 0.00025903499315193455\n",
      "TRAIN: EPOCH 10/100 | BATCH 70/71 | LOSS: 0.00026069201662657704\n",
      "VAL: EPOCH 10/100 | BATCH 0/8 | LOSS: 0.0003119196626357734\n",
      "VAL: EPOCH 10/100 | BATCH 1/8 | LOSS: 0.00024026336905080825\n",
      "VAL: EPOCH 10/100 | BATCH 2/8 | LOSS: 0.00024955198750831187\n",
      "VAL: EPOCH 10/100 | BATCH 3/8 | LOSS: 0.0002384321269346401\n",
      "VAL: EPOCH 10/100 | BATCH 4/8 | LOSS: 0.0002538334985729307\n",
      "VAL: EPOCH 10/100 | BATCH 5/8 | LOSS: 0.00024790461611701176\n",
      "VAL: EPOCH 10/100 | BATCH 6/8 | LOSS: 0.00024286003775029843\n",
      "VAL: EPOCH 10/100 | BATCH 7/8 | LOSS: 0.0002407530937489355\n",
      "TRAIN: EPOCH 11/100 | BATCH 0/71 | LOSS: 0.0002096704120049253\n",
      "TRAIN: EPOCH 11/100 | BATCH 1/71 | LOSS: 0.0002241488837171346\n",
      "TRAIN: EPOCH 11/100 | BATCH 2/71 | LOSS: 0.0002202004640518377\n",
      "TRAIN: EPOCH 11/100 | BATCH 3/71 | LOSS: 0.0002413513502688147\n",
      "TRAIN: EPOCH 11/100 | BATCH 4/71 | LOSS: 0.0002455658453982323\n",
      "TRAIN: EPOCH 11/100 | BATCH 5/71 | LOSS: 0.0002523138440058877\n",
      "TRAIN: EPOCH 11/100 | BATCH 6/71 | LOSS: 0.00025283167737403085\n",
      "TRAIN: EPOCH 11/100 | BATCH 7/71 | LOSS: 0.0002502127881598426\n",
      "TRAIN: EPOCH 11/100 | BATCH 8/71 | LOSS: 0.00024546008434198383\n",
      "TRAIN: EPOCH 11/100 | BATCH 9/71 | LOSS: 0.00025526363897370173\n",
      "TRAIN: EPOCH 11/100 | BATCH 10/71 | LOSS: 0.00025029874285048044\n",
      "TRAIN: EPOCH 11/100 | BATCH 11/71 | LOSS: 0.0002521659977598271\n",
      "TRAIN: EPOCH 11/100 | BATCH 12/71 | LOSS: 0.00025057394948537246\n",
      "TRAIN: EPOCH 11/100 | BATCH 13/71 | LOSS: 0.0002525613994553818\n",
      "TRAIN: EPOCH 11/100 | BATCH 14/71 | LOSS: 0.00025253667748377967\n",
      "TRAIN: EPOCH 11/100 | BATCH 15/71 | LOSS: 0.0002567346245996305\n",
      "TRAIN: EPOCH 11/100 | BATCH 16/71 | LOSS: 0.00025922858570565414\n",
      "TRAIN: EPOCH 11/100 | BATCH 17/71 | LOSS: 0.00025947009190632444\n",
      "TRAIN: EPOCH 11/100 | BATCH 18/71 | LOSS: 0.00026590057026815454\n",
      "TRAIN: EPOCH 11/100 | BATCH 19/71 | LOSS: 0.00026384139855508695\n",
      "TRAIN: EPOCH 11/100 | BATCH 20/71 | LOSS: 0.0002643410331248084\n",
      "TRAIN: EPOCH 11/100 | BATCH 21/71 | LOSS: 0.0002620727973408066\n",
      "TRAIN: EPOCH 11/100 | BATCH 22/71 | LOSS: 0.00026231148576303184\n",
      "TRAIN: EPOCH 11/100 | BATCH 23/71 | LOSS: 0.0002603409175208071\n",
      "TRAIN: EPOCH 11/100 | BATCH 24/71 | LOSS: 0.00025731243193149566\n",
      "TRAIN: EPOCH 11/100 | BATCH 25/71 | LOSS: 0.0002573627436784311\n",
      "TRAIN: EPOCH 11/100 | BATCH 26/71 | LOSS: 0.0002574261894484085\n",
      "TRAIN: EPOCH 11/100 | BATCH 27/71 | LOSS: 0.0002582481933391786\n",
      "TRAIN: EPOCH 11/100 | BATCH 28/71 | LOSS: 0.0002552513429565869\n",
      "TRAIN: EPOCH 11/100 | BATCH 29/71 | LOSS: 0.000253936213751634\n",
      "TRAIN: EPOCH 11/100 | BATCH 30/71 | LOSS: 0.0002538846418892424\n",
      "TRAIN: EPOCH 11/100 | BATCH 31/71 | LOSS: 0.0002543887349020224\n",
      "TRAIN: EPOCH 11/100 | BATCH 32/71 | LOSS: 0.00025396343147749025\n",
      "TRAIN: EPOCH 11/100 | BATCH 33/71 | LOSS: 0.0002537351683713496\n",
      "TRAIN: EPOCH 11/100 | BATCH 34/71 | LOSS: 0.0002532190880239276\n",
      "TRAIN: EPOCH 11/100 | BATCH 35/71 | LOSS: 0.0002543136639966785\n",
      "TRAIN: EPOCH 11/100 | BATCH 36/71 | LOSS: 0.0002549123455537483\n",
      "TRAIN: EPOCH 11/100 | BATCH 37/71 | LOSS: 0.00025503626238787547\n",
      "TRAIN: EPOCH 11/100 | BATCH 38/71 | LOSS: 0.00025526262358583225\n",
      "TRAIN: EPOCH 11/100 | BATCH 39/71 | LOSS: 0.0002547640931879869\n",
      "TRAIN: EPOCH 11/100 | BATCH 40/71 | LOSS: 0.0002540320714526787\n",
      "TRAIN: EPOCH 11/100 | BATCH 41/71 | LOSS: 0.0002537254542749863\n",
      "TRAIN: EPOCH 11/100 | BATCH 42/71 | LOSS: 0.00025362142281834207\n",
      "TRAIN: EPOCH 11/100 | BATCH 43/71 | LOSS: 0.00025272093262174167\n",
      "TRAIN: EPOCH 11/100 | BATCH 44/71 | LOSS: 0.0002509097824157733\n",
      "TRAIN: EPOCH 11/100 | BATCH 45/71 | LOSS: 0.00025031586277612445\n",
      "TRAIN: EPOCH 11/100 | BATCH 46/71 | LOSS: 0.00024969013627033996\n",
      "TRAIN: EPOCH 11/100 | BATCH 47/71 | LOSS: 0.0002499388565411209\n",
      "TRAIN: EPOCH 11/100 | BATCH 48/71 | LOSS: 0.000250346530098183\n",
      "TRAIN: EPOCH 11/100 | BATCH 49/71 | LOSS: 0.00024895658425521104\n",
      "TRAIN: EPOCH 11/100 | BATCH 50/71 | LOSS: 0.00024809826572891325\n",
      "TRAIN: EPOCH 11/100 | BATCH 51/71 | LOSS: 0.0002470613765651181\n",
      "TRAIN: EPOCH 11/100 | BATCH 52/71 | LOSS: 0.0002458079123346768\n",
      "TRAIN: EPOCH 11/100 | BATCH 53/71 | LOSS: 0.0002449403184948972\n",
      "TRAIN: EPOCH 11/100 | BATCH 54/71 | LOSS: 0.0002445479336364025\n",
      "TRAIN: EPOCH 11/100 | BATCH 55/71 | LOSS: 0.0002431582719769462\n",
      "TRAIN: EPOCH 11/100 | BATCH 56/71 | LOSS: 0.0002416799395353321\n",
      "TRAIN: EPOCH 11/100 | BATCH 57/71 | LOSS: 0.00024109957002079244\n",
      "TRAIN: EPOCH 11/100 | BATCH 58/71 | LOSS: 0.0002406011352816723\n",
      "TRAIN: EPOCH 11/100 | BATCH 59/71 | LOSS: 0.0002393454393313732\n",
      "TRAIN: EPOCH 11/100 | BATCH 60/71 | LOSS: 0.00023890481846689505\n",
      "TRAIN: EPOCH 11/100 | BATCH 61/71 | LOSS: 0.00023819652391642693\n",
      "TRAIN: EPOCH 11/100 | BATCH 62/71 | LOSS: 0.00023723405458787013\n",
      "TRAIN: EPOCH 11/100 | BATCH 63/71 | LOSS: 0.00023697473397987778\n",
      "TRAIN: EPOCH 11/100 | BATCH 64/71 | LOSS: 0.00023767890211624595\n",
      "TRAIN: EPOCH 11/100 | BATCH 65/71 | LOSS: 0.00023745580101879597\n",
      "TRAIN: EPOCH 11/100 | BATCH 66/71 | LOSS: 0.00023769374125386908\n",
      "TRAIN: EPOCH 11/100 | BATCH 67/71 | LOSS: 0.0002385278798208353\n",
      "TRAIN: EPOCH 11/100 | BATCH 68/71 | LOSS: 0.00023833408962726\n",
      "TRAIN: EPOCH 11/100 | BATCH 69/71 | LOSS: 0.00023781792135975724\n",
      "TRAIN: EPOCH 11/100 | BATCH 70/71 | LOSS: 0.00023777731083510575\n",
      "VAL: EPOCH 11/100 | BATCH 0/8 | LOSS: 0.00028774066595360637\n",
      "VAL: EPOCH 11/100 | BATCH 1/8 | LOSS: 0.00021650963026331738\n",
      "VAL: EPOCH 11/100 | BATCH 2/8 | LOSS: 0.00022415028070099652\n",
      "VAL: EPOCH 11/100 | BATCH 3/8 | LOSS: 0.00021479838324012235\n",
      "VAL: EPOCH 11/100 | BATCH 4/8 | LOSS: 0.00023077467340044678\n",
      "VAL: EPOCH 11/100 | BATCH 5/8 | LOSS: 0.0002237822482129559\n",
      "VAL: EPOCH 11/100 | BATCH 6/8 | LOSS: 0.00021987503610684404\n",
      "VAL: EPOCH 11/100 | BATCH 7/8 | LOSS: 0.00021747932805737946\n",
      "TRAIN: EPOCH 12/100 | BATCH 0/71 | LOSS: 0.0002909489267040044\n",
      "TRAIN: EPOCH 12/100 | BATCH 1/71 | LOSS: 0.0002433315821690485\n",
      "TRAIN: EPOCH 12/100 | BATCH 2/71 | LOSS: 0.00023524914286099374\n",
      "TRAIN: EPOCH 12/100 | BATCH 3/71 | LOSS: 0.00022633259504800662\n",
      "TRAIN: EPOCH 12/100 | BATCH 4/71 | LOSS: 0.0002206144912634045\n",
      "TRAIN: EPOCH 12/100 | BATCH 5/71 | LOSS: 0.00022982149190890291\n",
      "TRAIN: EPOCH 12/100 | BATCH 6/71 | LOSS: 0.00023527790160317506\n",
      "TRAIN: EPOCH 12/100 | BATCH 7/71 | LOSS: 0.00023234794934978709\n",
      "TRAIN: EPOCH 12/100 | BATCH 8/71 | LOSS: 0.00023530339563472403\n",
      "TRAIN: EPOCH 12/100 | BATCH 9/71 | LOSS: 0.00023749432002659888\n",
      "TRAIN: EPOCH 12/100 | BATCH 10/71 | LOSS: 0.00024032856030812994\n",
      "TRAIN: EPOCH 12/100 | BATCH 11/71 | LOSS: 0.00023916540158097632\n",
      "TRAIN: EPOCH 12/100 | BATCH 12/71 | LOSS: 0.0002372166502307384\n",
      "TRAIN: EPOCH 12/100 | BATCH 13/71 | LOSS: 0.00024135534788781245\n",
      "TRAIN: EPOCH 12/100 | BATCH 14/71 | LOSS: 0.00023637921549379826\n",
      "TRAIN: EPOCH 12/100 | BATCH 15/71 | LOSS: 0.000237446418395848\n",
      "TRAIN: EPOCH 12/100 | BATCH 16/71 | LOSS: 0.00023368016905699144\n",
      "TRAIN: EPOCH 12/100 | BATCH 17/71 | LOSS: 0.00023074857987618694\n",
      "TRAIN: EPOCH 12/100 | BATCH 18/71 | LOSS: 0.00023091837494192938\n",
      "TRAIN: EPOCH 12/100 | BATCH 19/71 | LOSS: 0.00022812026872998103\n",
      "TRAIN: EPOCH 12/100 | BATCH 20/71 | LOSS: 0.00022799025395042483\n",
      "TRAIN: EPOCH 12/100 | BATCH 21/71 | LOSS: 0.00022585143795533276\n",
      "TRAIN: EPOCH 12/100 | BATCH 22/71 | LOSS: 0.00022495176476102483\n",
      "TRAIN: EPOCH 12/100 | BATCH 23/71 | LOSS: 0.0002243799905651637\n",
      "TRAIN: EPOCH 12/100 | BATCH 24/71 | LOSS: 0.00022340525640174747\n",
      "TRAIN: EPOCH 12/100 | BATCH 25/71 | LOSS: 0.000221911927488131\n",
      "TRAIN: EPOCH 12/100 | BATCH 26/71 | LOSS: 0.000222526926706821\n",
      "TRAIN: EPOCH 12/100 | BATCH 27/71 | LOSS: 0.00022024306495690586\n",
      "TRAIN: EPOCH 12/100 | BATCH 28/71 | LOSS: 0.00022377841353239813\n",
      "TRAIN: EPOCH 12/100 | BATCH 29/71 | LOSS: 0.00022254494688240812\n",
      "TRAIN: EPOCH 12/100 | BATCH 30/71 | LOSS: 0.00022184117011831052\n",
      "TRAIN: EPOCH 12/100 | BATCH 31/71 | LOSS: 0.00022177778691911954\n",
      "TRAIN: EPOCH 12/100 | BATCH 32/71 | LOSS: 0.0002211319837387595\n",
      "TRAIN: EPOCH 12/100 | BATCH 33/71 | LOSS: 0.00021955959527986124\n",
      "TRAIN: EPOCH 12/100 | BATCH 34/71 | LOSS: 0.0002194755249157814\n",
      "TRAIN: EPOCH 12/100 | BATCH 35/71 | LOSS: 0.00021833203168676441\n",
      "TRAIN: EPOCH 12/100 | BATCH 36/71 | LOSS: 0.0002183684693001261\n",
      "TRAIN: EPOCH 12/100 | BATCH 37/71 | LOSS: 0.00021706311503964427\n",
      "TRAIN: EPOCH 12/100 | BATCH 38/71 | LOSS: 0.00021721752814184397\n",
      "TRAIN: EPOCH 12/100 | BATCH 39/71 | LOSS: 0.00021690179310098756\n",
      "TRAIN: EPOCH 12/100 | BATCH 40/71 | LOSS: 0.00021735527204838013\n",
      "TRAIN: EPOCH 12/100 | BATCH 41/71 | LOSS: 0.00021824163108942143\n",
      "TRAIN: EPOCH 12/100 | BATCH 42/71 | LOSS: 0.00021778542560601044\n",
      "TRAIN: EPOCH 12/100 | BATCH 43/71 | LOSS: 0.0002171675084603273\n",
      "TRAIN: EPOCH 12/100 | BATCH 44/71 | LOSS: 0.0002176807045341573\n",
      "TRAIN: EPOCH 12/100 | BATCH 45/71 | LOSS: 0.00021818940149387345\n",
      "TRAIN: EPOCH 12/100 | BATCH 46/71 | LOSS: 0.00021806500816192636\n",
      "TRAIN: EPOCH 12/100 | BATCH 47/71 | LOSS: 0.00021730627334666983\n",
      "TRAIN: EPOCH 12/100 | BATCH 48/71 | LOSS: 0.00021655143717332382\n",
      "TRAIN: EPOCH 12/100 | BATCH 49/71 | LOSS: 0.00021566084993537515\n",
      "TRAIN: EPOCH 12/100 | BATCH 50/71 | LOSS: 0.0002153942340930157\n",
      "TRAIN: EPOCH 12/100 | BATCH 51/71 | LOSS: 0.00021649249650251406\n",
      "TRAIN: EPOCH 12/100 | BATCH 52/71 | LOSS: 0.00021620436114663222\n",
      "TRAIN: EPOCH 12/100 | BATCH 53/71 | LOSS: 0.00021543801563826424\n",
      "TRAIN: EPOCH 12/100 | BATCH 54/71 | LOSS: 0.0002160315904554657\n",
      "TRAIN: EPOCH 12/100 | BATCH 55/71 | LOSS: 0.00021599067518504204\n",
      "TRAIN: EPOCH 12/100 | BATCH 56/71 | LOSS: 0.0002148066889372115\n",
      "TRAIN: EPOCH 12/100 | BATCH 57/71 | LOSS: 0.000215661597080882\n",
      "TRAIN: EPOCH 12/100 | BATCH 58/71 | LOSS: 0.0002150156094211201\n",
      "TRAIN: EPOCH 12/100 | BATCH 59/71 | LOSS: 0.00021511683977829912\n",
      "TRAIN: EPOCH 12/100 | BATCH 60/71 | LOSS: 0.00021465619158602824\n",
      "TRAIN: EPOCH 12/100 | BATCH 61/71 | LOSS: 0.00021482680970239603\n",
      "TRAIN: EPOCH 12/100 | BATCH 62/71 | LOSS: 0.00021471707949540503\n",
      "TRAIN: EPOCH 12/100 | BATCH 63/71 | LOSS: 0.00021530758317567233\n",
      "TRAIN: EPOCH 12/100 | BATCH 64/71 | LOSS: 0.00021507665425395735\n",
      "TRAIN: EPOCH 12/100 | BATCH 65/71 | LOSS: 0.0002145235244604533\n",
      "TRAIN: EPOCH 12/100 | BATCH 66/71 | LOSS: 0.00021461511674552544\n",
      "TRAIN: EPOCH 12/100 | BATCH 67/71 | LOSS: 0.00021551760544432052\n",
      "TRAIN: EPOCH 12/100 | BATCH 68/71 | LOSS: 0.00021579292829762605\n",
      "TRAIN: EPOCH 12/100 | BATCH 69/71 | LOSS: 0.00021549554387872505\n",
      "TRAIN: EPOCH 12/100 | BATCH 70/71 | LOSS: 0.00021544352139372537\n",
      "VAL: EPOCH 12/100 | BATCH 0/8 | LOSS: 0.00026845320826396346\n",
      "VAL: EPOCH 12/100 | BATCH 1/8 | LOSS: 0.0001994040358113125\n",
      "VAL: EPOCH 12/100 | BATCH 2/8 | LOSS: 0.00020604581610920528\n",
      "VAL: EPOCH 12/100 | BATCH 3/8 | LOSS: 0.0001969489749171771\n",
      "VAL: EPOCH 12/100 | BATCH 4/8 | LOSS: 0.00021004973677918315\n",
      "VAL: EPOCH 12/100 | BATCH 5/8 | LOSS: 0.00020402129545497397\n",
      "VAL: EPOCH 12/100 | BATCH 6/8 | LOSS: 0.00020135932468942234\n",
      "VAL: EPOCH 12/100 | BATCH 7/8 | LOSS: 0.00019869361130986363\n",
      "TRAIN: EPOCH 13/100 | BATCH 0/71 | LOSS: 0.0002148235507775098\n",
      "TRAIN: EPOCH 13/100 | BATCH 1/71 | LOSS: 0.0002484890428604558\n",
      "TRAIN: EPOCH 13/100 | BATCH 2/71 | LOSS: 0.0002365052690341448\n",
      "TRAIN: EPOCH 13/100 | BATCH 3/71 | LOSS: 0.0002449013954901602\n",
      "TRAIN: EPOCH 13/100 | BATCH 4/71 | LOSS: 0.00024013004440348595\n",
      "TRAIN: EPOCH 13/100 | BATCH 5/71 | LOSS: 0.0002344161742560876\n",
      "TRAIN: EPOCH 13/100 | BATCH 6/71 | LOSS: 0.00022406263243673102\n",
      "TRAIN: EPOCH 13/100 | BATCH 7/71 | LOSS: 0.0002193085711041931\n",
      "TRAIN: EPOCH 13/100 | BATCH 8/71 | LOSS: 0.00021247733901772235\n",
      "TRAIN: EPOCH 13/100 | BATCH 9/71 | LOSS: 0.00021576950093731283\n",
      "TRAIN: EPOCH 13/100 | BATCH 10/71 | LOSS: 0.0002153089973779226\n",
      "TRAIN: EPOCH 13/100 | BATCH 11/71 | LOSS: 0.00021115681859858645\n",
      "TRAIN: EPOCH 13/100 | BATCH 12/71 | LOSS: 0.00021777878842280747\n",
      "TRAIN: EPOCH 13/100 | BATCH 13/71 | LOSS: 0.00021937724938782464\n",
      "TRAIN: EPOCH 13/100 | BATCH 14/71 | LOSS: 0.00021750218293163924\n",
      "TRAIN: EPOCH 13/100 | BATCH 15/71 | LOSS: 0.00021605541314784205\n",
      "TRAIN: EPOCH 13/100 | BATCH 16/71 | LOSS: 0.00021589733917019605\n",
      "TRAIN: EPOCH 13/100 | BATCH 17/71 | LOSS: 0.00021648767789075564\n",
      "TRAIN: EPOCH 13/100 | BATCH 18/71 | LOSS: 0.00021453298426135197\n",
      "TRAIN: EPOCH 13/100 | BATCH 19/71 | LOSS: 0.0002134768321411684\n",
      "TRAIN: EPOCH 13/100 | BATCH 20/71 | LOSS: 0.00021354984519781457\n",
      "TRAIN: EPOCH 13/100 | BATCH 21/71 | LOSS: 0.00021153127057583663\n",
      "TRAIN: EPOCH 13/100 | BATCH 22/71 | LOSS: 0.00021057347539285928\n",
      "TRAIN: EPOCH 13/100 | BATCH 23/71 | LOSS: 0.00021023823986373222\n",
      "TRAIN: EPOCH 13/100 | BATCH 24/71 | LOSS: 0.00021096205397043378\n",
      "TRAIN: EPOCH 13/100 | BATCH 25/71 | LOSS: 0.0002091144756727422\n",
      "TRAIN: EPOCH 13/100 | BATCH 26/71 | LOSS: 0.0002075117399390028\n",
      "TRAIN: EPOCH 13/100 | BATCH 27/71 | LOSS: 0.00020905272633951557\n",
      "TRAIN: EPOCH 13/100 | BATCH 28/71 | LOSS: 0.00020870085352036203\n",
      "TRAIN: EPOCH 13/100 | BATCH 29/71 | LOSS: 0.0002082813962867173\n",
      "TRAIN: EPOCH 13/100 | BATCH 30/71 | LOSS: 0.00020740653206068543\n",
      "TRAIN: EPOCH 13/100 | BATCH 31/71 | LOSS: 0.0002042388427980768\n",
      "TRAIN: EPOCH 13/100 | BATCH 32/71 | LOSS: 0.00020413535865813947\n",
      "TRAIN: EPOCH 13/100 | BATCH 33/71 | LOSS: 0.00020325211345848134\n",
      "TRAIN: EPOCH 13/100 | BATCH 34/71 | LOSS: 0.00020182141784711607\n",
      "TRAIN: EPOCH 13/100 | BATCH 35/71 | LOSS: 0.0002007990078709554\n",
      "TRAIN: EPOCH 13/100 | BATCH 36/71 | LOSS: 0.00020040635521385215\n",
      "TRAIN: EPOCH 13/100 | BATCH 37/71 | LOSS: 0.00020021454800275693\n",
      "TRAIN: EPOCH 13/100 | BATCH 38/71 | LOSS: 0.00020076326006510033\n",
      "TRAIN: EPOCH 13/100 | BATCH 39/71 | LOSS: 0.00020242384234734346\n",
      "TRAIN: EPOCH 13/100 | BATCH 40/71 | LOSS: 0.00020232913723192746\n",
      "TRAIN: EPOCH 13/100 | BATCH 41/71 | LOSS: 0.00020190735605345772\n",
      "TRAIN: EPOCH 13/100 | BATCH 42/71 | LOSS: 0.00020128623230469434\n",
      "TRAIN: EPOCH 13/100 | BATCH 43/71 | LOSS: 0.00020080858865350655\n",
      "TRAIN: EPOCH 13/100 | BATCH 44/71 | LOSS: 0.00020120113007982986\n",
      "TRAIN: EPOCH 13/100 | BATCH 45/71 | LOSS: 0.00020082700053614604\n",
      "TRAIN: EPOCH 13/100 | BATCH 46/71 | LOSS: 0.00020078648850222692\n",
      "TRAIN: EPOCH 13/100 | BATCH 47/71 | LOSS: 0.0002012592703977134\n",
      "TRAIN: EPOCH 13/100 | BATCH 48/71 | LOSS: 0.0002010685204034101\n",
      "TRAIN: EPOCH 13/100 | BATCH 49/71 | LOSS: 0.0002003403112757951\n",
      "TRAIN: EPOCH 13/100 | BATCH 50/71 | LOSS: 0.0002001059579996246\n",
      "TRAIN: EPOCH 13/100 | BATCH 51/71 | LOSS: 0.00019934521808933752\n",
      "TRAIN: EPOCH 13/100 | BATCH 52/71 | LOSS: 0.00020012502006105728\n",
      "TRAIN: EPOCH 13/100 | BATCH 53/71 | LOSS: 0.00019914719100429297\n",
      "TRAIN: EPOCH 13/100 | BATCH 54/71 | LOSS: 0.00019819930550345983\n",
      "TRAIN: EPOCH 13/100 | BATCH 55/71 | LOSS: 0.0001977124772695658\n",
      "TRAIN: EPOCH 13/100 | BATCH 56/71 | LOSS: 0.00019796316311776376\n",
      "TRAIN: EPOCH 13/100 | BATCH 57/71 | LOSS: 0.0001975048608387467\n",
      "TRAIN: EPOCH 13/100 | BATCH 58/71 | LOSS: 0.00019634909576625895\n",
      "TRAIN: EPOCH 13/100 | BATCH 59/71 | LOSS: 0.00019659461759147234\n",
      "TRAIN: EPOCH 13/100 | BATCH 60/71 | LOSS: 0.00019594628223576813\n",
      "TRAIN: EPOCH 13/100 | BATCH 61/71 | LOSS: 0.0001948445694119249\n",
      "TRAIN: EPOCH 13/100 | BATCH 62/71 | LOSS: 0.00019470262613760988\n",
      "TRAIN: EPOCH 13/100 | BATCH 63/71 | LOSS: 0.0001943511472290993\n",
      "TRAIN: EPOCH 13/100 | BATCH 64/71 | LOSS: 0.00019418111165018322\n",
      "TRAIN: EPOCH 13/100 | BATCH 65/71 | LOSS: 0.00019374219559584603\n",
      "TRAIN: EPOCH 13/100 | BATCH 66/71 | LOSS: 0.00019271307952403187\n",
      "TRAIN: EPOCH 13/100 | BATCH 67/71 | LOSS: 0.00019307323300916536\n",
      "TRAIN: EPOCH 13/100 | BATCH 68/71 | LOSS: 0.00019263212490604138\n",
      "TRAIN: EPOCH 13/100 | BATCH 69/71 | LOSS: 0.00019253613843050386\n",
      "TRAIN: EPOCH 13/100 | BATCH 70/71 | LOSS: 0.0001935954082717883\n",
      "VAL: EPOCH 13/100 | BATCH 0/8 | LOSS: 0.0002481365227140486\n",
      "VAL: EPOCH 13/100 | BATCH 1/8 | LOSS: 0.00018455412282492034\n",
      "VAL: EPOCH 13/100 | BATCH 2/8 | LOSS: 0.00019120535337909436\n",
      "VAL: EPOCH 13/100 | BATCH 3/8 | LOSS: 0.0001830206219892716\n",
      "VAL: EPOCH 13/100 | BATCH 4/8 | LOSS: 0.00019648658781079575\n",
      "VAL: EPOCH 13/100 | BATCH 5/8 | LOSS: 0.00019046417219215073\n",
      "VAL: EPOCH 13/100 | BATCH 6/8 | LOSS: 0.00018793554357086708\n",
      "VAL: EPOCH 13/100 | BATCH 7/8 | LOSS: 0.00018648645254870644\n",
      "TRAIN: EPOCH 14/100 | BATCH 0/71 | LOSS: 0.00015027043991722167\n",
      "TRAIN: EPOCH 14/100 | BATCH 1/71 | LOSS: 0.00016163635882548988\n",
      "TRAIN: EPOCH 14/100 | BATCH 2/71 | LOSS: 0.00015079408573607603\n",
      "TRAIN: EPOCH 14/100 | BATCH 3/71 | LOSS: 0.00015800733672222123\n",
      "TRAIN: EPOCH 14/100 | BATCH 4/71 | LOSS: 0.0001692015037406236\n",
      "TRAIN: EPOCH 14/100 | BATCH 5/71 | LOSS: 0.00017946408964538327\n",
      "TRAIN: EPOCH 14/100 | BATCH 6/71 | LOSS: 0.00018264405246424888\n",
      "TRAIN: EPOCH 14/100 | BATCH 7/71 | LOSS: 0.00018306724268768448\n",
      "TRAIN: EPOCH 14/100 | BATCH 8/71 | LOSS: 0.00018074865187777\n",
      "TRAIN: EPOCH 14/100 | BATCH 9/71 | LOSS: 0.00017880146770039574\n",
      "TRAIN: EPOCH 14/100 | BATCH 10/71 | LOSS: 0.000185735382150266\n",
      "TRAIN: EPOCH 14/100 | BATCH 11/71 | LOSS: 0.00018428563635097817\n",
      "TRAIN: EPOCH 14/100 | BATCH 12/71 | LOSS: 0.00018095393338276504\n",
      "TRAIN: EPOCH 14/100 | BATCH 13/71 | LOSS: 0.00018566745088069832\n",
      "TRAIN: EPOCH 14/100 | BATCH 14/71 | LOSS: 0.0001860331423813477\n",
      "TRAIN: EPOCH 14/100 | BATCH 15/71 | LOSS: 0.00018554188409325434\n",
      "TRAIN: EPOCH 14/100 | BATCH 16/71 | LOSS: 0.00018692451703827828\n",
      "TRAIN: EPOCH 14/100 | BATCH 17/71 | LOSS: 0.00018712431645124324\n",
      "TRAIN: EPOCH 14/100 | BATCH 18/71 | LOSS: 0.000186124979671532\n",
      "TRAIN: EPOCH 14/100 | BATCH 19/71 | LOSS: 0.00018532432513893582\n",
      "TRAIN: EPOCH 14/100 | BATCH 20/71 | LOSS: 0.00018616442199951658\n",
      "TRAIN: EPOCH 14/100 | BATCH 21/71 | LOSS: 0.00018729616766160524\n",
      "TRAIN: EPOCH 14/100 | BATCH 22/71 | LOSS: 0.00018784826249390355\n",
      "TRAIN: EPOCH 14/100 | BATCH 23/71 | LOSS: 0.00018677754087548237\n",
      "TRAIN: EPOCH 14/100 | BATCH 24/71 | LOSS: 0.0001848014065762982\n",
      "TRAIN: EPOCH 14/100 | BATCH 25/71 | LOSS: 0.00018973796921692646\n",
      "TRAIN: EPOCH 14/100 | BATCH 26/71 | LOSS: 0.00019007644840274697\n",
      "TRAIN: EPOCH 14/100 | BATCH 27/71 | LOSS: 0.00018998439606678273\n",
      "TRAIN: EPOCH 14/100 | BATCH 28/71 | LOSS: 0.00018894765074445126\n",
      "TRAIN: EPOCH 14/100 | BATCH 29/71 | LOSS: 0.0001905157056171447\n",
      "TRAIN: EPOCH 14/100 | BATCH 30/71 | LOSS: 0.00018984707158749864\n",
      "TRAIN: EPOCH 14/100 | BATCH 31/71 | LOSS: 0.00018780106529447949\n",
      "TRAIN: EPOCH 14/100 | BATCH 32/71 | LOSS: 0.0001876962086305756\n",
      "TRAIN: EPOCH 14/100 | BATCH 33/71 | LOSS: 0.00018786793610642608\n",
      "TRAIN: EPOCH 14/100 | BATCH 34/71 | LOSS: 0.00018678084639499762\n",
      "TRAIN: EPOCH 14/100 | BATCH 35/71 | LOSS: 0.00018840863413061015\n",
      "TRAIN: EPOCH 14/100 | BATCH 36/71 | LOSS: 0.00018698738996455495\n",
      "TRAIN: EPOCH 14/100 | BATCH 37/71 | LOSS: 0.00018724818111973276\n",
      "TRAIN: EPOCH 14/100 | BATCH 38/71 | LOSS: 0.00018614905666953957\n",
      "TRAIN: EPOCH 14/100 | BATCH 39/71 | LOSS: 0.00018573696906969416\n",
      "TRAIN: EPOCH 14/100 | BATCH 40/71 | LOSS: 0.00018617241762623917\n",
      "TRAIN: EPOCH 14/100 | BATCH 41/71 | LOSS: 0.00018494959173646444\n",
      "TRAIN: EPOCH 14/100 | BATCH 42/71 | LOSS: 0.00018424067753204686\n",
      "TRAIN: EPOCH 14/100 | BATCH 43/71 | LOSS: 0.00018427038528236815\n",
      "TRAIN: EPOCH 14/100 | BATCH 44/71 | LOSS: 0.00018408330894696215\n",
      "TRAIN: EPOCH 14/100 | BATCH 45/71 | LOSS: 0.00018323436708432501\n",
      "TRAIN: EPOCH 14/100 | BATCH 46/71 | LOSS: 0.0001826095332867129\n",
      "TRAIN: EPOCH 14/100 | BATCH 47/71 | LOSS: 0.0001821354483884837\n",
      "TRAIN: EPOCH 14/100 | BATCH 48/71 | LOSS: 0.00018185269318007845\n",
      "TRAIN: EPOCH 14/100 | BATCH 49/71 | LOSS: 0.0001811139367055148\n",
      "TRAIN: EPOCH 14/100 | BATCH 50/71 | LOSS: 0.0001815700368252674\n",
      "TRAIN: EPOCH 14/100 | BATCH 51/71 | LOSS: 0.00018158786518212693\n",
      "TRAIN: EPOCH 14/100 | BATCH 52/71 | LOSS: 0.0001814820894857271\n",
      "TRAIN: EPOCH 14/100 | BATCH 53/71 | LOSS: 0.00018139097853606841\n",
      "TRAIN: EPOCH 14/100 | BATCH 54/71 | LOSS: 0.00018168149108532816\n",
      "TRAIN: EPOCH 14/100 | BATCH 55/71 | LOSS: 0.00018153735826282564\n",
      "TRAIN: EPOCH 14/100 | BATCH 56/71 | LOSS: 0.00018049072522237047\n",
      "TRAIN: EPOCH 14/100 | BATCH 57/71 | LOSS: 0.0001801303193948617\n",
      "TRAIN: EPOCH 14/100 | BATCH 58/71 | LOSS: 0.00017970259981111512\n",
      "TRAIN: EPOCH 14/100 | BATCH 59/71 | LOSS: 0.00017987327931526427\n",
      "TRAIN: EPOCH 14/100 | BATCH 60/71 | LOSS: 0.00018004315989244668\n",
      "TRAIN: EPOCH 14/100 | BATCH 61/71 | LOSS: 0.00017910550565331153\n",
      "TRAIN: EPOCH 14/100 | BATCH 62/71 | LOSS: 0.00017944549645541148\n",
      "TRAIN: EPOCH 14/100 | BATCH 63/71 | LOSS: 0.00017908223685481062\n",
      "TRAIN: EPOCH 14/100 | BATCH 64/71 | LOSS: 0.0001785781039731004\n",
      "TRAIN: EPOCH 14/100 | BATCH 65/71 | LOSS: 0.00017869476869236678\n",
      "TRAIN: EPOCH 14/100 | BATCH 66/71 | LOSS: 0.0001784283009708734\n",
      "TRAIN: EPOCH 14/100 | BATCH 67/71 | LOSS: 0.00017912215138066504\n",
      "TRAIN: EPOCH 14/100 | BATCH 68/71 | LOSS: 0.0001786191046383043\n",
      "TRAIN: EPOCH 14/100 | BATCH 69/71 | LOSS: 0.00017868912733060176\n",
      "TRAIN: EPOCH 14/100 | BATCH 70/71 | LOSS: 0.00017835639751660572\n",
      "VAL: EPOCH 14/100 | BATCH 0/8 | LOSS: 0.00023955282813403755\n",
      "VAL: EPOCH 14/100 | BATCH 1/8 | LOSS: 0.0001837413219618611\n",
      "VAL: EPOCH 14/100 | BATCH 2/8 | LOSS: 0.00018641844993301979\n",
      "VAL: EPOCH 14/100 | BATCH 3/8 | LOSS: 0.00017839900465332903\n",
      "VAL: EPOCH 14/100 | BATCH 4/8 | LOSS: 0.00018454134988132864\n",
      "VAL: EPOCH 14/100 | BATCH 5/8 | LOSS: 0.00018075373615526283\n",
      "VAL: EPOCH 14/100 | BATCH 6/8 | LOSS: 0.00017956908309965262\n",
      "VAL: EPOCH 14/100 | BATCH 7/8 | LOSS: 0.0001764211538102245\n",
      "TRAIN: EPOCH 15/100 | BATCH 0/71 | LOSS: 0.00019032218551728874\n",
      "TRAIN: EPOCH 15/100 | BATCH 1/71 | LOSS: 0.00018021181313088164\n",
      "TRAIN: EPOCH 15/100 | BATCH 2/71 | LOSS: 0.00017819210673527172\n",
      "TRAIN: EPOCH 15/100 | BATCH 3/71 | LOSS: 0.0001901191244542133\n",
      "TRAIN: EPOCH 15/100 | BATCH 4/71 | LOSS: 0.00018008705519605427\n",
      "TRAIN: EPOCH 15/100 | BATCH 5/71 | LOSS: 0.00018304638797417283\n",
      "TRAIN: EPOCH 15/100 | BATCH 6/71 | LOSS: 0.00017806627978903374\n",
      "TRAIN: EPOCH 15/100 | BATCH 7/71 | LOSS: 0.00017252881298190914\n",
      "TRAIN: EPOCH 15/100 | BATCH 8/71 | LOSS: 0.00016917719040066004\n",
      "TRAIN: EPOCH 15/100 | BATCH 9/71 | LOSS: 0.00016877838061191142\n",
      "TRAIN: EPOCH 15/100 | BATCH 10/71 | LOSS: 0.00016682878115468404\n",
      "TRAIN: EPOCH 15/100 | BATCH 11/71 | LOSS: 0.00017391538373582685\n",
      "TRAIN: EPOCH 15/100 | BATCH 12/71 | LOSS: 0.0001728896918043924\n",
      "TRAIN: EPOCH 15/100 | BATCH 13/71 | LOSS: 0.0001749407435584414\n",
      "TRAIN: EPOCH 15/100 | BATCH 14/71 | LOSS: 0.00017229689207548898\n",
      "TRAIN: EPOCH 15/100 | BATCH 15/71 | LOSS: 0.00017412241777492454\n",
      "TRAIN: EPOCH 15/100 | BATCH 16/71 | LOSS: 0.00017068700810127398\n",
      "TRAIN: EPOCH 15/100 | BATCH 17/71 | LOSS: 0.00017039153883363016\n",
      "TRAIN: EPOCH 15/100 | BATCH 18/71 | LOSS: 0.0001682969366237031\n",
      "TRAIN: EPOCH 15/100 | BATCH 19/71 | LOSS: 0.00016807426291052253\n",
      "TRAIN: EPOCH 15/100 | BATCH 20/71 | LOSS: 0.00016626532375258173\n",
      "TRAIN: EPOCH 15/100 | BATCH 21/71 | LOSS: 0.0001661278431790627\n",
      "TRAIN: EPOCH 15/100 | BATCH 22/71 | LOSS: 0.0001653693224121209\n",
      "TRAIN: EPOCH 15/100 | BATCH 23/71 | LOSS: 0.00016407842546565612\n",
      "TRAIN: EPOCH 15/100 | BATCH 24/71 | LOSS: 0.00016639044566545635\n",
      "TRAIN: EPOCH 15/100 | BATCH 25/71 | LOSS: 0.00016489144596002565\n",
      "TRAIN: EPOCH 15/100 | BATCH 26/71 | LOSS: 0.00016611330351292122\n",
      "TRAIN: EPOCH 15/100 | BATCH 27/71 | LOSS: 0.00016456738947973854\n",
      "TRAIN: EPOCH 15/100 | BATCH 28/71 | LOSS: 0.00016538387090208587\n",
      "TRAIN: EPOCH 15/100 | BATCH 29/71 | LOSS: 0.00016400648552613954\n",
      "TRAIN: EPOCH 15/100 | BATCH 30/71 | LOSS: 0.0001640793484365267\n",
      "TRAIN: EPOCH 15/100 | BATCH 31/71 | LOSS: 0.000166076037203311\n",
      "TRAIN: EPOCH 15/100 | BATCH 32/71 | LOSS: 0.00016574904491955584\n",
      "TRAIN: EPOCH 15/100 | BATCH 33/71 | LOSS: 0.00016619855373659555\n",
      "TRAIN: EPOCH 15/100 | BATCH 34/71 | LOSS: 0.00016678565880283712\n",
      "TRAIN: EPOCH 15/100 | BATCH 35/71 | LOSS: 0.00016734060854004283\n",
      "TRAIN: EPOCH 15/100 | BATCH 36/71 | LOSS: 0.00016805159739797582\n",
      "TRAIN: EPOCH 15/100 | BATCH 37/71 | LOSS: 0.00016684555157553405\n",
      "TRAIN: EPOCH 15/100 | BATCH 38/71 | LOSS: 0.00016664474708541558\n",
      "TRAIN: EPOCH 15/100 | BATCH 39/71 | LOSS: 0.0001663024853769457\n",
      "TRAIN: EPOCH 15/100 | BATCH 40/71 | LOSS: 0.0001669563605058257\n",
      "TRAIN: EPOCH 15/100 | BATCH 41/71 | LOSS: 0.00016601000658868412\n",
      "TRAIN: EPOCH 15/100 | BATCH 42/71 | LOSS: 0.00016725163393773053\n",
      "TRAIN: EPOCH 15/100 | BATCH 43/71 | LOSS: 0.00016581327508902177\n",
      "TRAIN: EPOCH 15/100 | BATCH 44/71 | LOSS: 0.00016574128886633036\n",
      "TRAIN: EPOCH 15/100 | BATCH 45/71 | LOSS: 0.00016607217517246127\n",
      "TRAIN: EPOCH 15/100 | BATCH 46/71 | LOSS: 0.00016620461773275932\n",
      "TRAIN: EPOCH 15/100 | BATCH 47/71 | LOSS: 0.00016645254557564235\n",
      "TRAIN: EPOCH 15/100 | BATCH 48/71 | LOSS: 0.0001658614925631531\n",
      "TRAIN: EPOCH 15/100 | BATCH 49/71 | LOSS: 0.0001654933035024442\n",
      "TRAIN: EPOCH 15/100 | BATCH 50/71 | LOSS: 0.00016574336897225722\n",
      "TRAIN: EPOCH 15/100 | BATCH 51/71 | LOSS: 0.0001652654718782287\n",
      "TRAIN: EPOCH 15/100 | BATCH 52/71 | LOSS: 0.00016459148022442846\n",
      "TRAIN: EPOCH 15/100 | BATCH 53/71 | LOSS: 0.00016448250783098587\n",
      "TRAIN: EPOCH 15/100 | BATCH 54/71 | LOSS: 0.00016423612647816876\n",
      "TRAIN: EPOCH 15/100 | BATCH 55/71 | LOSS: 0.00016486321640383852\n",
      "TRAIN: EPOCH 15/100 | BATCH 56/71 | LOSS: 0.0001658605189029977\n",
      "TRAIN: EPOCH 15/100 | BATCH 57/71 | LOSS: 0.00016560496517521297\n",
      "TRAIN: EPOCH 15/100 | BATCH 58/71 | LOSS: 0.00016676946848993962\n",
      "TRAIN: EPOCH 15/100 | BATCH 59/71 | LOSS: 0.0001664271022794613\n",
      "TRAIN: EPOCH 15/100 | BATCH 60/71 | LOSS: 0.00016656063080070632\n",
      "TRAIN: EPOCH 15/100 | BATCH 61/71 | LOSS: 0.0001658342314899088\n",
      "TRAIN: EPOCH 15/100 | BATCH 62/71 | LOSS: 0.0001656115135928202\n",
      "TRAIN: EPOCH 15/100 | BATCH 63/71 | LOSS: 0.00016567947773182823\n",
      "TRAIN: EPOCH 15/100 | BATCH 64/71 | LOSS: 0.00016557751268220062\n",
      "TRAIN: EPOCH 15/100 | BATCH 65/71 | LOSS: 0.0001652820889527599\n",
      "TRAIN: EPOCH 15/100 | BATCH 66/71 | LOSS: 0.00016474006732286356\n",
      "TRAIN: EPOCH 15/100 | BATCH 67/71 | LOSS: 0.00016411351038293694\n",
      "TRAIN: EPOCH 15/100 | BATCH 68/71 | LOSS: 0.00016385620878334495\n",
      "TRAIN: EPOCH 15/100 | BATCH 69/71 | LOSS: 0.00016343435064689922\n",
      "TRAIN: EPOCH 15/100 | BATCH 70/71 | LOSS: 0.0001640055866823466\n",
      "VAL: EPOCH 15/100 | BATCH 0/8 | LOSS: 0.00021981418831273913\n",
      "VAL: EPOCH 15/100 | BATCH 1/8 | LOSS: 0.000165480152645614\n",
      "VAL: EPOCH 15/100 | BATCH 2/8 | LOSS: 0.00016766391248287013\n",
      "VAL: EPOCH 15/100 | BATCH 3/8 | LOSS: 0.00016122240413096733\n",
      "VAL: EPOCH 15/100 | BATCH 4/8 | LOSS: 0.0001664191804593429\n",
      "VAL: EPOCH 15/100 | BATCH 5/8 | LOSS: 0.00016326436646825945\n",
      "VAL: EPOCH 15/100 | BATCH 6/8 | LOSS: 0.00016308963988974159\n",
      "VAL: EPOCH 15/100 | BATCH 7/8 | LOSS: 0.00016009418504836503\n",
      "TRAIN: EPOCH 16/100 | BATCH 0/71 | LOSS: 0.0001812247501220554\n",
      "TRAIN: EPOCH 16/100 | BATCH 1/71 | LOSS: 0.00018144387286156416\n",
      "TRAIN: EPOCH 16/100 | BATCH 2/71 | LOSS: 0.00016824196791276336\n",
      "TRAIN: EPOCH 16/100 | BATCH 3/71 | LOSS: 0.00016145404515555128\n",
      "TRAIN: EPOCH 16/100 | BATCH 4/71 | LOSS: 0.00016402016626670957\n",
      "TRAIN: EPOCH 16/100 | BATCH 5/71 | LOSS: 0.00016236299173518395\n",
      "TRAIN: EPOCH 16/100 | BATCH 6/71 | LOSS: 0.0001670145076267155\n",
      "TRAIN: EPOCH 16/100 | BATCH 7/71 | LOSS: 0.00016196050819416996\n",
      "TRAIN: EPOCH 16/100 | BATCH 8/71 | LOSS: 0.0001645829163155415\n",
      "TRAIN: EPOCH 16/100 | BATCH 9/71 | LOSS: 0.00016307727637467905\n",
      "TRAIN: EPOCH 16/100 | BATCH 10/71 | LOSS: 0.00016042586569462648\n",
      "TRAIN: EPOCH 16/100 | BATCH 11/71 | LOSS: 0.00016185410762166916\n",
      "TRAIN: EPOCH 16/100 | BATCH 12/71 | LOSS: 0.0001596844164081491\n",
      "TRAIN: EPOCH 16/100 | BATCH 13/71 | LOSS: 0.00015991594201685593\n",
      "TRAIN: EPOCH 16/100 | BATCH 14/71 | LOSS: 0.00015902141264329355\n",
      "TRAIN: EPOCH 16/100 | BATCH 15/71 | LOSS: 0.0001592474327480886\n",
      "TRAIN: EPOCH 16/100 | BATCH 16/71 | LOSS: 0.00015970977256074548\n",
      "TRAIN: EPOCH 16/100 | BATCH 17/71 | LOSS: 0.00015721633907459263\n",
      "TRAIN: EPOCH 16/100 | BATCH 18/71 | LOSS: 0.00015819051355049995\n",
      "TRAIN: EPOCH 16/100 | BATCH 19/71 | LOSS: 0.00015929978471831418\n",
      "TRAIN: EPOCH 16/100 | BATCH 20/71 | LOSS: 0.0001583708563841702\n",
      "TRAIN: EPOCH 16/100 | BATCH 21/71 | LOSS: 0.00015896642567399382\n",
      "TRAIN: EPOCH 16/100 | BATCH 22/71 | LOSS: 0.000158553354001766\n",
      "TRAIN: EPOCH 16/100 | BATCH 23/71 | LOSS: 0.00015595390747572915\n",
      "TRAIN: EPOCH 16/100 | BATCH 24/71 | LOSS: 0.00015778575208969415\n",
      "TRAIN: EPOCH 16/100 | BATCH 25/71 | LOSS: 0.0001588600039562712\n",
      "TRAIN: EPOCH 16/100 | BATCH 26/71 | LOSS: 0.0001596363446744228\n",
      "TRAIN: EPOCH 16/100 | BATCH 27/71 | LOSS: 0.00015869751015478478\n",
      "TRAIN: EPOCH 16/100 | BATCH 28/71 | LOSS: 0.0001587087392871236\n",
      "TRAIN: EPOCH 16/100 | BATCH 29/71 | LOSS: 0.00015741129561016958\n",
      "TRAIN: EPOCH 16/100 | BATCH 30/71 | LOSS: 0.00015801252541883338\n",
      "TRAIN: EPOCH 16/100 | BATCH 31/71 | LOSS: 0.00015701402435297496\n",
      "TRAIN: EPOCH 16/100 | BATCH 32/71 | LOSS: 0.00015721344265550601\n",
      "TRAIN: EPOCH 16/100 | BATCH 33/71 | LOSS: 0.00015676960859255975\n",
      "TRAIN: EPOCH 16/100 | BATCH 34/71 | LOSS: 0.00015597628163439888\n",
      "TRAIN: EPOCH 16/100 | BATCH 35/71 | LOSS: 0.000155579481442045\n",
      "TRAIN: EPOCH 16/100 | BATCH 36/71 | LOSS: 0.00015566463470521912\n",
      "TRAIN: EPOCH 16/100 | BATCH 37/71 | LOSS: 0.0001560259502423976\n",
      "TRAIN: EPOCH 16/100 | BATCH 38/71 | LOSS: 0.00015550170754954123\n",
      "TRAIN: EPOCH 16/100 | BATCH 39/71 | LOSS: 0.00015467111952602864\n",
      "TRAIN: EPOCH 16/100 | BATCH 40/71 | LOSS: 0.00015404900016331274\n",
      "TRAIN: EPOCH 16/100 | BATCH 41/71 | LOSS: 0.00015357471974788322\n",
      "TRAIN: EPOCH 16/100 | BATCH 42/71 | LOSS: 0.00015287153655663133\n",
      "TRAIN: EPOCH 16/100 | BATCH 43/71 | LOSS: 0.00015271315384201114\n",
      "TRAIN: EPOCH 16/100 | BATCH 44/71 | LOSS: 0.0001528930481678496\n",
      "TRAIN: EPOCH 16/100 | BATCH 45/71 | LOSS: 0.00015231279489260328\n",
      "TRAIN: EPOCH 16/100 | BATCH 46/71 | LOSS: 0.00015254470859059785\n",
      "TRAIN: EPOCH 16/100 | BATCH 47/71 | LOSS: 0.00015239789930395395\n",
      "TRAIN: EPOCH 16/100 | BATCH 48/71 | LOSS: 0.00015270424744218817\n",
      "TRAIN: EPOCH 16/100 | BATCH 49/71 | LOSS: 0.00015205465009785258\n",
      "TRAIN: EPOCH 16/100 | BATCH 50/71 | LOSS: 0.0001511084541847345\n",
      "TRAIN: EPOCH 16/100 | BATCH 51/71 | LOSS: 0.00015107660118911343\n",
      "TRAIN: EPOCH 16/100 | BATCH 52/71 | LOSS: 0.00015099021910364686\n",
      "TRAIN: EPOCH 16/100 | BATCH 53/71 | LOSS: 0.00015042728531019142\n",
      "TRAIN: EPOCH 16/100 | BATCH 54/71 | LOSS: 0.00014982159283879974\n",
      "TRAIN: EPOCH 16/100 | BATCH 55/71 | LOSS: 0.00014969928153212613\n",
      "TRAIN: EPOCH 16/100 | BATCH 56/71 | LOSS: 0.00014907286848092713\n",
      "TRAIN: EPOCH 16/100 | BATCH 57/71 | LOSS: 0.00014918618286048575\n",
      "TRAIN: EPOCH 16/100 | BATCH 58/71 | LOSS: 0.0001485227271509988\n",
      "TRAIN: EPOCH 16/100 | BATCH 59/71 | LOSS: 0.0001479017139596787\n",
      "TRAIN: EPOCH 16/100 | BATCH 60/71 | LOSS: 0.0001478165384051848\n",
      "TRAIN: EPOCH 16/100 | BATCH 61/71 | LOSS: 0.00014758223868181717\n",
      "TRAIN: EPOCH 16/100 | BATCH 62/71 | LOSS: 0.00014806885005710352\n",
      "TRAIN: EPOCH 16/100 | BATCH 63/71 | LOSS: 0.00014786427061608265\n",
      "TRAIN: EPOCH 16/100 | BATCH 64/71 | LOSS: 0.00014785028436633114\n",
      "TRAIN: EPOCH 16/100 | BATCH 65/71 | LOSS: 0.00014779166612601276\n",
      "TRAIN: EPOCH 16/100 | BATCH 66/71 | LOSS: 0.00014685762843287398\n",
      "TRAIN: EPOCH 16/100 | BATCH 67/71 | LOSS: 0.00014734235778365366\n",
      "TRAIN: EPOCH 16/100 | BATCH 68/71 | LOSS: 0.0001470619888319904\n",
      "TRAIN: EPOCH 16/100 | BATCH 69/71 | LOSS: 0.00014676336878827507\n",
      "TRAIN: EPOCH 16/100 | BATCH 70/71 | LOSS: 0.00014601636560745274\n",
      "VAL: EPOCH 16/100 | BATCH 0/8 | LOSS: 0.00019174671615473926\n",
      "VAL: EPOCH 16/100 | BATCH 1/8 | LOSS: 0.00014366547839017585\n",
      "VAL: EPOCH 16/100 | BATCH 2/8 | LOSS: 0.00014546546056711426\n",
      "VAL: EPOCH 16/100 | BATCH 3/8 | LOSS: 0.00013905119885748718\n",
      "VAL: EPOCH 16/100 | BATCH 4/8 | LOSS: 0.00014503013371722773\n",
      "VAL: EPOCH 16/100 | BATCH 5/8 | LOSS: 0.00014128313462909622\n",
      "VAL: EPOCH 16/100 | BATCH 6/8 | LOSS: 0.0001405505557028976\n",
      "VAL: EPOCH 16/100 | BATCH 7/8 | LOSS: 0.00013837035930919228\n",
      "TRAIN: EPOCH 17/100 | BATCH 0/71 | LOSS: 0.00012267049169167876\n",
      "TRAIN: EPOCH 17/100 | BATCH 1/71 | LOSS: 0.00013064064842183143\n",
      "TRAIN: EPOCH 17/100 | BATCH 2/71 | LOSS: 0.00014406093396246433\n",
      "TRAIN: EPOCH 17/100 | BATCH 3/71 | LOSS: 0.0001346483131783316\n",
      "TRAIN: EPOCH 17/100 | BATCH 4/71 | LOSS: 0.00013924017403041944\n",
      "TRAIN: EPOCH 17/100 | BATCH 5/71 | LOSS: 0.00013221893095760606\n",
      "TRAIN: EPOCH 17/100 | BATCH 6/71 | LOSS: 0.00012727812697578753\n",
      "TRAIN: EPOCH 17/100 | BATCH 7/71 | LOSS: 0.00012841515490435995\n",
      "TRAIN: EPOCH 17/100 | BATCH 8/71 | LOSS: 0.00013268229991404546\n",
      "TRAIN: EPOCH 17/100 | BATCH 9/71 | LOSS: 0.0001306641919654794\n",
      "TRAIN: EPOCH 17/100 | BATCH 10/71 | LOSS: 0.0001286404380648905\n",
      "TRAIN: EPOCH 17/100 | BATCH 11/71 | LOSS: 0.00013376116536771102\n",
      "TRAIN: EPOCH 17/100 | BATCH 12/71 | LOSS: 0.00013288047529595831\n",
      "TRAIN: EPOCH 17/100 | BATCH 13/71 | LOSS: 0.00013427467898249494\n",
      "TRAIN: EPOCH 17/100 | BATCH 14/71 | LOSS: 0.00013531443109968678\n",
      "TRAIN: EPOCH 17/100 | BATCH 15/71 | LOSS: 0.00013530242040360463\n",
      "TRAIN: EPOCH 17/100 | BATCH 16/71 | LOSS: 0.00013383380171623737\n",
      "TRAIN: EPOCH 17/100 | BATCH 17/71 | LOSS: 0.00013361694679285088\n",
      "TRAIN: EPOCH 17/100 | BATCH 18/71 | LOSS: 0.00013383401643582866\n",
      "TRAIN: EPOCH 17/100 | BATCH 19/71 | LOSS: 0.00013409342645900323\n",
      "TRAIN: EPOCH 17/100 | BATCH 20/71 | LOSS: 0.0001357346849371901\n",
      "TRAIN: EPOCH 17/100 | BATCH 21/71 | LOSS: 0.00013410592939842238\n",
      "TRAIN: EPOCH 17/100 | BATCH 22/71 | LOSS: 0.00013558871432906017\n",
      "TRAIN: EPOCH 17/100 | BATCH 23/71 | LOSS: 0.00013547220108497035\n",
      "TRAIN: EPOCH 17/100 | BATCH 24/71 | LOSS: 0.00013481254281941802\n",
      "TRAIN: EPOCH 17/100 | BATCH 25/71 | LOSS: 0.0001367910439372421\n",
      "TRAIN: EPOCH 17/100 | BATCH 26/71 | LOSS: 0.00013605477498559695\n",
      "TRAIN: EPOCH 17/100 | BATCH 27/71 | LOSS: 0.00013727013278444896\n",
      "TRAIN: EPOCH 17/100 | BATCH 28/71 | LOSS: 0.00013824607218527394\n",
      "TRAIN: EPOCH 17/100 | BATCH 29/71 | LOSS: 0.00013799455846310593\n",
      "TRAIN: EPOCH 17/100 | BATCH 30/71 | LOSS: 0.00013873421924091094\n",
      "TRAIN: EPOCH 17/100 | BATCH 31/71 | LOSS: 0.00014022329719409754\n",
      "TRAIN: EPOCH 17/100 | BATCH 32/71 | LOSS: 0.00014091602762081575\n",
      "TRAIN: EPOCH 17/100 | BATCH 33/71 | LOSS: 0.00014088085149791476\n",
      "TRAIN: EPOCH 17/100 | BATCH 34/71 | LOSS: 0.00014000273097605843\n",
      "TRAIN: EPOCH 17/100 | BATCH 35/71 | LOSS: 0.00014081303561397363\n",
      "TRAIN: EPOCH 17/100 | BATCH 36/71 | LOSS: 0.00013987183204310872\n",
      "TRAIN: EPOCH 17/100 | BATCH 37/71 | LOSS: 0.00014070654709173955\n",
      "TRAIN: EPOCH 17/100 | BATCH 38/71 | LOSS: 0.0001404732055305384\n",
      "TRAIN: EPOCH 17/100 | BATCH 39/71 | LOSS: 0.00014120220330369194\n",
      "TRAIN: EPOCH 17/100 | BATCH 40/71 | LOSS: 0.00014030626121760777\n",
      "TRAIN: EPOCH 17/100 | BATCH 41/71 | LOSS: 0.00014055522548178922\n",
      "TRAIN: EPOCH 17/100 | BATCH 42/71 | LOSS: 0.00014006361506776452\n",
      "TRAIN: EPOCH 17/100 | BATCH 43/71 | LOSS: 0.00013956323511427564\n",
      "TRAIN: EPOCH 17/100 | BATCH 44/71 | LOSS: 0.0001391120796648061\n",
      "TRAIN: EPOCH 17/100 | BATCH 45/71 | LOSS: 0.0001389044822565705\n",
      "TRAIN: EPOCH 17/100 | BATCH 46/71 | LOSS: 0.00013903058595346048\n",
      "TRAIN: EPOCH 17/100 | BATCH 47/71 | LOSS: 0.00013826731249840427\n",
      "TRAIN: EPOCH 17/100 | BATCH 48/71 | LOSS: 0.00013769413294075817\n",
      "TRAIN: EPOCH 17/100 | BATCH 49/71 | LOSS: 0.00013742913914029486\n",
      "TRAIN: EPOCH 17/100 | BATCH 50/71 | LOSS: 0.0001372326835681794\n",
      "TRAIN: EPOCH 17/100 | BATCH 51/71 | LOSS: 0.00013690176680169176\n",
      "TRAIN: EPOCH 17/100 | BATCH 52/71 | LOSS: 0.00013655765497736316\n",
      "TRAIN: EPOCH 17/100 | BATCH 53/71 | LOSS: 0.00013602510080090724\n",
      "TRAIN: EPOCH 17/100 | BATCH 54/71 | LOSS: 0.000136006508810996\n",
      "TRAIN: EPOCH 17/100 | BATCH 55/71 | LOSS: 0.00013575327011494664\n",
      "TRAIN: EPOCH 17/100 | BATCH 56/71 | LOSS: 0.00013531787510401684\n",
      "TRAIN: EPOCH 17/100 | BATCH 57/71 | LOSS: 0.00013566843712208633\n",
      "TRAIN: EPOCH 17/100 | BATCH 58/71 | LOSS: 0.0001350750013844552\n",
      "TRAIN: EPOCH 17/100 | BATCH 59/71 | LOSS: 0.00013458607536449564\n",
      "TRAIN: EPOCH 17/100 | BATCH 60/71 | LOSS: 0.00013452732363490572\n",
      "TRAIN: EPOCH 17/100 | BATCH 61/71 | LOSS: 0.00013444191250938832\n",
      "TRAIN: EPOCH 17/100 | BATCH 62/71 | LOSS: 0.00013420738763497431\n",
      "TRAIN: EPOCH 17/100 | BATCH 63/71 | LOSS: 0.000133598510842603\n",
      "TRAIN: EPOCH 17/100 | BATCH 64/71 | LOSS: 0.00013377220837775474\n",
      "TRAIN: EPOCH 17/100 | BATCH 65/71 | LOSS: 0.00013363562318387045\n",
      "TRAIN: EPOCH 17/100 | BATCH 66/71 | LOSS: 0.00013333033006238655\n",
      "TRAIN: EPOCH 17/100 | BATCH 67/71 | LOSS: 0.0001334426974876003\n",
      "TRAIN: EPOCH 17/100 | BATCH 68/71 | LOSS: 0.00013340574925836256\n",
      "TRAIN: EPOCH 17/100 | BATCH 69/71 | LOSS: 0.0001331225733987854\n",
      "TRAIN: EPOCH 17/100 | BATCH 70/71 | LOSS: 0.0001334801478586881\n",
      "VAL: EPOCH 17/100 | BATCH 0/8 | LOSS: 0.00017273479897994548\n",
      "VAL: EPOCH 17/100 | BATCH 1/8 | LOSS: 0.00012882138253189623\n",
      "VAL: EPOCH 17/100 | BATCH 2/8 | LOSS: 0.00013171579727592567\n",
      "VAL: EPOCH 17/100 | BATCH 3/8 | LOSS: 0.00012511543536675163\n",
      "VAL: EPOCH 17/100 | BATCH 4/8 | LOSS: 0.00013044369698036462\n",
      "VAL: EPOCH 17/100 | BATCH 5/8 | LOSS: 0.00012660221788488948\n",
      "VAL: EPOCH 17/100 | BATCH 6/8 | LOSS: 0.00012511992203404328\n",
      "VAL: EPOCH 17/100 | BATCH 7/8 | LOSS: 0.00012423295538610546\n",
      "TRAIN: EPOCH 18/100 | BATCH 0/71 | LOSS: 0.00011048396117985249\n",
      "TRAIN: EPOCH 18/100 | BATCH 1/71 | LOSS: 0.00010389647650299594\n",
      "TRAIN: EPOCH 18/100 | BATCH 2/71 | LOSS: 0.0001084406806815726\n",
      "TRAIN: EPOCH 18/100 | BATCH 3/71 | LOSS: 0.00011600698235270102\n",
      "TRAIN: EPOCH 18/100 | BATCH 4/71 | LOSS: 0.00011653737165033818\n",
      "TRAIN: EPOCH 18/100 | BATCH 5/71 | LOSS: 0.00012215742996583381\n",
      "TRAIN: EPOCH 18/100 | BATCH 6/71 | LOSS: 0.00012643155475546206\n",
      "TRAIN: EPOCH 18/100 | BATCH 7/71 | LOSS: 0.00012677485938183963\n",
      "TRAIN: EPOCH 18/100 | BATCH 8/71 | LOSS: 0.00012527252692962065\n",
      "TRAIN: EPOCH 18/100 | BATCH 9/71 | LOSS: 0.0001237623961060308\n",
      "TRAIN: EPOCH 18/100 | BATCH 10/71 | LOSS: 0.00012074407343541019\n",
      "TRAIN: EPOCH 18/100 | BATCH 11/71 | LOSS: 0.00012204372372555856\n",
      "TRAIN: EPOCH 18/100 | BATCH 12/71 | LOSS: 0.00012058522910452806\n",
      "TRAIN: EPOCH 18/100 | BATCH 13/71 | LOSS: 0.00012114204790642751\n",
      "TRAIN: EPOCH 18/100 | BATCH 14/71 | LOSS: 0.00012059171276632697\n",
      "TRAIN: EPOCH 18/100 | BATCH 15/71 | LOSS: 0.000120315171443508\n",
      "TRAIN: EPOCH 18/100 | BATCH 16/71 | LOSS: 0.0001191492603324792\n",
      "TRAIN: EPOCH 18/100 | BATCH 17/71 | LOSS: 0.00011982868858871775\n",
      "TRAIN: EPOCH 18/100 | BATCH 18/71 | LOSS: 0.0001196305617524654\n",
      "TRAIN: EPOCH 18/100 | BATCH 19/71 | LOSS: 0.00011855070151796099\n",
      "TRAIN: EPOCH 18/100 | BATCH 20/71 | LOSS: 0.00011861195872327135\n",
      "TRAIN: EPOCH 18/100 | BATCH 21/71 | LOSS: 0.00011859797582887536\n",
      "TRAIN: EPOCH 18/100 | BATCH 22/71 | LOSS: 0.0001170294446005162\n",
      "TRAIN: EPOCH 18/100 | BATCH 23/71 | LOSS: 0.00011671300641561781\n",
      "TRAIN: EPOCH 18/100 | BATCH 24/71 | LOSS: 0.00011646328581264242\n",
      "TRAIN: EPOCH 18/100 | BATCH 25/71 | LOSS: 0.000116180401024534\n",
      "TRAIN: EPOCH 18/100 | BATCH 26/71 | LOSS: 0.00011631162730433668\n",
      "TRAIN: EPOCH 18/100 | BATCH 27/71 | LOSS: 0.00011651300077833835\n",
      "TRAIN: EPOCH 18/100 | BATCH 28/71 | LOSS: 0.00011708135721635009\n",
      "TRAIN: EPOCH 18/100 | BATCH 29/71 | LOSS: 0.00011736384694813751\n",
      "TRAIN: EPOCH 18/100 | BATCH 30/71 | LOSS: 0.00011755207871770366\n",
      "TRAIN: EPOCH 18/100 | BATCH 31/71 | LOSS: 0.00011787444304900419\n",
      "TRAIN: EPOCH 18/100 | BATCH 32/71 | LOSS: 0.0001175932748909955\n",
      "TRAIN: EPOCH 18/100 | BATCH 33/71 | LOSS: 0.0001191238648864194\n",
      "TRAIN: EPOCH 18/100 | BATCH 34/71 | LOSS: 0.00011885628357828993\n",
      "TRAIN: EPOCH 18/100 | BATCH 35/71 | LOSS: 0.00011842185126119552\n",
      "TRAIN: EPOCH 18/100 | BATCH 36/71 | LOSS: 0.0001187108166443461\n",
      "TRAIN: EPOCH 18/100 | BATCH 37/71 | LOSS: 0.00011943382732600807\n",
      "TRAIN: EPOCH 18/100 | BATCH 38/71 | LOSS: 0.00012051814085750196\n",
      "TRAIN: EPOCH 18/100 | BATCH 39/71 | LOSS: 0.00012026380372844869\n",
      "TRAIN: EPOCH 18/100 | BATCH 40/71 | LOSS: 0.0001198526199625964\n",
      "TRAIN: EPOCH 18/100 | BATCH 41/71 | LOSS: 0.00011965619740381261\n",
      "TRAIN: EPOCH 18/100 | BATCH 42/71 | LOSS: 0.00011918873632418771\n",
      "TRAIN: EPOCH 18/100 | BATCH 43/71 | LOSS: 0.00011904021002092949\n",
      "TRAIN: EPOCH 18/100 | BATCH 44/71 | LOSS: 0.00011918141503378543\n",
      "TRAIN: EPOCH 18/100 | BATCH 45/71 | LOSS: 0.00012013040917657275\n",
      "TRAIN: EPOCH 18/100 | BATCH 46/71 | LOSS: 0.00012000296261790704\n",
      "TRAIN: EPOCH 18/100 | BATCH 47/71 | LOSS: 0.00011993766550707126\n",
      "TRAIN: EPOCH 18/100 | BATCH 48/71 | LOSS: 0.00011959400640358692\n",
      "TRAIN: EPOCH 18/100 | BATCH 49/71 | LOSS: 0.00011913708483916707\n",
      "TRAIN: EPOCH 18/100 | BATCH 50/71 | LOSS: 0.00011914107787614579\n",
      "TRAIN: EPOCH 18/100 | BATCH 51/71 | LOSS: 0.00011876166368333193\n",
      "TRAIN: EPOCH 18/100 | BATCH 52/71 | LOSS: 0.00011883984058480358\n",
      "TRAIN: EPOCH 18/100 | BATCH 53/71 | LOSS: 0.00011983874557902002\n",
      "TRAIN: EPOCH 18/100 | BATCH 54/71 | LOSS: 0.00011950724980455231\n",
      "TRAIN: EPOCH 18/100 | BATCH 55/71 | LOSS: 0.00011945606651611993\n",
      "TRAIN: EPOCH 18/100 | BATCH 56/71 | LOSS: 0.00011984078382578956\n",
      "TRAIN: EPOCH 18/100 | BATCH 57/71 | LOSS: 0.00011958309714200681\n",
      "TRAIN: EPOCH 18/100 | BATCH 58/71 | LOSS: 0.00011968569360452436\n",
      "TRAIN: EPOCH 18/100 | BATCH 59/71 | LOSS: 0.00012009309042089929\n",
      "TRAIN: EPOCH 18/100 | BATCH 60/71 | LOSS: 0.000119844414093379\n",
      "TRAIN: EPOCH 18/100 | BATCH 61/71 | LOSS: 0.0001193920504574048\n",
      "TRAIN: EPOCH 18/100 | BATCH 62/71 | LOSS: 0.00011929743916317377\n",
      "TRAIN: EPOCH 18/100 | BATCH 63/71 | LOSS: 0.00011909221859696117\n",
      "TRAIN: EPOCH 18/100 | BATCH 64/71 | LOSS: 0.00011879551669923016\n",
      "TRAIN: EPOCH 18/100 | BATCH 65/71 | LOSS: 0.00011856714450969417\n",
      "TRAIN: EPOCH 18/100 | BATCH 66/71 | LOSS: 0.00011871787973544074\n",
      "TRAIN: EPOCH 18/100 | BATCH 67/71 | LOSS: 0.00011866953922007644\n",
      "TRAIN: EPOCH 18/100 | BATCH 68/71 | LOSS: 0.00011822529990425117\n",
      "TRAIN: EPOCH 18/100 | BATCH 69/71 | LOSS: 0.00011794873010200848\n",
      "TRAIN: EPOCH 18/100 | BATCH 70/71 | LOSS: 0.00011815315827360035\n",
      "VAL: EPOCH 18/100 | BATCH 0/8 | LOSS: 0.00015858965343795717\n",
      "VAL: EPOCH 18/100 | BATCH 1/8 | LOSS: 0.0001213618234032765\n",
      "VAL: EPOCH 18/100 | BATCH 2/8 | LOSS: 0.00012454731040634215\n",
      "VAL: EPOCH 18/100 | BATCH 3/8 | LOSS: 0.00011820479812740814\n",
      "VAL: EPOCH 18/100 | BATCH 4/8 | LOSS: 0.00012272756575839593\n",
      "VAL: EPOCH 18/100 | BATCH 5/8 | LOSS: 0.00011911370650826332\n",
      "VAL: EPOCH 18/100 | BATCH 6/8 | LOSS: 0.00011698567478950801\n",
      "VAL: EPOCH 18/100 | BATCH 7/8 | LOSS: 0.00011698411253746599\n",
      "TRAIN: EPOCH 19/100 | BATCH 0/71 | LOSS: 0.00011601320875342935\n",
      "TRAIN: EPOCH 19/100 | BATCH 1/71 | LOSS: 0.0001096508203772828\n",
      "TRAIN: EPOCH 19/100 | BATCH 2/71 | LOSS: 0.00010869303757014374\n",
      "TRAIN: EPOCH 19/100 | BATCH 3/71 | LOSS: 0.00011224576883250847\n",
      "TRAIN: EPOCH 19/100 | BATCH 4/71 | LOSS: 0.00011680836032610387\n",
      "TRAIN: EPOCH 19/100 | BATCH 5/71 | LOSS: 0.00011690004976117052\n",
      "TRAIN: EPOCH 19/100 | BATCH 6/71 | LOSS: 0.00011422829473823575\n",
      "TRAIN: EPOCH 19/100 | BATCH 7/71 | LOSS: 0.00011113629352621501\n",
      "TRAIN: EPOCH 19/100 | BATCH 8/71 | LOSS: 0.0001077553355975801\n",
      "TRAIN: EPOCH 19/100 | BATCH 9/71 | LOSS: 0.0001094285114959348\n",
      "TRAIN: EPOCH 19/100 | BATCH 10/71 | LOSS: 0.00011044791873163459\n",
      "TRAIN: EPOCH 19/100 | BATCH 11/71 | LOSS: 0.00011030799578293227\n",
      "TRAIN: EPOCH 19/100 | BATCH 12/71 | LOSS: 0.0001101949315992757\n",
      "TRAIN: EPOCH 19/100 | BATCH 13/71 | LOSS: 0.0001100570454062628\n",
      "TRAIN: EPOCH 19/100 | BATCH 14/71 | LOSS: 0.00011033887518957878\n",
      "TRAIN: EPOCH 19/100 | BATCH 15/71 | LOSS: 0.00010868644312722608\n",
      "TRAIN: EPOCH 19/100 | BATCH 16/71 | LOSS: 0.00010845798875807839\n",
      "TRAIN: EPOCH 19/100 | BATCH 17/71 | LOSS: 0.00010762093976760904\n",
      "TRAIN: EPOCH 19/100 | BATCH 18/71 | LOSS: 0.00010858339756898778\n",
      "TRAIN: EPOCH 19/100 | BATCH 19/71 | LOSS: 0.00010962686865241267\n",
      "TRAIN: EPOCH 19/100 | BATCH 20/71 | LOSS: 0.00010848685321564387\n",
      "TRAIN: EPOCH 19/100 | BATCH 21/71 | LOSS: 0.00010888187171076424\n",
      "TRAIN: EPOCH 19/100 | BATCH 22/71 | LOSS: 0.00010859762234916991\n",
      "TRAIN: EPOCH 19/100 | BATCH 23/71 | LOSS: 0.00010842757971355847\n",
      "TRAIN: EPOCH 19/100 | BATCH 24/71 | LOSS: 0.000108357175195124\n",
      "TRAIN: EPOCH 19/100 | BATCH 25/71 | LOSS: 0.00010765975737460674\n",
      "TRAIN: EPOCH 19/100 | BATCH 26/71 | LOSS: 0.00010677139733969752\n",
      "TRAIN: EPOCH 19/100 | BATCH 27/71 | LOSS: 0.00010643953542707354\n",
      "TRAIN: EPOCH 19/100 | BATCH 28/71 | LOSS: 0.00010671885232521414\n",
      "TRAIN: EPOCH 19/100 | BATCH 29/71 | LOSS: 0.00010692031792132184\n",
      "TRAIN: EPOCH 19/100 | BATCH 30/71 | LOSS: 0.00010770470121755235\n",
      "TRAIN: EPOCH 19/100 | BATCH 31/71 | LOSS: 0.00010788836425490445\n",
      "TRAIN: EPOCH 19/100 | BATCH 32/71 | LOSS: 0.00010799266293002857\n",
      "TRAIN: EPOCH 19/100 | BATCH 33/71 | LOSS: 0.00010721786234438802\n",
      "TRAIN: EPOCH 19/100 | BATCH 34/71 | LOSS: 0.00010748432520943294\n",
      "TRAIN: EPOCH 19/100 | BATCH 35/71 | LOSS: 0.00010817025455859645\n",
      "TRAIN: EPOCH 19/100 | BATCH 36/71 | LOSS: 0.00010781482076807249\n",
      "TRAIN: EPOCH 19/100 | BATCH 37/71 | LOSS: 0.00010773514326214546\n",
      "TRAIN: EPOCH 19/100 | BATCH 38/71 | LOSS: 0.00010791157714825553\n",
      "TRAIN: EPOCH 19/100 | BATCH 39/71 | LOSS: 0.00010799719584611011\n",
      "TRAIN: EPOCH 19/100 | BATCH 40/71 | LOSS: 0.00010940864900121374\n",
      "TRAIN: EPOCH 19/100 | BATCH 41/71 | LOSS: 0.00010924122779258705\n",
      "TRAIN: EPOCH 19/100 | BATCH 42/71 | LOSS: 0.00010919999307546722\n",
      "TRAIN: EPOCH 19/100 | BATCH 43/71 | LOSS: 0.00010886918872446668\n",
      "TRAIN: EPOCH 19/100 | BATCH 44/71 | LOSS: 0.00010924629944687088\n",
      "TRAIN: EPOCH 19/100 | BATCH 45/71 | LOSS: 0.00010909796005960189\n",
      "TRAIN: EPOCH 19/100 | BATCH 46/71 | LOSS: 0.00010936254120872733\n",
      "TRAIN: EPOCH 19/100 | BATCH 47/71 | LOSS: 0.00010947810596917407\n",
      "TRAIN: EPOCH 19/100 | BATCH 48/71 | LOSS: 0.00010980999917480904\n",
      "TRAIN: EPOCH 19/100 | BATCH 49/71 | LOSS: 0.0001097241295792628\n",
      "TRAIN: EPOCH 19/100 | BATCH 50/71 | LOSS: 0.00010923626527359124\n",
      "TRAIN: EPOCH 19/100 | BATCH 51/71 | LOSS: 0.00010931846456794749\n",
      "TRAIN: EPOCH 19/100 | BATCH 52/71 | LOSS: 0.00010964228512987249\n",
      "TRAIN: EPOCH 19/100 | BATCH 53/71 | LOSS: 0.00010936577422803061\n",
      "TRAIN: EPOCH 19/100 | BATCH 54/71 | LOSS: 0.00010930759440684182\n",
      "TRAIN: EPOCH 19/100 | BATCH 55/71 | LOSS: 0.00010867146271006536\n",
      "TRAIN: EPOCH 19/100 | BATCH 56/71 | LOSS: 0.00010880313212717802\n",
      "TRAIN: EPOCH 19/100 | BATCH 57/71 | LOSS: 0.0001086376600894788\n",
      "TRAIN: EPOCH 19/100 | BATCH 58/71 | LOSS: 0.00010876806136674515\n",
      "TRAIN: EPOCH 19/100 | BATCH 59/71 | LOSS: 0.00010863640551785162\n",
      "TRAIN: EPOCH 19/100 | BATCH 60/71 | LOSS: 0.00010850682335077464\n",
      "TRAIN: EPOCH 19/100 | BATCH 61/71 | LOSS: 0.00010839532643008317\n",
      "TRAIN: EPOCH 19/100 | BATCH 62/71 | LOSS: 0.0001079065787341697\n",
      "TRAIN: EPOCH 19/100 | BATCH 63/71 | LOSS: 0.00010774688200854143\n",
      "TRAIN: EPOCH 19/100 | BATCH 64/71 | LOSS: 0.00010764251671319541\n",
      "TRAIN: EPOCH 19/100 | BATCH 65/71 | LOSS: 0.00010792979501826322\n",
      "TRAIN: EPOCH 19/100 | BATCH 66/71 | LOSS: 0.00010776873441228865\n",
      "TRAIN: EPOCH 19/100 | BATCH 67/71 | LOSS: 0.0001080720669372514\n",
      "TRAIN: EPOCH 19/100 | BATCH 68/71 | LOSS: 0.00010884271572306888\n",
      "TRAIN: EPOCH 19/100 | BATCH 69/71 | LOSS: 0.00010868982405684489\n",
      "TRAIN: EPOCH 19/100 | BATCH 70/71 | LOSS: 0.00010813773508449281\n",
      "VAL: EPOCH 19/100 | BATCH 0/8 | LOSS: 0.00014156801626086235\n",
      "VAL: EPOCH 19/100 | BATCH 1/8 | LOSS: 0.00010822084732353687\n",
      "VAL: EPOCH 19/100 | BATCH 2/8 | LOSS: 0.00011014264115753274\n",
      "VAL: EPOCH 19/100 | BATCH 3/8 | LOSS: 0.00010503167140996084\n",
      "VAL: EPOCH 19/100 | BATCH 4/8 | LOSS: 0.00010665566369425506\n",
      "VAL: EPOCH 19/100 | BATCH 5/8 | LOSS: 0.00010349440951055537\n",
      "VAL: EPOCH 19/100 | BATCH 6/8 | LOSS: 0.00010239550361542829\n",
      "VAL: EPOCH 19/100 | BATCH 7/8 | LOSS: 0.00010165647108806297\n",
      "TRAIN: EPOCH 20/100 | BATCH 0/71 | LOSS: 0.00010358364670537412\n",
      "TRAIN: EPOCH 20/100 | BATCH 1/71 | LOSS: 0.00010139775258721784\n",
      "TRAIN: EPOCH 20/100 | BATCH 2/71 | LOSS: 0.00010372816177550703\n",
      "TRAIN: EPOCH 20/100 | BATCH 3/71 | LOSS: 0.00010295015454175882\n",
      "TRAIN: EPOCH 20/100 | BATCH 4/71 | LOSS: 0.00010342583409510553\n",
      "TRAIN: EPOCH 20/100 | BATCH 5/71 | LOSS: 0.00010139642351229365\n",
      "TRAIN: EPOCH 20/100 | BATCH 6/71 | LOSS: 0.00010124019796161779\n",
      "TRAIN: EPOCH 20/100 | BATCH 7/71 | LOSS: 9.791624688659795e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 8/71 | LOSS: 9.732992152243646e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 9/71 | LOSS: 9.722054746816867e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 10/71 | LOSS: 9.728186705615371e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 11/71 | LOSS: 9.721668235821805e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 12/71 | LOSS: 9.650969933807993e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 13/71 | LOSS: 9.49713950311499e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 14/71 | LOSS: 9.540737955830992e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 15/71 | LOSS: 9.54334036578075e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 16/71 | LOSS: 9.464258704717984e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 17/71 | LOSS: 9.46990375167742e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 18/71 | LOSS: 9.669757510939809e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 19/71 | LOSS: 9.724567426019348e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 20/71 | LOSS: 9.800949157492834e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 21/71 | LOSS: 9.747785522697747e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 22/71 | LOSS: 0.00010000799659831935\n",
      "TRAIN: EPOCH 20/100 | BATCH 23/71 | LOSS: 9.958085229300195e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 24/71 | LOSS: 0.00010009980906033888\n",
      "TRAIN: EPOCH 20/100 | BATCH 25/71 | LOSS: 9.966835988542208e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 26/71 | LOSS: 9.882239801636724e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 27/71 | LOSS: 9.84862724310785e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 28/71 | LOSS: 9.796526413491189e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 29/71 | LOSS: 9.805056736998571e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 30/71 | LOSS: 9.785139123015406e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 31/71 | LOSS: 9.83741840627772e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 32/71 | LOSS: 9.807274137997548e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 33/71 | LOSS: 9.768055550760918e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 34/71 | LOSS: 9.749732998898253e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 35/71 | LOSS: 9.749585883077493e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 36/71 | LOSS: 9.767206270102965e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 37/71 | LOSS: 9.720000047022232e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 38/71 | LOSS: 9.677002638524685e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 39/71 | LOSS: 9.679726445028791e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 40/71 | LOSS: 9.66257707105109e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 41/71 | LOSS: 9.621557754664016e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 42/71 | LOSS: 9.586169387127251e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 43/71 | LOSS: 9.645370350468015e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 44/71 | LOSS: 9.746239116389511e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 45/71 | LOSS: 9.749144769359745e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 46/71 | LOSS: 9.71855205283104e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 47/71 | LOSS: 9.716905894189647e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 48/71 | LOSS: 9.76526743568461e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 49/71 | LOSS: 9.702272072900086e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 50/71 | LOSS: 9.776664042945805e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 51/71 | LOSS: 9.797104324276846e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 52/71 | LOSS: 9.772736515480815e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 53/71 | LOSS: 9.713335128725264e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 54/71 | LOSS: 9.744436472167515e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 55/71 | LOSS: 9.752800373722234e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 56/71 | LOSS: 9.732860627608668e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 57/71 | LOSS: 9.744158486337883e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 58/71 | LOSS: 9.715864365778359e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 59/71 | LOSS: 9.703800630328866e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 60/71 | LOSS: 9.711530559589384e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 61/71 | LOSS: 9.721815654155498e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 62/71 | LOSS: 9.698587948352926e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 63/71 | LOSS: 9.688686873232655e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 64/71 | LOSS: 9.70762048382312e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 65/71 | LOSS: 9.710185512438218e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 66/71 | LOSS: 9.71533984046624e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 67/71 | LOSS: 9.787573200061589e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 68/71 | LOSS: 9.800504873661946e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 69/71 | LOSS: 9.765776828446958e-05\n",
      "TRAIN: EPOCH 20/100 | BATCH 70/71 | LOSS: 9.758977338747108e-05\n",
      "VAL: EPOCH 20/100 | BATCH 0/8 | LOSS: 0.00013704424782190472\n",
      "VAL: EPOCH 20/100 | BATCH 1/8 | LOSS: 0.00010484169615665451\n",
      "VAL: EPOCH 20/100 | BATCH 2/8 | LOSS: 0.00010392354064000149\n",
      "VAL: EPOCH 20/100 | BATCH 3/8 | LOSS: 0.00010010939513449557\n",
      "VAL: EPOCH 20/100 | BATCH 4/8 | LOSS: 9.983155323425307e-05\n",
      "VAL: EPOCH 20/100 | BATCH 5/8 | LOSS: 9.704905460239388e-05\n",
      "VAL: EPOCH 20/100 | BATCH 6/8 | LOSS: 9.679291127083291e-05\n",
      "VAL: EPOCH 20/100 | BATCH 7/8 | LOSS: 9.544880867906613e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 0/71 | LOSS: 8.970993803814054e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 1/71 | LOSS: 8.078197424765676e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 2/71 | LOSS: 8.458082932823648e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 3/71 | LOSS: 8.1241327279713e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 4/71 | LOSS: 8.049695898080245e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 5/71 | LOSS: 8.097370907004613e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 6/71 | LOSS: 7.942715663895277e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 7/71 | LOSS: 8.227992930187611e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 8/71 | LOSS: 8.794265886535868e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 9/71 | LOSS: 8.838250723783859e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 10/71 | LOSS: 8.803974329070611e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 11/71 | LOSS: 8.680320570420008e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 12/71 | LOSS: 8.686259780705978e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 13/71 | LOSS: 8.545963698582324e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 14/71 | LOSS: 8.695422438904643e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 15/71 | LOSS: 8.726297983230324e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 16/71 | LOSS: 8.712789113975733e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 17/71 | LOSS: 8.957381578511558e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 18/71 | LOSS: 8.861659750293352e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 19/71 | LOSS: 8.875497514964082e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 20/71 | LOSS: 8.85582433381517e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 21/71 | LOSS: 8.942299642843533e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 22/71 | LOSS: 8.938698896520731e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 23/71 | LOSS: 9.064409186976263e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 24/71 | LOSS: 8.983738080132752e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 25/71 | LOSS: 9.106355719268322e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 26/71 | LOSS: 9.107171111584951e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 27/71 | LOSS: 9.068524663494568e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 28/71 | LOSS: 9.096172866050219e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 29/71 | LOSS: 9.020412714259389e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 30/71 | LOSS: 9.063796366512355e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 31/71 | LOSS: 9.07502385416592e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 32/71 | LOSS: 9.170743843159553e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 33/71 | LOSS: 9.116568531573969e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 34/71 | LOSS: 9.032863287887137e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 35/71 | LOSS: 9.069936620815295e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 36/71 | LOSS: 9.073033465569638e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 37/71 | LOSS: 9.074500102494647e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 38/71 | LOSS: 9.043264118447088e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 39/71 | LOSS: 9.053319790837122e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 40/71 | LOSS: 9.048416931608056e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 41/71 | LOSS: 9.076931313583849e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 42/71 | LOSS: 9.059789793587529e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 43/71 | LOSS: 9.067025877646466e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 44/71 | LOSS: 9.071806157913266e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 45/71 | LOSS: 9.076057745994109e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 46/71 | LOSS: 9.179387942570796e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 47/71 | LOSS: 9.170131306746043e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 48/71 | LOSS: 9.209533282603156e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 49/71 | LOSS: 9.148115990683437e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 50/71 | LOSS: 9.131586457620941e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 51/71 | LOSS: 9.146105087263725e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 52/71 | LOSS: 9.084305476687216e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 53/71 | LOSS: 9.050427871881295e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 54/71 | LOSS: 9.064137372082438e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 55/71 | LOSS: 9.024356014768793e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 56/71 | LOSS: 9.06202233939742e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 57/71 | LOSS: 9.015182237409794e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 58/71 | LOSS: 9.019674239502588e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 59/71 | LOSS: 9.02153596446927e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 60/71 | LOSS: 9.006659152486254e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 61/71 | LOSS: 8.975117370643983e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 62/71 | LOSS: 8.961827667903096e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 63/71 | LOSS: 8.961760318015877e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 64/71 | LOSS: 8.923662871194001e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 65/71 | LOSS: 8.889714257692862e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 66/71 | LOSS: 8.910879299023759e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 67/71 | LOSS: 8.933601801639751e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 68/71 | LOSS: 8.896934509655271e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 69/71 | LOSS: 8.88257786365492e-05\n",
      "TRAIN: EPOCH 21/100 | BATCH 70/71 | LOSS: 8.842031262170615e-05\n",
      "VAL: EPOCH 21/100 | BATCH 0/8 | LOSS: 0.00011944510333705693\n",
      "VAL: EPOCH 21/100 | BATCH 1/8 | LOSS: 9.695287008071318e-05\n",
      "VAL: EPOCH 21/100 | BATCH 2/8 | LOSS: 9.656983699339132e-05\n",
      "VAL: EPOCH 21/100 | BATCH 3/8 | LOSS: 9.157454769592732e-05\n",
      "VAL: EPOCH 21/100 | BATCH 4/8 | LOSS: 9.215991594828666e-05\n",
      "VAL: EPOCH 21/100 | BATCH 5/8 | LOSS: 8.984602876201582e-05\n",
      "VAL: EPOCH 21/100 | BATCH 6/8 | LOSS: 8.860690494267536e-05\n",
      "VAL: EPOCH 21/100 | BATCH 7/8 | LOSS: 8.936524773162091e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 0/71 | LOSS: 8.206117490772158e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 1/71 | LOSS: 8.120574784697965e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 2/71 | LOSS: 8.56447295518592e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 3/71 | LOSS: 8.198436626116745e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 4/71 | LOSS: 8.18240296212025e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 5/71 | LOSS: 8.2612749489878e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 6/71 | LOSS: 8.088962411940364e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 7/71 | LOSS: 8.488351977575803e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 8/71 | LOSS: 8.54087718633107e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 9/71 | LOSS: 8.680905593791977e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 10/71 | LOSS: 8.604695110327818e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 11/71 | LOSS: 8.597562070159863e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 12/71 | LOSS: 8.57334626534094e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 13/71 | LOSS: 8.501973674615979e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 14/71 | LOSS: 8.440283175635462e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 15/71 | LOSS: 8.424788893535151e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 16/71 | LOSS: 8.369469972368439e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 17/71 | LOSS: 8.265914099562603e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 18/71 | LOSS: 8.216624892635369e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 19/71 | LOSS: 8.120703969325405e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 20/71 | LOSS: 8.04314944876491e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 21/71 | LOSS: 8.09618885052094e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 22/71 | LOSS: 8.223226715294321e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 23/71 | LOSS: 8.148931222725271e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 24/71 | LOSS: 8.25904481462203e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 25/71 | LOSS: 8.23945063949885e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 26/71 | LOSS: 8.24347745812567e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 27/71 | LOSS: 8.183394181417367e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 28/71 | LOSS: 8.148638805543105e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 29/71 | LOSS: 8.11037056943557e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 30/71 | LOSS: 8.096334814552157e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 31/71 | LOSS: 8.050702967921097e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 32/71 | LOSS: 8.032635356137303e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 33/71 | LOSS: 8.007286348455476e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 34/71 | LOSS: 7.956890331115574e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 35/71 | LOSS: 8.010009474269787e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 36/71 | LOSS: 8.00690013912154e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 37/71 | LOSS: 8.001820874357548e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 38/71 | LOSS: 8.044763392088219e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 39/71 | LOSS: 8.034788952500093e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 40/71 | LOSS: 8.049560817265202e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 41/71 | LOSS: 8.043836940140907e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 42/71 | LOSS: 8.058839671849815e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 43/71 | LOSS: 8.032014226376883e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 44/71 | LOSS: 8.073394985533215e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 45/71 | LOSS: 8.083540555874251e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 46/71 | LOSS: 8.06533881064326e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 47/71 | LOSS: 8.053366173044196e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 48/71 | LOSS: 8.030679318649999e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 49/71 | LOSS: 8.014796228962951e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 50/71 | LOSS: 8.015474618421685e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 51/71 | LOSS: 7.973538042558805e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 52/71 | LOSS: 7.953930088540733e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 53/71 | LOSS: 7.934440596997135e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 54/71 | LOSS: 7.940153359827077e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 55/71 | LOSS: 7.906875518009266e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 56/71 | LOSS: 7.951419377748512e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 57/71 | LOSS: 7.926803827386124e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 58/71 | LOSS: 7.977131549446544e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 59/71 | LOSS: 7.984927457679683e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 60/71 | LOSS: 7.968213930191686e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 61/71 | LOSS: 7.976627614792256e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 62/71 | LOSS: 7.990657133401356e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 63/71 | LOSS: 8.025174327030982e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 64/71 | LOSS: 8.037271741509007e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 65/71 | LOSS: 8.017706535280344e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 66/71 | LOSS: 8.055498207130456e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 67/71 | LOSS: 8.051944025603862e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 68/71 | LOSS: 8.04997899399166e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 69/71 | LOSS: 8.053990708763844e-05\n",
      "TRAIN: EPOCH 22/100 | BATCH 70/71 | LOSS: 8.030468218838534e-05\n",
      "VAL: EPOCH 22/100 | BATCH 0/8 | LOSS: 0.00011292081035207957\n",
      "VAL: EPOCH 22/100 | BATCH 1/8 | LOSS: 8.740650082472712e-05\n",
      "VAL: EPOCH 22/100 | BATCH 2/8 | LOSS: 8.50277065183036e-05\n",
      "VAL: EPOCH 22/100 | BATCH 3/8 | LOSS: 8.230001367337536e-05\n",
      "VAL: EPOCH 22/100 | BATCH 4/8 | LOSS: 8.103851723717526e-05\n",
      "VAL: EPOCH 22/100 | BATCH 5/8 | LOSS: 7.834335337975062e-05\n",
      "VAL: EPOCH 22/100 | BATCH 6/8 | LOSS: 7.851620258796694e-05\n",
      "VAL: EPOCH 22/100 | BATCH 7/8 | LOSS: 7.851348709664308e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 0/71 | LOSS: 8.701771002961323e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 1/71 | LOSS: 8.286982847494073e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 2/71 | LOSS: 8.225684723583981e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 3/71 | LOSS: 8.280619658762589e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 4/71 | LOSS: 8.573673258069903e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 5/71 | LOSS: 8.275957710187261e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 6/71 | LOSS: 7.977877352719329e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 7/71 | LOSS: 8.084998989943415e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 8/71 | LOSS: 8.013987503040375e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 9/71 | LOSS: 7.999562585609965e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 10/71 | LOSS: 7.878211512103339e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 11/71 | LOSS: 7.924357244822507e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 12/71 | LOSS: 7.944193594784547e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 13/71 | LOSS: 8.044005153351463e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 14/71 | LOSS: 8.12713949320217e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 15/71 | LOSS: 8.0322253779741e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 16/71 | LOSS: 7.903712021823808e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 17/71 | LOSS: 7.860166988393757e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 18/71 | LOSS: 7.826125262286759e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 19/71 | LOSS: 7.817203822924057e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 20/71 | LOSS: 7.879138398233667e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 21/71 | LOSS: 8.017767702933105e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 22/71 | LOSS: 7.97805236209609e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 23/71 | LOSS: 7.953225773841648e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 24/71 | LOSS: 7.949114587972872e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 25/71 | LOSS: 7.996150214659373e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 26/71 | LOSS: 7.99493283449448e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 27/71 | LOSS: 7.985506519097336e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 28/71 | LOSS: 8.044298349353806e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 29/71 | LOSS: 8.03847153292736e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 30/71 | LOSS: 8.039349385428303e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 31/71 | LOSS: 8.048290226270183e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 32/71 | LOSS: 8.056579639687146e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 33/71 | LOSS: 8.018651511948145e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 34/71 | LOSS: 8.034386399750864e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 35/71 | LOSS: 8.08423759129558e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 36/71 | LOSS: 8.04368381166667e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 37/71 | LOSS: 7.991966164186841e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 38/71 | LOSS: 7.944605936361954e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 39/71 | LOSS: 7.954350858199177e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 40/71 | LOSS: 7.94415648603562e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 41/71 | LOSS: 7.89502219482702e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 42/71 | LOSS: 7.878578803366657e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 43/71 | LOSS: 7.832445904072797e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 44/71 | LOSS: 7.814605325822615e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 45/71 | LOSS: 7.811180124838796e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 46/71 | LOSS: 7.798223343846249e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 47/71 | LOSS: 7.818229611681697e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 48/71 | LOSS: 7.786568683662395e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 49/71 | LOSS: 7.741815657936968e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 50/71 | LOSS: 7.729953772379268e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 51/71 | LOSS: 7.677640419737705e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 52/71 | LOSS: 7.723986997899494e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 53/71 | LOSS: 7.69319055877902e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 54/71 | LOSS: 7.67748943127861e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 55/71 | LOSS: 7.6566351578679e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 56/71 | LOSS: 7.645452199524203e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 57/71 | LOSS: 7.63117968745064e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 58/71 | LOSS: 7.616446940011297e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 59/71 | LOSS: 7.609077583765611e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 60/71 | LOSS: 7.606576114308974e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 61/71 | LOSS: 7.604882780616472e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 62/71 | LOSS: 7.577687834522553e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 63/71 | LOSS: 7.556095829386322e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 64/71 | LOSS: 7.567728988271064e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 65/71 | LOSS: 7.60322863326027e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 66/71 | LOSS: 7.60757175157778e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 67/71 | LOSS: 7.601419522817356e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 68/71 | LOSS: 7.587264690959853e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 69/71 | LOSS: 7.565556755123128e-05\n",
      "TRAIN: EPOCH 23/100 | BATCH 70/71 | LOSS: 7.539777929401091e-05\n",
      "VAL: EPOCH 23/100 | BATCH 0/8 | LOSS: 0.00010160765668842942\n",
      "VAL: EPOCH 23/100 | BATCH 1/8 | LOSS: 8.05065719760023e-05\n",
      "VAL: EPOCH 23/100 | BATCH 2/8 | LOSS: 7.848172632899757e-05\n",
      "VAL: EPOCH 23/100 | BATCH 3/8 | LOSS: 7.572178765258286e-05\n",
      "VAL: EPOCH 23/100 | BATCH 4/8 | LOSS: 7.468128751497716e-05\n",
      "VAL: EPOCH 23/100 | BATCH 5/8 | LOSS: 7.207164768866885e-05\n",
      "VAL: EPOCH 23/100 | BATCH 6/8 | LOSS: 7.202399969433568e-05\n",
      "VAL: EPOCH 23/100 | BATCH 7/8 | LOSS: 7.221833311632508e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 0/71 | LOSS: 6.224031676538289e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 1/71 | LOSS: 6.616775135626085e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 2/71 | LOSS: 8.077579453432311e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 3/71 | LOSS: 7.51918905734783e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 4/71 | LOSS: 7.24195400835015e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 5/71 | LOSS: 7.085004714705671e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 6/71 | LOSS: 7.260360871441662e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 7/71 | LOSS: 7.488178198400419e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 8/71 | LOSS: 7.528147398261353e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 9/71 | LOSS: 7.52280859160237e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 10/71 | LOSS: 7.568101996598257e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 11/71 | LOSS: 7.777620097234224e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 12/71 | LOSS: 7.765125113879688e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 13/71 | LOSS: 7.619005191372707e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 14/71 | LOSS: 7.594846247229725e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 15/71 | LOSS: 7.5821662449016e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 16/71 | LOSS: 7.526504743830574e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 17/71 | LOSS: 7.4261921529089e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 18/71 | LOSS: 7.446591878665219e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 19/71 | LOSS: 7.382300718745682e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 20/71 | LOSS: 7.34418333325136e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 21/71 | LOSS: 7.291545857283795e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 22/71 | LOSS: 7.287574787725411e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 23/71 | LOSS: 7.238741151619858e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 24/71 | LOSS: 7.311319030122831e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 25/71 | LOSS: 7.346497435579434e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 26/71 | LOSS: 7.301955284438682e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 27/71 | LOSS: 7.265891508723143e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 28/71 | LOSS: 7.298320295000128e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 29/71 | LOSS: 7.246116268409727e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 30/71 | LOSS: 7.321998137124484e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 31/71 | LOSS: 7.271405047504231e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 32/71 | LOSS: 7.238906718240204e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 33/71 | LOSS: 7.175537126279189e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 34/71 | LOSS: 7.138956186411503e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 35/71 | LOSS: 7.16848338318717e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 36/71 | LOSS: 7.205957956250279e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 37/71 | LOSS: 7.157465513859949e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 38/71 | LOSS: 7.13714990781274e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 39/71 | LOSS: 7.09590021870099e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 40/71 | LOSS: 7.107336373670345e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 41/71 | LOSS: 7.067409032537224e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 42/71 | LOSS: 7.061141759338573e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 43/71 | LOSS: 7.014108864082532e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 44/71 | LOSS: 6.99145621587781e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 45/71 | LOSS: 6.97463967061713e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 46/71 | LOSS: 6.962572715178311e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 47/71 | LOSS: 6.988316188956863e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 48/71 | LOSS: 7.00562269805826e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 49/71 | LOSS: 6.988520988670643e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 50/71 | LOSS: 6.967683343556892e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 51/71 | LOSS: 6.937719591895494e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 52/71 | LOSS: 6.940187113458414e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 53/71 | LOSS: 6.938151913076311e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 54/71 | LOSS: 6.965106738128022e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 55/71 | LOSS: 6.955503334081316e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 56/71 | LOSS: 6.925748618270613e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 57/71 | LOSS: 6.930764106198631e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 58/71 | LOSS: 6.909521273929247e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 59/71 | LOSS: 6.870875962097975e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 60/71 | LOSS: 6.833747424114663e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 61/71 | LOSS: 6.837596230334677e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 62/71 | LOSS: 6.828549284985038e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 63/71 | LOSS: 6.807493008409438e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 64/71 | LOSS: 6.800940850203356e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 65/71 | LOSS: 6.802770026710773e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 66/71 | LOSS: 6.793316755810433e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 67/71 | LOSS: 6.775710372654859e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 68/71 | LOSS: 6.76735637724683e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 69/71 | LOSS: 6.79136675898917e-05\n",
      "TRAIN: EPOCH 24/100 | BATCH 70/71 | LOSS: 6.786728178193583e-05\n",
      "VAL: EPOCH 24/100 | BATCH 0/8 | LOSS: 9.211139695253223e-05\n",
      "VAL: EPOCH 24/100 | BATCH 1/8 | LOSS: 7.269940033438616e-05\n",
      "VAL: EPOCH 24/100 | BATCH 2/8 | LOSS: 7.084775279508904e-05\n",
      "VAL: EPOCH 24/100 | BATCH 3/8 | LOSS: 6.753359957656357e-05\n",
      "VAL: EPOCH 24/100 | BATCH 4/8 | LOSS: 6.621053762501106e-05\n",
      "VAL: EPOCH 24/100 | BATCH 5/8 | LOSS: 6.376130113494582e-05\n",
      "VAL: EPOCH 24/100 | BATCH 6/8 | LOSS: 6.380164164251514e-05\n",
      "VAL: EPOCH 24/100 | BATCH 7/8 | LOSS: 6.37319944871706e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 0/71 | LOSS: 5.675961438100785e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 1/71 | LOSS: 5.945708107901737e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 2/71 | LOSS: 6.036804794954757e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 3/71 | LOSS: 6.342278902593534e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 4/71 | LOSS: 6.38617595541291e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 5/71 | LOSS: 6.399876050030191e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 6/71 | LOSS: 6.703238302309598e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 7/71 | LOSS: 6.697873504890595e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 8/71 | LOSS: 6.602302083693858e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 9/71 | LOSS: 6.481133168563246e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 10/71 | LOSS: 6.503403446086767e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 11/71 | LOSS: 6.51365662633907e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 12/71 | LOSS: 6.540902596987927e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 13/71 | LOSS: 6.531204053317197e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 14/71 | LOSS: 6.53960058116354e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 15/71 | LOSS: 6.495645607174083e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 16/71 | LOSS: 6.41366234905196e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 17/71 | LOSS: 6.419636621204618e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 18/71 | LOSS: 6.356102714658176e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 19/71 | LOSS: 6.386183231370524e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 20/71 | LOSS: 6.361283591277676e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 21/71 | LOSS: 6.47436663712142e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 22/71 | LOSS: 6.402922973274659e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 23/71 | LOSS: 6.435996616952859e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 24/71 | LOSS: 6.396461903932505e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 25/71 | LOSS: 6.468975208498215e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 26/71 | LOSS: 6.42742898317347e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 27/71 | LOSS: 6.438777005054622e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 28/71 | LOSS: 6.410995072279736e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 29/71 | LOSS: 6.367362960493969e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 30/71 | LOSS: 6.436385275822343e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 31/71 | LOSS: 6.477235581314744e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 32/71 | LOSS: 6.485947244183274e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 33/71 | LOSS: 6.463420921885271e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 34/71 | LOSS: 6.500483796116896e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 35/71 | LOSS: 6.48674626467659e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 36/71 | LOSS: 6.48899437471079e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 37/71 | LOSS: 6.468792028164516e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 38/71 | LOSS: 6.459576634836431e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 39/71 | LOSS: 6.444512109737843e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 40/71 | LOSS: 6.42145790085878e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 41/71 | LOSS: 6.422650802137686e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 42/71 | LOSS: 6.447174027291408e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 43/71 | LOSS: 6.453008162762589e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 44/71 | LOSS: 6.441495174335109e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 45/71 | LOSS: 6.440177790152475e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 46/71 | LOSS: 6.4078904967012e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 47/71 | LOSS: 6.381389812304405e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 48/71 | LOSS: 6.402656703246567e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 49/71 | LOSS: 6.406268919818104e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 50/71 | LOSS: 6.37210310635376e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 51/71 | LOSS: 6.334624784078467e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 52/71 | LOSS: 6.31561225366928e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 53/71 | LOSS: 6.284278660368054e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 54/71 | LOSS: 6.26797060910824e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 55/71 | LOSS: 6.284910555872816e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 56/71 | LOSS: 6.283280852489712e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 57/71 | LOSS: 6.27126026866508e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 58/71 | LOSS: 6.28063968733995e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 59/71 | LOSS: 6.263296333296846e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 60/71 | LOSS: 6.255527996919957e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 61/71 | LOSS: 6.264668592203018e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 62/71 | LOSS: 6.249378185159349e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 63/71 | LOSS: 6.251941812251971e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 64/71 | LOSS: 6.243235603995765e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 65/71 | LOSS: 6.2261966322143e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 66/71 | LOSS: 6.21442192837026e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 67/71 | LOSS: 6.20412219552022e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 68/71 | LOSS: 6.170877547803052e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 69/71 | LOSS: 6.216274258414549e-05\n",
      "TRAIN: EPOCH 25/100 | BATCH 70/71 | LOSS: 6.213589217046499e-05\n",
      "VAL: EPOCH 25/100 | BATCH 0/8 | LOSS: 8.735476149013266e-05\n",
      "VAL: EPOCH 25/100 | BATCH 1/8 | LOSS: 6.7252598455525e-05\n",
      "VAL: EPOCH 25/100 | BATCH 2/8 | LOSS: 6.47911147098057e-05\n",
      "VAL: EPOCH 25/100 | BATCH 3/8 | LOSS: 6.211151685420191e-05\n",
      "VAL: EPOCH 25/100 | BATCH 4/8 | LOSS: 6.030851291143336e-05\n",
      "VAL: EPOCH 25/100 | BATCH 5/8 | LOSS: 5.7841149706897944e-05\n",
      "VAL: EPOCH 25/100 | BATCH 6/8 | LOSS: 5.8290312673696985e-05\n",
      "VAL: EPOCH 25/100 | BATCH 7/8 | LOSS: 5.8141171848546946e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 0/71 | LOSS: 6.121827755123377e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 1/71 | LOSS: 6.156501331133768e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 2/71 | LOSS: 6.217038268611456e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 3/71 | LOSS: 6.0867461797897704e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 4/71 | LOSS: 5.889051244594157e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 5/71 | LOSS: 5.785213033959735e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 6/71 | LOSS: 5.800100448790805e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 7/71 | LOSS: 5.773371003670036e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 8/71 | LOSS: 5.700380279449746e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 9/71 | LOSS: 5.7678131270222366e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 10/71 | LOSS: 5.790946571241048e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 11/71 | LOSS: 5.700381128311468e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 12/71 | LOSS: 5.7639145569947476e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 13/71 | LOSS: 5.7011315740445366e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 14/71 | LOSS: 5.727511209746202e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 15/71 | LOSS: 5.7826974625641014e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 16/71 | LOSS: 5.75977036907502e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 17/71 | LOSS: 5.7259755420899535e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 18/71 | LOSS: 5.8294501573790945e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 19/71 | LOSS: 5.850312609254615e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 20/71 | LOSS: 5.9212865813779424e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 21/71 | LOSS: 5.916368089939087e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 22/71 | LOSS: 5.955618041258751e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 23/71 | LOSS: 5.899622662279095e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 24/71 | LOSS: 5.857913798536174e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 25/71 | LOSS: 5.813073599151371e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 26/71 | LOSS: 5.847645782826779e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 27/71 | LOSS: 5.798449040282451e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 28/71 | LOSS: 5.7512906190321045e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 29/71 | LOSS: 5.7245065181632524e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 30/71 | LOSS: 5.725973871097179e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 31/71 | LOSS: 5.72197914152639e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 32/71 | LOSS: 5.721833671982908e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 33/71 | LOSS: 5.719043249829316e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 34/71 | LOSS: 5.67665343364102e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 35/71 | LOSS: 5.685213717596424e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 36/71 | LOSS: 5.6549119649020757e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 37/71 | LOSS: 5.672136572492922e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 38/71 | LOSS: 5.67171782910382e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 39/71 | LOSS: 5.685178666681168e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 40/71 | LOSS: 5.687469420968782e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 41/71 | LOSS: 5.694894320012758e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 42/71 | LOSS: 5.7300688853882644e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 43/71 | LOSS: 5.7767236698551145e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 44/71 | LOSS: 5.7472438735809796e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 45/71 | LOSS: 5.764527423193415e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 46/71 | LOSS: 5.7971027498306866e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 47/71 | LOSS: 5.7754405361265526e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 48/71 | LOSS: 5.7776858367866894e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 49/71 | LOSS: 5.7695825307746415e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 50/71 | LOSS: 5.758730385066741e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 51/71 | LOSS: 5.755903272914181e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 52/71 | LOSS: 5.723547058911414e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 53/71 | LOSS: 5.760897686370838e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 54/71 | LOSS: 5.752294494992715e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 55/71 | LOSS: 5.7873454547266844e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 56/71 | LOSS: 5.768955599163299e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 57/71 | LOSS: 5.774163614189782e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 58/71 | LOSS: 5.755799947557103e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 59/71 | LOSS: 5.7432475963044756e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 60/71 | LOSS: 5.730937136533731e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 61/71 | LOSS: 5.750652223556948e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 62/71 | LOSS: 5.753503039216859e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 63/71 | LOSS: 5.729639275386944e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 64/71 | LOSS: 5.7304612206420506e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 65/71 | LOSS: 5.712017065723723e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 66/71 | LOSS: 5.7203368031430236e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 67/71 | LOSS: 5.718433224982993e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 68/71 | LOSS: 5.723144736755099e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 69/71 | LOSS: 5.705423463950865e-05\n",
      "TRAIN: EPOCH 26/100 | BATCH 70/71 | LOSS: 5.7034808775478924e-05\n",
      "VAL: EPOCH 26/100 | BATCH 0/8 | LOSS: 8.06633906904608e-05\n",
      "VAL: EPOCH 26/100 | BATCH 1/8 | LOSS: 6.432519148802385e-05\n",
      "VAL: EPOCH 26/100 | BATCH 2/8 | LOSS: 6.227490181724231e-05\n",
      "VAL: EPOCH 26/100 | BATCH 3/8 | LOSS: 5.907873583055334e-05\n",
      "VAL: EPOCH 26/100 | BATCH 4/8 | LOSS: 5.741905042668805e-05\n",
      "VAL: EPOCH 26/100 | BATCH 5/8 | LOSS: 5.5247726171122245e-05\n",
      "VAL: EPOCH 26/100 | BATCH 6/8 | LOSS: 5.519069080557009e-05\n",
      "VAL: EPOCH 26/100 | BATCH 7/8 | LOSS: 5.5410124332411215e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 0/71 | LOSS: 5.312472421792336e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 1/71 | LOSS: 5.0646938689169474e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 2/71 | LOSS: 5.0801622516398005e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 3/71 | LOSS: 5.001993940823013e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 4/71 | LOSS: 5.1905612781411034e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 5/71 | LOSS: 4.97599927863727e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 6/71 | LOSS: 4.8410316334671476e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 7/71 | LOSS: 4.899586747342255e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 8/71 | LOSS: 5.115872772876173e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 9/71 | LOSS: 4.9790563934948295e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 10/71 | LOSS: 5.097750346811319e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 11/71 | LOSS: 5.0975272946137316e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 12/71 | LOSS: 5.178215752284114e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 13/71 | LOSS: 5.201487484945184e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 14/71 | LOSS: 5.130323479534127e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 15/71 | LOSS: 5.1078182195851696e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 16/71 | LOSS: 5.1008562235545145e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 17/71 | LOSS: 5.084229714056063e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 18/71 | LOSS: 5.12088452906985e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 19/71 | LOSS: 5.093864310765639e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 20/71 | LOSS: 5.143305107984426e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 21/71 | LOSS: 5.132393363799731e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 22/71 | LOSS: 5.131024543357932e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 23/71 | LOSS: 5.107766113117881e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 24/71 | LOSS: 5.274940369417891e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 25/71 | LOSS: 5.2731982577824965e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 26/71 | LOSS: 5.321403942717653e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 27/71 | LOSS: 5.304395004454169e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 28/71 | LOSS: 5.329064640983265e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 29/71 | LOSS: 5.309548420579328e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 30/71 | LOSS: 5.285744285379957e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 31/71 | LOSS: 5.27027008274672e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 32/71 | LOSS: 5.312595958733282e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 33/71 | LOSS: 5.296032684461405e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 34/71 | LOSS: 5.308086266658003e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 35/71 | LOSS: 5.336494773978807e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 36/71 | LOSS: 5.3624492244813846e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 37/71 | LOSS: 5.347550244263594e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 38/71 | LOSS: 5.375660643599426e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 39/71 | LOSS: 5.3721555923402774e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 40/71 | LOSS: 5.3609047005543624e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 41/71 | LOSS: 5.338228506194095e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 42/71 | LOSS: 5.325370376488345e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 43/71 | LOSS: 5.334711063782875e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 44/71 | LOSS: 5.338902402501036e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 45/71 | LOSS: 5.344101634324508e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 46/71 | LOSS: 5.319852193388989e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 47/71 | LOSS: 5.309296201024457e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 48/71 | LOSS: 5.289143808447395e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 49/71 | LOSS: 5.293044770951383e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 50/71 | LOSS: 5.2881962748121145e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 51/71 | LOSS: 5.288004432748914e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 52/71 | LOSS: 5.294969817458757e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 53/71 | LOSS: 5.3187419729915244e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 54/71 | LOSS: 5.315025383341973e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 55/71 | LOSS: 5.28994924025028e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 56/71 | LOSS: 5.281518985468306e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 57/71 | LOSS: 5.2857934672923374e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 58/71 | LOSS: 5.28122200502351e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 59/71 | LOSS: 5.2785398535585654e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 60/71 | LOSS: 5.28330809950111e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 61/71 | LOSS: 5.278913170425973e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 62/71 | LOSS: 5.304698759804494e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 63/71 | LOSS: 5.308821721428103e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 64/71 | LOSS: 5.302816201807358e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 65/71 | LOSS: 5.289115649771482e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 66/71 | LOSS: 5.291635290034977e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 67/71 | LOSS: 5.302241189543993e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 68/71 | LOSS: 5.3222082752272136e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 69/71 | LOSS: 5.325071007454036e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 70/71 | LOSS: 5.307305528590469e-05\n",
      "VAL: EPOCH 27/100 | BATCH 0/8 | LOSS: 8.437746873823926e-05\n",
      "VAL: EPOCH 27/100 | BATCH 1/8 | LOSS: 6.425941319321282e-05\n",
      "VAL: EPOCH 27/100 | BATCH 2/8 | LOSS: 6.137140007922426e-05\n",
      "VAL: EPOCH 27/100 | BATCH 3/8 | LOSS: 5.918088754697237e-05\n",
      "VAL: EPOCH 27/100 | BATCH 4/8 | LOSS: 5.730579723604023e-05\n",
      "VAL: EPOCH 27/100 | BATCH 5/8 | LOSS: 5.5320301423004516e-05\n",
      "VAL: EPOCH 27/100 | BATCH 6/8 | LOSS: 5.552710146210822e-05\n",
      "VAL: EPOCH 27/100 | BATCH 7/8 | LOSS: 5.5304073612205684e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 0/71 | LOSS: 4.560566594591364e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 1/71 | LOSS: 5.017195326217916e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 2/71 | LOSS: 5.851462022595418e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 3/71 | LOSS: 6.012584071868332e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 4/71 | LOSS: 6.041531232767738e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 5/71 | LOSS: 6.074696587650882e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 6/71 | LOSS: 5.830664511969579e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 7/71 | LOSS: 5.736514685850125e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 8/71 | LOSS: 5.74894867087197e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 9/71 | LOSS: 5.595587790594436e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 10/71 | LOSS: 5.496542540971529e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 11/71 | LOSS: 5.430465201546516e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 12/71 | LOSS: 5.402369508654094e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 13/71 | LOSS: 5.4110407290863805e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 14/71 | LOSS: 5.416390461808381e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 15/71 | LOSS: 5.4379838047680096e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 16/71 | LOSS: 5.3310804265921056e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 17/71 | LOSS: 5.2944066030451926e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 18/71 | LOSS: 5.284819513020155e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 19/71 | LOSS: 5.19062630701228e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 20/71 | LOSS: 5.182067061901935e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 21/71 | LOSS: 5.147808984805703e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 22/71 | LOSS: 5.094794038956499e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 23/71 | LOSS: 5.077278319731704e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 24/71 | LOSS: 5.051521889981814e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 25/71 | LOSS: 5.0359308331210814e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 26/71 | LOSS: 5.0306969502276775e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 27/71 | LOSS: 4.984362619455039e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 28/71 | LOSS: 4.966694641866368e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 29/71 | LOSS: 4.968312180911501e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 30/71 | LOSS: 4.991505592178944e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 31/71 | LOSS: 5.045232808242872e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 32/71 | LOSS: 5.025195467610364e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 33/71 | LOSS: 4.99638875172583e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 34/71 | LOSS: 5.013102615651275e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 35/71 | LOSS: 5.021304130221122e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 36/71 | LOSS: 5.011953426465216e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 37/71 | LOSS: 5.036054512617595e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 38/71 | LOSS: 5.0078974219594296e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 39/71 | LOSS: 4.9746482454793294e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 40/71 | LOSS: 4.9622780529827607e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 41/71 | LOSS: 4.962689646552982e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 42/71 | LOSS: 4.953306627210749e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 43/71 | LOSS: 4.954543874191586e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 44/71 | LOSS: 4.978725070638272e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 45/71 | LOSS: 4.9768008149998344e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 46/71 | LOSS: 4.966024069754565e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 47/71 | LOSS: 4.966302503817133e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 48/71 | LOSS: 5.0158013603935134e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 49/71 | LOSS: 5.0059125933330505e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 50/71 | LOSS: 5.016942903988392e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 51/71 | LOSS: 5.008731036818622e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 52/71 | LOSS: 4.96992470975526e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 53/71 | LOSS: 4.9695043142289957e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 54/71 | LOSS: 4.9670109133744106e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 55/71 | LOSS: 4.949503675431645e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 56/71 | LOSS: 5.0069653330035835e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 57/71 | LOSS: 4.993633410776965e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 58/71 | LOSS: 5.006212900221048e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 59/71 | LOSS: 4.997840351279592e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 60/71 | LOSS: 5.0032900319030115e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 61/71 | LOSS: 5.000220018348867e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 62/71 | LOSS: 4.996121574621943e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 63/71 | LOSS: 4.9848351920900313e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 64/71 | LOSS: 4.9801871444707594e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 65/71 | LOSS: 4.969129087950216e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 66/71 | LOSS: 4.982245342550563e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 67/71 | LOSS: 4.984155614639509e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 68/71 | LOSS: 4.9837852334283106e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 69/71 | LOSS: 4.971607865757376e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 70/71 | LOSS: 4.9678758444661324e-05\n",
      "VAL: EPOCH 28/100 | BATCH 0/8 | LOSS: 7.674866355955601e-05\n",
      "VAL: EPOCH 28/100 | BATCH 1/8 | LOSS: 6.03877615503734e-05\n",
      "VAL: EPOCH 28/100 | BATCH 2/8 | LOSS: 5.7313912596631177e-05\n",
      "VAL: EPOCH 28/100 | BATCH 3/8 | LOSS: 5.595902166533051e-05\n",
      "VAL: EPOCH 28/100 | BATCH 4/8 | LOSS: 5.391132435761392e-05\n",
      "VAL: EPOCH 28/100 | BATCH 5/8 | LOSS: 5.197118116484489e-05\n",
      "VAL: EPOCH 28/100 | BATCH 6/8 | LOSS: 5.2741851894617344e-05\n",
      "VAL: EPOCH 28/100 | BATCH 7/8 | LOSS: 5.295587970977067e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 0/71 | LOSS: 6.055885023670271e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 1/71 | LOSS: 4.5981334551470354e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 2/71 | LOSS: 4.687810845401449e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 3/71 | LOSS: 4.908544451609487e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 4/71 | LOSS: 4.7613802598789336e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 5/71 | LOSS: 5.110049702731582e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 6/71 | LOSS: 4.917297649496634e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 7/71 | LOSS: 4.8742956550995586e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 8/71 | LOSS: 4.8152143184173234e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 9/71 | LOSS: 4.794130545633379e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 10/71 | LOSS: 4.754660238342529e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 11/71 | LOSS: 4.7370234824484214e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 12/71 | LOSS: 4.6586908306030986e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 13/71 | LOSS: 4.64082882639819e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 14/71 | LOSS: 4.764396338335549e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 15/71 | LOSS: 4.698508564615622e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 16/71 | LOSS: 4.651011969305246e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 17/71 | LOSS: 4.6895829453003695e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 18/71 | LOSS: 4.698172625993673e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 19/71 | LOSS: 4.691817375714891e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 20/71 | LOSS: 4.680329445926916e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 21/71 | LOSS: 4.683320416502697e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 22/71 | LOSS: 4.6886508916150376e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 23/71 | LOSS: 4.6976927175516416e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 24/71 | LOSS: 4.659076919779181e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 25/71 | LOSS: 4.64686952629843e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 26/71 | LOSS: 4.672279330272296e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 27/71 | LOSS: 4.64213499071775e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 28/71 | LOSS: 4.6619493489209886e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 29/71 | LOSS: 4.6733126388668704e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 30/71 | LOSS: 4.672447257983168e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 31/71 | LOSS: 4.65403766156669e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 32/71 | LOSS: 4.644448662557724e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 33/71 | LOSS: 4.6658221810289164e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 34/71 | LOSS: 4.623661453868928e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 35/71 | LOSS: 4.60432939790836e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 36/71 | LOSS: 4.610029468863472e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 37/71 | LOSS: 4.603106036053137e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 38/71 | LOSS: 4.646360545078866e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 39/71 | LOSS: 4.646500892704353e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 40/71 | LOSS: 4.6715150635530494e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 41/71 | LOSS: 4.6829239917791534e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 42/71 | LOSS: 4.7175811761367504e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 43/71 | LOSS: 4.6909970726780806e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 44/71 | LOSS: 4.675030480332983e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 45/71 | LOSS: 4.722907531528187e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 46/71 | LOSS: 4.7069598989997135e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 47/71 | LOSS: 4.7030552498957455e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 48/71 | LOSS: 4.6756916367021216e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 49/71 | LOSS: 4.668142122682184e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 50/71 | LOSS: 4.655582404880346e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 51/71 | LOSS: 4.6635736138201675e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 52/71 | LOSS: 4.6589243075291995e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 53/71 | LOSS: 4.6548338138597535e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 54/71 | LOSS: 4.659016029274261e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 55/71 | LOSS: 4.658842551309915e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 56/71 | LOSS: 4.6758820022468604e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 57/71 | LOSS: 4.6620430331476485e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 58/71 | LOSS: 4.6571804347676936e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 59/71 | LOSS: 4.6987720270408315e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 60/71 | LOSS: 4.6867817367396514e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 61/71 | LOSS: 4.6782824586355875e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 62/71 | LOSS: 4.660236727287402e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 63/71 | LOSS: 4.637785377781256e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 64/71 | LOSS: 4.681629249646973e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 65/71 | LOSS: 4.674414407363105e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 66/71 | LOSS: 4.69071878321858e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 67/71 | LOSS: 4.675097614595228e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 68/71 | LOSS: 4.6656059355987914e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 69/71 | LOSS: 4.648319471536005e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 70/71 | LOSS: 4.6592102720956385e-05\n",
      "VAL: EPOCH 29/100 | BATCH 0/8 | LOSS: 6.732391921104863e-05\n",
      "VAL: EPOCH 29/100 | BATCH 1/8 | LOSS: 5.189096191315912e-05\n",
      "VAL: EPOCH 29/100 | BATCH 2/8 | LOSS: 4.9868152321626745e-05\n",
      "VAL: EPOCH 29/100 | BATCH 3/8 | LOSS: 4.748333685711259e-05\n",
      "VAL: EPOCH 29/100 | BATCH 4/8 | LOSS: 4.5943850273033605e-05\n",
      "VAL: EPOCH 29/100 | BATCH 5/8 | LOSS: 4.393821473058779e-05\n",
      "VAL: EPOCH 29/100 | BATCH 6/8 | LOSS: 4.433402656494374e-05\n",
      "VAL: EPOCH 29/100 | BATCH 7/8 | LOSS: 4.424117696544272e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 0/71 | LOSS: 4.369170346762985e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 1/71 | LOSS: 4.113961222174112e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 2/71 | LOSS: 4.1791339754126966e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 3/71 | LOSS: 4.757816714118235e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 4/71 | LOSS: 4.369299640529789e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 5/71 | LOSS: 4.24290028604446e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 6/71 | LOSS: 4.400557996372559e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 7/71 | LOSS: 4.4004429582855664e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 8/71 | LOSS: 4.45478605494524e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 9/71 | LOSS: 4.412775015225634e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 10/71 | LOSS: 4.4318697208919645e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 11/71 | LOSS: 4.340408001250277e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 12/71 | LOSS: 4.2394625499862464e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 13/71 | LOSS: 4.1760267777135596e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 14/71 | LOSS: 4.184238447730119e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 15/71 | LOSS: 4.264537301423843e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 16/71 | LOSS: 4.2470741284298984e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 17/71 | LOSS: 4.296822933409001e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 18/71 | LOSS: 4.263441166442231e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 19/71 | LOSS: 4.2571319681883324e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 20/71 | LOSS: 4.2444119615192035e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 21/71 | LOSS: 4.243036958293735e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 22/71 | LOSS: 4.264729653621781e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 23/71 | LOSS: 4.2696352617592005e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 24/71 | LOSS: 4.32253438339103e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 25/71 | LOSS: 4.272101796232164e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 26/71 | LOSS: 4.241995823880037e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 27/71 | LOSS: 4.2420065645793716e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 28/71 | LOSS: 4.262063558944019e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 29/71 | LOSS: 4.285781954725583e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 30/71 | LOSS: 4.3050459005884946e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 31/71 | LOSS: 4.296599877307017e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 32/71 | LOSS: 4.286628675551831e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 33/71 | LOSS: 4.2633935342618153e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 34/71 | LOSS: 4.260820569470525e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 35/71 | LOSS: 4.268496316702416e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 36/71 | LOSS: 4.2603841024715255e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 37/71 | LOSS: 4.324249522668604e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 38/71 | LOSS: 4.2972464983064965e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 39/71 | LOSS: 4.286124913051026e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 40/71 | LOSS: 4.284292627722252e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 41/71 | LOSS: 4.254238475758549e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 42/71 | LOSS: 4.237722003931277e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 43/71 | LOSS: 4.232180894673961e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 44/71 | LOSS: 4.27010463883764e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 45/71 | LOSS: 4.258688801675579e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 46/71 | LOSS: 4.283836116225389e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 47/71 | LOSS: 4.290461966623601e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 48/71 | LOSS: 4.293919902215284e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 49/71 | LOSS: 4.281844670913415e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 50/71 | LOSS: 4.308985069870045e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 51/71 | LOSS: 4.2938422001707993e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 52/71 | LOSS: 4.339533034170635e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 53/71 | LOSS: 4.340779085194097e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 54/71 | LOSS: 4.3634108144694686e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 55/71 | LOSS: 4.358737835121117e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 56/71 | LOSS: 4.353230277749326e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 57/71 | LOSS: 4.344463575072386e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 58/71 | LOSS: 4.337897722908272e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 59/71 | LOSS: 4.3393210792904334e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 60/71 | LOSS: 4.310823348279256e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 61/71 | LOSS: 4.305170005541332e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 62/71 | LOSS: 4.301589805912781e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 63/71 | LOSS: 4.300604823015419e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 64/71 | LOSS: 4.3044230523474445e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 65/71 | LOSS: 4.333534710013103e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 66/71 | LOSS: 4.3337433263372786e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 67/71 | LOSS: 4.3268121691439504e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 68/71 | LOSS: 4.329862373276188e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 69/71 | LOSS: 4.332857235047933e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 70/71 | LOSS: 4.314747282888406e-05\n",
      "VAL: EPOCH 30/100 | BATCH 0/8 | LOSS: 6.368479807861149e-05\n",
      "VAL: EPOCH 30/100 | BATCH 1/8 | LOSS: 4.9520740503794514e-05\n",
      "VAL: EPOCH 30/100 | BATCH 2/8 | LOSS: 4.738668576464988e-05\n",
      "VAL: EPOCH 30/100 | BATCH 3/8 | LOSS: 4.555610303214053e-05\n",
      "VAL: EPOCH 30/100 | BATCH 4/8 | LOSS: 4.395434443722479e-05\n",
      "VAL: EPOCH 30/100 | BATCH 5/8 | LOSS: 4.207290597454024e-05\n",
      "VAL: EPOCH 30/100 | BATCH 6/8 | LOSS: 4.244519836252688e-05\n",
      "VAL: EPOCH 30/100 | BATCH 7/8 | LOSS: 4.2662740725063486e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 0/71 | LOSS: 4.10127904615365e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 1/71 | LOSS: 3.7199251892161556e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 2/71 | LOSS: 3.82733718045832e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 3/71 | LOSS: 4.277925927453907e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 4/71 | LOSS: 4.066085093654692e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 5/71 | LOSS: 4.013087285178093e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 6/71 | LOSS: 3.889911334096853e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 7/71 | LOSS: 3.9868201383796986e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 8/71 | LOSS: 4.071331770521485e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 9/71 | LOSS: 4.0098573299474084e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 10/71 | LOSS: 3.9286696185379036e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 11/71 | LOSS: 3.887235592022383e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 12/71 | LOSS: 3.880303260162831e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 13/71 | LOSS: 3.8254011087701656e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 14/71 | LOSS: 3.81218756956514e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 15/71 | LOSS: 3.839499277091818e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 16/71 | LOSS: 3.92557393591714e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 17/71 | LOSS: 3.919280647500677e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 18/71 | LOSS: 3.932875322214769e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 19/71 | LOSS: 3.9754759563948026e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 20/71 | LOSS: 3.9952596818606945e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 21/71 | LOSS: 4.0034251552159816e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 22/71 | LOSS: 4.0069770361499295e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 23/71 | LOSS: 4.0008958724987075e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 24/71 | LOSS: 4.03822872613091e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 25/71 | LOSS: 4.015705767065251e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 26/71 | LOSS: 3.998787710938982e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 27/71 | LOSS: 3.981722277655665e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 28/71 | LOSS: 4.0103879090848156e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 29/71 | LOSS: 4.027680624858476e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 30/71 | LOSS: 4.031440485343938e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 31/71 | LOSS: 4.06303130375818e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 32/71 | LOSS: 4.040995797064776e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 33/71 | LOSS: 4.06351970441927e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 34/71 | LOSS: 4.096760146369758e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 35/71 | LOSS: 4.1055126530409325e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 36/71 | LOSS: 4.114975515246467e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 37/71 | LOSS: 4.0882607400386694e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 38/71 | LOSS: 4.075783465687448e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 39/71 | LOSS: 4.0660148533788744e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 40/71 | LOSS: 4.084377360413782e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 41/71 | LOSS: 4.101730510661755e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 42/71 | LOSS: 4.106615631107436e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 43/71 | LOSS: 4.099589998738586e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 44/71 | LOSS: 4.095177298748038e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 45/71 | LOSS: 4.1130020532443226e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 46/71 | LOSS: 4.096598291033978e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 47/71 | LOSS: 4.086436145674573e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 48/71 | LOSS: 4.094341962553123e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 49/71 | LOSS: 4.088044253876433e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 50/71 | LOSS: 4.07675577341305e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 51/71 | LOSS: 4.079182025844393e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 52/71 | LOSS: 4.0795350390187695e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 53/71 | LOSS: 4.080746502613356e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 54/71 | LOSS: 4.0786353334128346e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 55/71 | LOSS: 4.098373035178936e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 56/71 | LOSS: 4.109493232119763e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 57/71 | LOSS: 4.105713033540865e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 58/71 | LOSS: 4.132705164966726e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 59/71 | LOSS: 4.108545878504325e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 60/71 | LOSS: 4.098657596155955e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 61/71 | LOSS: 4.078732402582142e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 62/71 | LOSS: 4.077202151714277e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 63/71 | LOSS: 4.070931163369096e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 64/71 | LOSS: 4.056962677098524e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 65/71 | LOSS: 4.044265055695591e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 66/71 | LOSS: 4.0341208171127226e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 67/71 | LOSS: 4.026776225467021e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 68/71 | LOSS: 4.020428958397858e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 69/71 | LOSS: 4.0187865514391365e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 70/71 | LOSS: 4.0221478729377646e-05\n",
      "VAL: EPOCH 31/100 | BATCH 0/8 | LOSS: 5.831506132381037e-05\n",
      "VAL: EPOCH 31/100 | BATCH 1/8 | LOSS: 4.609164534485899e-05\n",
      "VAL: EPOCH 31/100 | BATCH 2/8 | LOSS: 4.4072052939251684e-05\n",
      "VAL: EPOCH 31/100 | BATCH 3/8 | LOSS: 4.171134696662193e-05\n",
      "VAL: EPOCH 31/100 | BATCH 4/8 | LOSS: 4.012777280877344e-05\n",
      "VAL: EPOCH 31/100 | BATCH 5/8 | LOSS: 3.8398889046220575e-05\n",
      "VAL: EPOCH 31/100 | BATCH 6/8 | LOSS: 3.8669767621156226e-05\n",
      "VAL: EPOCH 31/100 | BATCH 7/8 | LOSS: 3.912671377293009e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 0/71 | LOSS: 2.7848625904880464e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 1/71 | LOSS: 2.8604676117538475e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 2/71 | LOSS: 2.780661573827577e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 3/71 | LOSS: 3.179666146024829e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 4/71 | LOSS: 3.2603710860712456e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 5/71 | LOSS: 3.389675657672342e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 6/71 | LOSS: 3.648263529092739e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 7/71 | LOSS: 3.655204045571736e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 8/71 | LOSS: 3.931188216989135e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 9/71 | LOSS: 4.0720149263506757e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 10/71 | LOSS: 4.087132401764393e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 11/71 | LOSS: 4.118806797729727e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 12/71 | LOSS: 4.047240564922014e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 13/71 | LOSS: 4.0436707422486506e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 14/71 | LOSS: 4.0455317503074184e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 15/71 | LOSS: 3.9612678165212856e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 16/71 | LOSS: 3.982850488254746e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 17/71 | LOSS: 3.971246143110976e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 18/71 | LOSS: 3.942048830427475e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 19/71 | LOSS: 3.9304539131990166e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 20/71 | LOSS: 3.935957318476756e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 21/71 | LOSS: 3.900296889531257e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 22/71 | LOSS: 3.883834205739929e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 23/71 | LOSS: 3.866553076174265e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 24/71 | LOSS: 3.851498389849439e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 25/71 | LOSS: 3.8544504954748285e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 26/71 | LOSS: 3.857151737234003e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 27/71 | LOSS: 3.836322464069651e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 28/71 | LOSS: 3.799821106126885e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 29/71 | LOSS: 3.786841977368264e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 30/71 | LOSS: 3.760409409522019e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 31/71 | LOSS: 3.731977034249212e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 32/71 | LOSS: 3.7029290703657985e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 33/71 | LOSS: 3.7793730638548045e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 34/71 | LOSS: 3.766705771600495e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 35/71 | LOSS: 3.781005564936398e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 36/71 | LOSS: 3.7691776309952115e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 37/71 | LOSS: 3.756069231473559e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 38/71 | LOSS: 3.75846907095715e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 39/71 | LOSS: 3.757528415917477e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 40/71 | LOSS: 3.8091984562215215e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 41/71 | LOSS: 3.8018955690079436e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 42/71 | LOSS: 3.7888566217842775e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 43/71 | LOSS: 3.771609635805362e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 44/71 | LOSS: 3.7724404561837826e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 45/71 | LOSS: 3.758979006385704e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 46/71 | LOSS: 3.7675704663072e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 47/71 | LOSS: 3.799821763550426e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 48/71 | LOSS: 3.81143978599112e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 49/71 | LOSS: 3.7931031292828266e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 50/71 | LOSS: 3.828809966245328e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 51/71 | LOSS: 3.839143374324731e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 52/71 | LOSS: 3.868942580736058e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 53/71 | LOSS: 3.8581548795735684e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 54/71 | LOSS: 3.8540926768539725e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 55/71 | LOSS: 3.8474029971829236e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 56/71 | LOSS: 3.8313665483432836e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 57/71 | LOSS: 3.80361853817233e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 58/71 | LOSS: 3.785767885501165e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 59/71 | LOSS: 3.789902348216856e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 60/71 | LOSS: 3.80383943160805e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 61/71 | LOSS: 3.8025205398181213e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 62/71 | LOSS: 3.810011831875373e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 63/71 | LOSS: 3.81743178081706e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 64/71 | LOSS: 3.8191641774924044e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 65/71 | LOSS: 3.813395905484812e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 66/71 | LOSS: 3.8141940462788734e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 67/71 | LOSS: 3.813053214852356e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 68/71 | LOSS: 3.8238874011490815e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 69/71 | LOSS: 3.8201459964121955e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 70/71 | LOSS: 3.795709908701329e-05\n",
      "VAL: EPOCH 32/100 | BATCH 0/8 | LOSS: 5.8486777561483905e-05\n",
      "VAL: EPOCH 32/100 | BATCH 1/8 | LOSS: 4.717476258520037e-05\n",
      "VAL: EPOCH 32/100 | BATCH 2/8 | LOSS: 4.4955088621160634e-05\n",
      "VAL: EPOCH 32/100 | BATCH 3/8 | LOSS: 4.245954733050894e-05\n",
      "VAL: EPOCH 32/100 | BATCH 4/8 | LOSS: 4.113655086257495e-05\n",
      "VAL: EPOCH 32/100 | BATCH 5/8 | LOSS: 3.93609649715169e-05\n",
      "VAL: EPOCH 32/100 | BATCH 6/8 | LOSS: 3.934260270658082e-05\n",
      "VAL: EPOCH 32/100 | BATCH 7/8 | LOSS: 3.9833830214774935e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 0/71 | LOSS: 3.60872145392932e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 1/71 | LOSS: 4.492970401770435e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 2/71 | LOSS: 3.935466399222302e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 3/71 | LOSS: 3.8046920963097364e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 4/71 | LOSS: 3.9557149284519257e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 5/71 | LOSS: 3.933091041593192e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 6/71 | LOSS: 3.8069702246243e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 7/71 | LOSS: 3.636504152382258e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 8/71 | LOSS: 3.843863103409401e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 9/71 | LOSS: 3.7865008562221195e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 10/71 | LOSS: 3.731915024797093e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 11/71 | LOSS: 3.7060988991773534e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 12/71 | LOSS: 3.7738282956594885e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 13/71 | LOSS: 3.7547391912085004e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 14/71 | LOSS: 3.708954051641437e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 15/71 | LOSS: 3.736961184586107e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 16/71 | LOSS: 3.77413525711745e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 17/71 | LOSS: 3.727780464638878e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 18/71 | LOSS: 3.761409674265642e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 19/71 | LOSS: 3.7833389342267765e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 20/71 | LOSS: 3.77843184001644e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 21/71 | LOSS: 3.736173221113859e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 22/71 | LOSS: 3.723574493209447e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 23/71 | LOSS: 3.810007236400755e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 24/71 | LOSS: 3.837054326140787e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 25/71 | LOSS: 3.801862616315842e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 26/71 | LOSS: 3.773943993750166e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 27/71 | LOSS: 3.8107397813291755e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 28/71 | LOSS: 3.822030063521855e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 29/71 | LOSS: 3.7929872754223955e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 30/71 | LOSS: 3.799520251754251e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 31/71 | LOSS: 3.784000011819444e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 32/71 | LOSS: 3.82928382984546e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 33/71 | LOSS: 3.824622238907443e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 34/71 | LOSS: 3.825995175118026e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 35/71 | LOSS: 3.838935410587712e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 36/71 | LOSS: 3.851041627648236e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 37/71 | LOSS: 3.838100048214026e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 38/71 | LOSS: 3.823326038307128e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 39/71 | LOSS: 3.8454963123513154e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 40/71 | LOSS: 3.8440109542201886e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 41/71 | LOSS: 3.829976378169487e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 42/71 | LOSS: 3.828595021501357e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 43/71 | LOSS: 3.835882605265149e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 44/71 | LOSS: 3.842655844184467e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 45/71 | LOSS: 3.835673376907716e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 46/71 | LOSS: 3.838045248561143e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 47/71 | LOSS: 3.827393046170376e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 48/71 | LOSS: 3.835491135797216e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 49/71 | LOSS: 3.8363041676348074e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 50/71 | LOSS: 3.820484619377656e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 51/71 | LOSS: 3.7877068315048324e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 52/71 | LOSS: 3.7695948982595926e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 53/71 | LOSS: 3.7540837790099126e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 54/71 | LOSS: 3.746505565438631e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 55/71 | LOSS: 3.7481926645861154e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 56/71 | LOSS: 3.7324935443680975e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 57/71 | LOSS: 3.737244593327793e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 58/71 | LOSS: 3.753728092074891e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 59/71 | LOSS: 3.7624372847252134e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 60/71 | LOSS: 3.746688451458601e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 61/71 | LOSS: 3.752408593432084e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 62/71 | LOSS: 3.737229711814074e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 63/71 | LOSS: 3.732714918669444e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 64/71 | LOSS: 3.724823086835946e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 65/71 | LOSS: 3.7174981612638064e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 66/71 | LOSS: 3.7286903480252026e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 67/71 | LOSS: 3.730971906513921e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 68/71 | LOSS: 3.7242797944876976e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 69/71 | LOSS: 3.7203829664836774e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 70/71 | LOSS: 3.710049555004037e-05\n",
      "VAL: EPOCH 33/100 | BATCH 0/8 | LOSS: 5.432938996818848e-05\n",
      "VAL: EPOCH 33/100 | BATCH 1/8 | LOSS: 4.22979865106754e-05\n",
      "VAL: EPOCH 33/100 | BATCH 2/8 | LOSS: 4.015956922861127e-05\n",
      "VAL: EPOCH 33/100 | BATCH 3/8 | LOSS: 3.7915221582807135e-05\n",
      "VAL: EPOCH 33/100 | BATCH 4/8 | LOSS: 3.650235157692805e-05\n",
      "VAL: EPOCH 33/100 | BATCH 5/8 | LOSS: 3.47786274990843e-05\n",
      "VAL: EPOCH 33/100 | BATCH 6/8 | LOSS: 3.51378096508727e-05\n",
      "VAL: EPOCH 33/100 | BATCH 7/8 | LOSS: 3.543438833730761e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 0/71 | LOSS: 2.4274839233839884e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 1/71 | LOSS: 3.3947924748645164e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 2/71 | LOSS: 3.271559762652032e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 3/71 | LOSS: 3.3746720873750746e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 4/71 | LOSS: 3.5501860838849095e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 5/71 | LOSS: 3.792652569245547e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 6/71 | LOSS: 3.751513265472438e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 7/71 | LOSS: 3.770178091144771e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 8/71 | LOSS: 3.75878116756212e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 9/71 | LOSS: 3.688478063850198e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 10/71 | LOSS: 3.620976878499443e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 11/71 | LOSS: 3.6034066397405695e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 12/71 | LOSS: 3.6051815326433054e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 13/71 | LOSS: 3.5826325984089635e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 14/71 | LOSS: 3.596648578726066e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 15/71 | LOSS: 3.56998959887278e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 16/71 | LOSS: 3.535539529847858e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 17/71 | LOSS: 3.602823198889382e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 18/71 | LOSS: 3.570646769826693e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 19/71 | LOSS: 3.6062410617887505e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 20/71 | LOSS: 3.656382527662485e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 21/71 | LOSS: 3.647632415364073e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 22/71 | LOSS: 3.6353914291122116e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 23/71 | LOSS: 3.606602028109288e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 24/71 | LOSS: 3.587942497688346e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 25/71 | LOSS: 3.579911670105783e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 26/71 | LOSS: 3.575284629025393e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 27/71 | LOSS: 3.550929997280556e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 28/71 | LOSS: 3.5498894343116516e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 29/71 | LOSS: 3.541886671882821e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 30/71 | LOSS: 3.529433448537959e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 31/71 | LOSS: 3.5255660407074174e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 32/71 | LOSS: 3.509798416141138e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 33/71 | LOSS: 3.519756565390922e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 34/71 | LOSS: 3.50126697282706e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 35/71 | LOSS: 3.503403119490637e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 36/71 | LOSS: 3.51150102269949e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 37/71 | LOSS: 3.509202485331212e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 38/71 | LOSS: 3.501117764524399e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 39/71 | LOSS: 3.4814762648238685e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 40/71 | LOSS: 3.476807583732496e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 41/71 | LOSS: 3.495681764997287e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 42/71 | LOSS: 3.465930519247872e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 43/71 | LOSS: 3.466932457740768e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 44/71 | LOSS: 3.476141763611748e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 45/71 | LOSS: 3.4656511793993715e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 46/71 | LOSS: 3.452367951816067e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 47/71 | LOSS: 3.451259529659486e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 48/71 | LOSS: 3.4473518507165494e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 49/71 | LOSS: 3.436623370362213e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 50/71 | LOSS: 3.4319882829817396e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 51/71 | LOSS: 3.410196481686748e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 52/71 | LOSS: 3.399967534038159e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 53/71 | LOSS: 3.395132743780441e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 54/71 | LOSS: 3.40735094306927e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 55/71 | LOSS: 3.409504243531306e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 56/71 | LOSS: 3.393710603791477e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 57/71 | LOSS: 3.39618502729191e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 58/71 | LOSS: 3.44961392360254e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 59/71 | LOSS: 3.439501715547521e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 60/71 | LOSS: 3.4405443464075676e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 61/71 | LOSS: 3.435439157328801e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 62/71 | LOSS: 3.441153098214677e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 63/71 | LOSS: 3.436155051872447e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 64/71 | LOSS: 3.437471009070018e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 65/71 | LOSS: 3.436703647104988e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 66/71 | LOSS: 3.433067899665608e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 67/71 | LOSS: 3.43733069498472e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 68/71 | LOSS: 3.4283034213897906e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 69/71 | LOSS: 3.428603904467309e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 70/71 | LOSS: 3.43912122100936e-05\n",
      "VAL: EPOCH 34/100 | BATCH 0/8 | LOSS: 5.3628540626959875e-05\n",
      "VAL: EPOCH 34/100 | BATCH 1/8 | LOSS: 4.21575805376051e-05\n",
      "VAL: EPOCH 34/100 | BATCH 2/8 | LOSS: 4.0431310480926186e-05\n",
      "VAL: EPOCH 34/100 | BATCH 3/8 | LOSS: 3.786964589380659e-05\n",
      "VAL: EPOCH 34/100 | BATCH 4/8 | LOSS: 3.669060388347134e-05\n",
      "VAL: EPOCH 34/100 | BATCH 5/8 | LOSS: 3.5145340916642454e-05\n",
      "VAL: EPOCH 34/100 | BATCH 6/8 | LOSS: 3.5582076244671566e-05\n",
      "VAL: EPOCH 34/100 | BATCH 7/8 | LOSS: 3.56698076302564e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 0/71 | LOSS: 3.8027217669878155e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 1/71 | LOSS: 3.7798639823449776e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 2/71 | LOSS: 3.758932386214534e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 3/71 | LOSS: 3.688629021780798e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 4/71 | LOSS: 3.516360593494028e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 5/71 | LOSS: 3.379385786198933e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 6/71 | LOSS: 3.35602887519469e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 7/71 | LOSS: 3.307518431938661e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 8/71 | LOSS: 3.377889091401206e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 9/71 | LOSS: 3.336000427225372e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 10/71 | LOSS: 3.342633582095996e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 11/71 | LOSS: 3.3397044565693555e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 12/71 | LOSS: 3.3485451170306005e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 13/71 | LOSS: 3.309203774343976e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 14/71 | LOSS: 3.290136676999585e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 15/71 | LOSS: 3.348203301811736e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 16/71 | LOSS: 3.384232064383789e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 17/71 | LOSS: 3.34960989979057e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 18/71 | LOSS: 3.3246480502145315e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 19/71 | LOSS: 3.297221619504853e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 20/71 | LOSS: 3.303581364716159e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 21/71 | LOSS: 3.275484157876979e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 22/71 | LOSS: 3.2643932844739936e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 23/71 | LOSS: 3.240382003847723e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 24/71 | LOSS: 3.262217265728395e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 25/71 | LOSS: 3.2366595755425245e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 26/71 | LOSS: 3.257207390561234e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 27/71 | LOSS: 3.244738575501417e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 28/71 | LOSS: 3.257727721888685e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 29/71 | LOSS: 3.286743904027389e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 30/71 | LOSS: 3.299692117809994e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 31/71 | LOSS: 3.3108586137586826e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 32/71 | LOSS: 3.3041034398730986e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 33/71 | LOSS: 3.314352135716677e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 34/71 | LOSS: 3.321715490268876e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 35/71 | LOSS: 3.337519092383445e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 36/71 | LOSS: 3.310882533008447e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 37/71 | LOSS: 3.290725496634351e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 38/71 | LOSS: 3.2952817724808716e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 39/71 | LOSS: 3.284557797087473e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 40/71 | LOSS: 3.270422827726423e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 41/71 | LOSS: 3.249842280811641e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 42/71 | LOSS: 3.259263914599636e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 43/71 | LOSS: 3.2558957545089385e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 44/71 | LOSS: 3.253644038826072e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 45/71 | LOSS: 3.27269914202549e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 46/71 | LOSS: 3.2582323717391815e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 47/71 | LOSS: 3.253825711150663e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 48/71 | LOSS: 3.253138133078785e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 49/71 | LOSS: 3.24595746860723e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 50/71 | LOSS: 3.2400449616069834e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 51/71 | LOSS: 3.215678802567149e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 52/71 | LOSS: 3.2589696343595005e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 53/71 | LOSS: 3.2510499564792825e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 54/71 | LOSS: 3.246138221584261e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 55/71 | LOSS: 3.2413860351100864e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 56/71 | LOSS: 3.2424913290908496e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 57/71 | LOSS: 3.259951781415268e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 58/71 | LOSS: 3.25327043463989e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 59/71 | LOSS: 3.261960670594514e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 60/71 | LOSS: 3.272732480030247e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 61/71 | LOSS: 3.2915124921278365e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 62/71 | LOSS: 3.29504054250373e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 63/71 | LOSS: 3.3026194671492703e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 64/71 | LOSS: 3.298889714642428e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 65/71 | LOSS: 3.28744467741203e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 66/71 | LOSS: 3.279077279344097e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 67/71 | LOSS: 3.281964957161964e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 68/71 | LOSS: 3.2716702596677564e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 69/71 | LOSS: 3.27400550564302e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 70/71 | LOSS: 3.291011028484487e-05\n",
      "VAL: EPOCH 35/100 | BATCH 0/8 | LOSS: 4.804058698937297e-05\n",
      "VAL: EPOCH 35/100 | BATCH 1/8 | LOSS: 3.7756359233753756e-05\n",
      "VAL: EPOCH 35/100 | BATCH 2/8 | LOSS: 3.613501273017997e-05\n",
      "VAL: EPOCH 35/100 | BATCH 3/8 | LOSS: 3.4441877687640954e-05\n",
      "VAL: EPOCH 35/100 | BATCH 4/8 | LOSS: 3.3399776657461186e-05\n",
      "VAL: EPOCH 35/100 | BATCH 5/8 | LOSS: 3.194505855693327e-05\n",
      "VAL: EPOCH 35/100 | BATCH 6/8 | LOSS: 3.244527085501302e-05\n",
      "VAL: EPOCH 35/100 | BATCH 7/8 | LOSS: 3.275920107626007e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 0/71 | LOSS: 4.285999602871016e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 1/71 | LOSS: 3.734550591616426e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 2/71 | LOSS: 3.968724073880973e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 3/71 | LOSS: 3.977087271778146e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 4/71 | LOSS: 4.0513926796847956e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 5/71 | LOSS: 3.9041685643799916e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 6/71 | LOSS: 4.077490874416461e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 7/71 | LOSS: 3.971586420448148e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 8/71 | LOSS: 3.871556732014546e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 9/71 | LOSS: 3.8746362406527624e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 10/71 | LOSS: 3.824879769870842e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 11/71 | LOSS: 3.888401988660917e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 12/71 | LOSS: 3.824699804401742e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 13/71 | LOSS: 3.8231935572444596e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 14/71 | LOSS: 3.790895328468953e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 15/71 | LOSS: 3.811994224633963e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 16/71 | LOSS: 3.806617164895322e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 17/71 | LOSS: 3.790757242920032e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 18/71 | LOSS: 3.755806507175102e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 19/71 | LOSS: 3.728280953509966e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 20/71 | LOSS: 3.703311380204035e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 21/71 | LOSS: 3.6607892401331234e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 22/71 | LOSS: 3.62698411544957e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 23/71 | LOSS: 3.6086424264188587e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 24/71 | LOSS: 3.610936422774103e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 25/71 | LOSS: 3.60031439973122e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 26/71 | LOSS: 3.542071586977087e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 27/71 | LOSS: 3.527371187114373e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 28/71 | LOSS: 3.5011492118114956e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 29/71 | LOSS: 3.478426339521927e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 30/71 | LOSS: 3.4324456024595776e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 31/71 | LOSS: 3.410522140256944e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 32/71 | LOSS: 3.3928459026466236e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 33/71 | LOSS: 3.381947130144588e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 34/71 | LOSS: 3.3911907407205684e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 35/71 | LOSS: 3.366812072474406e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 36/71 | LOSS: 3.360813079288582e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 37/71 | LOSS: 3.383326829383517e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 38/71 | LOSS: 3.3981409125352424e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 39/71 | LOSS: 3.406515388633124e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 40/71 | LOSS: 3.408337730301044e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 41/71 | LOSS: 3.389842255447071e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 42/71 | LOSS: 3.3649437809376544e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 43/71 | LOSS: 3.363976260185485e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 44/71 | LOSS: 3.3568067778509836e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 45/71 | LOSS: 3.348543997044148e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 46/71 | LOSS: 3.345386706295106e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 47/71 | LOSS: 3.3422899681075556e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 48/71 | LOSS: 3.327538975934992e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 49/71 | LOSS: 3.319215011288179e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 50/71 | LOSS: 3.311071600810643e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 51/71 | LOSS: 3.3205874108087024e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 52/71 | LOSS: 3.30985399245346e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 53/71 | LOSS: 3.304267728607455e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 54/71 | LOSS: 3.3096404943287116e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 55/71 | LOSS: 3.300860735829961e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 56/71 | LOSS: 3.314675641855789e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 57/71 | LOSS: 3.319466251359072e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 58/71 | LOSS: 3.303952618838501e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 59/71 | LOSS: 3.304641471307453e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 60/71 | LOSS: 3.293454490810037e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 61/71 | LOSS: 3.284530213286709e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 62/71 | LOSS: 3.2836940388531526e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 63/71 | LOSS: 3.281026368995299e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 64/71 | LOSS: 3.273808260228879e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 65/71 | LOSS: 3.266696140516697e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 66/71 | LOSS: 3.264356203531019e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 67/71 | LOSS: 3.271899231549469e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 68/71 | LOSS: 3.2737392889364436e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 69/71 | LOSS: 3.264152242731403e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 70/71 | LOSS: 3.282046187886293e-05\n",
      "VAL: EPOCH 36/100 | BATCH 0/8 | LOSS: 4.7244546294678e-05\n",
      "VAL: EPOCH 36/100 | BATCH 1/8 | LOSS: 3.7553552829194814e-05\n",
      "VAL: EPOCH 36/100 | BATCH 2/8 | LOSS: 3.555535901493082e-05\n",
      "VAL: EPOCH 36/100 | BATCH 3/8 | LOSS: 3.453130648267688e-05\n",
      "VAL: EPOCH 36/100 | BATCH 4/8 | LOSS: 3.338744281791151e-05\n",
      "VAL: EPOCH 36/100 | BATCH 5/8 | LOSS: 3.203926492763761e-05\n",
      "VAL: EPOCH 36/100 | BATCH 6/8 | LOSS: 3.2668323391200306e-05\n",
      "VAL: EPOCH 36/100 | BATCH 7/8 | LOSS: 3.3369921538906056e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 0/71 | LOSS: 2.4650324121466838e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 1/71 | LOSS: 3.0924299608159345e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 2/71 | LOSS: 3.144079103852467e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 3/71 | LOSS: 2.8705602744594216e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 4/71 | LOSS: 2.869763411581516e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 5/71 | LOSS: 2.9265675038914196e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 6/71 | LOSS: 2.890095363129928e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 7/71 | LOSS: 3.1021873610370676e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 8/71 | LOSS: 3.046310363667241e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 9/71 | LOSS: 2.9591813108709176e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 10/71 | LOSS: 3.053073057592635e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 11/71 | LOSS: 3.0759693042152016e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 12/71 | LOSS: 3.0225280799920885e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 13/71 | LOSS: 3.0743543123078e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 14/71 | LOSS: 3.052167315521122e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 15/71 | LOSS: 3.128172818378516e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 16/71 | LOSS: 3.09434695629855e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 17/71 | LOSS: 3.083669788692431e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 18/71 | LOSS: 3.129821609610716e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 19/71 | LOSS: 3.1649817992729366e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 20/71 | LOSS: 3.133987575849806e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 21/71 | LOSS: 3.107199881924316e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 22/71 | LOSS: 3.116411702965548e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 23/71 | LOSS: 3.1181714499931935e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 24/71 | LOSS: 3.1259435781976206e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 25/71 | LOSS: 3.107911036819972e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 26/71 | LOSS: 3.078545799966108e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 27/71 | LOSS: 3.056353469738886e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 28/71 | LOSS: 3.058651362090163e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 29/71 | LOSS: 3.0415870484527355e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 30/71 | LOSS: 3.036259389069352e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 31/71 | LOSS: 3.0234265295803198e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 32/71 | LOSS: 2.9974852871170945e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 33/71 | LOSS: 2.9925374345860773e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 34/71 | LOSS: 2.9836951268537503e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 35/71 | LOSS: 3.024092610657034e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 36/71 | LOSS: 3.0411387588419774e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 37/71 | LOSS: 3.07339786989982e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 38/71 | LOSS: 3.088167226777818e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 39/71 | LOSS: 3.079617526964284e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 40/71 | LOSS: 3.088540324824862e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 41/71 | LOSS: 3.085120823968845e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 42/71 | LOSS: 3.108569750153503e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 43/71 | LOSS: 3.100647427345393e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 44/71 | LOSS: 3.110486019674378e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 45/71 | LOSS: 3.1058799045833595e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 46/71 | LOSS: 3.091339199949908e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 47/71 | LOSS: 3.078234840359073e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 48/71 | LOSS: 3.073470324292133e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 49/71 | LOSS: 3.061887753574411e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 50/71 | LOSS: 3.080603635359747e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 51/71 | LOSS: 3.086461769131487e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 52/71 | LOSS: 3.090407078144501e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 53/71 | LOSS: 3.083329391773549e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 54/71 | LOSS: 3.083823089995845e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 55/71 | LOSS: 3.112622217875989e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 56/71 | LOSS: 3.111545843319363e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 57/71 | LOSS: 3.107539223806114e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 58/71 | LOSS: 3.104521487854319e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 59/71 | LOSS: 3.1066717171294536e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 60/71 | LOSS: 3.1118296661946666e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 61/71 | LOSS: 3.0938807662272254e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 62/71 | LOSS: 3.092866262748084e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 63/71 | LOSS: 3.0881124303050456e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 64/71 | LOSS: 3.080047172261402e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 65/71 | LOSS: 3.099057975112263e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 66/71 | LOSS: 3.1035768568390676e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 67/71 | LOSS: 3.098166989195092e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 68/71 | LOSS: 3.08437858251821e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 69/71 | LOSS: 3.0919066557544286e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 70/71 | LOSS: 3.1139015512783604e-05\n",
      "VAL: EPOCH 37/100 | BATCH 0/8 | LOSS: 4.508066922426224e-05\n",
      "VAL: EPOCH 37/100 | BATCH 1/8 | LOSS: 3.5601964555098675e-05\n",
      "VAL: EPOCH 37/100 | BATCH 2/8 | LOSS: 3.42371677106712e-05\n",
      "VAL: EPOCH 37/100 | BATCH 3/8 | LOSS: 3.349193048052257e-05\n",
      "VAL: EPOCH 37/100 | BATCH 4/8 | LOSS: 3.253307149861939e-05\n",
      "VAL: EPOCH 37/100 | BATCH 5/8 | LOSS: 3.1147887057159096e-05\n",
      "VAL: EPOCH 37/100 | BATCH 6/8 | LOSS: 3.162102906831673e-05\n",
      "VAL: EPOCH 37/100 | BATCH 7/8 | LOSS: 3.204608856322011e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 0/71 | LOSS: 2.5460089091211557e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 1/71 | LOSS: 3.028668652405031e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 2/71 | LOSS: 3.137101884931326e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 3/71 | LOSS: 3.1914698411128484e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 4/71 | LOSS: 3.046080601052381e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 5/71 | LOSS: 3.038330335887925e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 6/71 | LOSS: 2.9720647424775442e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 7/71 | LOSS: 2.972107904497534e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 8/71 | LOSS: 2.950912676169537e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 9/71 | LOSS: 3.0395479188882746e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 10/71 | LOSS: 3.0102363862996835e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 11/71 | LOSS: 2.9676478182712646e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 12/71 | LOSS: 2.9673087528163495e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 13/71 | LOSS: 3.0509731654352174e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 14/71 | LOSS: 2.994276280029832e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 15/71 | LOSS: 2.991121198192559e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 16/71 | LOSS: 3.0083288305938957e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 17/71 | LOSS: 3.0013262302317242e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 18/71 | LOSS: 2.932126402524055e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 19/71 | LOSS: 2.9428297420963644e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 20/71 | LOSS: 2.9033392257544966e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 21/71 | LOSS: 2.955384479719214e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 22/71 | LOSS: 2.9838953038130928e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 23/71 | LOSS: 2.969583374579088e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 24/71 | LOSS: 2.9565611403086222e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 25/71 | LOSS: 2.9719862630792286e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 26/71 | LOSS: 2.9784389240645664e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 27/71 | LOSS: 2.9825671910137836e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 28/71 | LOSS: 2.958827845477258e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 29/71 | LOSS: 2.9401310954805618e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 30/71 | LOSS: 2.924610682032747e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 31/71 | LOSS: 2.937323159812877e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 32/71 | LOSS: 2.9670829572944168e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 33/71 | LOSS: 2.9551992584482138e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 34/71 | LOSS: 2.9714483753195962e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 35/71 | LOSS: 2.9755798954688038e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 36/71 | LOSS: 2.98523446454547e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 37/71 | LOSS: 3.033345049181279e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 38/71 | LOSS: 3.0279828607513664e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 39/71 | LOSS: 3.0357433251992917e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 40/71 | LOSS: 3.0457584704513797e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 41/71 | LOSS: 3.051680665694654e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 42/71 | LOSS: 3.0444586437349858e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 43/71 | LOSS: 3.0417865880074466e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 44/71 | LOSS: 3.0347770088054756e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 45/71 | LOSS: 3.0317441245563753e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 46/71 | LOSS: 3.0189991408173847e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 47/71 | LOSS: 3.0092620742531533e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 48/71 | LOSS: 2.9985055947208263e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 49/71 | LOSS: 2.9834935558028518e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 50/71 | LOSS: 2.997097088190654e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 51/71 | LOSS: 2.9933796633066286e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 52/71 | LOSS: 2.9816079370233775e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 53/71 | LOSS: 2.9777126824397043e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 54/71 | LOSS: 2.977448675665073e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 55/71 | LOSS: 2.9784962407575222e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 56/71 | LOSS: 2.974967240956367e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 57/71 | LOSS: 2.978785075889579e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 58/71 | LOSS: 2.9637664106380495e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 59/71 | LOSS: 2.9621711170572478e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 60/71 | LOSS: 2.978465868852895e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 61/71 | LOSS: 2.9843621192530037e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 62/71 | LOSS: 2.985101881287327e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 63/71 | LOSS: 2.981689874559379e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 64/71 | LOSS: 2.978247368726163e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 65/71 | LOSS: 2.991270164123059e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 66/71 | LOSS: 3.0047198813564992e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 67/71 | LOSS: 3.0007005400822143e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 68/71 | LOSS: 2.9998880326393586e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 69/71 | LOSS: 2.995022982109471e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 70/71 | LOSS: 2.9820633180477185e-05\n",
      "VAL: EPOCH 38/100 | BATCH 0/8 | LOSS: 4.089848880539648e-05\n",
      "VAL: EPOCH 38/100 | BATCH 1/8 | LOSS: 3.27185625792481e-05\n",
      "VAL: EPOCH 38/100 | BATCH 2/8 | LOSS: 3.149657216757381e-05\n",
      "VAL: EPOCH 38/100 | BATCH 3/8 | LOSS: 3.049253882636549e-05\n",
      "VAL: EPOCH 38/100 | BATCH 4/8 | LOSS: 2.9513176559703425e-05\n",
      "VAL: EPOCH 38/100 | BATCH 5/8 | LOSS: 2.8493932404671796e-05\n",
      "VAL: EPOCH 38/100 | BATCH 6/8 | LOSS: 2.8764793699208114e-05\n",
      "VAL: EPOCH 38/100 | BATCH 7/8 | LOSS: 2.9177137548686005e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 0/71 | LOSS: 2.7506364858709276e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 1/71 | LOSS: 2.675755968084559e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 2/71 | LOSS: 2.4291701265610754e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 3/71 | LOSS: 2.5897435079969e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 4/71 | LOSS: 2.508077086531557e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 5/71 | LOSS: 2.5870003203939024e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 6/71 | LOSS: 2.6131425849078888e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 7/71 | LOSS: 2.745115648394858e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 8/71 | LOSS: 2.7161086109117605e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 9/71 | LOSS: 2.7403100102674215e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 10/71 | LOSS: 2.7194621493849397e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 11/71 | LOSS: 2.717320482285383e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 12/71 | LOSS: 2.7127739695760494e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 13/71 | LOSS: 2.666325755334193e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 14/71 | LOSS: 2.687115193111822e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 15/71 | LOSS: 2.7103954380436335e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 16/71 | LOSS: 2.691941913593944e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 17/71 | LOSS: 2.6749973686593068e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 18/71 | LOSS: 2.714978070702943e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 19/71 | LOSS: 2.6954324312100653e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 20/71 | LOSS: 2.6847087740731824e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 21/71 | LOSS: 2.7141517586476933e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 22/71 | LOSS: 2.7189528618296407e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 23/71 | LOSS: 2.765648097617183e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 24/71 | LOSS: 2.7518778297235257e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 25/71 | LOSS: 2.771210326430334e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 26/71 | LOSS: 2.790798027018792e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 27/71 | LOSS: 2.8219652514443233e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 28/71 | LOSS: 2.8499203961410816e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 29/71 | LOSS: 2.8464435733136876e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 30/71 | LOSS: 2.8277056645170126e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 31/71 | LOSS: 2.836092409097546e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 32/71 | LOSS: 2.8541140140574e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 33/71 | LOSS: 2.8610760583593647e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 34/71 | LOSS: 2.8778860775803748e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 35/71 | LOSS: 2.9150928084062696e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 36/71 | LOSS: 2.9263046451740445e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 37/71 | LOSS: 2.9138803065028408e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 38/71 | LOSS: 2.9301319395577033e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 39/71 | LOSS: 2.914577730734891e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 40/71 | LOSS: 2.9017426818538253e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 41/71 | LOSS: 2.928714582681312e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 42/71 | LOSS: 2.92186160909613e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 43/71 | LOSS: 2.9520742290597727e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 44/71 | LOSS: 2.9446886077898346e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 45/71 | LOSS: 2.946613779425641e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 46/71 | LOSS: 2.9371952823179952e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 47/71 | LOSS: 2.9415952819059992e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 48/71 | LOSS: 2.949554009876233e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 49/71 | LOSS: 2.963603474199772e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 50/71 | LOSS: 2.9556096206743306e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 51/71 | LOSS: 2.9533691164173295e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 52/71 | LOSS: 2.9586681056622733e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 53/71 | LOSS: 2.9574728277684363e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 54/71 | LOSS: 2.9509179677750747e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 55/71 | LOSS: 2.9529948863325573e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 56/71 | LOSS: 2.9564467120891142e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 57/71 | LOSS: 2.9581304888465796e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 58/71 | LOSS: 2.969698095003585e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 59/71 | LOSS: 2.9742462599339583e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 60/71 | LOSS: 2.9632570658577606e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 61/71 | LOSS: 2.9527625326497195e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 62/71 | LOSS: 2.9407730431153646e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 63/71 | LOSS: 2.9346927135520673e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 64/71 | LOSS: 2.9280645601549902e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 65/71 | LOSS: 2.9267972229607757e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 66/71 | LOSS: 2.9256056548737392e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 67/71 | LOSS: 2.9194107997575294e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 68/71 | LOSS: 2.9132671640937865e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 69/71 | LOSS: 2.9101858542292446e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 70/71 | LOSS: 2.9069724046743252e-05\n",
      "VAL: EPOCH 39/100 | BATCH 0/8 | LOSS: 3.912337342626415e-05\n",
      "VAL: EPOCH 39/100 | BATCH 1/8 | LOSS: 3.110456145805074e-05\n",
      "VAL: EPOCH 39/100 | BATCH 2/8 | LOSS: 2.9994613214512356e-05\n",
      "VAL: EPOCH 39/100 | BATCH 3/8 | LOSS: 2.8899049993924564e-05\n",
      "VAL: EPOCH 39/100 | BATCH 4/8 | LOSS: 2.80868978734361e-05\n",
      "VAL: EPOCH 39/100 | BATCH 5/8 | LOSS: 2.7103662129472166e-05\n",
      "VAL: EPOCH 39/100 | BATCH 6/8 | LOSS: 2.740028529452892e-05\n",
      "VAL: EPOCH 39/100 | BATCH 7/8 | LOSS: 2.792746749946673e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 0/71 | LOSS: 2.3703594706603326e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 1/71 | LOSS: 2.564031365182018e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 2/71 | LOSS: 2.4556148976747256e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 3/71 | LOSS: 2.479819477230194e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 4/71 | LOSS: 2.7254021188127808e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 5/71 | LOSS: 2.7938977837038692e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 6/71 | LOSS: 2.7611745411247413e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 7/71 | LOSS: 2.821724024215655e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 8/71 | LOSS: 2.759586176479287e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 9/71 | LOSS: 2.7318923275743146e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 10/71 | LOSS: 2.72265743578001e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 11/71 | LOSS: 2.703052647727115e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 12/71 | LOSS: 2.7212530487584165e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 13/71 | LOSS: 2.7823659365822095e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 14/71 | LOSS: 2.8427398137864657e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 15/71 | LOSS: 2.8484839617703983e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 16/71 | LOSS: 2.872047512295574e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 17/71 | LOSS: 2.8845848343432106e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 18/71 | LOSS: 2.8433041638651813e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 19/71 | LOSS: 2.858732959793997e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 20/71 | LOSS: 2.854869412008806e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 21/71 | LOSS: 2.864133288808675e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 22/71 | LOSS: 2.862864138201669e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 23/71 | LOSS: 2.8571387019837857e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 24/71 | LOSS: 2.8608277134480886e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 25/71 | LOSS: 2.8529265784107756e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 26/71 | LOSS: 2.8377122160034356e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 27/71 | LOSS: 2.8540536277432693e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 28/71 | LOSS: 2.8387525037095626e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 29/71 | LOSS: 2.8315247557960296e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 30/71 | LOSS: 2.8266052932820223e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 31/71 | LOSS: 2.822166760552136e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 32/71 | LOSS: 2.8075366241285916e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 33/71 | LOSS: 2.827457148455056e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 34/71 | LOSS: 2.8172771167841608e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 35/71 | LOSS: 2.823418577059379e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 36/71 | LOSS: 2.8057141084777744e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 37/71 | LOSS: 2.8181307769907443e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 38/71 | LOSS: 2.8083567462416773e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 39/71 | LOSS: 2.8225705455042772e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 40/71 | LOSS: 2.8200747515145894e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 41/71 | LOSS: 2.8218506875938537e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 42/71 | LOSS: 2.8035391149514486e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 43/71 | LOSS: 2.809710552107638e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 44/71 | LOSS: 2.8132386392422226e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 45/71 | LOSS: 2.7972803289468057e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 46/71 | LOSS: 2.8010051151950586e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 47/71 | LOSS: 2.815855744605263e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 48/71 | LOSS: 2.8206172550087605e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 49/71 | LOSS: 2.822529189870693e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 50/71 | LOSS: 2.8367410670719383e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 51/71 | LOSS: 2.8335227748007477e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 52/71 | LOSS: 2.8481054199938575e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 53/71 | LOSS: 2.856855657969222e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 54/71 | LOSS: 2.846116856909992e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 55/71 | LOSS: 2.8367366407370094e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 56/71 | LOSS: 2.8328420803677945e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 57/71 | LOSS: 2.8503263004902943e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 58/71 | LOSS: 2.8456508424192256e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 59/71 | LOSS: 2.8351784718931108e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 60/71 | LOSS: 2.8357287922771037e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 61/71 | LOSS: 2.8351777972045143e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 62/71 | LOSS: 2.8226662776894897e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 63/71 | LOSS: 2.8199045800647582e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 64/71 | LOSS: 2.83481884532823e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 65/71 | LOSS: 2.8251576843489822e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 66/71 | LOSS: 2.824669791695683e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 67/71 | LOSS: 2.8186686262522606e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 68/71 | LOSS: 2.817031802977368e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 69/71 | LOSS: 2.807336763258458e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 70/71 | LOSS: 2.794256025981027e-05\n",
      "VAL: EPOCH 40/100 | BATCH 0/8 | LOSS: 3.876844857586548e-05\n",
      "VAL: EPOCH 40/100 | BATCH 1/8 | LOSS: 3.171771277266089e-05\n",
      "VAL: EPOCH 40/100 | BATCH 2/8 | LOSS: 3.018186059004317e-05\n",
      "VAL: EPOCH 40/100 | BATCH 3/8 | LOSS: 2.964845543829142e-05\n",
      "VAL: EPOCH 40/100 | BATCH 4/8 | LOSS: 2.880004649341572e-05\n",
      "VAL: EPOCH 40/100 | BATCH 5/8 | LOSS: 2.8006154631536145e-05\n",
      "VAL: EPOCH 40/100 | BATCH 6/8 | LOSS: 2.8691122549519476e-05\n",
      "VAL: EPOCH 40/100 | BATCH 7/8 | LOSS: 2.936747523563099e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 0/71 | LOSS: 2.2055148292565718e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 1/71 | LOSS: 2.4648064936627634e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 2/71 | LOSS: 2.3048172806738876e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 3/71 | LOSS: 2.3891476757853525e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 4/71 | LOSS: 2.4535915508749893e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 5/71 | LOSS: 2.4141133508237544e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 6/71 | LOSS: 2.418381960264274e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 7/71 | LOSS: 2.39572295868129e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 8/71 | LOSS: 2.3271186768801676e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 9/71 | LOSS: 2.3200624491437337e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 10/71 | LOSS: 2.3357070445358246e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 11/71 | LOSS: 2.3309398026564548e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 12/71 | LOSS: 2.3438481198704372e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 13/71 | LOSS: 2.371004718172896e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 14/71 | LOSS: 2.410183127115791e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 15/71 | LOSS: 2.4980786974992952e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 16/71 | LOSS: 2.5346960148239948e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 17/71 | LOSS: 2.547106659120699e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 18/71 | LOSS: 2.5683418483056715e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 19/71 | LOSS: 2.569773714640178e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 20/71 | LOSS: 2.6678405577383402e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 21/71 | LOSS: 2.6504535030240234e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 22/71 | LOSS: 2.641940077212538e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 23/71 | LOSS: 2.6467632551430142e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 24/71 | LOSS: 2.665355284989346e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 25/71 | LOSS: 2.6886081706414607e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 26/71 | LOSS: 2.6890610469207684e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 27/71 | LOSS: 2.7072885976459865e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 28/71 | LOSS: 2.7432115279350848e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 29/71 | LOSS: 2.7312035126669797e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 30/71 | LOSS: 2.6984489464197276e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 31/71 | LOSS: 2.7108338429115975e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 32/71 | LOSS: 2.7071961360916525e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 33/71 | LOSS: 2.691504387792734e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 34/71 | LOSS: 2.716862680764669e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 35/71 | LOSS: 2.734404941091068e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 36/71 | LOSS: 2.7403732609535518e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 37/71 | LOSS: 2.7248025042737074e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 38/71 | LOSS: 2.725385037103059e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 39/71 | LOSS: 2.7190944683752606e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 40/71 | LOSS: 2.7200588880166436e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 41/71 | LOSS: 2.6999402940592042e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 42/71 | LOSS: 2.7020316380468673e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 43/71 | LOSS: 2.7035120555493368e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 44/71 | LOSS: 2.681842884663234e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 45/71 | LOSS: 2.6717873365471505e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 46/71 | LOSS: 2.6777685057010225e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 47/71 | LOSS: 2.666364074836262e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 48/71 | LOSS: 2.6645168668901718e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 49/71 | LOSS: 2.670078356459271e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 50/71 | LOSS: 2.665202633829276e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 51/71 | LOSS: 2.6512704817622973e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 52/71 | LOSS: 2.640589905841241e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 53/71 | LOSS: 2.639934333543621e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 54/71 | LOSS: 2.6343999872386288e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 55/71 | LOSS: 2.630684561414195e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 56/71 | LOSS: 2.6251518296735493e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 57/71 | LOSS: 2.6122326556199777e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 58/71 | LOSS: 2.6182730912065163e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 59/71 | LOSS: 2.6206786636369846e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 60/71 | LOSS: 2.6244795603294032e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 61/71 | LOSS: 2.6313011902299964e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 62/71 | LOSS: 2.6283304650282915e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 63/71 | LOSS: 2.6271834968838448e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 64/71 | LOSS: 2.633077123251636e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 65/71 | LOSS: 2.6261910947637674e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 66/71 | LOSS: 2.6363473698981592e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 67/71 | LOSS: 2.6279651111133932e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 68/71 | LOSS: 2.6319819617260624e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 69/71 | LOSS: 2.6243537649861538e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 70/71 | LOSS: 2.627226109632855e-05\n",
      "VAL: EPOCH 41/100 | BATCH 0/8 | LOSS: 3.588050458347425e-05\n",
      "VAL: EPOCH 41/100 | BATCH 1/8 | LOSS: 2.9210075808805414e-05\n",
      "VAL: EPOCH 41/100 | BATCH 2/8 | LOSS: 2.7978909201920033e-05\n",
      "VAL: EPOCH 41/100 | BATCH 3/8 | LOSS: 2.7013622457161546e-05\n",
      "VAL: EPOCH 41/100 | BATCH 4/8 | LOSS: 2.609916846267879e-05\n",
      "VAL: EPOCH 41/100 | BATCH 5/8 | LOSS: 2.5195021104688447e-05\n",
      "VAL: EPOCH 41/100 | BATCH 6/8 | LOSS: 2.5683680827829187e-05\n",
      "VAL: EPOCH 41/100 | BATCH 7/8 | LOSS: 2.62864489286585e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 0/71 | LOSS: 2.1161173208383843e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 1/71 | LOSS: 2.2543128579854965e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 2/71 | LOSS: 2.6376591752826545e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 3/71 | LOSS: 2.5634145913500106e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 4/71 | LOSS: 2.592175696918275e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 5/71 | LOSS: 2.6183422960457392e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 6/71 | LOSS: 2.6801572728020672e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 7/71 | LOSS: 2.5864616418402875e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 8/71 | LOSS: 2.554243110353127e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 9/71 | LOSS: 2.5953245676646473e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 10/71 | LOSS: 2.57385894325456e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 11/71 | LOSS: 2.6239324900719414e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 12/71 | LOSS: 2.624737988718642e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 13/71 | LOSS: 2.6138979592360556e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 14/71 | LOSS: 2.6142800440235684e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 15/71 | LOSS: 2.6599659577186685e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 16/71 | LOSS: 2.6625720398870352e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 17/71 | LOSS: 2.65392552844585e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 18/71 | LOSS: 2.605140061381175e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 19/71 | LOSS: 2.581998842288158e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 20/71 | LOSS: 2.5650679509410457e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 21/71 | LOSS: 2.6030083311277188e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 22/71 | LOSS: 2.6537822281091433e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 23/71 | LOSS: 2.659102218179517e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 24/71 | LOSS: 2.6418846973683686e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 25/71 | LOSS: 2.6160417921626224e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 26/71 | LOSS: 2.6428788260091096e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 27/71 | LOSS: 2.636777194961074e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 28/71 | LOSS: 2.638471314326282e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 29/71 | LOSS: 2.6305936262360775e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 30/71 | LOSS: 2.627997556344546e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 31/71 | LOSS: 2.635970321307468e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 32/71 | LOSS: 2.633497785030737e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 33/71 | LOSS: 2.621602230005117e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 34/71 | LOSS: 2.629575397544873e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 35/71 | LOSS: 2.6291814972662702e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 36/71 | LOSS: 2.6150753959931585e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 37/71 | LOSS: 2.597225983136971e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 38/71 | LOSS: 2.5764436055070313e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 39/71 | LOSS: 2.5803985363381798e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 40/71 | LOSS: 2.5769038363317277e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 41/71 | LOSS: 2.573609435785329e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 42/71 | LOSS: 2.556939406806419e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 43/71 | LOSS: 2.546481007150512e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 44/71 | LOSS: 2.5424240569312435e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 45/71 | LOSS: 2.5442710743252814e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 46/71 | LOSS: 2.551825259325461e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 47/71 | LOSS: 2.5466720558142697e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 48/71 | LOSS: 2.5466093723721594e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 49/71 | LOSS: 2.5830753329501022e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 50/71 | LOSS: 2.5852990659711625e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 51/71 | LOSS: 2.605664261724227e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 52/71 | LOSS: 2.590368305116602e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 53/71 | LOSS: 2.5762446259452394e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 54/71 | LOSS: 2.570077376731206e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 55/71 | LOSS: 2.5639306646293596e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 56/71 | LOSS: 2.565022767061907e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 57/71 | LOSS: 2.5689103620544332e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 58/71 | LOSS: 2.568542981476886e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 59/71 | LOSS: 2.5643163765683616e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 60/71 | LOSS: 2.567648456921442e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 61/71 | LOSS: 2.5603070929308514e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 62/71 | LOSS: 2.567180562769026e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 63/71 | LOSS: 2.5660639892066683e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 64/71 | LOSS: 2.5662741049577912e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 65/71 | LOSS: 2.5544829882026974e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 66/71 | LOSS: 2.5521260971827222e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 67/71 | LOSS: 2.5633400341843406e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 68/71 | LOSS: 2.5486794676407293e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 69/71 | LOSS: 2.5439429399349528e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 70/71 | LOSS: 2.5275108974954133e-05\n",
      "VAL: EPOCH 42/100 | BATCH 0/8 | LOSS: 3.445435140747577e-05\n",
      "VAL: EPOCH 42/100 | BATCH 1/8 | LOSS: 2.770442188193556e-05\n",
      "VAL: EPOCH 42/100 | BATCH 2/8 | LOSS: 2.676150506886188e-05\n",
      "VAL: EPOCH 42/100 | BATCH 3/8 | LOSS: 2.567580713730422e-05\n",
      "VAL: EPOCH 42/100 | BATCH 4/8 | LOSS: 2.498762696632184e-05\n",
      "VAL: EPOCH 42/100 | BATCH 5/8 | LOSS: 2.405212884089754e-05\n",
      "VAL: EPOCH 42/100 | BATCH 6/8 | LOSS: 2.4266200073595557e-05\n",
      "VAL: EPOCH 42/100 | BATCH 7/8 | LOSS: 2.468400361976819e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 0/71 | LOSS: 2.2414220438804477e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 1/71 | LOSS: 2.2524911400978453e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 2/71 | LOSS: 2.390065249831726e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 3/71 | LOSS: 2.2669732970825862e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 4/71 | LOSS: 2.314254961675033e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 5/71 | LOSS: 2.4545949903161574e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 6/71 | LOSS: 2.5552443860630903e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 7/71 | LOSS: 2.502452161934343e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 8/71 | LOSS: 2.470346710955103e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 9/71 | LOSS: 2.4141200992744415e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 10/71 | LOSS: 2.3915439504145812e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 11/71 | LOSS: 2.4500432270239496e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 12/71 | LOSS: 2.445319296384696e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 13/71 | LOSS: 2.4061425132718534e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 14/71 | LOSS: 2.3661678399851858e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 15/71 | LOSS: 2.3624365326213592e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 16/71 | LOSS: 2.4168144270387367e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 17/71 | LOSS: 2.4130200472427532e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 18/71 | LOSS: 2.3972198840439025e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 19/71 | LOSS: 2.4176781698770357e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 20/71 | LOSS: 2.4058118946240504e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 21/71 | LOSS: 2.4019202845176384e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 22/71 | LOSS: 2.4318695938310828e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 23/71 | LOSS: 2.4501668728286557e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 24/71 | LOSS: 2.4654880835441873e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 25/71 | LOSS: 2.4651162605620742e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 26/71 | LOSS: 2.469520478809028e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 27/71 | LOSS: 2.448717129612175e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 28/71 | LOSS: 2.419440214249209e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 29/71 | LOSS: 2.4375197851137877e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 30/71 | LOSS: 2.4089394379012858e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 31/71 | LOSS: 2.3995120443487394e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 32/71 | LOSS: 2.399171182443035e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 33/71 | LOSS: 2.3923814885218983e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 34/71 | LOSS: 2.3935867284308186e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 35/71 | LOSS: 2.3846541454177997e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 36/71 | LOSS: 2.3932593673659563e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 37/71 | LOSS: 2.385145906277773e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 38/71 | LOSS: 2.373706005388298e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 39/71 | LOSS: 2.3920215244288557e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 40/71 | LOSS: 2.4015096242670198e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 41/71 | LOSS: 2.396964252208515e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 42/71 | LOSS: 2.399057907601927e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 43/71 | LOSS: 2.4057332873119528e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 44/71 | LOSS: 2.4238053326068134e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 45/71 | LOSS: 2.4164480949010517e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 46/71 | LOSS: 2.409340142665005e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 47/71 | LOSS: 2.409873908012135e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 48/71 | LOSS: 2.4169120980350644e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 49/71 | LOSS: 2.423481950245332e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 50/71 | LOSS: 2.43152093496653e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 51/71 | LOSS: 2.423330449976045e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 52/71 | LOSS: 2.4148815907021754e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 53/71 | LOSS: 2.4277506377580317e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 54/71 | LOSS: 2.4294661885836502e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 55/71 | LOSS: 2.4240908844928655e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 56/71 | LOSS: 2.425139727915449e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 57/71 | LOSS: 2.42139711441143e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 58/71 | LOSS: 2.4202775590711714e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 59/71 | LOSS: 2.415036418218127e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 60/71 | LOSS: 2.412976752312999e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 61/71 | LOSS: 2.4098177985457192e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 62/71 | LOSS: 2.4007740677685464e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 63/71 | LOSS: 2.4080508296719927e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 64/71 | LOSS: 2.410982057111911e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 65/71 | LOSS: 2.4100360116625037e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 66/71 | LOSS: 2.4094257078918893e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 67/71 | LOSS: 2.411141766809605e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 68/71 | LOSS: 2.4008080984366234e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 69/71 | LOSS: 2.3960173374299693e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 70/71 | LOSS: 2.3846639777102483e-05\n",
      "VAL: EPOCH 43/100 | BATCH 0/8 | LOSS: 3.164083318552002e-05\n",
      "VAL: EPOCH 43/100 | BATCH 1/8 | LOSS: 2.576009137555957e-05\n",
      "VAL: EPOCH 43/100 | BATCH 2/8 | LOSS: 2.4953356235831354e-05\n",
      "VAL: EPOCH 43/100 | BATCH 3/8 | LOSS: 2.4138255867001135e-05\n",
      "VAL: EPOCH 43/100 | BATCH 4/8 | LOSS: 2.3518142552347853e-05\n",
      "VAL: EPOCH 43/100 | BATCH 5/8 | LOSS: 2.2605307700966176e-05\n",
      "VAL: EPOCH 43/100 | BATCH 6/8 | LOSS: 2.2929667466087267e-05\n",
      "VAL: EPOCH 43/100 | BATCH 7/8 | LOSS: 2.3350333776761545e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 0/71 | LOSS: 2.438444789731875e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 1/71 | LOSS: 2.352606043132255e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 2/71 | LOSS: 2.3539278724153217e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 3/71 | LOSS: 2.3929452254378702e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 4/71 | LOSS: 2.532130529289134e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 5/71 | LOSS: 2.5077050243756577e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 6/71 | LOSS: 2.573015210925535e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 7/71 | LOSS: 2.5727992806423572e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 8/71 | LOSS: 2.589441343540481e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 9/71 | LOSS: 2.6130362130061258e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 10/71 | LOSS: 2.6130195692530833e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 11/71 | LOSS: 2.5645117299670044e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 12/71 | LOSS: 2.526124766378556e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 13/71 | LOSS: 2.527876404201379e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 14/71 | LOSS: 2.501740527804941e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 15/71 | LOSS: 2.4747608904363005e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 16/71 | LOSS: 2.4648066541618283e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 17/71 | LOSS: 2.4744794144579726e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 18/71 | LOSS: 2.460366474559506e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 19/71 | LOSS: 2.442152053845348e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 20/71 | LOSS: 2.429637468248118e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 21/71 | LOSS: 2.4303534618494186e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 22/71 | LOSS: 2.4092710468128484e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 23/71 | LOSS: 2.4158362293746904e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 24/71 | LOSS: 2.4206908929045312e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 25/71 | LOSS: 2.420395300690031e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 26/71 | LOSS: 2.421318452063672e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 27/71 | LOSS: 2.405212788809357e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 28/71 | LOSS: 2.4267465991654497e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 29/71 | LOSS: 2.42335518123582e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 30/71 | LOSS: 2.3999603135669004e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 31/71 | LOSS: 2.407658718084349e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 32/71 | LOSS: 2.4257365746024966e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 33/71 | LOSS: 2.438587564350147e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 34/71 | LOSS: 2.431116814217863e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 35/71 | LOSS: 2.4095228430572508e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 36/71 | LOSS: 2.4153812528065583e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 37/71 | LOSS: 2.3993869227093744e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 38/71 | LOSS: 2.3881787012961024e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 39/71 | LOSS: 2.403623780082853e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 40/71 | LOSS: 2.3988401242205305e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 41/71 | LOSS: 2.3940078963711303e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 42/71 | LOSS: 2.3807082386135102e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 43/71 | LOSS: 2.3817725460420743e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 44/71 | LOSS: 2.3759841699049706e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 45/71 | LOSS: 2.3729681822220776e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 46/71 | LOSS: 2.3652872922421592e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 47/71 | LOSS: 2.3711171365903283e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 48/71 | LOSS: 2.368263386062831e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 49/71 | LOSS: 2.365016222029226e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 50/71 | LOSS: 2.362283157708589e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 51/71 | LOSS: 2.3636806732751072e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 52/71 | LOSS: 2.3639849321213814e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 53/71 | LOSS: 2.3615143449946857e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 54/71 | LOSS: 2.3550380071603946e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 55/71 | LOSS: 2.36187780891279e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 56/71 | LOSS: 2.353133245298778e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 57/71 | LOSS: 2.3547451589898817e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 58/71 | LOSS: 2.350053466092874e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 59/71 | LOSS: 2.3521838162802548e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 60/71 | LOSS: 2.3636865825821706e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 61/71 | LOSS: 2.3597676772624254e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 62/71 | LOSS: 2.3548425353568284e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 63/71 | LOSS: 2.3599313522026932e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 64/71 | LOSS: 2.3582744100937048e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 65/71 | LOSS: 2.3552080385729134e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 66/71 | LOSS: 2.3493271966337976e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 67/71 | LOSS: 2.344681254672811e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 68/71 | LOSS: 2.3445246383282896e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 69/71 | LOSS: 2.3349919112141443e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 70/71 | LOSS: 2.3325285184628355e-05\n",
      "VAL: EPOCH 44/100 | BATCH 0/8 | LOSS: 3.3626085496507585e-05\n",
      "VAL: EPOCH 44/100 | BATCH 1/8 | LOSS: 2.8709535399684682e-05\n",
      "VAL: EPOCH 44/100 | BATCH 2/8 | LOSS: 2.8382104574120604e-05\n",
      "VAL: EPOCH 44/100 | BATCH 3/8 | LOSS: 2.6824283850146458e-05\n",
      "VAL: EPOCH 44/100 | BATCH 4/8 | LOSS: 2.612167700135615e-05\n",
      "VAL: EPOCH 44/100 | BATCH 5/8 | LOSS: 2.5275232170921907e-05\n",
      "VAL: EPOCH 44/100 | BATCH 6/8 | LOSS: 2.554207899915387e-05\n",
      "VAL: EPOCH 44/100 | BATCH 7/8 | LOSS: 2.5834279540504212e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 0/71 | LOSS: 2.2946343960938975e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 1/71 | LOSS: 2.4470508833474014e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 2/71 | LOSS: 2.5289980233840954e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 3/71 | LOSS: 2.5983267278206768e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 4/71 | LOSS: 2.6376133246230892e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 5/71 | LOSS: 2.7654500627249945e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 6/71 | LOSS: 2.774280515690667e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 7/71 | LOSS: 2.6767785357151297e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 8/71 | LOSS: 2.7324808090472492e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 9/71 | LOSS: 2.7197755116503686e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 10/71 | LOSS: 2.7320063633272763e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 11/71 | LOSS: 2.6562467913511984e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 12/71 | LOSS: 2.649601670922353e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 13/71 | LOSS: 2.6205956341332888e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 14/71 | LOSS: 2.5645830949846033e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 15/71 | LOSS: 2.5459968810537248e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 16/71 | LOSS: 2.5390812772778136e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 17/71 | LOSS: 2.514250000887033e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 18/71 | LOSS: 2.4938779939762562e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 19/71 | LOSS: 2.4707536158530275e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 20/71 | LOSS: 2.4664423108333722e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 21/71 | LOSS: 2.435161695757415e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 22/71 | LOSS: 2.407262091646376e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 23/71 | LOSS: 2.3746570756581303e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 24/71 | LOSS: 2.3659447688260116e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 25/71 | LOSS: 2.3498638186608263e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 26/71 | LOSS: 2.331470292389255e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 27/71 | LOSS: 2.331333871552488e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 28/71 | LOSS: 2.3295389712137845e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 29/71 | LOSS: 2.3280583021308607e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 30/71 | LOSS: 2.3187887915898294e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 31/71 | LOSS: 2.314858045338042e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 32/71 | LOSS: 2.3080764852541808e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 33/71 | LOSS: 2.3009556678195676e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 34/71 | LOSS: 2.3217284199615408e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 35/71 | LOSS: 2.3155163792883588e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 36/71 | LOSS: 2.329981725779362e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 37/71 | LOSS: 2.3275649781632973e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 38/71 | LOSS: 2.328559322706543e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 39/71 | LOSS: 2.3367587709799408e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 40/71 | LOSS: 2.3287753808800495e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 41/71 | LOSS: 2.3498111096116536e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 42/71 | LOSS: 2.3492757633036045e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 43/71 | LOSS: 2.3654510187043343e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 44/71 | LOSS: 2.366761408565152e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 45/71 | LOSS: 2.372798893623479e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 46/71 | LOSS: 2.3768505420872665e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 47/71 | LOSS: 2.3812302477684472e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 48/71 | LOSS: 2.38851646697315e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 49/71 | LOSS: 2.3833472696423996e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 50/71 | LOSS: 2.3860675355535932e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 51/71 | LOSS: 2.382104800674894e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 52/71 | LOSS: 2.370981046795968e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 53/71 | LOSS: 2.3773780008519276e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 54/71 | LOSS: 2.37606237616009e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 55/71 | LOSS: 2.3626929630284265e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 56/71 | LOSS: 2.3531866374245862e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 57/71 | LOSS: 2.3559006124209002e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 58/71 | LOSS: 2.3554569560150472e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 59/71 | LOSS: 2.368358024493015e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 60/71 | LOSS: 2.3660609959271646e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 61/71 | LOSS: 2.3601520326572485e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 62/71 | LOSS: 2.354044221329599e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 63/71 | LOSS: 2.3633107218756777e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 64/71 | LOSS: 2.358035694319719e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 65/71 | LOSS: 2.359614995150354e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 66/71 | LOSS: 2.354262818047193e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 67/71 | LOSS: 2.3539030494515936e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 68/71 | LOSS: 2.3562247920578912e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 69/71 | LOSS: 2.350878590472608e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 70/71 | LOSS: 2.36395490632481e-05\n",
      "VAL: EPOCH 45/100 | BATCH 0/8 | LOSS: 3.3543663448654115e-05\n",
      "VAL: EPOCH 45/100 | BATCH 1/8 | LOSS: 2.7887644137081224e-05\n",
      "VAL: EPOCH 45/100 | BATCH 2/8 | LOSS: 2.7261361537966877e-05\n",
      "VAL: EPOCH 45/100 | BATCH 3/8 | LOSS: 2.5756535251275636e-05\n",
      "VAL: EPOCH 45/100 | BATCH 4/8 | LOSS: 2.5251763872802258e-05\n",
      "VAL: EPOCH 45/100 | BATCH 5/8 | LOSS: 2.434342968626879e-05\n",
      "VAL: EPOCH 45/100 | BATCH 6/8 | LOSS: 2.4555605445389767e-05\n",
      "VAL: EPOCH 45/100 | BATCH 7/8 | LOSS: 2.4806875671856687e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 0/71 | LOSS: 2.2715481463819742e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 1/71 | LOSS: 2.1997510884830263e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 2/71 | LOSS: 2.2282203038533528e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 3/71 | LOSS: 2.233581653854344e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 4/71 | LOSS: 2.352378869545646e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 5/71 | LOSS: 2.3009078480148066e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 6/71 | LOSS: 2.3497018316577722e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 7/71 | LOSS: 2.3531596298198565e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 8/71 | LOSS: 2.33994119803861e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 9/71 | LOSS: 2.3880863773229065e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 10/71 | LOSS: 2.398869153032799e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 11/71 | LOSS: 2.367969106368643e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 12/71 | LOSS: 2.3509169505604615e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 13/71 | LOSS: 2.3371162309818567e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 14/71 | LOSS: 2.3282299055911913e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 15/71 | LOSS: 2.3561340640299022e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 16/71 | LOSS: 2.3819249155862697e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 17/71 | LOSS: 2.3574820564438898e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 18/71 | LOSS: 2.352487756166068e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 19/71 | LOSS: 2.3955146934895312e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 20/71 | LOSS: 2.3800472609720946e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 21/71 | LOSS: 2.364136641765733e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 22/71 | LOSS: 2.40785115950174e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 23/71 | LOSS: 2.3903572885804653e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 24/71 | LOSS: 2.407632222457323e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 25/71 | LOSS: 2.3895442609500606e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 26/71 | LOSS: 2.3753621631099292e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 27/71 | LOSS: 2.373287549874346e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 28/71 | LOSS: 2.3637442139041576e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 29/71 | LOSS: 2.3542321529627466e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 30/71 | LOSS: 2.3355788265865656e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 31/71 | LOSS: 2.335576192535882e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 32/71 | LOSS: 2.326962815211366e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 33/71 | LOSS: 2.328946929457529e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 34/71 | LOSS: 2.312440841965976e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 35/71 | LOSS: 2.336492773085613e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 36/71 | LOSS: 2.3272024230430583e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 37/71 | LOSS: 2.325880751940483e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 38/71 | LOSS: 2.321108051826461e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 39/71 | LOSS: 2.3256993381437495e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 40/71 | LOSS: 2.32030672078708e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 41/71 | LOSS: 2.324013842423613e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 42/71 | LOSS: 2.3177169612947265e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 43/71 | LOSS: 2.3350839290453056e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 44/71 | LOSS: 2.3329551064913783e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 45/71 | LOSS: 2.3342830227984056e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 46/71 | LOSS: 2.341125737268854e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 47/71 | LOSS: 2.33036414556409e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 48/71 | LOSS: 2.3272992398326134e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 49/71 | LOSS: 2.3520145077782217e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 50/71 | LOSS: 2.3627392140185167e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 51/71 | LOSS: 2.365947998091892e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 52/71 | LOSS: 2.380002462025073e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 53/71 | LOSS: 2.3696445370862624e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 54/71 | LOSS: 2.3777694315702486e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 55/71 | LOSS: 2.3890930184019714e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 56/71 | LOSS: 2.3782993338771346e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 57/71 | LOSS: 2.375858567292794e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 58/71 | LOSS: 2.3771826571477103e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 59/71 | LOSS: 2.386489207613825e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 60/71 | LOSS: 2.377037964804197e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 61/71 | LOSS: 2.3815162979102048e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 62/71 | LOSS: 2.38740870820096e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 63/71 | LOSS: 2.3776838446565307e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 64/71 | LOSS: 2.3718377157750254e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 65/71 | LOSS: 2.3785944683891792e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 66/71 | LOSS: 2.3798071441575954e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 67/71 | LOSS: 2.3719032897668728e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 68/71 | LOSS: 2.375432442299232e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 69/71 | LOSS: 2.369266351576828e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 70/71 | LOSS: 2.368462982060368e-05\n",
      "VAL: EPOCH 46/100 | BATCH 0/8 | LOSS: 2.983490048791282e-05\n",
      "VAL: EPOCH 46/100 | BATCH 1/8 | LOSS: 2.5069019102375023e-05\n",
      "VAL: EPOCH 46/100 | BATCH 2/8 | LOSS: 2.4618466341053136e-05\n",
      "VAL: EPOCH 46/100 | BATCH 3/8 | LOSS: 2.4031108296185266e-05\n",
      "VAL: EPOCH 46/100 | BATCH 4/8 | LOSS: 2.334960845473688e-05\n",
      "VAL: EPOCH 46/100 | BATCH 5/8 | LOSS: 2.2719361065052606e-05\n",
      "VAL: EPOCH 46/100 | BATCH 6/8 | LOSS: 2.273797222837207e-05\n",
      "VAL: EPOCH 46/100 | BATCH 7/8 | LOSS: 2.3037277514958987e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 0/71 | LOSS: 2.169641084037721e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 1/71 | LOSS: 2.350392060179729e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 2/71 | LOSS: 2.401140106182235e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 3/71 | LOSS: 2.3342645363300107e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 4/71 | LOSS: 2.3119129036786034e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 5/71 | LOSS: 2.2832560413614072e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 6/71 | LOSS: 2.228743453867667e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 7/71 | LOSS: 2.1977465394229512e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 8/71 | LOSS: 2.1671505439573797e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 9/71 | LOSS: 2.169692270399537e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 10/71 | LOSS: 2.1130873475075614e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 11/71 | LOSS: 2.136053414384757e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 12/71 | LOSS: 2.1383126015559985e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 13/71 | LOSS: 2.1892510111294022e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 14/71 | LOSS: 2.1764266421087086e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 15/71 | LOSS: 2.2057145770304487e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 16/71 | LOSS: 2.1815346211523694e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 17/71 | LOSS: 2.1613212892488162e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 18/71 | LOSS: 2.155569046934895e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 19/71 | LOSS: 2.162073396902997e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 20/71 | LOSS: 2.1798914791101458e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 21/71 | LOSS: 2.2093889128882438e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 22/71 | LOSS: 2.210620866959368e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 23/71 | LOSS: 2.2036705937959294e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 24/71 | LOSS: 2.2046673984732478e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 25/71 | LOSS: 2.201643152287803e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 26/71 | LOSS: 2.214415198304104e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 27/71 | LOSS: 2.2186076096529306e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 28/71 | LOSS: 2.2223179069536353e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 29/71 | LOSS: 2.2033061274366143e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 30/71 | LOSS: 2.191261075729985e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 31/71 | LOSS: 2.197238984535943e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 32/71 | LOSS: 2.1921588499522343e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 33/71 | LOSS: 2.1911421968338e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 34/71 | LOSS: 2.213090613492698e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 35/71 | LOSS: 2.201713318249353e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 36/71 | LOSS: 2.1825633986146705e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 37/71 | LOSS: 2.1861760112522844e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 38/71 | LOSS: 2.1828754609323834e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 39/71 | LOSS: 2.199011485117808e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 40/71 | LOSS: 2.217215046030327e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 41/71 | LOSS: 2.219080928019442e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 42/71 | LOSS: 2.2197144252734074e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 43/71 | LOSS: 2.2153659901133242e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 44/71 | LOSS: 2.2230104130155977e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 45/71 | LOSS: 2.217548936918636e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 46/71 | LOSS: 2.2146294890881883e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 47/71 | LOSS: 2.2132794413209922e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 48/71 | LOSS: 2.2052396922603924e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 49/71 | LOSS: 2.1897140049986775e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 50/71 | LOSS: 2.1806085219306588e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 51/71 | LOSS: 2.1879147442842412e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 52/71 | LOSS: 2.18238685433017e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 53/71 | LOSS: 2.18926728073846e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 54/71 | LOSS: 2.1812147662620355e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 55/71 | LOSS: 2.1708996022035925e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 56/71 | LOSS: 2.1665070466712533e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 57/71 | LOSS: 2.1688803497391777e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 58/71 | LOSS: 2.1676057813392156e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 59/71 | LOSS: 2.1732872952876883e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 60/71 | LOSS: 2.1659469059129344e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 61/71 | LOSS: 2.1688013138402546e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 62/71 | LOSS: 2.178307400638929e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 63/71 | LOSS: 2.1790977726254823e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 64/71 | LOSS: 2.176938350413156e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 65/71 | LOSS: 2.1889885346988166e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 66/71 | LOSS: 2.195367826278749e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 67/71 | LOSS: 2.1882647522087447e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 68/71 | LOSS: 2.1894871797880825e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 69/71 | LOSS: 2.1852522500661766e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 70/71 | LOSS: 2.1869418522097912e-05\n",
      "VAL: EPOCH 47/100 | BATCH 0/8 | LOSS: 2.8522730644908734e-05\n",
      "VAL: EPOCH 47/100 | BATCH 1/8 | LOSS: 2.410978686384624e-05\n",
      "VAL: EPOCH 47/100 | BATCH 2/8 | LOSS: 2.3792358964177158e-05\n",
      "VAL: EPOCH 47/100 | BATCH 3/8 | LOSS: 2.3618539671588223e-05\n",
      "VAL: EPOCH 47/100 | BATCH 4/8 | LOSS: 2.3213607346406206e-05\n",
      "VAL: EPOCH 47/100 | BATCH 5/8 | LOSS: 2.2397458451450802e-05\n",
      "VAL: EPOCH 47/100 | BATCH 6/8 | LOSS: 2.2719739458157813e-05\n",
      "VAL: EPOCH 47/100 | BATCH 7/8 | LOSS: 2.3368553002001136e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 0/71 | LOSS: 1.8965005438076332e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 1/71 | LOSS: 2.512119317543693e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 2/71 | LOSS: 2.2261778819180716e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 3/71 | LOSS: 2.2780259314458817e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 4/71 | LOSS: 2.472929481882602e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 5/71 | LOSS: 2.3543129827885423e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 6/71 | LOSS: 2.2582505282896038e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 7/71 | LOSS: 2.244300753773132e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 8/71 | LOSS: 2.2623069324052063e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 9/71 | LOSS: 2.2020539290679153e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 10/71 | LOSS: 2.2384850374444134e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 11/71 | LOSS: 2.224510444648331e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 12/71 | LOSS: 2.1720079950262934e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 13/71 | LOSS: 2.180787617232584e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 14/71 | LOSS: 2.1429402719756276e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 15/71 | LOSS: 2.1059617210994475e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 16/71 | LOSS: 2.117481241892914e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 17/71 | LOSS: 2.1574725603083305e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 18/71 | LOSS: 2.1860469802815812e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 19/71 | LOSS: 2.143980282198754e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 20/71 | LOSS: 2.1330696957496304e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 21/71 | LOSS: 2.1144153420622885e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 22/71 | LOSS: 2.1287692304569013e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 23/71 | LOSS: 2.1703717872393707e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 24/71 | LOSS: 2.158255985705182e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 25/71 | LOSS: 2.14793551724422e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 26/71 | LOSS: 2.1394901489623374e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 27/71 | LOSS: 2.1176733948128197e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 28/71 | LOSS: 2.110794537594333e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 29/71 | LOSS: 2.101674975468389e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 30/71 | LOSS: 2.100257123886202e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 31/71 | LOSS: 2.082396406422049e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 32/71 | LOSS: 2.0903118693791896e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 33/71 | LOSS: 2.086574273990289e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 34/71 | LOSS: 2.0816485526406073e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 35/71 | LOSS: 2.072349121186158e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 36/71 | LOSS: 2.0782975224318687e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 37/71 | LOSS: 2.076974813366848e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 38/71 | LOSS: 2.0673303179836905e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 39/71 | LOSS: 2.0671618904088974e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 40/71 | LOSS: 2.0610999455107604e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 41/71 | LOSS: 2.0547173768309656e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 42/71 | LOSS: 2.0521452698144564e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 43/71 | LOSS: 2.0545617933914233e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 44/71 | LOSS: 2.0501776776250658e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 45/71 | LOSS: 2.057437738941466e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 46/71 | LOSS: 2.0605779587021196e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 47/71 | LOSS: 2.0552494371865276e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 48/71 | LOSS: 2.0539717000673942e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 49/71 | LOSS: 2.0822900514758656e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 50/71 | LOSS: 2.080788733877068e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 51/71 | LOSS: 2.082676767783526e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 52/71 | LOSS: 2.079113008496176e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 53/71 | LOSS: 2.0830140783800744e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 54/71 | LOSS: 2.07897144719027e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 55/71 | LOSS: 2.081110168156946e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 56/71 | LOSS: 2.0897012961397292e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 57/71 | LOSS: 2.092658137528282e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 58/71 | LOSS: 2.0935165193188398e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 59/71 | LOSS: 2.089601694024168e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 60/71 | LOSS: 2.0869298586163853e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 61/71 | LOSS: 2.0912982519451275e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 62/71 | LOSS: 2.100884041052297e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 63/71 | LOSS: 2.0998724210130604e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 64/71 | LOSS: 2.093459773347534e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 65/71 | LOSS: 2.0851198783158466e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 66/71 | LOSS: 2.0817963328471393e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 67/71 | LOSS: 2.0772683295517173e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 68/71 | LOSS: 2.0744488448026065e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 69/71 | LOSS: 2.0742983282876334e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 70/71 | LOSS: 2.076722447666943e-05\n",
      "VAL: EPOCH 48/100 | BATCH 0/8 | LOSS: 2.8433092666091397e-05\n",
      "VAL: EPOCH 48/100 | BATCH 1/8 | LOSS: 2.4439494154648855e-05\n",
      "VAL: EPOCH 48/100 | BATCH 2/8 | LOSS: 2.3902864389431972e-05\n",
      "VAL: EPOCH 48/100 | BATCH 3/8 | LOSS: 2.293593297508778e-05\n",
      "VAL: EPOCH 48/100 | BATCH 4/8 | LOSS: 2.2200578678166492e-05\n",
      "VAL: EPOCH 48/100 | BATCH 5/8 | LOSS: 2.1790277969557792e-05\n",
      "VAL: EPOCH 48/100 | BATCH 6/8 | LOSS: 2.203937037847936e-05\n",
      "VAL: EPOCH 48/100 | BATCH 7/8 | LOSS: 2.23498138893774e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 0/71 | LOSS: 2.6296591386198997e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 1/71 | LOSS: 2.3209564460557885e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 2/71 | LOSS: 2.2168285795487463e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 3/71 | LOSS: 2.2148650714370888e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 4/71 | LOSS: 2.2052479471312837e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 5/71 | LOSS: 2.1716353330702987e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 6/71 | LOSS: 2.0892974134767428e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 7/71 | LOSS: 2.1812918930663727e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 8/71 | LOSS: 2.163338892084236e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 9/71 | LOSS: 2.2891250773682258e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 10/71 | LOSS: 2.271592662013559e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 11/71 | LOSS: 2.2880605077565026e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 12/71 | LOSS: 2.2118239100494135e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 13/71 | LOSS: 2.1887465533966732e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 14/71 | LOSS: 2.222286530013662e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 15/71 | LOSS: 2.201661948220135e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 16/71 | LOSS: 2.1863369136767954e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 17/71 | LOSS: 2.1493202540215054e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 18/71 | LOSS: 2.1383653993804107e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 19/71 | LOSS: 2.1542448666878045e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 20/71 | LOSS: 2.1569370541588535e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 21/71 | LOSS: 2.1361908941550858e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 22/71 | LOSS: 2.125825915794374e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 23/71 | LOSS: 2.1335220102021896e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 24/71 | LOSS: 2.1476440670085138e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 25/71 | LOSS: 2.1386600075437364e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 26/71 | LOSS: 2.131972083858542e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 27/71 | LOSS: 2.1179908669312552e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 28/71 | LOSS: 2.1084152931821178e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 29/71 | LOSS: 2.110533738838664e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 30/71 | LOSS: 2.0884544556470394e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 31/71 | LOSS: 2.09069115442162e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 32/71 | LOSS: 2.109674126889634e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 33/71 | LOSS: 2.1105550411662346e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 34/71 | LOSS: 2.110629998891714e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 35/71 | LOSS: 2.1284858702428108e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 36/71 | LOSS: 2.121834441405136e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 37/71 | LOSS: 2.1383209562037863e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 38/71 | LOSS: 2.1448771267620406e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 39/71 | LOSS: 2.135055785856821e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 40/71 | LOSS: 2.139110076713245e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 41/71 | LOSS: 2.134293557295071e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 42/71 | LOSS: 2.130609118466964e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 43/71 | LOSS: 2.1229063039837374e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 44/71 | LOSS: 2.1212543106230442e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 45/71 | LOSS: 2.1329686067316366e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 46/71 | LOSS: 2.141476535201791e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 47/71 | LOSS: 2.148287791214898e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 48/71 | LOSS: 2.1570096262677203e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 49/71 | LOSS: 2.1483440559677546e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 50/71 | LOSS: 2.161832346210562e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 51/71 | LOSS: 2.1565381810153038e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 52/71 | LOSS: 2.1717848917518803e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 53/71 | LOSS: 2.1836544857689188e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 54/71 | LOSS: 2.189218402318974e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 55/71 | LOSS: 2.1825391399943328e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 56/71 | LOSS: 2.186305935761642e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 57/71 | LOSS: 2.184661999026189e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 58/71 | LOSS: 2.1852279780309262e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 59/71 | LOSS: 2.1820369223253996e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 60/71 | LOSS: 2.180112924030314e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 61/71 | LOSS: 2.178862373071869e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 62/71 | LOSS: 2.1858936576343637e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 63/71 | LOSS: 2.1851332220990116e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 64/71 | LOSS: 2.180056744691683e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 65/71 | LOSS: 2.1774882966521105e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 66/71 | LOSS: 2.179006630163414e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 67/71 | LOSS: 2.1679933029993073e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 68/71 | LOSS: 2.1669748133243285e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 69/71 | LOSS: 2.1563956839340142e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 70/71 | LOSS: 2.1598449627201494e-05\n",
      "VAL: EPOCH 49/100 | BATCH 0/8 | LOSS: 2.6442408852744848e-05\n",
      "VAL: EPOCH 49/100 | BATCH 1/8 | LOSS: 2.193544241890777e-05\n",
      "VAL: EPOCH 49/100 | BATCH 2/8 | LOSS: 2.105734286791024e-05\n",
      "VAL: EPOCH 49/100 | BATCH 3/8 | LOSS: 2.045501514658099e-05\n",
      "VAL: EPOCH 49/100 | BATCH 4/8 | LOSS: 1.9970756693510338e-05\n",
      "VAL: EPOCH 49/100 | BATCH 5/8 | LOSS: 1.9260237725878444e-05\n",
      "VAL: EPOCH 49/100 | BATCH 6/8 | LOSS: 1.9594652777803795e-05\n",
      "VAL: EPOCH 49/100 | BATCH 7/8 | LOSS: 2.0008927776871133e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 0/71 | LOSS: 2.6646819605957717e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 1/71 | LOSS: 2.2629372324445285e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 2/71 | LOSS: 2.0630316915533815e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 3/71 | LOSS: 2.063833198917564e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 4/71 | LOSS: 2.053556490864139e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 5/71 | LOSS: 2.0764088731084485e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 6/71 | LOSS: 2.0753546258284977e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 7/71 | LOSS: 2.108139642587048e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 8/71 | LOSS: 2.1501081088596646e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 9/71 | LOSS: 2.094837745971745e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 10/71 | LOSS: 2.109124083786314e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 11/71 | LOSS: 2.0987618730335573e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 12/71 | LOSS: 2.0780515954426777e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 13/71 | LOSS: 2.1325047133099622e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 14/71 | LOSS: 2.153122732124757e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 15/71 | LOSS: 2.170601931084093e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 16/71 | LOSS: 2.120917280776399e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 17/71 | LOSS: 2.1166617140503757e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 18/71 | LOSS: 2.1080700447016982e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 19/71 | LOSS: 2.1077433575555916e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 20/71 | LOSS: 2.084799797601244e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 21/71 | LOSS: 2.0815384811033834e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 22/71 | LOSS: 2.0814069221555457e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 23/71 | LOSS: 2.0817578236650054e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 24/71 | LOSS: 2.0825146784773096e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 25/71 | LOSS: 2.0779872451944706e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 26/71 | LOSS: 2.067069531390357e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 27/71 | LOSS: 2.0701963291815317e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 28/71 | LOSS: 2.096919694128607e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 29/71 | LOSS: 2.0851712300403354e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 30/71 | LOSS: 2.0675127269632575e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 31/71 | LOSS: 2.0782106730621308e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 32/71 | LOSS: 2.0706093588766333e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 33/71 | LOSS: 2.0736938334748094e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 34/71 | LOSS: 2.0677067316553023e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 35/71 | LOSS: 2.0581513631946615e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 36/71 | LOSS: 2.070713688570666e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 37/71 | LOSS: 2.058650472080396e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 38/71 | LOSS: 2.0610830268616646e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 39/71 | LOSS: 2.0597438015101945e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 40/71 | LOSS: 2.0538852124179644e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 41/71 | LOSS: 2.054480665676584e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 42/71 | LOSS: 2.0516057110250775e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 43/71 | LOSS: 2.0475687217020262e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 44/71 | LOSS: 2.057218710736682e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 45/71 | LOSS: 2.0506202378348224e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 46/71 | LOSS: 2.063005829652266e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 47/71 | LOSS: 2.0515689281334442e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 48/71 | LOSS: 2.052975201988783e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 49/71 | LOSS: 2.0699140804936178e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 50/71 | LOSS: 2.0682075189017948e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 51/71 | LOSS: 2.06606898175848e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 52/71 | LOSS: 2.0701709389576758e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 53/71 | LOSS: 2.0653886908635116e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 54/71 | LOSS: 2.062783248468556e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 55/71 | LOSS: 2.0611849939606535e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 56/71 | LOSS: 2.0601217903803398e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 57/71 | LOSS: 2.0662838198467367e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 58/71 | LOSS: 2.0644012662252585e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 59/71 | LOSS: 2.0643936143945513e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 60/71 | LOSS: 2.055184062347045e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 61/71 | LOSS: 2.0560383657744574e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 62/71 | LOSS: 2.0512915239484596e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 63/71 | LOSS: 2.0503927942172595e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 64/71 | LOSS: 2.044854176906833e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 65/71 | LOSS: 2.0355094337984074e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 66/71 | LOSS: 2.0270597190967997e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 67/71 | LOSS: 2.0251235448941932e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 68/71 | LOSS: 2.0256277096358236e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 69/71 | LOSS: 2.0192273456944218e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 70/71 | LOSS: 2.0116128543519716e-05\n",
      "VAL: EPOCH 50/100 | BATCH 0/8 | LOSS: 2.50105258601252e-05\n",
      "VAL: EPOCH 50/100 | BATCH 1/8 | LOSS: 2.1605086658382788e-05\n",
      "VAL: EPOCH 50/100 | BATCH 2/8 | LOSS: 2.1303560060914606e-05\n",
      "VAL: EPOCH 50/100 | BATCH 3/8 | LOSS: 2.1185395780776162e-05\n",
      "VAL: EPOCH 50/100 | BATCH 4/8 | LOSS: 2.0885784397250973e-05\n",
      "VAL: EPOCH 50/100 | BATCH 5/8 | LOSS: 2.010320592186569e-05\n",
      "VAL: EPOCH 50/100 | BATCH 6/8 | LOSS: 2.0443750275132644e-05\n",
      "VAL: EPOCH 50/100 | BATCH 7/8 | LOSS: 2.0933835230607656e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 0/71 | LOSS: 1.823273123591207e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 1/71 | LOSS: 1.939791582117323e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 2/71 | LOSS: 1.9861411904760946e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 3/71 | LOSS: 1.9725488527910784e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 4/71 | LOSS: 1.9562679881346412e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 5/71 | LOSS: 1.9307597843483865e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 6/71 | LOSS: 1.9920276047612007e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 7/71 | LOSS: 2.0354898651930853e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 8/71 | LOSS: 2.068431402019794e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 9/71 | LOSS: 2.0959623725502752e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 10/71 | LOSS: 2.114452846316536e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 11/71 | LOSS: 2.1571981505985605e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 12/71 | LOSS: 2.1481890144059435e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 13/71 | LOSS: 2.1324559254156027e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 14/71 | LOSS: 2.132759958233995e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 15/71 | LOSS: 2.084243237732153e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 16/71 | LOSS: 2.054670061197077e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 17/71 | LOSS: 2.0656094723866165e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 18/71 | LOSS: 2.04370030826938e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 19/71 | LOSS: 2.022702792601194e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 20/71 | LOSS: 2.0441194595020663e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 21/71 | LOSS: 2.0348430139578838e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 22/71 | LOSS: 2.0281940716971725e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 23/71 | LOSS: 2.0582435581673053e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 24/71 | LOSS: 2.047125861281529e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 25/71 | LOSS: 2.0369270644620814e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 26/71 | LOSS: 2.0401408349883882e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 27/71 | LOSS: 2.0310840487322173e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 28/71 | LOSS: 2.0264811860999605e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 29/71 | LOSS: 2.0054136014853915e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 30/71 | LOSS: 1.989131103454706e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 31/71 | LOSS: 2.0041347539745402e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 32/71 | LOSS: 2.006696897958384e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 33/71 | LOSS: 2.010726489863694e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 34/71 | LOSS: 2.0259876977693888e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 35/71 | LOSS: 2.027313411821928e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 36/71 | LOSS: 2.019869279556096e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 37/71 | LOSS: 2.0117417343020573e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 38/71 | LOSS: 2.0067687471918103e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 39/71 | LOSS: 1.9995667230432446e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 40/71 | LOSS: 2.0040857213266795e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 41/71 | LOSS: 1.9991621857549208e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 42/71 | LOSS: 2.0076369912922653e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 43/71 | LOSS: 2.024206516828351e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 44/71 | LOSS: 2.0063157494910733e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 45/71 | LOSS: 2.0009623629290296e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 46/71 | LOSS: 2.0011294935179746e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 47/71 | LOSS: 2.0019713228217977e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 48/71 | LOSS: 1.9928097349386046e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 49/71 | LOSS: 2.0067441600986057e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 50/71 | LOSS: 2.0107904103996607e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 51/71 | LOSS: 2.0011732693511967e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 52/71 | LOSS: 2.003455856023686e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 53/71 | LOSS: 1.9943103897848806e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 54/71 | LOSS: 1.989939704468601e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 55/71 | LOSS: 1.9990653072454734e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 56/71 | LOSS: 1.9937545738158453e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 57/71 | LOSS: 1.9912322323793417e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 58/71 | LOSS: 1.987272511449469e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 59/71 | LOSS: 1.9824957568440975e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 60/71 | LOSS: 1.976399841221225e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 61/71 | LOSS: 1.977325580575721e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 62/71 | LOSS: 1.9726036621178204e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 63/71 | LOSS: 1.9737427720656342e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 64/71 | LOSS: 1.9833095911263416e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 65/71 | LOSS: 1.982597856994923e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 66/71 | LOSS: 1.9809636009384447e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 67/71 | LOSS: 1.9825372044793123e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 68/71 | LOSS: 1.984513814470805e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 69/71 | LOSS: 1.981612604140537e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 70/71 | LOSS: 1.9885644037522454e-05\n",
      "VAL: EPOCH 51/100 | BATCH 0/8 | LOSS: 2.725532976910472e-05\n",
      "VAL: EPOCH 51/100 | BATCH 1/8 | LOSS: 2.4402516828558873e-05\n",
      "VAL: EPOCH 51/100 | BATCH 2/8 | LOSS: 2.397561001998838e-05\n",
      "VAL: EPOCH 51/100 | BATCH 3/8 | LOSS: 2.362846453252132e-05\n",
      "VAL: EPOCH 51/100 | BATCH 4/8 | LOSS: 2.288944051542785e-05\n",
      "VAL: EPOCH 51/100 | BATCH 5/8 | LOSS: 2.2553217604581732e-05\n",
      "VAL: EPOCH 51/100 | BATCH 6/8 | LOSS: 2.2600428305200433e-05\n",
      "VAL: EPOCH 51/100 | BATCH 7/8 | LOSS: 2.2981907250141376e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 0/71 | LOSS: 2.1842235582880676e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 1/71 | LOSS: 2.1054994249425363e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 2/71 | LOSS: 2.0780335641272057e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 3/71 | LOSS: 2.1162758002901683e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 4/71 | LOSS: 2.0384126764838582e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 5/71 | LOSS: 1.9787136807281058e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 6/71 | LOSS: 2.012766240763345e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 7/71 | LOSS: 1.9935947420890443e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 8/71 | LOSS: 1.98286082498574e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 9/71 | LOSS: 2.011707219935488e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 10/71 | LOSS: 1.9770167537816715e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 11/71 | LOSS: 2.007222186269549e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 12/71 | LOSS: 1.992454221740795e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 13/71 | LOSS: 1.99396181415068e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 14/71 | LOSS: 1.9863621370556455e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 15/71 | LOSS: 1.94219221612002e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 16/71 | LOSS: 1.979449756190126e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 17/71 | LOSS: 1.9874420710645307e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 18/71 | LOSS: 2.001422762987204e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 19/71 | LOSS: 2.0287140250729864e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 20/71 | LOSS: 2.031518341114168e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 21/71 | LOSS: 2.0220572051502213e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 22/71 | LOSS: 2.039000830099331e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 23/71 | LOSS: 2.0188335308072663e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 24/71 | LOSS: 2.021540713030845e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 25/71 | LOSS: 2.0253998417711744e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 26/71 | LOSS: 2.0094728824915364e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 27/71 | LOSS: 2.0262180961643545e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 28/71 | LOSS: 2.014922656285464e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 29/71 | LOSS: 2.0086787602243326e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 30/71 | LOSS: 2.0138839435305506e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 31/71 | LOSS: 2.002677285872778e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 32/71 | LOSS: 1.996675525857206e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 33/71 | LOSS: 1.986192728509195e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 34/71 | LOSS: 1.979892507993749e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 35/71 | LOSS: 1.995722181567948e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 36/71 | LOSS: 1.9954774649483403e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 37/71 | LOSS: 1.9923836733046062e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 38/71 | LOSS: 1.986506718728775e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 39/71 | LOSS: 1.981520604203979e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 40/71 | LOSS: 1.9951182788112997e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 41/71 | LOSS: 1.9878619068116504e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 42/71 | LOSS: 1.9893369024557193e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 43/71 | LOSS: 1.9949253741784062e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 44/71 | LOSS: 1.9884998538246793e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 45/71 | LOSS: 1.9851937988436397e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 46/71 | LOSS: 1.9758859475296824e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 47/71 | LOSS: 1.9810303532115842e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 48/71 | LOSS: 1.9862090548671477e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 49/71 | LOSS: 1.9855417922371997e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 50/71 | LOSS: 1.977022218531655e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 51/71 | LOSS: 1.9759402535922934e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 52/71 | LOSS: 1.9692742119692617e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 53/71 | LOSS: 1.9657785169665994e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 54/71 | LOSS: 1.960660857465965e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 55/71 | LOSS: 1.964506340560287e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 56/71 | LOSS: 1.9770487517490165e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 57/71 | LOSS: 1.9859783010204032e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 58/71 | LOSS: 1.9818454794881675e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 59/71 | LOSS: 1.9847524526994675e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 60/71 | LOSS: 1.9789752350036115e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 61/71 | LOSS: 1.9898597348665427e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 62/71 | LOSS: 1.9881182875935107e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 63/71 | LOSS: 1.9990998453067732e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 64/71 | LOSS: 2.000961540034041e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 65/71 | LOSS: 1.9971452330179147e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 66/71 | LOSS: 2.0051483302064407e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 67/71 | LOSS: 2.0103515319479973e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 68/71 | LOSS: 2.005203622093836e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 69/71 | LOSS: 2.007254578139899e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 70/71 | LOSS: 2.0277539155819832e-05\n",
      "VAL: EPOCH 52/100 | BATCH 0/8 | LOSS: 2.9410362913040444e-05\n",
      "VAL: EPOCH 52/100 | BATCH 1/8 | LOSS: 2.6460438675712794e-05\n",
      "VAL: EPOCH 52/100 | BATCH 2/8 | LOSS: 2.6708573083548497e-05\n",
      "VAL: EPOCH 52/100 | BATCH 3/8 | LOSS: 2.5603461835999042e-05\n",
      "VAL: EPOCH 52/100 | BATCH 4/8 | LOSS: 2.500460177543573e-05\n",
      "VAL: EPOCH 52/100 | BATCH 5/8 | LOSS: 2.4556692248249117e-05\n",
      "VAL: EPOCH 52/100 | BATCH 6/8 | LOSS: 2.4409265669029473e-05\n",
      "VAL: EPOCH 52/100 | BATCH 7/8 | LOSS: 2.4321897171830642e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 0/71 | LOSS: 2.4738288630032912e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 1/71 | LOSS: 2.3850913748901803e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 2/71 | LOSS: 2.2592964038873713e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 3/71 | LOSS: 2.294613659614697e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 4/71 | LOSS: 2.342695152037777e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 5/71 | LOSS: 2.2832674706781592e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 6/71 | LOSS: 2.343165553091759e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 7/71 | LOSS: 2.4459357973682927e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 8/71 | LOSS: 2.3627015656933912e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 9/71 | LOSS: 2.4834405303408856e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 10/71 | LOSS: 2.453403976687696e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 11/71 | LOSS: 2.4323344405274838e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 12/71 | LOSS: 2.5086052818761134e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 13/71 | LOSS: 2.476958979968913e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 14/71 | LOSS: 2.4328263316419906e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 15/71 | LOSS: 2.41453171838657e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 16/71 | LOSS: 2.4469871384687448e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 17/71 | LOSS: 2.3866986339271534e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 18/71 | LOSS: 2.4660538800066877e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 19/71 | LOSS: 2.4441894856863657e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 20/71 | LOSS: 2.433507290247473e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 21/71 | LOSS: 2.406335004872579e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 22/71 | LOSS: 2.430898190220393e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 23/71 | LOSS: 2.4153323996263982e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 24/71 | LOSS: 2.3858446002122947e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 25/71 | LOSS: 2.3737522916725047e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 26/71 | LOSS: 2.3848412402677006e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 27/71 | LOSS: 2.3906491995668538e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 28/71 | LOSS: 2.371157530307818e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 29/71 | LOSS: 2.3559510312528196e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 30/71 | LOSS: 2.3333938917212728e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 31/71 | LOSS: 2.3203024454687693e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 32/71 | LOSS: 2.3092496397139534e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 33/71 | LOSS: 2.3013149663764873e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 34/71 | LOSS: 2.294421782218186e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 35/71 | LOSS: 2.28236156342771e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 36/71 | LOSS: 2.2763761768834917e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 37/71 | LOSS: 2.2674642518962318e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 38/71 | LOSS: 2.2651512317213183e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 39/71 | LOSS: 2.2633400976701524e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 40/71 | LOSS: 2.2586559525069144e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 41/71 | LOSS: 2.2443689149873708e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 42/71 | LOSS: 2.22228045247395e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 43/71 | LOSS: 2.2114673707950384e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 44/71 | LOSS: 2.20181515639221e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 45/71 | LOSS: 2.184551701999305e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 46/71 | LOSS: 2.1741557905810855e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 47/71 | LOSS: 2.1752263118438957e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 48/71 | LOSS: 2.1675859656801438e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 49/71 | LOSS: 2.166117241358734e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 50/71 | LOSS: 2.1617415750726794e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 51/71 | LOSS: 2.144945415238908e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 52/71 | LOSS: 2.1530133672744994e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 53/71 | LOSS: 2.1452422532564486e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 54/71 | LOSS: 2.1336497957236135e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 55/71 | LOSS: 2.1436663798470234e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 56/71 | LOSS: 2.140574415215381e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 57/71 | LOSS: 2.1305909599348147e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 58/71 | LOSS: 2.1249857728450432e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 59/71 | LOSS: 2.123744670825545e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 60/71 | LOSS: 2.1212826513725363e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 61/71 | LOSS: 2.1137715755744026e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 62/71 | LOSS: 2.1082072057724295e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 63/71 | LOSS: 2.09949870395576e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 64/71 | LOSS: 2.0993642549281222e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 65/71 | LOSS: 2.0938323524949904e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 66/71 | LOSS: 2.0936367050058263e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 67/71 | LOSS: 2.088812794574872e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 68/71 | LOSS: 2.0881836251752194e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 69/71 | LOSS: 2.0979651656359367e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 70/71 | LOSS: 2.0858167541954457e-05\n",
      "VAL: EPOCH 53/100 | BATCH 0/8 | LOSS: 2.4279768695123494e-05\n",
      "VAL: EPOCH 53/100 | BATCH 1/8 | LOSS: 2.2511241695610806e-05\n",
      "VAL: EPOCH 53/100 | BATCH 2/8 | LOSS: 2.2404679233053077e-05\n",
      "VAL: EPOCH 53/100 | BATCH 3/8 | LOSS: 2.267372292408254e-05\n",
      "VAL: EPOCH 53/100 | BATCH 4/8 | LOSS: 2.225913995062001e-05\n",
      "VAL: EPOCH 53/100 | BATCH 5/8 | LOSS: 2.1903516123226534e-05\n",
      "VAL: EPOCH 53/100 | BATCH 6/8 | LOSS: 2.2335330608517063e-05\n",
      "VAL: EPOCH 53/100 | BATCH 7/8 | LOSS: 2.2982038672125782e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 0/71 | LOSS: 2.02307401195867e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 1/71 | LOSS: 2.0551865418383386e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 2/71 | LOSS: 2.312699628722233e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 3/71 | LOSS: 2.1645237666234607e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 4/71 | LOSS: 2.1344077322282828e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 5/71 | LOSS: 2.209419744758634e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 6/71 | LOSS: 2.1475059319137862e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 7/71 | LOSS: 2.199818163717282e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 8/71 | LOSS: 2.151730142132793e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 9/71 | LOSS: 2.1428689069580287e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 10/71 | LOSS: 2.1747171889397908e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 11/71 | LOSS: 2.1683355195515713e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 12/71 | LOSS: 2.123558064900303e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 13/71 | LOSS: 2.1200008242366103e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 14/71 | LOSS: 2.094607965166991e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 15/71 | LOSS: 2.1204126824159175e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 16/71 | LOSS: 2.100883017085246e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 17/71 | LOSS: 2.0754465771864892e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 18/71 | LOSS: 2.053768213571809e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 19/71 | LOSS: 2.0281119850551478e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 20/71 | LOSS: 2.0048323979911704e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 21/71 | LOSS: 1.991895599282791e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 22/71 | LOSS: 1.9848002604861826e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 23/71 | LOSS: 1.9822666445179493e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 24/71 | LOSS: 1.99139289179584e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 25/71 | LOSS: 1.986578039437658e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 26/71 | LOSS: 1.97653906879067e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 27/71 | LOSS: 1.9821910557636458e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 28/71 | LOSS: 1.9746364128438692e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 29/71 | LOSS: 1.975820002068455e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 30/71 | LOSS: 1.9790994561082052e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 31/71 | LOSS: 1.9652451669571747e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 32/71 | LOSS: 1.9485522099068586e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 33/71 | LOSS: 1.9300550106623524e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 34/71 | LOSS: 1.9291261482326912e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 35/71 | LOSS: 1.9194786015456582e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 36/71 | LOSS: 1.9137519155117696e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 37/71 | LOSS: 1.9121959978388315e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 38/71 | LOSS: 1.903824170604378e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 39/71 | LOSS: 1.9060697627537593e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 40/71 | LOSS: 1.8983486570061536e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 41/71 | LOSS: 1.8873297579245574e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 42/71 | LOSS: 1.8814159650287893e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 43/71 | LOSS: 1.876259090171185e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 44/71 | LOSS: 1.882888112353006e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 45/71 | LOSS: 1.8745211728737164e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 46/71 | LOSS: 1.8766361616608846e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 47/71 | LOSS: 1.8862832538009872e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 48/71 | LOSS: 1.8845408184071338e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 49/71 | LOSS: 1.890916722913971e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 50/71 | LOSS: 1.8854632192718174e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 51/71 | LOSS: 1.890541174221793e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 52/71 | LOSS: 1.8874424433375198e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 53/71 | LOSS: 1.8951393131929864e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 54/71 | LOSS: 1.893537208326796e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 55/71 | LOSS: 1.8874222730249e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 56/71 | LOSS: 1.8868715925603746e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 57/71 | LOSS: 1.8836206373666285e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 58/71 | LOSS: 1.878260544321689e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 59/71 | LOSS: 1.8711759973181568e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 60/71 | LOSS: 1.8682791259842084e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 61/71 | LOSS: 1.8653332459038092e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 62/71 | LOSS: 1.8638845133301358e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 63/71 | LOSS: 1.8604265818567e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 64/71 | LOSS: 1.868626649621337e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 65/71 | LOSS: 1.8685110262595117e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 66/71 | LOSS: 1.863127180065442e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 67/71 | LOSS: 1.8592667549241657e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 68/71 | LOSS: 1.863118876022729e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 69/71 | LOSS: 1.8579129040777585e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 70/71 | LOSS: 1.8639029253214355e-05\n",
      "VAL: EPOCH 54/100 | BATCH 0/8 | LOSS: 2.0937763110850938e-05\n",
      "VAL: EPOCH 54/100 | BATCH 1/8 | LOSS: 1.908760350488592e-05\n",
      "VAL: EPOCH 54/100 | BATCH 2/8 | LOSS: 1.9010072719538584e-05\n",
      "VAL: EPOCH 54/100 | BATCH 3/8 | LOSS: 1.8427880604576785e-05\n",
      "VAL: EPOCH 54/100 | BATCH 4/8 | LOSS: 1.8060112779494375e-05\n",
      "VAL: EPOCH 54/100 | BATCH 5/8 | LOSS: 1.7639293522127748e-05\n",
      "VAL: EPOCH 54/100 | BATCH 6/8 | LOSS: 1.796401478973816e-05\n",
      "VAL: EPOCH 54/100 | BATCH 7/8 | LOSS: 1.8367278926234576e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 0/71 | LOSS: 1.9777435227297246e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 1/71 | LOSS: 1.9299557607155293e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 2/71 | LOSS: 1.799481105990708e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 3/71 | LOSS: 1.8721276319411118e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 4/71 | LOSS: 1.8614351938595063e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 5/71 | LOSS: 1.8400295327107113e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 6/71 | LOSS: 1.790040265145113e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 7/71 | LOSS: 1.809895661608607e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 8/71 | LOSS: 1.8156340754810823e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 9/71 | LOSS: 1.8449050912749954e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 10/71 | LOSS: 1.8852910381445493e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 11/71 | LOSS: 1.893134215909716e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 12/71 | LOSS: 1.893224837728597e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 13/71 | LOSS: 1.9388647680378718e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 14/71 | LOSS: 1.9133877867716364e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 15/71 | LOSS: 1.9113422354166687e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 16/71 | LOSS: 1.9192434753195438e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 17/71 | LOSS: 1.922418879176904e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 18/71 | LOSS: 1.918791129259932e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 19/71 | LOSS: 1.8985594397236128e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 20/71 | LOSS: 1.9041494375588152e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 21/71 | LOSS: 1.9160294538199775e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 22/71 | LOSS: 1.9266139766525317e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 23/71 | LOSS: 1.936288481374504e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 24/71 | LOSS: 1.9284113732283005e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 25/71 | LOSS: 1.914257924139607e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 26/71 | LOSS: 1.8940386460733998e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 27/71 | LOSS: 1.8891257793386884e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 28/71 | LOSS: 1.8869029127958568e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 29/71 | LOSS: 1.8801333226292627e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 30/71 | LOSS: 1.8763900287364864e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 31/71 | LOSS: 1.870779780688281e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 32/71 | LOSS: 1.8680777843242552e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 33/71 | LOSS: 1.8636154918032628e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 34/71 | LOSS: 1.8509309796042672e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 35/71 | LOSS: 1.8473486887968546e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 36/71 | LOSS: 1.8366654207843443e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 37/71 | LOSS: 1.851824529359491e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 38/71 | LOSS: 1.8469377209839877e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 39/71 | LOSS: 1.8439706605022365e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 40/71 | LOSS: 1.84416950748816e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 41/71 | LOSS: 1.839328340446671e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 42/71 | LOSS: 1.8397005670670107e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 43/71 | LOSS: 1.8314625756168176e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 44/71 | LOSS: 1.8259839655204106e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 45/71 | LOSS: 1.8217015543126287e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 46/71 | LOSS: 1.8147797086288498e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 47/71 | LOSS: 1.8097834564893372e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 48/71 | LOSS: 1.8042638639228094e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 49/71 | LOSS: 1.7957782147277613e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 50/71 | LOSS: 1.786113816528015e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 51/71 | LOSS: 1.791609485315768e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 52/71 | LOSS: 1.797052748272863e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 53/71 | LOSS: 1.8052153892414128e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 54/71 | LOSS: 1.799923976994416e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 55/71 | LOSS: 1.8029940195317196e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 56/71 | LOSS: 1.8085489404926958e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 57/71 | LOSS: 1.8053245310448832e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 58/71 | LOSS: 1.802155683265991e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 59/71 | LOSS: 1.794578975022887e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 60/71 | LOSS: 1.7920964115755402e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 61/71 | LOSS: 1.7879404485058565e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 62/71 | LOSS: 1.784858310808583e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 63/71 | LOSS: 1.777377130451896e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 64/71 | LOSS: 1.7785884931352206e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 65/71 | LOSS: 1.7779940125447784e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 66/71 | LOSS: 1.7748265534790526e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 67/71 | LOSS: 1.776830795814235e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 68/71 | LOSS: 1.7700327088528287e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 69/71 | LOSS: 1.7667799407458265e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 70/71 | LOSS: 1.7838308977702647e-05\n",
      "VAL: EPOCH 55/100 | BATCH 0/8 | LOSS: 2.324709203094244e-05\n",
      "VAL: EPOCH 55/100 | BATCH 1/8 | LOSS: 2.063103238469921e-05\n",
      "VAL: EPOCH 55/100 | BATCH 2/8 | LOSS: 2.0278686861274764e-05\n",
      "VAL: EPOCH 55/100 | BATCH 3/8 | LOSS: 1.9905693534383317e-05\n",
      "VAL: EPOCH 55/100 | BATCH 4/8 | LOSS: 1.931850565597415e-05\n",
      "VAL: EPOCH 55/100 | BATCH 5/8 | LOSS: 1.9074745371957153e-05\n",
      "VAL: EPOCH 55/100 | BATCH 6/8 | LOSS: 1.9130973929090294e-05\n",
      "VAL: EPOCH 55/100 | BATCH 7/8 | LOSS: 1.9224144807594712e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 0/71 | LOSS: 2.0507854060269892e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 1/71 | LOSS: 2.136630610038992e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 2/71 | LOSS: 1.9433701406038988e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 3/71 | LOSS: 1.8700109194469405e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 4/71 | LOSS: 1.9027592134079897e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 5/71 | LOSS: 1.90291126879553e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 6/71 | LOSS: 1.9423145983767298e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 7/71 | LOSS: 1.9018947114091134e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 8/71 | LOSS: 1.8598586595746587e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 9/71 | LOSS: 1.87354366062209e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 10/71 | LOSS: 1.8680559812558137e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 11/71 | LOSS: 1.9068891864056543e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 12/71 | LOSS: 1.9064010232866098e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 13/71 | LOSS: 1.9114471277654438e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 14/71 | LOSS: 1.9236294610891493e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 15/71 | LOSS: 1.921985312947072e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 16/71 | LOSS: 1.9365273200331585e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 17/71 | LOSS: 1.954033935665696e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 18/71 | LOSS: 1.9563222897551513e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 19/71 | LOSS: 1.941214168255101e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 20/71 | LOSS: 1.9453665767546875e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 21/71 | LOSS: 1.93497593169344e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 22/71 | LOSS: 1.9415141334478825e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 23/71 | LOSS: 1.924638020985488e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 24/71 | LOSS: 1.914560212753713e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 25/71 | LOSS: 1.9281592638155696e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 26/71 | LOSS: 1.9144350584678318e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 27/71 | LOSS: 1.898041221336046e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 28/71 | LOSS: 1.888746010081377e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 29/71 | LOSS: 1.8784118219627997e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 30/71 | LOSS: 1.8643751753834002e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 31/71 | LOSS: 1.8574567803852915e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 32/71 | LOSS: 1.8650748643267434e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 33/71 | LOSS: 1.861864596803036e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 34/71 | LOSS: 1.8535538271992535e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 35/71 | LOSS: 1.859727806024441e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 36/71 | LOSS: 1.8520560130324355e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 37/71 | LOSS: 1.83852882736047e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 38/71 | LOSS: 1.8355711313816777e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 39/71 | LOSS: 1.8463488618181146e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 40/71 | LOSS: 1.852204072414812e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 41/71 | LOSS: 1.8489134704612386e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 42/71 | LOSS: 1.8440942936691964e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 43/71 | LOSS: 1.838756785136039e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 44/71 | LOSS: 1.83554151792325e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 45/71 | LOSS: 1.8368503827767437e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 46/71 | LOSS: 1.836237113317846e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 47/71 | LOSS: 1.8276620323831594e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 48/71 | LOSS: 1.8220204802506783e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 49/71 | LOSS: 1.8144634832424345e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 50/71 | LOSS: 1.8253489311153352e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 51/71 | LOSS: 1.8193663716094587e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 52/71 | LOSS: 1.812127271169063e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 53/71 | LOSS: 1.8060620648072842e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 54/71 | LOSS: 1.8051857643347996e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 55/71 | LOSS: 1.7999686617388632e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 56/71 | LOSS: 1.7975507258246614e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 57/71 | LOSS: 1.8044085177750495e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 58/71 | LOSS: 1.8013195015447633e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 59/71 | LOSS: 1.7973760031964046e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 60/71 | LOSS: 1.8024684725444742e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 61/71 | LOSS: 1.796434710946238e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 62/71 | LOSS: 1.7897169253422087e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 63/71 | LOSS: 1.785859323888417e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 64/71 | LOSS: 1.777742145350203e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 65/71 | LOSS: 1.778547619213703e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 66/71 | LOSS: 1.7780116283588474e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 67/71 | LOSS: 1.7736576142744845e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 68/71 | LOSS: 1.7743884872407246e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 69/71 | LOSS: 1.768570677346101e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 70/71 | LOSS: 1.771632935448048e-05\n",
      "VAL: EPOCH 56/100 | BATCH 0/8 | LOSS: 2.0773008145624772e-05\n",
      "VAL: EPOCH 56/100 | BATCH 1/8 | LOSS: 1.9740427887882106e-05\n",
      "VAL: EPOCH 56/100 | BATCH 2/8 | LOSS: 1.948323309382734e-05\n",
      "VAL: EPOCH 56/100 | BATCH 3/8 | LOSS: 1.9472214717097813e-05\n",
      "VAL: EPOCH 56/100 | BATCH 4/8 | LOSS: 1.902374824567232e-05\n",
      "VAL: EPOCH 56/100 | BATCH 5/8 | LOSS: 1.881333870793848e-05\n",
      "VAL: EPOCH 56/100 | BATCH 6/8 | LOSS: 1.9208894497881245e-05\n",
      "VAL: EPOCH 56/100 | BATCH 7/8 | LOSS: 1.9614961956904153e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 0/71 | LOSS: 1.7492253391537815e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 1/71 | LOSS: 1.608226648386335e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 2/71 | LOSS: 1.6546883974418353e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 3/71 | LOSS: 1.6182297258637846e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 4/71 | LOSS: 1.5873535812716e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 5/71 | LOSS: 1.6201852910550468e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 6/71 | LOSS: 1.632858376459418e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 7/71 | LOSS: 1.6145745348694618e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 8/71 | LOSS: 1.6686173290104813e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 9/71 | LOSS: 1.6775021686044055e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 10/71 | LOSS: 1.6745933780012738e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 11/71 | LOSS: 1.7271199794777203e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 12/71 | LOSS: 1.72648005200944e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 13/71 | LOSS: 1.7340595669728437e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 14/71 | LOSS: 1.7592260458817085e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 15/71 | LOSS: 1.7636006987231667e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 16/71 | LOSS: 1.7540175844367375e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 17/71 | LOSS: 1.7592737397838693e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 18/71 | LOSS: 1.7560753277441682e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 19/71 | LOSS: 1.7573633067513582e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 20/71 | LOSS: 1.7391880690064726e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 21/71 | LOSS: 1.7465725415439177e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 22/71 | LOSS: 1.7658246534942325e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 23/71 | LOSS: 1.757615094296246e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 24/71 | LOSS: 1.7580357671249658e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 25/71 | LOSS: 1.7689196913404605e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 26/71 | LOSS: 1.768610107117436e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 27/71 | LOSS: 1.7758570525724542e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 28/71 | LOSS: 1.7690025272494148e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 29/71 | LOSS: 1.7637364180700388e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 30/71 | LOSS: 1.7662935915136438e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 31/71 | LOSS: 1.7847259812242555e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 32/71 | LOSS: 1.7758258956856672e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 33/71 | LOSS: 1.7637726560839356e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 34/71 | LOSS: 1.7704370137445428e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 35/71 | LOSS: 1.7649949995757197e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 36/71 | LOSS: 1.7605939270602853e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 37/71 | LOSS: 1.7582174390745902e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 38/71 | LOSS: 1.7502687954290126e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 39/71 | LOSS: 1.7451341091145877e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 40/71 | LOSS: 1.7592649051150103e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 41/71 | LOSS: 1.7592208981416963e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 42/71 | LOSS: 1.7566363123139953e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 43/71 | LOSS: 1.7638069659204287e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 44/71 | LOSS: 1.7638778930025487e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 45/71 | LOSS: 1.762438189515429e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 46/71 | LOSS: 1.7750997706551718e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 47/71 | LOSS: 1.768573785435971e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 48/71 | LOSS: 1.7745877719752738e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 49/71 | LOSS: 1.7736448753566947e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 50/71 | LOSS: 1.7722803236478393e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 51/71 | LOSS: 1.761451928289786e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 52/71 | LOSS: 1.757368810567126e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 53/71 | LOSS: 1.7489221136202104e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 54/71 | LOSS: 1.7483514172702352e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 55/71 | LOSS: 1.752604047006961e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 56/71 | LOSS: 1.7492894696994898e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 57/71 | LOSS: 1.7486848891762905e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 58/71 | LOSS: 1.748945970128693e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 59/71 | LOSS: 1.750055674468361e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 60/71 | LOSS: 1.7440152793720014e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 61/71 | LOSS: 1.747262347400406e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 62/71 | LOSS: 1.7412543399139337e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 63/71 | LOSS: 1.7339841647867615e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 64/71 | LOSS: 1.7346959695994602e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 65/71 | LOSS: 1.73246672106457e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 66/71 | LOSS: 1.7293927225752547e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 67/71 | LOSS: 1.7314437320023633e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 68/71 | LOSS: 1.7270745047426317e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 69/71 | LOSS: 1.7268817061579156e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 70/71 | LOSS: 1.7375359534590117e-05\n",
      "VAL: EPOCH 57/100 | BATCH 0/8 | LOSS: 1.9856772269122303e-05\n",
      "VAL: EPOCH 57/100 | BATCH 1/8 | LOSS: 1.7275699065066874e-05\n",
      "VAL: EPOCH 57/100 | BATCH 2/8 | LOSS: 1.711630587427256e-05\n",
      "VAL: EPOCH 57/100 | BATCH 3/8 | LOSS: 1.682382435319596e-05\n",
      "VAL: EPOCH 57/100 | BATCH 4/8 | LOSS: 1.6641612819512375e-05\n",
      "VAL: EPOCH 57/100 | BATCH 5/8 | LOSS: 1.6202416190935764e-05\n",
      "VAL: EPOCH 57/100 | BATCH 6/8 | LOSS: 1.6497211618116125e-05\n",
      "VAL: EPOCH 57/100 | BATCH 7/8 | LOSS: 1.677669570199214e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 0/71 | LOSS: 1.7511389160063118e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 1/71 | LOSS: 1.7820950233726762e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 2/71 | LOSS: 1.6830409549584147e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 3/71 | LOSS: 1.633106194276479e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 4/71 | LOSS: 1.6904896256164648e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 5/71 | LOSS: 1.6318742685446825e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 6/71 | LOSS: 1.6281723219435662e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 7/71 | LOSS: 1.5914967320895812e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 8/71 | LOSS: 1.5982874275424467e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 9/71 | LOSS: 1.5841635195101846e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 10/71 | LOSS: 1.5967140660160855e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 11/71 | LOSS: 1.5747161796753062e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 12/71 | LOSS: 1.5801185537174417e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 13/71 | LOSS: 1.5658722466988756e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 14/71 | LOSS: 1.5932423169336594e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 15/71 | LOSS: 1.591032196301967e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 16/71 | LOSS: 1.6193519353263956e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 17/71 | LOSS: 1.618029879561315e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 18/71 | LOSS: 1.599917040469958e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 19/71 | LOSS: 1.619998201931594e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 20/71 | LOSS: 1.6021241424910148e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 21/71 | LOSS: 1.6114753965336025e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 22/71 | LOSS: 1.6166713976417668e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 23/71 | LOSS: 1.6046164167467698e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 24/71 | LOSS: 1.6238381576840765e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 25/71 | LOSS: 1.6196932077806334e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 26/71 | LOSS: 1.62712490068602e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 27/71 | LOSS: 1.6384560110184664e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 28/71 | LOSS: 1.6365464912739518e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 29/71 | LOSS: 1.642946851158437e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 30/71 | LOSS: 1.6425941794295795e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 31/71 | LOSS: 1.643588126398754e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 32/71 | LOSS: 1.663659774446084e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 33/71 | LOSS: 1.6694677630285322e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 34/71 | LOSS: 1.6649144397108883e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 35/71 | LOSS: 1.6869971331187924e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 36/71 | LOSS: 1.6860621439558895e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 37/71 | LOSS: 1.695972062072159e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 38/71 | LOSS: 1.69193742691184e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 39/71 | LOSS: 1.691766658495908e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 40/71 | LOSS: 1.6847140458838377e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 41/71 | LOSS: 1.6830510763351674e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 42/71 | LOSS: 1.6891549527171327e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 43/71 | LOSS: 1.6824053628542142e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 44/71 | LOSS: 1.677444967450962e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 45/71 | LOSS: 1.6713275548998194e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 46/71 | LOSS: 1.682463143298903e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 47/71 | LOSS: 1.679732829037069e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 48/71 | LOSS: 1.684567513973666e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 49/71 | LOSS: 1.695440858384245e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 50/71 | LOSS: 1.7043012603073078e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 51/71 | LOSS: 1.7024777434926364e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 52/71 | LOSS: 1.705343041770525e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 53/71 | LOSS: 1.7080090418420258e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 54/71 | LOSS: 1.7091652113198177e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 55/71 | LOSS: 1.7101221357864104e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 56/71 | LOSS: 1.7228474095182927e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 57/71 | LOSS: 1.723556693707059e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 58/71 | LOSS: 1.7267680019451013e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 59/71 | LOSS: 1.7173523565361394e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 60/71 | LOSS: 1.7266326185414132e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 61/71 | LOSS: 1.7324346103006035e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 62/71 | LOSS: 1.731605324392957e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 63/71 | LOSS: 1.735431941085608e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 64/71 | LOSS: 1.738295327063847e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 65/71 | LOSS: 1.7415052546063883e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 66/71 | LOSS: 1.7448987167501483e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 67/71 | LOSS: 1.742803133578598e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 68/71 | LOSS: 1.746618190868437e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 69/71 | LOSS: 1.741353887285056e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 70/71 | LOSS: 1.7363615626609443e-05\n",
      "VAL: EPOCH 58/100 | BATCH 0/8 | LOSS: 1.8917908164439723e-05\n",
      "VAL: EPOCH 58/100 | BATCH 1/8 | LOSS: 1.698354935797397e-05\n",
      "VAL: EPOCH 58/100 | BATCH 2/8 | LOSS: 1.7153843752263736e-05\n",
      "VAL: EPOCH 58/100 | BATCH 3/8 | LOSS: 1.6446060044472688e-05\n",
      "VAL: EPOCH 58/100 | BATCH 4/8 | LOSS: 1.6334230349457356e-05\n",
      "VAL: EPOCH 58/100 | BATCH 5/8 | LOSS: 1.5830051931213045e-05\n",
      "VAL: EPOCH 58/100 | BATCH 6/8 | LOSS: 1.5946867668909753e-05\n",
      "VAL: EPOCH 58/100 | BATCH 7/8 | LOSS: 1.616877818833018e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 0/71 | LOSS: 1.3571025192504749e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 1/71 | LOSS: 1.4636851119576022e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 2/71 | LOSS: 1.4539176542408919e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 3/71 | LOSS: 1.5253667470460641e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 4/71 | LOSS: 1.549712524138158e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 5/71 | LOSS: 1.5487665677937912e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 6/71 | LOSS: 1.5911659829725977e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 7/71 | LOSS: 1.610943752439198e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 8/71 | LOSS: 1.5892209881308696e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 9/71 | LOSS: 1.6289839368255344e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 10/71 | LOSS: 1.672162712070117e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 11/71 | LOSS: 1.6901300114113837e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 12/71 | LOSS: 1.6839182885283102e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 13/71 | LOSS: 1.6751565713743082e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 14/71 | LOSS: 1.7382591492302404e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 15/71 | LOSS: 1.7181628663820447e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 16/71 | LOSS: 1.6943388372430275e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 17/71 | LOSS: 1.6921955799868254e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 18/71 | LOSS: 1.693782202269894e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 19/71 | LOSS: 1.7070751573555755e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 20/71 | LOSS: 1.703950827158267e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 21/71 | LOSS: 1.7095195578969484e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 22/71 | LOSS: 1.7057401949031096e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 23/71 | LOSS: 1.7136912560999917e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 24/71 | LOSS: 1.7041671744664198e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 25/71 | LOSS: 1.7023982204241642e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 26/71 | LOSS: 1.6905987077161962e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 27/71 | LOSS: 1.6991766805014258e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 28/71 | LOSS: 1.715476496585701e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 29/71 | LOSS: 1.715476413058544e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 30/71 | LOSS: 1.716427290844994e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 31/71 | LOSS: 1.7102460020623766e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 32/71 | LOSS: 1.700501662994749e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 33/71 | LOSS: 1.7218064653313966e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 34/71 | LOSS: 1.7248628241109795e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 35/71 | LOSS: 1.7238208127107806e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 36/71 | LOSS: 1.7248505615940746e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 37/71 | LOSS: 1.725715111732777e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 38/71 | LOSS: 1.7257229746274577e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 39/71 | LOSS: 1.7291835729338344e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 40/71 | LOSS: 1.727027860974109e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 41/71 | LOSS: 1.7283775118162987e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 42/71 | LOSS: 1.7209380488100917e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 43/71 | LOSS: 1.7179714714901902e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 44/71 | LOSS: 1.7347117278000545e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 45/71 | LOSS: 1.7439779541113555e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 46/71 | LOSS: 1.7466669382980748e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 47/71 | LOSS: 1.7451159654531995e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 48/71 | LOSS: 1.749276352525993e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 49/71 | LOSS: 1.7486108517914545e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 50/71 | LOSS: 1.7552787325899664e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 51/71 | LOSS: 1.7494729508353674e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 52/71 | LOSS: 1.753870864771636e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 53/71 | LOSS: 1.7519223227416796e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 54/71 | LOSS: 1.7468210750270042e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 55/71 | LOSS: 1.752807062465373e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 56/71 | LOSS: 1.7478033489743738e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 57/71 | LOSS: 1.7498665987784927e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 58/71 | LOSS: 1.7597934826064097e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 59/71 | LOSS: 1.7550031437470655e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 60/71 | LOSS: 1.7483388964152613e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 61/71 | LOSS: 1.7483641899631158e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 62/71 | LOSS: 1.747749658014592e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 63/71 | LOSS: 1.7488350053440627e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 64/71 | LOSS: 1.7429505700979812e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 65/71 | LOSS: 1.7440184268652406e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 66/71 | LOSS: 1.743199182009679e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 67/71 | LOSS: 1.7443419277269105e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 68/71 | LOSS: 1.741552570319323e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 69/71 | LOSS: 1.7429597288095724e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 70/71 | LOSS: 1.748103104674415e-05\n",
      "VAL: EPOCH 59/100 | BATCH 0/8 | LOSS: 1.9432367480476387e-05\n",
      "VAL: EPOCH 59/100 | BATCH 1/8 | LOSS: 1.6812791272968752e-05\n",
      "VAL: EPOCH 59/100 | BATCH 2/8 | LOSS: 1.656646812383163e-05\n",
      "VAL: EPOCH 59/100 | BATCH 3/8 | LOSS: 1.5879200645940728e-05\n",
      "VAL: EPOCH 59/100 | BATCH 4/8 | LOSS: 1.5671378605475184e-05\n",
      "VAL: EPOCH 59/100 | BATCH 5/8 | LOSS: 1.5303067963638266e-05\n",
      "VAL: EPOCH 59/100 | BATCH 6/8 | LOSS: 1.5548722590860313e-05\n",
      "VAL: EPOCH 59/100 | BATCH 7/8 | LOSS: 1.584417702815699e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 0/71 | LOSS: 1.7701520846458152e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 1/71 | LOSS: 1.646381360842497e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 2/71 | LOSS: 1.664900173636852e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 3/71 | LOSS: 1.611630636944028e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 4/71 | LOSS: 1.54755167386611e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 5/71 | LOSS: 1.5569427432637895e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 6/71 | LOSS: 1.5881837238599212e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 7/71 | LOSS: 1.5523871411460277e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 8/71 | LOSS: 1.558716424672942e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 9/71 | LOSS: 1.5640878427802817e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 10/71 | LOSS: 1.5457174082588278e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 11/71 | LOSS: 1.525020646416427e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 12/71 | LOSS: 1.5225135030050296e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 13/71 | LOSS: 1.517239161330508e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 14/71 | LOSS: 1.5123161938390695e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 15/71 | LOSS: 1.5254925074259518e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 16/71 | LOSS: 1.5251434567343335e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 17/71 | LOSS: 1.527862797148474e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 18/71 | LOSS: 1.5126589890937076e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 19/71 | LOSS: 1.5158197675191331e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 20/71 | LOSS: 1.513581221280176e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 21/71 | LOSS: 1.5301893406351816e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 22/71 | LOSS: 1.528152666730381e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 23/71 | LOSS: 1.534180766308661e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 24/71 | LOSS: 1.5404518780997023e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 25/71 | LOSS: 1.5726017619062288e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 26/71 | LOSS: 1.5889200531765475e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 27/71 | LOSS: 1.596500858015913e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 28/71 | LOSS: 1.603671632343823e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 29/71 | LOSS: 1.608000420674216e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 30/71 | LOSS: 1.606220394023694e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 31/71 | LOSS: 1.6023704915824055e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 32/71 | LOSS: 1.6050800144457202e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 33/71 | LOSS: 1.6059393227110643e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 34/71 | LOSS: 1.6097339799411463e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 35/71 | LOSS: 1.606444920475446e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 36/71 | LOSS: 1.6287853434954053e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 37/71 | LOSS: 1.638066611887648e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 38/71 | LOSS: 1.6318149205173355e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 39/71 | LOSS: 1.6386867287110363e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 40/71 | LOSS: 1.6347174473261608e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 41/71 | LOSS: 1.6272124282425337e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 42/71 | LOSS: 1.6240998331002546e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 43/71 | LOSS: 1.6338202013602395e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 44/71 | LOSS: 1.640374062440565e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 45/71 | LOSS: 1.6483116862376768e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 46/71 | LOSS: 1.651007438882284e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 47/71 | LOSS: 1.6572586105212395e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 48/71 | LOSS: 1.6571599264018597e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 49/71 | LOSS: 1.657861579587916e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 50/71 | LOSS: 1.6545588639964278e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 51/71 | LOSS: 1.649189110909132e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 52/71 | LOSS: 1.6610135303981566e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 53/71 | LOSS: 1.6706217071192373e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 54/71 | LOSS: 1.6697034086312423e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 55/71 | LOSS: 1.6780831824040172e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 56/71 | LOSS: 1.6716281447371696e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 57/71 | LOSS: 1.674711736619617e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 58/71 | LOSS: 1.6787846168696533e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 59/71 | LOSS: 1.677150373022111e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 60/71 | LOSS: 1.6765476980477525e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 61/71 | LOSS: 1.6806766383993937e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 62/71 | LOSS: 1.6763178921662204e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 63/71 | LOSS: 1.6889640889417024e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 64/71 | LOSS: 1.6864011237115706e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 65/71 | LOSS: 1.6872492241501195e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 66/71 | LOSS: 1.6828316639380794e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 67/71 | LOSS: 1.6790801964816637e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 68/71 | LOSS: 1.6750696912809424e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 69/71 | LOSS: 1.678319022175856e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 70/71 | LOSS: 1.679915145923331e-05\n",
      "VAL: EPOCH 60/100 | BATCH 0/8 | LOSS: 1.785978201951366e-05\n",
      "VAL: EPOCH 60/100 | BATCH 1/8 | LOSS: 1.6949790733633563e-05\n",
      "VAL: EPOCH 60/100 | BATCH 2/8 | LOSS: 1.7090436813305132e-05\n",
      "VAL: EPOCH 60/100 | BATCH 3/8 | LOSS: 1.7196154658449814e-05\n",
      "VAL: EPOCH 60/100 | BATCH 4/8 | LOSS: 1.7028401998686604e-05\n",
      "VAL: EPOCH 60/100 | BATCH 5/8 | LOSS: 1.671448565806107e-05\n",
      "VAL: EPOCH 60/100 | BATCH 6/8 | LOSS: 1.7074433084677105e-05\n",
      "VAL: EPOCH 60/100 | BATCH 7/8 | LOSS: 1.75421082531102e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 0/71 | LOSS: 1.2756710930261761e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 1/71 | LOSS: 1.5180585251073353e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 2/71 | LOSS: 1.539043781425183e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 3/71 | LOSS: 1.5773802260810044e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 4/71 | LOSS: 1.6026967932702975e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 5/71 | LOSS: 1.597036862222012e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 6/71 | LOSS: 1.606671347482396e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 7/71 | LOSS: 1.5930084146020818e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 8/71 | LOSS: 1.6272820883184773e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 9/71 | LOSS: 1.610916215213365e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 10/71 | LOSS: 1.6350464069215708e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 11/71 | LOSS: 1.638673825254955e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 12/71 | LOSS: 1.6409851363837683e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 13/71 | LOSS: 1.6603283971302778e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 14/71 | LOSS: 1.6756455139936102e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 15/71 | LOSS: 1.6840755904468097e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 16/71 | LOSS: 1.6647504226592207e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 17/71 | LOSS: 1.648937025796234e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 18/71 | LOSS: 1.6770322824748674e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 19/71 | LOSS: 1.6749514634284425e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 20/71 | LOSS: 1.7048453151974607e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 21/71 | LOSS: 1.7088055838857226e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 22/71 | LOSS: 1.705028507390323e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 23/71 | LOSS: 1.6874420225576614e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 24/71 | LOSS: 1.7066993968910538e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 25/71 | LOSS: 1.7125746965645634e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 26/71 | LOSS: 1.699437618501381e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 27/71 | LOSS: 1.7116261265722903e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 28/71 | LOSS: 1.7167090343969776e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 29/71 | LOSS: 1.705059088029278e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 30/71 | LOSS: 1.707326009636745e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 31/71 | LOSS: 1.716508540994255e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 32/71 | LOSS: 1.718427180700625e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 33/71 | LOSS: 1.7156520377337822e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 34/71 | LOSS: 1.7306642725348607e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 35/71 | LOSS: 1.745469120982711e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 36/71 | LOSS: 1.7415052482376906e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 37/71 | LOSS: 1.744941513798518e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 38/71 | LOSS: 1.765183706317121e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 39/71 | LOSS: 1.762929614415043e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 40/71 | LOSS: 1.7724630441346275e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 41/71 | LOSS: 1.78374071505719e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 42/71 | LOSS: 1.7743872165230515e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 43/71 | LOSS: 1.7804034417746482e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 44/71 | LOSS: 1.792028601307215e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 45/71 | LOSS: 1.7976982551614206e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 46/71 | LOSS: 1.808922280225546e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 47/71 | LOSS: 1.8188882828932645e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 48/71 | LOSS: 1.811933937145169e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 49/71 | LOSS: 1.828137414122466e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 50/71 | LOSS: 1.8207087479965468e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 51/71 | LOSS: 1.8283098492807207e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 52/71 | LOSS: 1.8253160957993834e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 53/71 | LOSS: 1.8217436330882333e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 54/71 | LOSS: 1.8225870305534707e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 55/71 | LOSS: 1.8223391180072213e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 56/71 | LOSS: 1.815054032736123e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 57/71 | LOSS: 1.8163758513451443e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 58/71 | LOSS: 1.8148465811864914e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 59/71 | LOSS: 1.809250347832858e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 60/71 | LOSS: 1.7993442896590376e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 61/71 | LOSS: 1.8039785000808244e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 62/71 | LOSS: 1.8072843063453635e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 63/71 | LOSS: 1.8024208927158725e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 64/71 | LOSS: 1.803881645733222e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 65/71 | LOSS: 1.8032194392700067e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 66/71 | LOSS: 1.7984968739011805e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 67/71 | LOSS: 1.796631427703901e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 68/71 | LOSS: 1.794980684992523e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 69/71 | LOSS: 1.7962279798666714e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 70/71 | LOSS: 1.788724548020236e-05\n",
      "VAL: EPOCH 61/100 | BATCH 0/8 | LOSS: 1.5494377294089645e-05\n",
      "VAL: EPOCH 61/100 | BATCH 1/8 | LOSS: 1.528981920273509e-05\n",
      "VAL: EPOCH 61/100 | BATCH 2/8 | LOSS: 1.577266690825733e-05\n",
      "VAL: EPOCH 61/100 | BATCH 3/8 | LOSS: 1.5701554730185308e-05\n",
      "VAL: EPOCH 61/100 | BATCH 4/8 | LOSS: 1.5631646965630354e-05\n",
      "VAL: EPOCH 61/100 | BATCH 5/8 | LOSS: 1.5308294223359553e-05\n",
      "VAL: EPOCH 61/100 | BATCH 6/8 | LOSS: 1.557262593061231e-05\n",
      "VAL: EPOCH 61/100 | BATCH 7/8 | LOSS: 1.58079636776165e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 0/71 | LOSS: 1.7291949916398153e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 1/71 | LOSS: 1.4739158359589055e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 2/71 | LOSS: 1.5044137398945168e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 3/71 | LOSS: 1.6307687019434525e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 4/71 | LOSS: 1.5692064334871247e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 5/71 | LOSS: 1.4954802130281072e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 6/71 | LOSS: 1.567013701527945e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 7/71 | LOSS: 1.5462238934560446e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 8/71 | LOSS: 1.5611738490406424e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 9/71 | LOSS: 1.548950567666907e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 10/71 | LOSS: 1.5335587787293743e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 11/71 | LOSS: 1.5395631028998952e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 12/71 | LOSS: 1.538497865392576e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 13/71 | LOSS: 1.5287452957376706e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 14/71 | LOSS: 1.5440686305131142e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 15/71 | LOSS: 1.5396658000099706e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 16/71 | LOSS: 1.526941190711702e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 17/71 | LOSS: 1.5174608304530719e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 18/71 | LOSS: 1.5239392556204142e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 19/71 | LOSS: 1.5399116682601745e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 20/71 | LOSS: 1.5528894913413317e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 21/71 | LOSS: 1.5326617275687486e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 22/71 | LOSS: 1.5514317055931315e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 23/71 | LOSS: 1.577636521687964e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 24/71 | LOSS: 1.584569617989473e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 25/71 | LOSS: 1.601300349508305e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 26/71 | LOSS: 1.6031219476846876e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 27/71 | LOSS: 1.5980087742458897e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 28/71 | LOSS: 1.6065126868037122e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 29/71 | LOSS: 1.5994075844597926e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 30/71 | LOSS: 1.5992498828704663e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 31/71 | LOSS: 1.6026855149675612e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 32/71 | LOSS: 1.6020821352536917e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 33/71 | LOSS: 1.594213945816805e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 34/71 | LOSS: 1.5894173910575252e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 35/71 | LOSS: 1.5920757202062912e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 36/71 | LOSS: 1.5828090392853875e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 37/71 | LOSS: 1.5807085865641345e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 38/71 | LOSS: 1.581479439967855e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 39/71 | LOSS: 1.588126076512708e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 40/71 | LOSS: 1.5908832499074533e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 41/71 | LOSS: 1.5888597491062856e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 42/71 | LOSS: 1.588436788192349e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 43/71 | LOSS: 1.5846397611147207e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 44/71 | LOSS: 1.5797924970684107e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 45/71 | LOSS: 1.576673645578684e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 46/71 | LOSS: 1.595717051888177e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 47/71 | LOSS: 1.6055185843318515e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 48/71 | LOSS: 1.617062062738411e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 49/71 | LOSS: 1.610724177226075e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 50/71 | LOSS: 1.605884952761128e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 51/71 | LOSS: 1.608716696374848e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 52/71 | LOSS: 1.6046936286998834e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 53/71 | LOSS: 1.6034697839333807e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 54/71 | LOSS: 1.6090215897499795e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 55/71 | LOSS: 1.6078062880556638e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 56/71 | LOSS: 1.613371111026234e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 57/71 | LOSS: 1.6100173023256375e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 58/71 | LOSS: 1.6129781493041033e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 59/71 | LOSS: 1.612738415133208e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 60/71 | LOSS: 1.613209810113672e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 61/71 | LOSS: 1.612006564110127e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 62/71 | LOSS: 1.6125699692074993e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 63/71 | LOSS: 1.6079807267033175e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 64/71 | LOSS: 1.605673818490826e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 65/71 | LOSS: 1.6034542761300337e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 66/71 | LOSS: 1.600104840337681e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 67/71 | LOSS: 1.5953968453258686e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 68/71 | LOSS: 1.600080896173165e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 69/71 | LOSS: 1.6021493788035252e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 70/71 | LOSS: 1.6103157410635875e-05\n",
      "VAL: EPOCH 62/100 | BATCH 0/8 | LOSS: 1.798623998183757e-05\n",
      "VAL: EPOCH 62/100 | BATCH 1/8 | LOSS: 1.6897451132535934e-05\n",
      "VAL: EPOCH 62/100 | BATCH 2/8 | LOSS: 1.7157111869892105e-05\n",
      "VAL: EPOCH 62/100 | BATCH 3/8 | LOSS: 1.6112101093312958e-05\n",
      "VAL: EPOCH 62/100 | BATCH 4/8 | LOSS: 1.5918370263534597e-05\n",
      "VAL: EPOCH 62/100 | BATCH 5/8 | LOSS: 1.5703744187097374e-05\n",
      "VAL: EPOCH 62/100 | BATCH 6/8 | LOSS: 1.5820208968112377e-05\n",
      "VAL: EPOCH 62/100 | BATCH 7/8 | LOSS: 1.5809375099706813e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 0/71 | LOSS: 1.3128052160027437e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 1/71 | LOSS: 2.1199880393396597e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 2/71 | LOSS: 1.936123286820172e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 3/71 | LOSS: 1.8597551843413385e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 4/71 | LOSS: 1.8229429406346755e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 5/71 | LOSS: 1.7746537196217105e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 6/71 | LOSS: 1.7727478085102382e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 7/71 | LOSS: 1.7300378431173158e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 8/71 | LOSS: 1.7308656525629987e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 9/71 | LOSS: 1.7623886560613754e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 10/71 | LOSS: 1.7205860124456443e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 11/71 | LOSS: 1.7759202971016446e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 12/71 | LOSS: 1.7846871252494075e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 13/71 | LOSS: 1.7692715118755586e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 14/71 | LOSS: 1.8518008801038377e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 15/71 | LOSS: 1.8307879713574948e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 16/71 | LOSS: 1.8836200337425587e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 17/71 | LOSS: 1.917217529504948e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 18/71 | LOSS: 1.9179242946464863e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 19/71 | LOSS: 1.9230772340961267e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 20/71 | LOSS: 1.927650165660972e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 21/71 | LOSS: 1.9253368505319074e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 22/71 | LOSS: 1.933742967831558e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 23/71 | LOSS: 1.9457482115588693e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 24/71 | LOSS: 1.9546958137652838e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 25/71 | LOSS: 1.9397152055507133e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 26/71 | LOSS: 1.92355858383019e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 27/71 | LOSS: 1.9055789770001346e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 28/71 | LOSS: 1.889274131701376e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 29/71 | LOSS: 1.882957985799294e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 30/71 | LOSS: 1.867320949508388e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 31/71 | LOSS: 1.85781038908317e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 32/71 | LOSS: 1.8499306668562703e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 33/71 | LOSS: 1.849398497998824e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 34/71 | LOSS: 1.8532170101285113e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 35/71 | LOSS: 1.8481555748116483e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 36/71 | LOSS: 1.8427270776088778e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 37/71 | LOSS: 1.8324336499637483e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 38/71 | LOSS: 1.8184486856252755e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 39/71 | LOSS: 1.809083971693326e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 40/71 | LOSS: 1.790076254199049e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 41/71 | LOSS: 1.790217753036164e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 42/71 | LOSS: 1.7863783648144184e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 43/71 | LOSS: 1.782035060800395e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 44/71 | LOSS: 1.7819545509054378e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 45/71 | LOSS: 1.776070963466697e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 46/71 | LOSS: 1.774665692055157e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 47/71 | LOSS: 1.7753700357540463e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 48/71 | LOSS: 1.769703601671701e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 49/71 | LOSS: 1.7707369242998538e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 50/71 | LOSS: 1.7573527425245665e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 51/71 | LOSS: 1.754223699908135e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 52/71 | LOSS: 1.756836658353008e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 53/71 | LOSS: 1.7514462864777738e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 54/71 | LOSS: 1.7457652095551814e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 55/71 | LOSS: 1.7442860439587093e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 56/71 | LOSS: 1.7417898545733024e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 57/71 | LOSS: 1.7406895635915512e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 58/71 | LOSS: 1.740742915236466e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 59/71 | LOSS: 1.7414917404797355e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 60/71 | LOSS: 1.734906909107059e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 61/71 | LOSS: 1.733962294418879e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 62/71 | LOSS: 1.7314900813087457e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 63/71 | LOSS: 1.7270124402557485e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 64/71 | LOSS: 1.722580599072479e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 65/71 | LOSS: 1.7171009307220988e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 66/71 | LOSS: 1.7165580052889383e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 67/71 | LOSS: 1.7144531578678117e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 68/71 | LOSS: 1.709718758386283e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 69/71 | LOSS: 1.7055597748237364e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 70/71 | LOSS: 1.699449654859589e-05\n",
      "VAL: EPOCH 63/100 | BATCH 0/8 | LOSS: 1.5099638403626159e-05\n",
      "VAL: EPOCH 63/100 | BATCH 1/8 | LOSS: 1.4469048437604215e-05\n",
      "VAL: EPOCH 63/100 | BATCH 2/8 | LOSS: 1.4713065866089892e-05\n",
      "VAL: EPOCH 63/100 | BATCH 3/8 | LOSS: 1.4487071439361898e-05\n",
      "VAL: EPOCH 63/100 | BATCH 4/8 | LOSS: 1.4489590284938458e-05\n",
      "VAL: EPOCH 63/100 | BATCH 5/8 | LOSS: 1.423182917884939e-05\n",
      "VAL: EPOCH 63/100 | BATCH 6/8 | LOSS: 1.4422493872448935e-05\n",
      "VAL: EPOCH 63/100 | BATCH 7/8 | LOSS: 1.4780105630052276e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 0/71 | LOSS: 1.5870076822466217e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 1/71 | LOSS: 1.7449286133341957e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 2/71 | LOSS: 1.4755886392473863e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 3/71 | LOSS: 1.4693446701130597e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 4/71 | LOSS: 1.5548773808404805e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 5/71 | LOSS: 1.534221731465853e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 6/71 | LOSS: 1.5024372260086238e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 7/71 | LOSS: 1.4644553289144824e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 8/71 | LOSS: 1.4382243332672968e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 9/71 | LOSS: 1.4271674899646314e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 10/71 | LOSS: 1.4401682991857259e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 11/71 | LOSS: 1.4335321338876383e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 12/71 | LOSS: 1.427812493112959e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 13/71 | LOSS: 1.4208409668624102e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 14/71 | LOSS: 1.4358531128285298e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 15/71 | LOSS: 1.4259180886710965e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 16/71 | LOSS: 1.419008025122286e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 17/71 | LOSS: 1.4020393993834861e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 18/71 | LOSS: 1.3976382912292856e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 19/71 | LOSS: 1.399845896230545e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 20/71 | LOSS: 1.4197835688329568e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 21/71 | LOSS: 1.4072881640458945e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 22/71 | LOSS: 1.4040197564337058e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 23/71 | LOSS: 1.3983807813625996e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 24/71 | LOSS: 1.4063035523577128e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 25/71 | LOSS: 1.4104869653517828e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 26/71 | LOSS: 1.3996691147137122e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 27/71 | LOSS: 1.4095383773695045e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 28/71 | LOSS: 1.421856158513351e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 29/71 | LOSS: 1.4239727443055017e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 30/71 | LOSS: 1.4544982795703089e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 31/71 | LOSS: 1.4494779293272586e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 32/71 | LOSS: 1.455727672719425e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 33/71 | LOSS: 1.501969216084998e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 34/71 | LOSS: 1.505483872149073e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 35/71 | LOSS: 1.5263523639280014e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 36/71 | LOSS: 1.549830567209692e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 37/71 | LOSS: 1.5490612991255292e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 38/71 | LOSS: 1.5956662583705158e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 39/71 | LOSS: 1.613763065506646e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 40/71 | LOSS: 1.6198023309343404e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 41/71 | LOSS: 1.6290514005439555e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 42/71 | LOSS: 1.6249219486273306e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 43/71 | LOSS: 1.621658574632337e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 44/71 | LOSS: 1.6354804756701924e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 45/71 | LOSS: 1.643948294456451e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 46/71 | LOSS: 1.6458320811928367e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 47/71 | LOSS: 1.6384176925991294e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 48/71 | LOSS: 1.6442323798475292e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 49/71 | LOSS: 1.640409343963256e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 50/71 | LOSS: 1.63683195626386e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 51/71 | LOSS: 1.646184850519953e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 52/71 | LOSS: 1.6427853552158922e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 53/71 | LOSS: 1.6425488394344467e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 54/71 | LOSS: 1.6507551283046434e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 55/71 | LOSS: 1.6503920603229615e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 56/71 | LOSS: 1.644626930404532e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 57/71 | LOSS: 1.6539505175952118e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 58/71 | LOSS: 1.654463025168575e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 59/71 | LOSS: 1.6555114583146255e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 60/71 | LOSS: 1.6595702126338796e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 61/71 | LOSS: 1.6623513572760145e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 62/71 | LOSS: 1.666045328521351e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 63/71 | LOSS: 1.6605342821662816e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 64/71 | LOSS: 1.675986631408495e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 65/71 | LOSS: 1.6694506994650567e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 66/71 | LOSS: 1.6645546633869223e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 67/71 | LOSS: 1.6706726442220423e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 68/71 | LOSS: 1.677637259411352e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 69/71 | LOSS: 1.6783097232421694e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 70/71 | LOSS: 1.680760385464913e-05\n",
      "VAL: EPOCH 64/100 | BATCH 0/8 | LOSS: 2.622276770125609e-05\n",
      "VAL: EPOCH 64/100 | BATCH 1/8 | LOSS: 2.8335842216620222e-05\n",
      "VAL: EPOCH 64/100 | BATCH 2/8 | LOSS: 2.8149607411857385e-05\n",
      "VAL: EPOCH 64/100 | BATCH 3/8 | LOSS: 2.884472769437707e-05\n",
      "VAL: EPOCH 64/100 | BATCH 4/8 | LOSS: 2.8717932582367212e-05\n",
      "VAL: EPOCH 64/100 | BATCH 5/8 | LOSS: 2.9350476324907504e-05\n",
      "VAL: EPOCH 64/100 | BATCH 6/8 | LOSS: 2.9981105659057255e-05\n",
      "VAL: EPOCH 64/100 | BATCH 7/8 | LOSS: 3.099245759585756e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 0/71 | LOSS: 2.121806755894795e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 1/71 | LOSS: 2.0191282601444982e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 2/71 | LOSS: 1.9484531852261473e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 3/71 | LOSS: 2.0329411199782044e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 4/71 | LOSS: 1.9647787121357395e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 5/71 | LOSS: 1.9886757096780155e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 6/71 | LOSS: 2.0229838680409428e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 7/71 | LOSS: 1.9311898995511e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 8/71 | LOSS: 1.936726686027315e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 9/71 | LOSS: 1.913573796628043e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 10/71 | LOSS: 1.8847720473951828e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 11/71 | LOSS: 1.928481287905015e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 12/71 | LOSS: 1.937534020376356e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 13/71 | LOSS: 1.9313307607912327e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 14/71 | LOSS: 1.947133435654299e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 15/71 | LOSS: 1.9119750504614785e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 16/71 | LOSS: 1.926660789810258e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 17/71 | LOSS: 1.8998332456653265e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 18/71 | LOSS: 1.8731577495744107e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 19/71 | LOSS: 1.8760836428555195e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 20/71 | LOSS: 1.8604319761618083e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 21/71 | LOSS: 1.8383117135685595e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 22/71 | LOSS: 1.828055416769095e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 23/71 | LOSS: 1.8057828924611385e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 24/71 | LOSS: 1.805711475753924e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 25/71 | LOSS: 1.783162199969341e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 26/71 | LOSS: 1.7802288918270886e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 27/71 | LOSS: 1.7766927450273734e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 28/71 | LOSS: 1.757181953194206e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 29/71 | LOSS: 1.739011953153143e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 30/71 | LOSS: 1.7290958449819065e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 31/71 | LOSS: 1.7234885888228746e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 32/71 | LOSS: 1.727936113170334e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 33/71 | LOSS: 1.7328585914476527e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 34/71 | LOSS: 1.726116183168155e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 35/71 | LOSS: 1.7213700453390225e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 36/71 | LOSS: 1.733200398932142e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 37/71 | LOSS: 1.7198263178311447e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 38/71 | LOSS: 1.720777130779028e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 39/71 | LOSS: 1.7237831548300166e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 40/71 | LOSS: 1.719622477827304e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 41/71 | LOSS: 1.7157290888765903e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 42/71 | LOSS: 1.7043704585460566e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 43/71 | LOSS: 1.6990669626631476e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 44/71 | LOSS: 1.6973445599433035e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 45/71 | LOSS: 1.6955947103269864e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 46/71 | LOSS: 1.6853383482505685e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 47/71 | LOSS: 1.6740696537453914e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 48/71 | LOSS: 1.673937456834377e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 49/71 | LOSS: 1.6706050810171292e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 50/71 | LOSS: 1.669324514046357e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 51/71 | LOSS: 1.6597156288228758e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 52/71 | LOSS: 1.650836512917007e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 53/71 | LOSS: 1.648379756497215e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 54/71 | LOSS: 1.6416886286275588e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 55/71 | LOSS: 1.6385626996192775e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 56/71 | LOSS: 1.633877205645069e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 57/71 | LOSS: 1.6314297901207564e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 58/71 | LOSS: 1.6307173933393826e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 59/71 | LOSS: 1.629991202207748e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 60/71 | LOSS: 1.6226537078838475e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 61/71 | LOSS: 1.6183202821975422e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 62/71 | LOSS: 1.6125911474412692e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 63/71 | LOSS: 1.6092304235826305e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 64/71 | LOSS: 1.603549090549887e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 65/71 | LOSS: 1.5992350309909053e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 66/71 | LOSS: 1.592356723300164e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 67/71 | LOSS: 1.5993071790088672e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 68/71 | LOSS: 1.598705245893486e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 69/71 | LOSS: 1.598435798898988e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 70/71 | LOSS: 1.609146864877761e-05\n",
      "VAL: EPOCH 65/100 | BATCH 0/8 | LOSS: 1.4270093743107282e-05\n",
      "VAL: EPOCH 65/100 | BATCH 1/8 | LOSS: 1.5189581063168589e-05\n",
      "VAL: EPOCH 65/100 | BATCH 2/8 | LOSS: 1.5664640159229748e-05\n",
      "VAL: EPOCH 65/100 | BATCH 3/8 | LOSS: 1.5524481113970978e-05\n",
      "VAL: EPOCH 65/100 | BATCH 4/8 | LOSS: 1.5402453573187814e-05\n",
      "VAL: EPOCH 65/100 | BATCH 5/8 | LOSS: 1.5406101738335565e-05\n",
      "VAL: EPOCH 65/100 | BATCH 6/8 | LOSS: 1.5547623791332755e-05\n",
      "VAL: EPOCH 65/100 | BATCH 7/8 | LOSS: 1.5498997981922003e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 0/71 | LOSS: 2.11108781513758e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 1/71 | LOSS: 1.6510883142473176e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 2/71 | LOSS: 1.5688569267998293e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 3/71 | LOSS: 1.6308535350617603e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 4/71 | LOSS: 1.690795052127214e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 5/71 | LOSS: 1.65601932167192e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 6/71 | LOSS: 1.647559136992121e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 7/71 | LOSS: 1.614241205061262e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 8/71 | LOSS: 1.625031068720596e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 9/71 | LOSS: 1.6275543748633937e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 10/71 | LOSS: 1.6059921935348857e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 11/71 | LOSS: 1.5896672190744237e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 12/71 | LOSS: 1.5714152700988612e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 13/71 | LOSS: 1.561174336269947e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 14/71 | LOSS: 1.5702859855082352e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 15/71 | LOSS: 1.5454472816145426e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 16/71 | LOSS: 1.563435650014606e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 17/71 | LOSS: 1.53495954893717e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 18/71 | LOSS: 1.5093253320006433e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 19/71 | LOSS: 1.4828585744908195e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 20/71 | LOSS: 1.4804608153575655e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 21/71 | LOSS: 1.4600165038493948e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 22/71 | LOSS: 1.4562481856441794e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 23/71 | LOSS: 1.4515168724453057e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 24/71 | LOSS: 1.457817943446571e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 25/71 | LOSS: 1.4671337514580675e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 26/71 | LOSS: 1.4609905216881694e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 27/71 | LOSS: 1.4570371441940161e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 28/71 | LOSS: 1.4633981350173065e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 29/71 | LOSS: 1.4635220820006604e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 30/71 | LOSS: 1.4592501693480318e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 31/71 | LOSS: 1.456515673226022e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 32/71 | LOSS: 1.462235041165541e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 33/71 | LOSS: 1.4579750527713366e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 34/71 | LOSS: 1.4694351453467139e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 35/71 | LOSS: 1.4649427991268263e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 36/71 | LOSS: 1.4589415366424726e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 37/71 | LOSS: 1.4613422775399062e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 38/71 | LOSS: 1.46451896468464e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 39/71 | LOSS: 1.4685510541312396e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 40/71 | LOSS: 1.4758483791033874e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 41/71 | LOSS: 1.4739645025873047e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 42/71 | LOSS: 1.4801494661125701e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 43/71 | LOSS: 1.4804086545568266e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 44/71 | LOSS: 1.4741261717669355e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 45/71 | LOSS: 1.4770195498979023e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 46/71 | LOSS: 1.4747006253913995e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 47/71 | LOSS: 1.4741160119532045e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 48/71 | LOSS: 1.473991608004233e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 49/71 | LOSS: 1.4669717293145368e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 50/71 | LOSS: 1.4698516090849058e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 51/71 | LOSS: 1.466811431782844e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 52/71 | LOSS: 1.470606835113667e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 53/71 | LOSS: 1.470022870206675e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 54/71 | LOSS: 1.4614696043612308e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 55/71 | LOSS: 1.4627383231007635e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 56/71 | LOSS: 1.4597572133648659e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 57/71 | LOSS: 1.462368873976159e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 58/71 | LOSS: 1.4600674272144916e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 59/71 | LOSS: 1.4552305037796031e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 60/71 | LOSS: 1.4492974097250777e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 61/71 | LOSS: 1.4461250958124911e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 62/71 | LOSS: 1.4417072562205176e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 63/71 | LOSS: 1.4368155021315943e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 64/71 | LOSS: 1.4373341022851842e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 65/71 | LOSS: 1.4369833438236542e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 66/71 | LOSS: 1.430843704879204e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 67/71 | LOSS: 1.4304322736707571e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 68/71 | LOSS: 1.4279588630534304e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 69/71 | LOSS: 1.4266100229828486e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 70/71 | LOSS: 1.4243824743587051e-05\n",
      "VAL: EPOCH 66/100 | BATCH 0/8 | LOSS: 1.4887769793858752e-05\n",
      "VAL: EPOCH 66/100 | BATCH 1/8 | LOSS: 1.4302638646768173e-05\n",
      "VAL: EPOCH 66/100 | BATCH 2/8 | LOSS: 1.4584382370230742e-05\n",
      "VAL: EPOCH 66/100 | BATCH 3/8 | LOSS: 1.3976929949421901e-05\n",
      "VAL: EPOCH 66/100 | BATCH 4/8 | LOSS: 1.396002608089475e-05\n",
      "VAL: EPOCH 66/100 | BATCH 5/8 | LOSS: 1.3795928528755516e-05\n",
      "VAL: EPOCH 66/100 | BATCH 6/8 | LOSS: 1.3799238331557717e-05\n",
      "VAL: EPOCH 66/100 | BATCH 7/8 | LOSS: 1.410271090662718e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 0/71 | LOSS: 1.3361586752580479e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 1/71 | LOSS: 1.426949256710941e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 2/71 | LOSS: 1.4322209002178473e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 3/71 | LOSS: 1.4261304158935673e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 4/71 | LOSS: 1.4720342551299837e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 5/71 | LOSS: 1.4947574375886083e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 6/71 | LOSS: 1.4880373653016119e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 7/71 | LOSS: 1.5056897041176853e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 8/71 | LOSS: 1.4643611191584367e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 9/71 | LOSS: 1.5047247052279999e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 10/71 | LOSS: 1.487088543812702e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 11/71 | LOSS: 1.4976034284093961e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 12/71 | LOSS: 1.5624686476747647e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 13/71 | LOSS: 1.556554226616364e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 14/71 | LOSS: 1.591011059645098e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 15/71 | LOSS: 1.6410185594395443e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 16/71 | LOSS: 1.675024513507892e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 17/71 | LOSS: 1.7102211738044087e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 18/71 | LOSS: 1.7305734983988497e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 19/71 | LOSS: 1.7120734537456882e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 20/71 | LOSS: 1.706253161024679e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 21/71 | LOSS: 1.7064482314956628e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 22/71 | LOSS: 1.727079839126774e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 23/71 | LOSS: 1.716201048414708e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 24/71 | LOSS: 1.768257650837768e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 25/71 | LOSS: 1.7653447335200886e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 26/71 | LOSS: 1.7617080394066527e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 27/71 | LOSS: 1.7457342502374168e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 28/71 | LOSS: 1.7355006062204885e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 29/71 | LOSS: 1.7192020034902574e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 30/71 | LOSS: 1.704306886230986e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 31/71 | LOSS: 1.714166666033634e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 32/71 | LOSS: 1.7034503936443027e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 33/71 | LOSS: 1.6974564834312975e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 34/71 | LOSS: 1.692679538142069e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 35/71 | LOSS: 1.6883430059048504e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 36/71 | LOSS: 1.6794954324329927e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 37/71 | LOSS: 1.6654063361592737e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 38/71 | LOSS: 1.6606830472599726e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 39/71 | LOSS: 1.6631582207082828e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 40/71 | LOSS: 1.6619455935852317e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 41/71 | LOSS: 1.653324512690666e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 42/71 | LOSS: 1.651537725190919e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 43/71 | LOSS: 1.6475458737245802e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 44/71 | LOSS: 1.6596280571927004e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 45/71 | LOSS: 1.654656728572698e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 46/71 | LOSS: 1.6485833123411756e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 47/71 | LOSS: 1.6586736762747023e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 48/71 | LOSS: 1.6572111829246956e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 49/71 | LOSS: 1.6518096599611455e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 50/71 | LOSS: 1.6506400082589072e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 51/71 | LOSS: 1.648247521538556e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 52/71 | LOSS: 1.6505298501798744e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 53/71 | LOSS: 1.6507626343001095e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 54/71 | LOSS: 1.655036804467355e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 55/71 | LOSS: 1.655393621799054e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 56/71 | LOSS: 1.655582003111608e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 57/71 | LOSS: 1.656304159731572e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 58/71 | LOSS: 1.6569670475230125e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 59/71 | LOSS: 1.6570983825658912e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 60/71 | LOSS: 1.6551847588967485e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 61/71 | LOSS: 1.663306087603785e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 62/71 | LOSS: 1.6619345005315225e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 63/71 | LOSS: 1.651363589871835e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 64/71 | LOSS: 1.658467999032627e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 65/71 | LOSS: 1.6541542236430388e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 66/71 | LOSS: 1.6541546769917997e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 67/71 | LOSS: 1.654358774270376e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 68/71 | LOSS: 1.6455127769259505e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 69/71 | LOSS: 1.648569089281539e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 70/71 | LOSS: 1.6421707942741627e-05\n",
      "VAL: EPOCH 67/100 | BATCH 0/8 | LOSS: 1.5526562492595986e-05\n",
      "VAL: EPOCH 67/100 | BATCH 1/8 | LOSS: 1.622686795599293e-05\n",
      "VAL: EPOCH 67/100 | BATCH 2/8 | LOSS: 1.6434606853484485e-05\n",
      "VAL: EPOCH 67/100 | BATCH 3/8 | LOSS: 1.625630784474197e-05\n",
      "VAL: EPOCH 67/100 | BATCH 4/8 | LOSS: 1.6046537712099963e-05\n",
      "VAL: EPOCH 67/100 | BATCH 5/8 | LOSS: 1.645410369140639e-05\n",
      "VAL: EPOCH 67/100 | BATCH 6/8 | LOSS: 1.6729902199585922e-05\n",
      "VAL: EPOCH 67/100 | BATCH 7/8 | LOSS: 1.7158518403448397e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 0/71 | LOSS: 1.760696068231482e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 1/71 | LOSS: 1.62117203217349e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 2/71 | LOSS: 1.544593427145931e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 3/71 | LOSS: 1.583367998136964e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 4/71 | LOSS: 1.5924069339234846e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 5/71 | LOSS: 1.612369017796785e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 6/71 | LOSS: 1.5458787207275497e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 7/71 | LOSS: 1.580449975335796e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 8/71 | LOSS: 1.5350007945218953e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 9/71 | LOSS: 1.5122008881007787e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 10/71 | LOSS: 1.4758915726402352e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 11/71 | LOSS: 1.4752197027216122e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 12/71 | LOSS: 1.4759216495664217e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 13/71 | LOSS: 1.4657298509389096e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 14/71 | LOSS: 1.5020901385772352e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 15/71 | LOSS: 1.4926478456800396e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 16/71 | LOSS: 1.4975839375812725e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 17/71 | LOSS: 1.4948393577974963e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 18/71 | LOSS: 1.481774420665859e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 19/71 | LOSS: 1.4892036642777384e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 20/71 | LOSS: 1.4883934151536474e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 21/71 | LOSS: 1.4951159507538911e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 22/71 | LOSS: 1.4961621277704936e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 23/71 | LOSS: 1.4893449929331837e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 24/71 | LOSS: 1.4826087826804724e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 25/71 | LOSS: 1.4690288655681517e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 26/71 | LOSS: 1.4533263647872783e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 27/71 | LOSS: 1.4506139060748474e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 28/71 | LOSS: 1.4485441421004061e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 29/71 | LOSS: 1.4358256309302913e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 30/71 | LOSS: 1.4376429809940859e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 31/71 | LOSS: 1.437950473359706e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 32/71 | LOSS: 1.4331214460429637e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 33/71 | LOSS: 1.4331146062699298e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 34/71 | LOSS: 1.4361454746644346e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 35/71 | LOSS: 1.434855082354463e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 36/71 | LOSS: 1.4288586744687809e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 37/71 | LOSS: 1.4280271994051108e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 38/71 | LOSS: 1.41928550221611e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 39/71 | LOSS: 1.4249890182327362e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 40/71 | LOSS: 1.4201867088559084e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 41/71 | LOSS: 1.4148310626859755e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 42/71 | LOSS: 1.4151663192531613e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 43/71 | LOSS: 1.4156795817722344e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 44/71 | LOSS: 1.416198742339879e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 45/71 | LOSS: 1.417194789610221e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 46/71 | LOSS: 1.4215214736047547e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 47/71 | LOSS: 1.4222310350457216e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 48/71 | LOSS: 1.4166824498994286e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 49/71 | LOSS: 1.4162881434458541e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 50/71 | LOSS: 1.4106024768718235e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 51/71 | LOSS: 1.4108249860538099e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 52/71 | LOSS: 1.4107048645080726e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 53/71 | LOSS: 1.4178828560371013e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 54/71 | LOSS: 1.4179544367917432e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 55/71 | LOSS: 1.4264593135391936e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 56/71 | LOSS: 1.4202478348778913e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 57/71 | LOSS: 1.4179876227993599e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 58/71 | LOSS: 1.4165191421821965e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 59/71 | LOSS: 1.4142307721461596e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 60/71 | LOSS: 1.411351409156761e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 61/71 | LOSS: 1.410229188261456e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 62/71 | LOSS: 1.4172088759507806e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 63/71 | LOSS: 1.4185970073299359e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 64/71 | LOSS: 1.4157056242626054e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 65/71 | LOSS: 1.4204661676392954e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 66/71 | LOSS: 1.4167573844228607e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 67/71 | LOSS: 1.4121337993428289e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 68/71 | LOSS: 1.4115132599081203e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 69/71 | LOSS: 1.4125145844445796e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 70/71 | LOSS: 1.4154429087463708e-05\n",
      "VAL: EPOCH 68/100 | BATCH 0/8 | LOSS: 2.095230229315348e-05\n",
      "VAL: EPOCH 68/100 | BATCH 1/8 | LOSS: 2.0086967197130434e-05\n",
      "VAL: EPOCH 68/100 | BATCH 2/8 | LOSS: 2.039527134911623e-05\n",
      "VAL: EPOCH 68/100 | BATCH 3/8 | LOSS: 1.9413200334383873e-05\n",
      "VAL: EPOCH 68/100 | BATCH 4/8 | LOSS: 1.9479417096590625e-05\n",
      "VAL: EPOCH 68/100 | BATCH 5/8 | LOSS: 1.9261729903519154e-05\n",
      "VAL: EPOCH 68/100 | BATCH 6/8 | LOSS: 1.9134622821833807e-05\n",
      "VAL: EPOCH 68/100 | BATCH 7/8 | LOSS: 1.928139795381867e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 0/71 | LOSS: 2.2140604414744303e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 1/71 | LOSS: 1.8599489521875512e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 2/71 | LOSS: 1.785838261033253e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 3/71 | LOSS: 1.8642889244802063e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 4/71 | LOSS: 1.875974703580141e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 5/71 | LOSS: 1.8462830363811616e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 6/71 | LOSS: 1.8101141384769498e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 7/71 | LOSS: 1.7502850710116036e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 8/71 | LOSS: 1.7884684893942904e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 9/71 | LOSS: 1.721844682833762e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 10/71 | LOSS: 1.7045417073611382e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 11/71 | LOSS: 1.7432703240653307e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 12/71 | LOSS: 1.7241388727248144e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 13/71 | LOSS: 1.7130730157077778e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 14/71 | LOSS: 1.7055400773339593e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 15/71 | LOSS: 1.681493642990972e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 16/71 | LOSS: 1.6904300483635298e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 17/71 | LOSS: 1.6768969847665478e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 18/71 | LOSS: 1.6749594004451002e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 19/71 | LOSS: 1.6720326220820426e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 20/71 | LOSS: 1.6725781409020002e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 21/71 | LOSS: 1.658956671235501e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 22/71 | LOSS: 1.64817263636075e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 23/71 | LOSS: 1.626784171548934e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 24/71 | LOSS: 1.620703947992297e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 25/71 | LOSS: 1.610368602856537e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 26/71 | LOSS: 1.5958258093936212e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 27/71 | LOSS: 1.5975455588496906e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 28/71 | LOSS: 1.5810096054588812e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 29/71 | LOSS: 1.571993152538198e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 30/71 | LOSS: 1.5582344971217906e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 31/71 | LOSS: 1.5567868160815124e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 32/71 | LOSS: 1.550377623282133e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 33/71 | LOSS: 1.5546369200712754e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 34/71 | LOSS: 1.5469362136043077e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 35/71 | LOSS: 1.544250906388495e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 36/71 | LOSS: 1.5296043587826518e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 37/71 | LOSS: 1.5210471246973611e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 38/71 | LOSS: 1.5139028759082206e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 39/71 | LOSS: 1.5089233806975244e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 40/71 | LOSS: 1.5038380230934553e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 41/71 | LOSS: 1.5045120800918777e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 42/71 | LOSS: 1.4930012692402247e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 43/71 | LOSS: 1.4910952793136609e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 44/71 | LOSS: 1.4874250842290671e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 45/71 | LOSS: 1.4868391882254914e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 46/71 | LOSS: 1.4897159746126024e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 47/71 | LOSS: 1.4903240279788102e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 48/71 | LOSS: 1.483872833608873e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 49/71 | LOSS: 1.4822637367615244e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 50/71 | LOSS: 1.4806418667100005e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 51/71 | LOSS: 1.4755352997151651e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 52/71 | LOSS: 1.4838309721628045e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 53/71 | LOSS: 1.4859514521958772e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 54/71 | LOSS: 1.4901364887588318e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 55/71 | LOSS: 1.4839256128392922e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 56/71 | LOSS: 1.4779309120122139e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 57/71 | LOSS: 1.4802843170086973e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 58/71 | LOSS: 1.476851738809304e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 59/71 | LOSS: 1.4768709706913797e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 60/71 | LOSS: 1.4718440249199659e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 61/71 | LOSS: 1.4716194175801526e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 62/71 | LOSS: 1.47202350086041e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 63/71 | LOSS: 1.472985664463522e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 64/71 | LOSS: 1.4747438931058591e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 65/71 | LOSS: 1.4768689459632503e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 66/71 | LOSS: 1.4734092338354702e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 67/71 | LOSS: 1.4746655270916081e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 68/71 | LOSS: 1.4710266691838404e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 69/71 | LOSS: 1.4717436377915355e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 70/71 | LOSS: 1.470337822315165e-05\n",
      "VAL: EPOCH 69/100 | BATCH 0/8 | LOSS: 1.3042087630310562e-05\n",
      "VAL: EPOCH 69/100 | BATCH 1/8 | LOSS: 1.2785385933966609e-05\n",
      "VAL: EPOCH 69/100 | BATCH 2/8 | LOSS: 1.3177000255382154e-05\n",
      "VAL: EPOCH 69/100 | BATCH 3/8 | LOSS: 1.279944467569294e-05\n",
      "VAL: EPOCH 69/100 | BATCH 4/8 | LOSS: 1.2802398669009562e-05\n",
      "VAL: EPOCH 69/100 | BATCH 5/8 | LOSS: 1.2699201003367003e-05\n",
      "VAL: EPOCH 69/100 | BATCH 6/8 | LOSS: 1.2716670685871837e-05\n",
      "VAL: EPOCH 69/100 | BATCH 7/8 | LOSS: 1.3117124240125122e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 0/71 | LOSS: 1.1040221579605713e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 1/71 | LOSS: 1.2696822523139417e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 2/71 | LOSS: 1.2662543667829596e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 3/71 | LOSS: 1.2261199117347132e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 4/71 | LOSS: 1.2825041631003842e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 5/71 | LOSS: 1.268886777931281e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 6/71 | LOSS: 1.2992181577179249e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 7/71 | LOSS: 1.284634424791875e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 8/71 | LOSS: 1.3296601031874565e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 9/71 | LOSS: 1.3202597256167791e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 10/71 | LOSS: 1.3783145153535191e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 11/71 | LOSS: 1.368293722710708e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 12/71 | LOSS: 1.3610315223699077e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 13/71 | LOSS: 1.353903098788578e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 14/71 | LOSS: 1.3681468165790042e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 15/71 | LOSS: 1.3762132311967434e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 16/71 | LOSS: 1.3699285398829756e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 17/71 | LOSS: 1.3648919725205309e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 18/71 | LOSS: 1.3727005448584495e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 19/71 | LOSS: 1.3725221469940152e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 20/71 | LOSS: 1.3721092857719799e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 21/71 | LOSS: 1.3683748627293177e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 22/71 | LOSS: 1.369911763543749e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 23/71 | LOSS: 1.3779186626076504e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 24/71 | LOSS: 1.3709596314583906e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 25/71 | LOSS: 1.3660639454498709e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 26/71 | LOSS: 1.371376651217428e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 27/71 | LOSS: 1.3739835984389564e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 28/71 | LOSS: 1.366018941866812e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 29/71 | LOSS: 1.3610109332754898e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 30/71 | LOSS: 1.3609430102552587e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 31/71 | LOSS: 1.3571704926107486e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 32/71 | LOSS: 1.3571479719372071e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 33/71 | LOSS: 1.3696185573387671e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 34/71 | LOSS: 1.3689525258087087e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 35/71 | LOSS: 1.363454356982806e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 36/71 | LOSS: 1.364398930663073e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 37/71 | LOSS: 1.3624652709902301e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 38/71 | LOSS: 1.3592239063142095e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 39/71 | LOSS: 1.3598963209915383e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 40/71 | LOSS: 1.357338754953561e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 41/71 | LOSS: 1.3561727623060246e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 42/71 | LOSS: 1.352753199255879e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 43/71 | LOSS: 1.3568916921775037e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 44/71 | LOSS: 1.3471199680578947e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 45/71 | LOSS: 1.348852975238317e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 46/71 | LOSS: 1.3560460463347052e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 47/71 | LOSS: 1.3555581820886195e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 48/71 | LOSS: 1.3698677488422135e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 49/71 | LOSS: 1.3717842139158165e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 50/71 | LOSS: 1.3717390281860443e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 51/71 | LOSS: 1.3793495537146764e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 52/71 | LOSS: 1.387193014176413e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 53/71 | LOSS: 1.3956684631045425e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 54/71 | LOSS: 1.3979477774806913e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 55/71 | LOSS: 1.398906303100895e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 56/71 | LOSS: 1.3929305472023165e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 57/71 | LOSS: 1.3976054733261819e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 58/71 | LOSS: 1.3970848682466736e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 59/71 | LOSS: 1.3952388220180486e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 60/71 | LOSS: 1.3978320850106146e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 61/71 | LOSS: 1.398566850653996e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 62/71 | LOSS: 1.394708404347064e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 63/71 | LOSS: 1.3936647007994907e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 64/71 | LOSS: 1.3945065074949525e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 65/71 | LOSS: 1.3951925225366281e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 66/71 | LOSS: 1.3943161632854374e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 67/71 | LOSS: 1.393224828033828e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 68/71 | LOSS: 1.3908083269927664e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 69/71 | LOSS: 1.3916874429144497e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 70/71 | LOSS: 1.3824734164508057e-05\n",
      "VAL: EPOCH 70/100 | BATCH 0/8 | LOSS: 1.1115963388874661e-05\n",
      "VAL: EPOCH 70/100 | BATCH 1/8 | LOSS: 1.1864503449032782e-05\n",
      "VAL: EPOCH 70/100 | BATCH 2/8 | LOSS: 1.2146089223582143e-05\n",
      "VAL: EPOCH 70/100 | BATCH 3/8 | LOSS: 1.2098577826691326e-05\n",
      "VAL: EPOCH 70/100 | BATCH 4/8 | LOSS: 1.213096475112252e-05\n",
      "VAL: EPOCH 70/100 | BATCH 5/8 | LOSS: 1.2228593277541222e-05\n",
      "VAL: EPOCH 70/100 | BATCH 6/8 | LOSS: 1.2408240763761569e-05\n",
      "VAL: EPOCH 70/100 | BATCH 7/8 | LOSS: 1.2855801173827786e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 0/71 | LOSS: 1.3057408068561926e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 1/71 | LOSS: 1.3093930192553671e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 2/71 | LOSS: 1.2753697774314787e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 3/71 | LOSS: 1.2280236887818319e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 4/71 | LOSS: 1.237968372151954e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 5/71 | LOSS: 1.2597192380781053e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 6/71 | LOSS: 1.230642257204246e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 7/71 | LOSS: 1.2184240858914563e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 8/71 | LOSS: 1.240314213646343e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 9/71 | LOSS: 1.22672259749379e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 10/71 | LOSS: 1.2257553449175743e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 11/71 | LOSS: 1.2383062994558713e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 12/71 | LOSS: 1.239488070635931e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 13/71 | LOSS: 1.2346734105709142e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 14/71 | LOSS: 1.2322291331656743e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 15/71 | LOSS: 1.237427608202779e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 16/71 | LOSS: 1.2268895796510419e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 17/71 | LOSS: 1.2265402347111376e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 18/71 | LOSS: 1.224597494192973e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 19/71 | LOSS: 1.2266869271115866e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 20/71 | LOSS: 1.2257900380063802e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 21/71 | LOSS: 1.2223824548775816e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 22/71 | LOSS: 1.214924609287308e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 23/71 | LOSS: 1.2080092233190953e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 24/71 | LOSS: 1.2057923668180592e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 25/71 | LOSS: 1.2143815883274119e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 26/71 | LOSS: 1.2223000272502691e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 27/71 | LOSS: 1.23483811681321e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 28/71 | LOSS: 1.2415488025249817e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 29/71 | LOSS: 1.245116839830492e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 30/71 | LOSS: 1.2454098982890436e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 31/71 | LOSS: 1.24428644312502e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 32/71 | LOSS: 1.2440159948847864e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 33/71 | LOSS: 1.2411568417626446e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 34/71 | LOSS: 1.246596311830217e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 35/71 | LOSS: 1.2480980356283706e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 36/71 | LOSS: 1.2423397812518806e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 37/71 | LOSS: 1.2437350266593153e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 38/71 | LOSS: 1.2436236829671543e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 39/71 | LOSS: 1.2395641260809497e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 40/71 | LOSS: 1.2469414464359899e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 41/71 | LOSS: 1.2453864850873028e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 42/71 | LOSS: 1.2530193512353689e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 43/71 | LOSS: 1.255980336364618e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 44/71 | LOSS: 1.2601114111829601e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 45/71 | LOSS: 1.2571343552775469e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 46/71 | LOSS: 1.2526080160781968e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 47/71 | LOSS: 1.2510115652730747e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 48/71 | LOSS: 1.2462927250973216e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 49/71 | LOSS: 1.2526106329460163e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 50/71 | LOSS: 1.2495894528304538e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 51/71 | LOSS: 1.252494416803529e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 52/71 | LOSS: 1.2494535062529085e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 53/71 | LOSS: 1.255169335881537e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 54/71 | LOSS: 1.2568260370409751e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 55/71 | LOSS: 1.2612651760589091e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 56/71 | LOSS: 1.2629961642424428e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 57/71 | LOSS: 1.2621347885779588e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 58/71 | LOSS: 1.2568218091658664e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 59/71 | LOSS: 1.2618060645763763e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 60/71 | LOSS: 1.2620566491529986e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 61/71 | LOSS: 1.260978481901299e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 62/71 | LOSS: 1.2687072162479458e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 63/71 | LOSS: 1.2676920846388384e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 64/71 | LOSS: 1.265902991200654e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 65/71 | LOSS: 1.2683785633161085e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 66/71 | LOSS: 1.273308206786603e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 67/71 | LOSS: 1.2731843954123392e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 68/71 | LOSS: 1.278714035583687e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 69/71 | LOSS: 1.2768840679200367e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 70/71 | LOSS: 1.2724514343856972e-05\n",
      "VAL: EPOCH 71/100 | BATCH 0/8 | LOSS: 1.2064872862538323e-05\n",
      "VAL: EPOCH 71/100 | BATCH 1/8 | LOSS: 1.2150717338954564e-05\n",
      "VAL: EPOCH 71/100 | BATCH 2/8 | LOSS: 1.2407596538347812e-05\n",
      "VAL: EPOCH 71/100 | BATCH 3/8 | LOSS: 1.1882820217579138e-05\n",
      "VAL: EPOCH 71/100 | BATCH 4/8 | LOSS: 1.1730169717338867e-05\n",
      "VAL: EPOCH 71/100 | BATCH 5/8 | LOSS: 1.1980020038511915e-05\n",
      "VAL: EPOCH 71/100 | BATCH 6/8 | LOSS: 1.2137604893983475e-05\n",
      "VAL: EPOCH 71/100 | BATCH 7/8 | LOSS: 1.2269228932382248e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 0/71 | LOSS: 1.355927361146314e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 1/71 | LOSS: 1.4199522411217913e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 2/71 | LOSS: 1.3520487603576234e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 3/71 | LOSS: 1.4050029676582199e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 4/71 | LOSS: 1.3823565677739679e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 5/71 | LOSS: 1.4023199582879897e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 6/71 | LOSS: 1.4037624948416902e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 7/71 | LOSS: 1.3907752759223513e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 8/71 | LOSS: 1.3579715010160322e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 9/71 | LOSS: 1.362950106340577e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 10/71 | LOSS: 1.354237900324568e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 11/71 | LOSS: 1.3284521704311677e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 12/71 | LOSS: 1.321550518276546e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 13/71 | LOSS: 1.3034557209071604e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 14/71 | LOSS: 1.2913648545994267e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 15/71 | LOSS: 1.3065043674487242e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 16/71 | LOSS: 1.3345598638218635e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 17/71 | LOSS: 1.3257328343267242e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 18/71 | LOSS: 1.3376179930147422e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 19/71 | LOSS: 1.3396704707702156e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 20/71 | LOSS: 1.3541307583052134e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 21/71 | LOSS: 1.3498141346314117e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 22/71 | LOSS: 1.3446090119744085e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 23/71 | LOSS: 1.3325864301805268e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 24/71 | LOSS: 1.3327697306522168e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 25/71 | LOSS: 1.3362543541006744e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 26/71 | LOSS: 1.333545225390009e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 27/71 | LOSS: 1.3384896647039568e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 28/71 | LOSS: 1.3290118721638934e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 29/71 | LOSS: 1.3326575450870829e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 30/71 | LOSS: 1.318861720186936e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 31/71 | LOSS: 1.3216445921671038e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 32/71 | LOSS: 1.319961255362476e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 33/71 | LOSS: 1.332332867070149e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 34/71 | LOSS: 1.3251053236932162e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 35/71 | LOSS: 1.3274289762598022e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 36/71 | LOSS: 1.3286234133301076e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 37/71 | LOSS: 1.3228835685272075e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 38/71 | LOSS: 1.3150716068384309e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 39/71 | LOSS: 1.3186803175813112e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 40/71 | LOSS: 1.3222632443968195e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 41/71 | LOSS: 1.3264190415828212e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 42/71 | LOSS: 1.3338893654161988e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 43/71 | LOSS: 1.3521483961912137e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 44/71 | LOSS: 1.3533250704414159e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 45/71 | LOSS: 1.3590201314914536e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 46/71 | LOSS: 1.3670389481821404e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 47/71 | LOSS: 1.3718643098551789e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 48/71 | LOSS: 1.3720919979481996e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 49/71 | LOSS: 1.3763864590146113e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 50/71 | LOSS: 1.3717770521978701e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 51/71 | LOSS: 1.3691926666909309e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 52/71 | LOSS: 1.369113709548607e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 53/71 | LOSS: 1.3714047360769689e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 54/71 | LOSS: 1.369094764165559e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 55/71 | LOSS: 1.3681305014350592e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 56/71 | LOSS: 1.365560961985785e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 57/71 | LOSS: 1.3643931589317147e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 58/71 | LOSS: 1.365308410532958e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 59/71 | LOSS: 1.3655821688492626e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 60/71 | LOSS: 1.3617790098011983e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 61/71 | LOSS: 1.3580299500679229e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 62/71 | LOSS: 1.3591856706683295e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 63/71 | LOSS: 1.3576794415826043e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 64/71 | LOSS: 1.359595013267468e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 65/71 | LOSS: 1.356133223894038e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 66/71 | LOSS: 1.3559467795369243e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 67/71 | LOSS: 1.3551365207283251e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 68/71 | LOSS: 1.3525457240799712e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 69/71 | LOSS: 1.3538321309169987e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 70/71 | LOSS: 1.3560937051653115e-05\n",
      "VAL: EPOCH 72/100 | BATCH 0/8 | LOSS: 1.4848937098577153e-05\n",
      "VAL: EPOCH 72/100 | BATCH 1/8 | LOSS: 1.4938830190658337e-05\n",
      "VAL: EPOCH 72/100 | BATCH 2/8 | LOSS: 1.585322115715826e-05\n",
      "VAL: EPOCH 72/100 | BATCH 3/8 | LOSS: 1.5957717323544784e-05\n",
      "VAL: EPOCH 72/100 | BATCH 4/8 | LOSS: 1.6037919158407023e-05\n",
      "VAL: EPOCH 72/100 | BATCH 5/8 | LOSS: 1.5950399908130446e-05\n",
      "VAL: EPOCH 72/100 | BATCH 6/8 | LOSS: 1.5855891953313922e-05\n",
      "VAL: EPOCH 72/100 | BATCH 7/8 | LOSS: 1.6363407553399156e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 0/71 | LOSS: 1.8672062651603483e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 1/71 | LOSS: 1.5127618098631501e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 2/71 | LOSS: 1.5122866595144538e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 3/71 | LOSS: 1.5007003867140156e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 4/71 | LOSS: 1.4303455282060895e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 5/71 | LOSS: 1.421543962957609e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 6/71 | LOSS: 1.4243888991976356e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 7/71 | LOSS: 1.40796361165485e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 8/71 | LOSS: 1.4137544894765597e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 9/71 | LOSS: 1.3987647344038124e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 10/71 | LOSS: 1.3870159405500585e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 11/71 | LOSS: 1.3711763510097322e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 12/71 | LOSS: 1.3507925639434969e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 13/71 | LOSS: 1.3387935788549449e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 14/71 | LOSS: 1.3476668088211834e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 15/71 | LOSS: 1.355630922716955e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 16/71 | LOSS: 1.3708405758199446e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 17/71 | LOSS: 1.3633404705615249e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 18/71 | LOSS: 1.3600808798448844e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 19/71 | LOSS: 1.362101702397922e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 20/71 | LOSS: 1.4038279394818736e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 21/71 | LOSS: 1.3991773895130875e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 22/71 | LOSS: 1.3938684906347392e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 23/71 | LOSS: 1.3875543610690025e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 24/71 | LOSS: 1.3913790935475845e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 25/71 | LOSS: 1.383014320638792e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 26/71 | LOSS: 1.3920302990300115e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 27/71 | LOSS: 1.391579608675134e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 28/71 | LOSS: 1.4007249303866611e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 29/71 | LOSS: 1.4089106692457183e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 30/71 | LOSS: 1.4000053705521783e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 31/71 | LOSS: 1.3957620154769756e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 32/71 | LOSS: 1.3870548724353835e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 33/71 | LOSS: 1.3746778677159455e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 34/71 | LOSS: 1.3669366489921231e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 35/71 | LOSS: 1.3561254920407211e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 36/71 | LOSS: 1.3484573935636797e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 37/71 | LOSS: 1.343729380957819e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 38/71 | LOSS: 1.3417787391890903e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 39/71 | LOSS: 1.3363394873522339e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 40/71 | LOSS: 1.3450981253011311e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 41/71 | LOSS: 1.3432767554193214e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 42/71 | LOSS: 1.3392699910645905e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 43/71 | LOSS: 1.3370726161486925e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 44/71 | LOSS: 1.3401072182104043e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 45/71 | LOSS: 1.3373842529804476e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 46/71 | LOSS: 1.335060108218347e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 47/71 | LOSS: 1.3391421646247181e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 48/71 | LOSS: 1.3376997490662948e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 49/71 | LOSS: 1.3387715989665594e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 50/71 | LOSS: 1.3345190239431081e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 51/71 | LOSS: 1.3308865688416587e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 52/71 | LOSS: 1.3338409429844381e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 53/71 | LOSS: 1.329671815458116e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 54/71 | LOSS: 1.3273394275033339e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 55/71 | LOSS: 1.32493988920552e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 56/71 | LOSS: 1.3302231181704995e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 57/71 | LOSS: 1.321670543833973e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 58/71 | LOSS: 1.3262112819790935e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 59/71 | LOSS: 1.3287908859638265e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 60/71 | LOSS: 1.3306453198668752e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 61/71 | LOSS: 1.3354496346554726e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 62/71 | LOSS: 1.3369749640765977e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 63/71 | LOSS: 1.3335925572732776e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 64/71 | LOSS: 1.3342053814715025e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 65/71 | LOSS: 1.333217613302457e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 66/71 | LOSS: 1.3365835219525225e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 67/71 | LOSS: 1.3365213121955202e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 68/71 | LOSS: 1.3354887137804484e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 69/71 | LOSS: 1.3428762460015215e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 70/71 | LOSS: 1.3388170450077307e-05\n",
      "VAL: EPOCH 73/100 | BATCH 0/8 | LOSS: 1.18271418614313e-05\n",
      "VAL: EPOCH 73/100 | BATCH 1/8 | LOSS: 1.3089306776237208e-05\n",
      "VAL: EPOCH 73/100 | BATCH 2/8 | LOSS: 1.3592315553978551e-05\n",
      "VAL: EPOCH 73/100 | BATCH 3/8 | LOSS: 1.37831341362471e-05\n",
      "VAL: EPOCH 73/100 | BATCH 4/8 | LOSS: 1.3669662621396128e-05\n",
      "VAL: EPOCH 73/100 | BATCH 5/8 | LOSS: 1.3943265154618226e-05\n",
      "VAL: EPOCH 73/100 | BATCH 6/8 | LOSS: 1.4175022832724998e-05\n",
      "VAL: EPOCH 73/100 | BATCH 7/8 | LOSS: 1.4723489698553749e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 0/71 | LOSS: 1.542886639072094e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 1/71 | LOSS: 1.4395808648259845e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 2/71 | LOSS: 1.4747111587591158e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 3/71 | LOSS: 1.479379693591909e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 4/71 | LOSS: 1.4748888133908622e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 5/71 | LOSS: 1.4146684558606163e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 6/71 | LOSS: 1.4764249629349382e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 7/71 | LOSS: 1.4407512026082259e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 8/71 | LOSS: 1.446389968704251e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 9/71 | LOSS: 1.4346639636642066e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 10/71 | LOSS: 1.4161015893808905e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 11/71 | LOSS: 1.4046164475682113e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 12/71 | LOSS: 1.387239948459095e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 13/71 | LOSS: 1.392419855099953e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 14/71 | LOSS: 1.3853895325155463e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 15/71 | LOSS: 1.397233592115299e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 16/71 | LOSS: 1.3915437621643137e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 17/71 | LOSS: 1.4225151820331424e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 18/71 | LOSS: 1.4068211672374195e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 19/71 | LOSS: 1.4157346140564186e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 20/71 | LOSS: 1.4197952883218282e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 21/71 | LOSS: 1.4351535521034913e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 22/71 | LOSS: 1.432924016072597e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 23/71 | LOSS: 1.4167087253251035e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 24/71 | LOSS: 1.4157726036501117e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 25/71 | LOSS: 1.4019092409748388e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 26/71 | LOSS: 1.3967326392755947e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 27/71 | LOSS: 1.393053756666112e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 28/71 | LOSS: 1.3792223304837284e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 29/71 | LOSS: 1.3672406688177337e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 30/71 | LOSS: 1.3549182151637656e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 31/71 | LOSS: 1.3618271935911253e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 32/71 | LOSS: 1.3578487486291014e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 33/71 | LOSS: 1.3569877650939708e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 34/71 | LOSS: 1.349861821446601e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 35/71 | LOSS: 1.3427540150385337e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 36/71 | LOSS: 1.3410969463497954e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 37/71 | LOSS: 1.34969084587953e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 38/71 | LOSS: 1.3482324226434008e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 39/71 | LOSS: 1.3527033752325224e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 40/71 | LOSS: 1.3441210617978557e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 41/71 | LOSS: 1.3444332797094138e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 42/71 | LOSS: 1.3423095519693709e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 43/71 | LOSS: 1.3472490061096984e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 44/71 | LOSS: 1.3470624111909678e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 45/71 | LOSS: 1.3436303660178648e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 46/71 | LOSS: 1.3518980596656663e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 47/71 | LOSS: 1.3555914298043112e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 48/71 | LOSS: 1.3662745041816675e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 49/71 | LOSS: 1.36399677467125e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 50/71 | LOSS: 1.3657492708458163e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 51/71 | LOSS: 1.3729587023520322e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 52/71 | LOSS: 1.3705446930681268e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 53/71 | LOSS: 1.3678508804770428e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 54/71 | LOSS: 1.3700428098673001e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 55/71 | LOSS: 1.3652568018837233e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 56/71 | LOSS: 1.3637572042250867e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 57/71 | LOSS: 1.3616285082991517e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 58/71 | LOSS: 1.3652444869290714e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 59/71 | LOSS: 1.3573453982947587e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 60/71 | LOSS: 1.3610907623589184e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 61/71 | LOSS: 1.3649283734975466e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 62/71 | LOSS: 1.362160922340875e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 63/71 | LOSS: 1.3628489369921226e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 64/71 | LOSS: 1.3610046894777602e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 65/71 | LOSS: 1.3634261022437414e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 66/71 | LOSS: 1.3659559248372349e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 67/71 | LOSS: 1.3652645400404307e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 68/71 | LOSS: 1.3693737710654851e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 69/71 | LOSS: 1.3750637832085236e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 70/71 | LOSS: 1.3834986879066465e-05\n",
      "VAL: EPOCH 74/100 | BATCH 0/8 | LOSS: 1.0701176506699994e-05\n",
      "VAL: EPOCH 74/100 | BATCH 1/8 | LOSS: 1.1355628885212354e-05\n",
      "VAL: EPOCH 74/100 | BATCH 2/8 | LOSS: 1.1690756764437538e-05\n",
      "VAL: EPOCH 74/100 | BATCH 3/8 | LOSS: 1.1533069709912525e-05\n",
      "VAL: EPOCH 74/100 | BATCH 4/8 | LOSS: 1.156323432951467e-05\n",
      "VAL: EPOCH 74/100 | BATCH 5/8 | LOSS: 1.180946386133049e-05\n",
      "VAL: EPOCH 74/100 | BATCH 6/8 | LOSS: 1.198754164631412e-05\n",
      "VAL: EPOCH 74/100 | BATCH 7/8 | LOSS: 1.2490505014284281e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 0/71 | LOSS: 1.1390384315745905e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 1/71 | LOSS: 1.3328044587979093e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 2/71 | LOSS: 1.4521361663355492e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 3/71 | LOSS: 1.4374533520822297e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 4/71 | LOSS: 1.4065437426324934e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 5/71 | LOSS: 1.4019782914450237e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 6/71 | LOSS: 1.4068400040352052e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 7/71 | LOSS: 1.4186284261086257e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 8/71 | LOSS: 1.3837292903594465e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 9/71 | LOSS: 1.3818074694427197e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 10/71 | LOSS: 1.4000208631676452e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 11/71 | LOSS: 1.4145316830157148e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 12/71 | LOSS: 1.414731524654109e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 13/71 | LOSS: 1.426245561171007e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 14/71 | LOSS: 1.4169672977004665e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 15/71 | LOSS: 1.400444375576626e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 16/71 | LOSS: 1.392127973411013e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 17/71 | LOSS: 1.3807676623400766e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 18/71 | LOSS: 1.369980538201114e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 19/71 | LOSS: 1.365068587801943e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 20/71 | LOSS: 1.3464552510066868e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 21/71 | LOSS: 1.3409485001664672e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 22/71 | LOSS: 1.3429463805273965e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 23/71 | LOSS: 1.339403786460025e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 24/71 | LOSS: 1.342713410849683e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 25/71 | LOSS: 1.3480318662354526e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 26/71 | LOSS: 1.3390526301346082e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 27/71 | LOSS: 1.323382551683088e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 28/71 | LOSS: 1.3256049457815042e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 29/71 | LOSS: 1.3222728891075045e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 30/71 | LOSS: 1.3134049866846087e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 31/71 | LOSS: 1.3234094154768172e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 32/71 | LOSS: 1.3192929944841925e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 33/71 | LOSS: 1.3192701584664652e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 34/71 | LOSS: 1.3248512784360042e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 35/71 | LOSS: 1.3321203141458682e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 36/71 | LOSS: 1.3243527531370558e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 37/71 | LOSS: 1.3340756332150991e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 38/71 | LOSS: 1.3405066965788137e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 39/71 | LOSS: 1.3346121681934164e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 40/71 | LOSS: 1.3300393841679424e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 41/71 | LOSS: 1.3250342189643388e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 42/71 | LOSS: 1.3293204024511249e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 43/71 | LOSS: 1.3267906896627126e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 44/71 | LOSS: 1.3202594143674813e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 45/71 | LOSS: 1.3184579707423994e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 46/71 | LOSS: 1.3285468702769263e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 47/71 | LOSS: 1.3284673305709779e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 48/71 | LOSS: 1.3354663957327328e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 49/71 | LOSS: 1.335238534011296e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 50/71 | LOSS: 1.3333583785210708e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 51/71 | LOSS: 1.3340208047688626e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 52/71 | LOSS: 1.3454478886280481e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 53/71 | LOSS: 1.347033884047522e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 54/71 | LOSS: 1.3545750136307271e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 55/71 | LOSS: 1.3558027246420221e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 56/71 | LOSS: 1.3553915873911281e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 57/71 | LOSS: 1.3662303021631268e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 58/71 | LOSS: 1.3632460819681617e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 59/71 | LOSS: 1.3637241014900307e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 60/71 | LOSS: 1.3714016605546454e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 61/71 | LOSS: 1.3726351983589872e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 62/71 | LOSS: 1.3716920774539984e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 63/71 | LOSS: 1.3721291864499108e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 64/71 | LOSS: 1.375336873584624e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 65/71 | LOSS: 1.368559761624166e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 66/71 | LOSS: 1.3665372426291484e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 67/71 | LOSS: 1.371303944645607e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 68/71 | LOSS: 1.3697077783813322e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 69/71 | LOSS: 1.3697587408907046e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 70/71 | LOSS: 1.3661789138572664e-05\n",
      "VAL: EPOCH 75/100 | BATCH 0/8 | LOSS: 1.7104634025599808e-05\n",
      "VAL: EPOCH 75/100 | BATCH 1/8 | LOSS: 1.5924370472930605e-05\n",
      "VAL: EPOCH 75/100 | BATCH 2/8 | LOSS: 1.5576163605146576e-05\n",
      "VAL: EPOCH 75/100 | BATCH 3/8 | LOSS: 1.4496835092359106e-05\n",
      "VAL: EPOCH 75/100 | BATCH 4/8 | LOSS: 1.4302256749942898e-05\n",
      "VAL: EPOCH 75/100 | BATCH 5/8 | LOSS: 1.4562032447429374e-05\n",
      "VAL: EPOCH 75/100 | BATCH 6/8 | LOSS: 1.449121996951622e-05\n",
      "VAL: EPOCH 75/100 | BATCH 7/8 | LOSS: 1.4428844792746531e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 0/71 | LOSS: 1.532928945380263e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 1/71 | LOSS: 1.6572970707784407e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 2/71 | LOSS: 1.473949881377242e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 3/71 | LOSS: 1.4918512306394405e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 4/71 | LOSS: 1.4593116429750807e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 5/71 | LOSS: 1.5197626150135571e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 6/71 | LOSS: 1.538045970456941e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 7/71 | LOSS: 1.5130497104109963e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 8/71 | LOSS: 1.5017039913800545e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 9/71 | LOSS: 1.4596687469747848e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 10/71 | LOSS: 1.4355052123639986e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 11/71 | LOSS: 1.4308708841781481e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 12/71 | LOSS: 1.4133224655009913e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 13/71 | LOSS: 1.415781980540487e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 14/71 | LOSS: 1.4005196074625322e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 15/71 | LOSS: 1.4081689243994333e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 16/71 | LOSS: 1.3905900125203462e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 17/71 | LOSS: 1.3824306582440235e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 18/71 | LOSS: 1.3872758915424224e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 19/71 | LOSS: 1.37402867039782e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 20/71 | LOSS: 1.3716418444736128e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 21/71 | LOSS: 1.3762832581547132e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 22/71 | LOSS: 1.365780882105869e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 23/71 | LOSS: 1.3749004930711331e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 24/71 | LOSS: 1.3674926340172533e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 25/71 | LOSS: 1.3545850449120804e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 26/71 | LOSS: 1.3506401417005151e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 27/71 | LOSS: 1.339637801070889e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 28/71 | LOSS: 1.3441433048714755e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 29/71 | LOSS: 1.3328873289234858e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 30/71 | LOSS: 1.3276977369969245e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 31/71 | LOSS: 1.3357472226971367e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 32/71 | LOSS: 1.3270942486776745e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 33/71 | LOSS: 1.3206612600376556e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 34/71 | LOSS: 1.3213462105860734e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 35/71 | LOSS: 1.3280611230786437e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 36/71 | LOSS: 1.332973905035131e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 37/71 | LOSS: 1.3430388782197903e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 38/71 | LOSS: 1.3414520743446281e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 39/71 | LOSS: 1.3503939044312574e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 40/71 | LOSS: 1.339009200242666e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 41/71 | LOSS: 1.3340376253403346e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 42/71 | LOSS: 1.3239772099125752e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 43/71 | LOSS: 1.3308382242691799e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 44/71 | LOSS: 1.3355850569496397e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 45/71 | LOSS: 1.3296015209641116e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 46/71 | LOSS: 1.3235749510715051e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 47/71 | LOSS: 1.3253368043327404e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 48/71 | LOSS: 1.3237047935444006e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 49/71 | LOSS: 1.3235286678536795e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 50/71 | LOSS: 1.323325735332522e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 51/71 | LOSS: 1.3219076576066213e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 52/71 | LOSS: 1.32110037912015e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 53/71 | LOSS: 1.3237746518118203e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 54/71 | LOSS: 1.3221595575239255e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 55/71 | LOSS: 1.3262877116306169e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 56/71 | LOSS: 1.3268303965767764e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 57/71 | LOSS: 1.3216466899832775e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 58/71 | LOSS: 1.3219420090258532e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 59/71 | LOSS: 1.323583557374756e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 60/71 | LOSS: 1.3238408711436395e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 61/71 | LOSS: 1.3174492235726398e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 62/71 | LOSS: 1.3180979381320954e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 63/71 | LOSS: 1.3180344083707496e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 64/71 | LOSS: 1.3174467690427823e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 65/71 | LOSS: 1.3151466288091438e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 66/71 | LOSS: 1.3105212182476386e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 67/71 | LOSS: 1.3149977602926597e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 68/71 | LOSS: 1.3170208504838545e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 69/71 | LOSS: 1.3172604251719478e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 70/71 | LOSS: 1.3094421499029612e-05\n",
      "VAL: EPOCH 76/100 | BATCH 0/8 | LOSS: 1.7127589671872556e-05\n",
      "VAL: EPOCH 76/100 | BATCH 1/8 | LOSS: 1.691170473350212e-05\n",
      "VAL: EPOCH 76/100 | BATCH 2/8 | LOSS: 1.6973651023969676e-05\n",
      "VAL: EPOCH 76/100 | BATCH 3/8 | LOSS: 1.6053953459049808e-05\n",
      "VAL: EPOCH 76/100 | BATCH 4/8 | LOSS: 1.5785063442308456e-05\n",
      "VAL: EPOCH 76/100 | BATCH 5/8 | LOSS: 1.6063746443251148e-05\n",
      "VAL: EPOCH 76/100 | BATCH 6/8 | LOSS: 1.596545397270737e-05\n",
      "VAL: EPOCH 76/100 | BATCH 7/8 | LOSS: 1.5807233808118326e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 0/71 | LOSS: 1.7319292965112254e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 1/71 | LOSS: 1.4708828530274332e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 2/71 | LOSS: 1.5122530991599584e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 3/71 | LOSS: 1.4963423609515303e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 4/71 | LOSS: 1.4442427891481202e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 5/71 | LOSS: 1.5932688105143217e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 6/71 | LOSS: 1.5117038206621406e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 7/71 | LOSS: 1.4768542428100773e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 8/71 | LOSS: 1.5344104617219677e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 9/71 | LOSS: 1.5134860586840659e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 10/71 | LOSS: 1.5441100086635824e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 11/71 | LOSS: 1.5083882620577546e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 12/71 | LOSS: 1.554561738605396e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 13/71 | LOSS: 1.5335829110491822e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 14/71 | LOSS: 1.516369611636037e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 15/71 | LOSS: 1.5189210046173685e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 16/71 | LOSS: 1.4885775164978387e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 17/71 | LOSS: 1.485329438663838e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 18/71 | LOSS: 1.4859057237596349e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 19/71 | LOSS: 1.463883204451122e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 20/71 | LOSS: 1.4575389156691796e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 21/71 | LOSS: 1.4529151940223528e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 22/71 | LOSS: 1.4494563678770488e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 23/71 | LOSS: 1.4435831189985038e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 24/71 | LOSS: 1.4265815298131202e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 25/71 | LOSS: 1.421942372941815e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 26/71 | LOSS: 1.4176463250974107e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 27/71 | LOSS: 1.4031728207036004e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 28/71 | LOSS: 1.401184212666302e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 29/71 | LOSS: 1.391283170354048e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 30/71 | LOSS: 1.3842746328361588e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 31/71 | LOSS: 1.3741293457769643e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 32/71 | LOSS: 1.367784404988704e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 33/71 | LOSS: 1.368547937625622e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 34/71 | LOSS: 1.3552333413307288e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 35/71 | LOSS: 1.3479610869479882e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 36/71 | LOSS: 1.3483444908578767e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 37/71 | LOSS: 1.3342343879087007e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 38/71 | LOSS: 1.3349135350346422e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 39/71 | LOSS: 1.327107529505156e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 40/71 | LOSS: 1.3211171080467643e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 41/71 | LOSS: 1.3241554088814328e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 42/71 | LOSS: 1.3166759725758904e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 43/71 | LOSS: 1.3213412392525987e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 44/71 | LOSS: 1.3180609777110577e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 45/71 | LOSS: 1.3140902054779556e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 46/71 | LOSS: 1.3094393347892812e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 47/71 | LOSS: 1.3123139979143161e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 48/71 | LOSS: 1.3019693958957927e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 49/71 | LOSS: 1.2981592699361499e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 50/71 | LOSS: 1.2910120109582375e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 51/71 | LOSS: 1.2942652329701768e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 52/71 | LOSS: 1.2906649282980789e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 53/71 | LOSS: 1.2862844481855131e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 54/71 | LOSS: 1.282304266086695e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 55/71 | LOSS: 1.2813011803051008e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 56/71 | LOSS: 1.2841324924170694e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 57/71 | LOSS: 1.281859784222465e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 58/71 | LOSS: 1.2790350657438868e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 59/71 | LOSS: 1.2759858933956518e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 60/71 | LOSS: 1.2745637862087174e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 61/71 | LOSS: 1.2730497327274567e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 62/71 | LOSS: 1.272576564732599e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 63/71 | LOSS: 1.2665412299384116e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 64/71 | LOSS: 1.263998686996414e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 65/71 | LOSS: 1.258126883730063e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 66/71 | LOSS: 1.2534702201169714e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 67/71 | LOSS: 1.2505643593720057e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 68/71 | LOSS: 1.2479575464870626e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 69/71 | LOSS: 1.2531718272969427e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 70/71 | LOSS: 1.2476785933602155e-05\n",
      "VAL: EPOCH 77/100 | BATCH 0/8 | LOSS: 1.2040768524457235e-05\n",
      "VAL: EPOCH 77/100 | BATCH 1/8 | LOSS: 1.1974879726039944e-05\n",
      "VAL: EPOCH 77/100 | BATCH 2/8 | LOSS: 1.2381363376334775e-05\n",
      "VAL: EPOCH 77/100 | BATCH 3/8 | LOSS: 1.204141244670609e-05\n",
      "VAL: EPOCH 77/100 | BATCH 4/8 | LOSS: 1.2069745389453601e-05\n",
      "VAL: EPOCH 77/100 | BATCH 5/8 | LOSS: 1.1979277890835268e-05\n",
      "VAL: EPOCH 77/100 | BATCH 6/8 | LOSS: 1.1998572777624109e-05\n",
      "VAL: EPOCH 77/100 | BATCH 7/8 | LOSS: 1.2150974612268328e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 0/71 | LOSS: 1.2041347872582264e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 1/71 | LOSS: 1.154319124907488e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 2/71 | LOSS: 1.1933352349539442e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 3/71 | LOSS: 1.1419692782510538e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 4/71 | LOSS: 1.2191187852295116e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 5/71 | LOSS: 1.2120003096545892e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 6/71 | LOSS: 1.214064702383309e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 7/71 | LOSS: 1.2474773598114552e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 8/71 | LOSS: 1.255307516758977e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 9/71 | LOSS: 1.2943026649736567e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 10/71 | LOSS: 1.3132284948369488e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 11/71 | LOSS: 1.2964685235298626e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 12/71 | LOSS: 1.2975942809134722e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 13/71 | LOSS: 1.2802961594258835e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 14/71 | LOSS: 1.2591164644012073e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 15/71 | LOSS: 1.2564401401959913e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 16/71 | LOSS: 1.2605358653458739e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 17/71 | LOSS: 1.253147391051041e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 18/71 | LOSS: 1.2506723339212936e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 19/71 | LOSS: 1.2552982434499427e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 20/71 | LOSS: 1.2462204419231663e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 21/71 | LOSS: 1.2525088508978529e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 22/71 | LOSS: 1.2608734323293902e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 23/71 | LOSS: 1.2445470323048843e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 24/71 | LOSS: 1.2566200384753756e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 25/71 | LOSS: 1.258038589814812e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 26/71 | LOSS: 1.2542344881728275e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 27/71 | LOSS: 1.24821839498119e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 28/71 | LOSS: 1.2446256262372681e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 29/71 | LOSS: 1.253779197819919e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 30/71 | LOSS: 1.2619998033774355e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 31/71 | LOSS: 1.2615216235190019e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 32/71 | LOSS: 1.2516221955372727e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 33/71 | LOSS: 1.2588506224881926e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 34/71 | LOSS: 1.2527981665958317e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 35/71 | LOSS: 1.243559015367484e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 36/71 | LOSS: 1.2439819599692143e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 37/71 | LOSS: 1.2441714117908545e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 38/71 | LOSS: 1.2559345892581265e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 39/71 | LOSS: 1.2494321731537639e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 40/71 | LOSS: 1.2575824479338225e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 41/71 | LOSS: 1.2545549019532267e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 42/71 | LOSS: 1.2599545317603177e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 43/71 | LOSS: 1.2575673095356747e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 44/71 | LOSS: 1.2504989071102398e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 45/71 | LOSS: 1.2585632525176903e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 46/71 | LOSS: 1.2585769369213673e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 47/71 | LOSS: 1.266791334349667e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 48/71 | LOSS: 1.258854018135248e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 49/71 | LOSS: 1.2552350399346324e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 50/71 | LOSS: 1.252403274288643e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 51/71 | LOSS: 1.2468821995963271e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 52/71 | LOSS: 1.2445627499749568e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 53/71 | LOSS: 1.2397169366859426e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 54/71 | LOSS: 1.2359186819759833e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 55/71 | LOSS: 1.2291509619899443e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 56/71 | LOSS: 1.2250007960558338e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 57/71 | LOSS: 1.2199329062398113e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 58/71 | LOSS: 1.2186543815113185e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 59/71 | LOSS: 1.2203836665018267e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 60/71 | LOSS: 1.2185371502471485e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 61/71 | LOSS: 1.2151916991804742e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 62/71 | LOSS: 1.2109261834190138e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 63/71 | LOSS: 1.2088356754702545e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 64/71 | LOSS: 1.2096904594307908e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 65/71 | LOSS: 1.2123749594607172e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 66/71 | LOSS: 1.2135528614540383e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 67/71 | LOSS: 1.2113942094562363e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 68/71 | LOSS: 1.2097200601272272e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 69/71 | LOSS: 1.2089469003383004e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 70/71 | LOSS: 1.2018678492969345e-05\n",
      "VAL: EPOCH 78/100 | BATCH 0/8 | LOSS: 1.1067122613894753e-05\n",
      "VAL: EPOCH 78/100 | BATCH 1/8 | LOSS: 1.1929198080906644e-05\n",
      "VAL: EPOCH 78/100 | BATCH 2/8 | LOSS: 1.2092031283827964e-05\n",
      "VAL: EPOCH 78/100 | BATCH 3/8 | LOSS: 1.2244639719938277e-05\n",
      "VAL: EPOCH 78/100 | BATCH 4/8 | LOSS: 1.2114170567656402e-05\n",
      "VAL: EPOCH 78/100 | BATCH 5/8 | LOSS: 1.2294667158130324e-05\n",
      "VAL: EPOCH 78/100 | BATCH 6/8 | LOSS: 1.2122191817200343e-05\n",
      "VAL: EPOCH 78/100 | BATCH 7/8 | LOSS: 1.207741786402039e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 0/71 | LOSS: 1.152643199020531e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 1/71 | LOSS: 1.2179538771306397e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 2/71 | LOSS: 1.1950456306900984e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 3/71 | LOSS: 1.1240058711337042e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 4/71 | LOSS: 1.0949397074000445e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 5/71 | LOSS: 1.1551157664750159e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 6/71 | LOSS: 1.125286222044711e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 7/71 | LOSS: 1.1295018339296803e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 8/71 | LOSS: 1.122458444216237e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 9/71 | LOSS: 1.1385199104552158e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 10/71 | LOSS: 1.1273293239355553e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 11/71 | LOSS: 1.1297700363381106e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 12/71 | LOSS: 1.1595791945113047e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 13/71 | LOSS: 1.1757109794936177e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 14/71 | LOSS: 1.1733706257170222e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 15/71 | LOSS: 1.1763766792682873e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 16/71 | LOSS: 1.1669816665423295e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 17/71 | LOSS: 1.1713207489568353e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 18/71 | LOSS: 1.184311407230684e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 19/71 | LOSS: 1.1881739874297637e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 20/71 | LOSS: 1.1807159873049905e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 21/71 | LOSS: 1.1915745620750716e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 22/71 | LOSS: 1.1870691512582278e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 23/71 | LOSS: 1.2045881135236414e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 24/71 | LOSS: 1.2053293503413442e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 25/71 | LOSS: 1.200945602109781e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 26/71 | LOSS: 1.2029676764979269e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 27/71 | LOSS: 1.2054535675685787e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 28/71 | LOSS: 1.2000757155404025e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 29/71 | LOSS: 1.2246035142500962e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 30/71 | LOSS: 1.222793739983782e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 31/71 | LOSS: 1.2265524105714576e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 32/71 | LOSS: 1.2310696575062167e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 33/71 | LOSS: 1.245536941496539e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 34/71 | LOSS: 1.262465326103016e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 35/71 | LOSS: 1.2581145180471745e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 36/71 | LOSS: 1.2584942507257094e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 37/71 | LOSS: 1.2515793801675931e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 38/71 | LOSS: 1.2503442378193797e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 39/71 | LOSS: 1.2626959619410628e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 40/71 | LOSS: 1.2609214536496438e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 41/71 | LOSS: 1.2707625457897804e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 42/71 | LOSS: 1.2637310837182963e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 43/71 | LOSS: 1.2668486698947328e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 44/71 | LOSS: 1.2687122297292162e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 45/71 | LOSS: 1.2778526702870453e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 46/71 | LOSS: 1.2757485316622944e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 47/71 | LOSS: 1.2749090785746375e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 48/71 | LOSS: 1.272695462088153e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 49/71 | LOSS: 1.2678810417128262e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 50/71 | LOSS: 1.269871439695171e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 51/71 | LOSS: 1.2661530473241537e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 52/71 | LOSS: 1.2631076987297933e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 53/71 | LOSS: 1.2598578854926422e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 54/71 | LOSS: 1.2592136259958021e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 55/71 | LOSS: 1.2577419787313765e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 56/71 | LOSS: 1.259894334157651e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 57/71 | LOSS: 1.2576689500590498e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 58/71 | LOSS: 1.2568170736273514e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 59/71 | LOSS: 1.2586164833313282e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 60/71 | LOSS: 1.2586876079292212e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 61/71 | LOSS: 1.2535710392248508e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 62/71 | LOSS: 1.2556393523973095e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 63/71 | LOSS: 1.2567501855187402e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 64/71 | LOSS: 1.2536516708048742e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 65/71 | LOSS: 1.2589616179673502e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 66/71 | LOSS: 1.2577617210931137e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 67/71 | LOSS: 1.257861946910185e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 68/71 | LOSS: 1.2592606161479784e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 69/71 | LOSS: 1.2555963543750944e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 70/71 | LOSS: 1.2481999483882224e-05\n",
      "VAL: EPOCH 79/100 | BATCH 0/8 | LOSS: 1.2826016245526262e-05\n",
      "VAL: EPOCH 79/100 | BATCH 1/8 | LOSS: 1.2570953913382255e-05\n",
      "VAL: EPOCH 79/100 | BATCH 2/8 | LOSS: 1.2925081440092375e-05\n",
      "VAL: EPOCH 79/100 | BATCH 3/8 | LOSS: 1.2908717053505825e-05\n",
      "VAL: EPOCH 79/100 | BATCH 4/8 | LOSS: 1.3015802323934622e-05\n",
      "VAL: EPOCH 79/100 | BATCH 5/8 | LOSS: 1.303085749289797e-05\n",
      "VAL: EPOCH 79/100 | BATCH 6/8 | LOSS: 1.302896453125868e-05\n",
      "VAL: EPOCH 79/100 | BATCH 7/8 | LOSS: 1.370568793390703e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 0/71 | LOSS: 1.3346098057809286e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 1/71 | LOSS: 1.3556401427194942e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 2/71 | LOSS: 1.319919526091932e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 3/71 | LOSS: 1.2506479833973572e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 4/71 | LOSS: 1.2239944408065639e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 5/71 | LOSS: 1.2127896904227478e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 6/71 | LOSS: 1.1926359677870226e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 7/71 | LOSS: 1.1816323763014225e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 8/71 | LOSS: 1.1396619508256359e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 9/71 | LOSS: 1.1502230609039544e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 10/71 | LOSS: 1.1256823282482484e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 11/71 | LOSS: 1.1229764443972575e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 12/71 | LOSS: 1.1383729682935294e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 13/71 | LOSS: 1.1580336636792968e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 14/71 | LOSS: 1.1674427029599126e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 15/71 | LOSS: 1.1625514616753208e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 16/71 | LOSS: 1.1781207029022934e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 17/71 | LOSS: 1.1786338846933278e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 18/71 | LOSS: 1.1901591278729029e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 19/71 | LOSS: 1.1872789264089078e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 20/71 | LOSS: 1.1970287882182415e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 21/71 | LOSS: 1.1853342626140643e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 22/71 | LOSS: 1.1916450286820611e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 23/71 | LOSS: 1.1857967706419004e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 24/71 | LOSS: 1.1934747271880042e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 25/71 | LOSS: 1.1925153243142771e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 26/71 | LOSS: 1.2064445972673301e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 27/71 | LOSS: 1.1982948990407749e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 28/71 | LOSS: 1.1872420992435294e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 29/71 | LOSS: 1.1960254687437554e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 30/71 | LOSS: 1.207572703635613e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 31/71 | LOSS: 1.200940238277326e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 32/71 | LOSS: 1.1925409732308859e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 33/71 | LOSS: 1.191429809670594e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 34/71 | LOSS: 1.1868462648375758e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 35/71 | LOSS: 1.1850978024894073e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 36/71 | LOSS: 1.1846568225804288e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 37/71 | LOSS: 1.1822982335280885e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 38/71 | LOSS: 1.1743695489339865e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 39/71 | LOSS: 1.1818611847047578e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 40/71 | LOSS: 1.17770834505081e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 41/71 | LOSS: 1.1843966040898868e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 42/71 | LOSS: 1.1838991625598621e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 43/71 | LOSS: 1.1824404137686626e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 44/71 | LOSS: 1.1797806514045482e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 45/71 | LOSS: 1.1864056096269254e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 46/71 | LOSS: 1.1844525652896723e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 47/71 | LOSS: 1.1849074477747005e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 48/71 | LOSS: 1.18559719339828e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 49/71 | LOSS: 1.1815434281743365e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 50/71 | LOSS: 1.181863107751055e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 51/71 | LOSS: 1.1833381572264792e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 52/71 | LOSS: 1.1838684241531223e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 53/71 | LOSS: 1.1788344232223204e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 54/71 | LOSS: 1.1830357206318612e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 55/71 | LOSS: 1.1815643087434832e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 56/71 | LOSS: 1.1791667642874496e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 57/71 | LOSS: 1.1781979851391576e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 58/71 | LOSS: 1.179788697777825e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 59/71 | LOSS: 1.1737449176507652e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 60/71 | LOSS: 1.171438127696957e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 61/71 | LOSS: 1.1726704806571962e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 62/71 | LOSS: 1.1663843199332607e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 63/71 | LOSS: 1.168409202989551e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 64/71 | LOSS: 1.1697797288853997e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 65/71 | LOSS: 1.1681033825108457e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 66/71 | LOSS: 1.1650047197880626e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 67/71 | LOSS: 1.1692210243970348e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 68/71 | LOSS: 1.1706251337971585e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 69/71 | LOSS: 1.1683006219495188e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 70/71 | LOSS: 1.1713053616301589e-05\n",
      "VAL: EPOCH 80/100 | BATCH 0/8 | LOSS: 1.0340976587031037e-05\n",
      "VAL: EPOCH 80/100 | BATCH 1/8 | LOSS: 1.0371969437983353e-05\n",
      "VAL: EPOCH 80/100 | BATCH 2/8 | LOSS: 1.0352305556201221e-05\n",
      "VAL: EPOCH 80/100 | BATCH 3/8 | LOSS: 9.887691476251348e-06\n",
      "VAL: EPOCH 80/100 | BATCH 4/8 | LOSS: 9.75435377768008e-06\n",
      "VAL: EPOCH 80/100 | BATCH 5/8 | LOSS: 9.969453913072357e-06\n",
      "VAL: EPOCH 80/100 | BATCH 6/8 | LOSS: 1.0024836358622582e-05\n",
      "VAL: EPOCH 80/100 | BATCH 7/8 | LOSS: 1.0225253049611638e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 0/71 | LOSS: 1.1585396350710653e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 1/71 | LOSS: 1.0774342626973521e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 2/71 | LOSS: 1.3424570473337857e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 3/71 | LOSS: 1.2579266240209108e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 4/71 | LOSS: 1.295509900955949e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 5/71 | LOSS: 1.3201141882746015e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 6/71 | LOSS: 1.3415978693436565e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 7/71 | LOSS: 1.2956699379174097e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 8/71 | LOSS: 1.2843672468281713e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 9/71 | LOSS: 1.2487860203691525e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 10/71 | LOSS: 1.2413712779975453e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 11/71 | LOSS: 1.2114837848760848e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 12/71 | LOSS: 1.2165019353023336e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 13/71 | LOSS: 1.209653971402856e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 14/71 | LOSS: 1.2122771901582988e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 15/71 | LOSS: 1.2007716122752754e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 16/71 | LOSS: 1.1940707147861009e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 17/71 | LOSS: 1.2018331845966815e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 18/71 | LOSS: 1.2026518752942425e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 19/71 | LOSS: 1.2086848028047825e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 20/71 | LOSS: 1.2077642731171745e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 21/71 | LOSS: 1.2089703928226795e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 22/71 | LOSS: 1.2064787805012083e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 23/71 | LOSS: 1.2081733492171528e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 24/71 | LOSS: 1.2064244401699398e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 25/71 | LOSS: 1.206149020687847e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 26/71 | LOSS: 1.1935542884371157e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 27/71 | LOSS: 1.1910279681615066e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 28/71 | LOSS: 1.1935969394408889e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 29/71 | LOSS: 1.1875499634091587e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 30/71 | LOSS: 1.1808871931862086e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 31/71 | LOSS: 1.1781849821090873e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 32/71 | LOSS: 1.1794791520735474e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 33/71 | LOSS: 1.1822717303456397e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 34/71 | LOSS: 1.1800729537623868e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 35/71 | LOSS: 1.1760385480859744e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 36/71 | LOSS: 1.1742166026198066e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 37/71 | LOSS: 1.1663723542273816e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 38/71 | LOSS: 1.1612362845297843e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 39/71 | LOSS: 1.1577954887798115e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 40/71 | LOSS: 1.14939909574942e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 41/71 | LOSS: 1.1474231953235132e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 42/71 | LOSS: 1.1485755918371534e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 43/71 | LOSS: 1.1408429104035763e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 44/71 | LOSS: 1.1396089960019002e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 45/71 | LOSS: 1.1400817524902422e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 46/71 | LOSS: 1.1366093608654757e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 47/71 | LOSS: 1.1322924725239622e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 48/71 | LOSS: 1.1365553309959217e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 49/71 | LOSS: 1.1359763357177144e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 50/71 | LOSS: 1.1323638300738963e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 51/71 | LOSS: 1.1312955288863472e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 52/71 | LOSS: 1.1353363488352735e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 53/71 | LOSS: 1.1317916451608417e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 54/71 | LOSS: 1.1344989434292075e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 55/71 | LOSS: 1.1310983105821443e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 56/71 | LOSS: 1.133281936442169e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 57/71 | LOSS: 1.1391347313825652e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 58/71 | LOSS: 1.1427835858539458e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 59/71 | LOSS: 1.1377308176937125e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 60/71 | LOSS: 1.141899743653666e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 61/71 | LOSS: 1.1489993466379782e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 62/71 | LOSS: 1.1470224091594683e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 63/71 | LOSS: 1.1468449940821301e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 64/71 | LOSS: 1.142690268282492e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 65/71 | LOSS: 1.1459372575645279e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 66/71 | LOSS: 1.1478444253382927e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 67/71 | LOSS: 1.1509153597109938e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 68/71 | LOSS: 1.1575972983838005e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 69/71 | LOSS: 1.1600643197847863e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 70/71 | LOSS: 1.1782346096161988e-05\n",
      "VAL: EPOCH 81/100 | BATCH 0/8 | LOSS: 1.1076583177782595e-05\n",
      "VAL: EPOCH 81/100 | BATCH 1/8 | LOSS: 1.2395997146086302e-05\n",
      "VAL: EPOCH 81/100 | BATCH 2/8 | LOSS: 1.2444746971596032e-05\n",
      "VAL: EPOCH 81/100 | BATCH 3/8 | LOSS: 1.2504095138865523e-05\n",
      "VAL: EPOCH 81/100 | BATCH 4/8 | LOSS: 1.25868999020895e-05\n",
      "VAL: EPOCH 81/100 | BATCH 5/8 | LOSS: 1.3173342874021424e-05\n",
      "VAL: EPOCH 81/100 | BATCH 6/8 | LOSS: 1.3378953943694277e-05\n",
      "VAL: EPOCH 81/100 | BATCH 7/8 | LOSS: 1.3744285297434544e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 0/71 | LOSS: 1.0627371011651121e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 1/71 | LOSS: 1.5696653463237453e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 2/71 | LOSS: 1.546606836200226e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 3/71 | LOSS: 1.593235538166482e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 4/71 | LOSS: 1.6977734048850834e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 5/71 | LOSS: 1.6455505829071626e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 6/71 | LOSS: 1.7389286118226926e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 7/71 | LOSS: 1.6529049048585875e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 8/71 | LOSS: 1.591458820055575e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 9/71 | LOSS: 1.5651032754249172e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 10/71 | LOSS: 1.529330612274035e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 11/71 | LOSS: 1.5430933293221944e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 12/71 | LOSS: 1.5153560245660348e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 13/71 | LOSS: 1.5348649347808013e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 14/71 | LOSS: 1.4938079887845864e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 15/71 | LOSS: 1.4671545727651392e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 16/71 | LOSS: 1.4514339454763103e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 17/71 | LOSS: 1.432298222425743e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 18/71 | LOSS: 1.4208928453291791e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 19/71 | LOSS: 1.4314594818642944e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 20/71 | LOSS: 1.4032798823055123e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 21/71 | LOSS: 1.4258673094115114e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 22/71 | LOSS: 1.4035882723755607e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 23/71 | LOSS: 1.4186289414889567e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 24/71 | LOSS: 1.429023322998546e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 25/71 | LOSS: 1.4302506805016981e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 26/71 | LOSS: 1.4538062276659292e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 27/71 | LOSS: 1.4415632319599223e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 28/71 | LOSS: 1.4631392269652209e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 29/71 | LOSS: 1.4476394517259905e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 30/71 | LOSS: 1.4514400889663281e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 31/71 | LOSS: 1.4614672494417391e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 32/71 | LOSS: 1.4461665958868142e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 33/71 | LOSS: 1.4582224353302189e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 34/71 | LOSS: 1.4567264068838474e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 35/71 | LOSS: 1.4746625412145579e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 36/71 | LOSS: 1.4688710138757046e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 37/71 | LOSS: 1.474295796517828e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 38/71 | LOSS: 1.4890504877630752e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 39/71 | LOSS: 1.4808743367211719e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 40/71 | LOSS: 1.4909784201846356e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 41/71 | LOSS: 1.4823600546757612e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 42/71 | LOSS: 1.480123372075045e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 43/71 | LOSS: 1.4821624290561886e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 44/71 | LOSS: 1.4846070290433191e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 45/71 | LOSS: 1.4825114822483359e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 46/71 | LOSS: 1.4852343156235293e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 47/71 | LOSS: 1.4970479620994107e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 48/71 | LOSS: 1.491907294955442e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 49/71 | LOSS: 1.5004014694568468e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 50/71 | LOSS: 1.5043876088634088e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 51/71 | LOSS: 1.5037492718105428e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 52/71 | LOSS: 1.5164075536957617e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 53/71 | LOSS: 1.5158960683981638e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 54/71 | LOSS: 1.5231926276597237e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 55/71 | LOSS: 1.523253998974334e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 56/71 | LOSS: 1.5246079544196714e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 57/71 | LOSS: 1.524454604617043e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 58/71 | LOSS: 1.5255546220602918e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 59/71 | LOSS: 1.532025679201373e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 60/71 | LOSS: 1.5257107675171908e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 61/71 | LOSS: 1.5339582953475335e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 62/71 | LOSS: 1.5332660503132963e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 63/71 | LOSS: 1.5328259792113386e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 64/71 | LOSS: 1.525934595310201e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 65/71 | LOSS: 1.526462520731994e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 66/71 | LOSS: 1.5283897170517246e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 67/71 | LOSS: 1.5269662967779476e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 68/71 | LOSS: 1.5244741007336415e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 69/71 | LOSS: 1.5196825604237218e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 70/71 | LOSS: 1.5096072966628873e-05\n",
      "VAL: EPOCH 82/100 | BATCH 0/8 | LOSS: 1.2202606740174815e-05\n",
      "VAL: EPOCH 82/100 | BATCH 1/8 | LOSS: 1.2057421827194048e-05\n",
      "VAL: EPOCH 82/100 | BATCH 2/8 | LOSS: 1.3197797367562695e-05\n",
      "VAL: EPOCH 82/100 | BATCH 3/8 | LOSS: 1.3321936421561986e-05\n",
      "VAL: EPOCH 82/100 | BATCH 4/8 | LOSS: 1.3498918633558787e-05\n",
      "VAL: EPOCH 82/100 | BATCH 5/8 | LOSS: 1.3513176175668681e-05\n",
      "VAL: EPOCH 82/100 | BATCH 6/8 | LOSS: 1.3490333133501866e-05\n",
      "VAL: EPOCH 82/100 | BATCH 7/8 | LOSS: 1.3836852644999453e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 0/71 | LOSS: 1.386815529258456e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 1/71 | LOSS: 1.3173864772397792e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 2/71 | LOSS: 1.272270401386777e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 3/71 | LOSS: 1.2410438785082079e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 4/71 | LOSS: 1.2044214781781192e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 5/71 | LOSS: 1.1934221068562087e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 6/71 | LOSS: 1.1671395051442751e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 7/71 | LOSS: 1.14397446395742e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 8/71 | LOSS: 1.1773613247593554e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 9/71 | LOSS: 1.163193319371203e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 10/71 | LOSS: 1.1757605451831212e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 11/71 | LOSS: 1.1785791154276618e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 12/71 | LOSS: 1.186737993591053e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 13/71 | LOSS: 1.1969126912195602e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 14/71 | LOSS: 1.1892391194123775e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 15/71 | LOSS: 1.178008653823781e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 16/71 | LOSS: 1.1796505050904884e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 17/71 | LOSS: 1.1782102269029969e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 18/71 | LOSS: 1.1642399266221267e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 19/71 | LOSS: 1.1692006592056713e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 20/71 | LOSS: 1.1644593368622563e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 21/71 | LOSS: 1.1627236248882996e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 22/71 | LOSS: 1.1689650542077446e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 23/71 | LOSS: 1.1709506907209288e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 24/71 | LOSS: 1.1795014797826298e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 25/71 | LOSS: 1.1836172374247466e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 26/71 | LOSS: 1.1888874454328928e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 27/71 | LOSS: 1.1816956137928563e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 28/71 | LOSS: 1.1900079761258455e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 29/71 | LOSS: 1.1868835827044677e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 30/71 | LOSS: 1.1784786634492658e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 31/71 | LOSS: 1.1641669601658577e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 32/71 | LOSS: 1.17643420515268e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 33/71 | LOSS: 1.1736272935034803e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 34/71 | LOSS: 1.1866686769021077e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 35/71 | LOSS: 1.1843572110592504e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 36/71 | LOSS: 1.1823738286447888e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 37/71 | LOSS: 1.176154828978102e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 38/71 | LOSS: 1.1691556303561903e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 39/71 | LOSS: 1.167966684079147e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 40/71 | LOSS: 1.1613789621794529e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 41/71 | LOSS: 1.1622635871130374e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 42/71 | LOSS: 1.1693729183482287e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 43/71 | LOSS: 1.1713177276429963e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 44/71 | LOSS: 1.177757813921845e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 45/71 | LOSS: 1.1815598258102039e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 46/71 | LOSS: 1.1829642488087399e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 47/71 | LOSS: 1.1824412865735212e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 48/71 | LOSS: 1.1943956451432314e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 49/71 | LOSS: 1.1948630708502606e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 50/71 | LOSS: 1.1990639869313158e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 51/71 | LOSS: 1.2003485555676609e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 52/71 | LOSS: 1.200789998654487e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 53/71 | LOSS: 1.2045887615386164e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 54/71 | LOSS: 1.2052975811953233e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 55/71 | LOSS: 1.203019883178058e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 56/71 | LOSS: 1.2056008432537374e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 57/71 | LOSS: 1.2076481417697605e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 58/71 | LOSS: 1.2078347119036076e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 59/71 | LOSS: 1.2069810948863353e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 60/71 | LOSS: 1.204847310754169e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 61/71 | LOSS: 1.2049821356002564e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 62/71 | LOSS: 1.2041066088025378e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 63/71 | LOSS: 1.2042489871078033e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 64/71 | LOSS: 1.2086401357709503e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 65/71 | LOSS: 1.205592040302387e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 66/71 | LOSS: 1.202931395452122e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 67/71 | LOSS: 1.2010094237399136e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 68/71 | LOSS: 1.2025459316488275e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 69/71 | LOSS: 1.2030328161927172e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 70/71 | LOSS: 1.1950222402370394e-05\n",
      "VAL: EPOCH 83/100 | BATCH 0/8 | LOSS: 1.2530482308648061e-05\n",
      "VAL: EPOCH 83/100 | BATCH 1/8 | LOSS: 1.2054619674017886e-05\n",
      "VAL: EPOCH 83/100 | BATCH 2/8 | LOSS: 1.2239143264499338e-05\n",
      "VAL: EPOCH 83/100 | BATCH 3/8 | LOSS: 1.2019619362035883e-05\n",
      "VAL: EPOCH 83/100 | BATCH 4/8 | LOSS: 1.1918526615772862e-05\n",
      "VAL: EPOCH 83/100 | BATCH 5/8 | LOSS: 1.2055876292530835e-05\n",
      "VAL: EPOCH 83/100 | BATCH 6/8 | LOSS: 1.189036954331511e-05\n",
      "VAL: EPOCH 83/100 | BATCH 7/8 | LOSS: 1.173921157260338e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 0/71 | LOSS: 1.2008809790131636e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 1/71 | LOSS: 1.1384898698452162e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 2/71 | LOSS: 1.2014435014862102e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 3/71 | LOSS: 1.1867382227137568e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 4/71 | LOSS: 1.2439223064575345e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 5/71 | LOSS: 1.2624882704888781e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 6/71 | LOSS: 1.2237375943056708e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 7/71 | LOSS: 1.182782852993114e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 8/71 | LOSS: 1.1773030564654619e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 9/71 | LOSS: 1.1790553344326327e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 10/71 | LOSS: 1.1707762744415298e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 11/71 | LOSS: 1.1799998219430563e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 12/71 | LOSS: 1.1959654539868307e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 13/71 | LOSS: 1.2087534092383326e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 14/71 | LOSS: 1.2222742043377366e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 15/71 | LOSS: 1.2295514181914768e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 16/71 | LOSS: 1.2336795674962237e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 17/71 | LOSS: 1.2572195525232624e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 18/71 | LOSS: 1.2484720556178791e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 19/71 | LOSS: 1.269172284992237e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 20/71 | LOSS: 1.2728319277465788e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 21/71 | LOSS: 1.2819265231717674e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 22/71 | LOSS: 1.3053263303489707e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 23/71 | LOSS: 1.2966761687494e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 24/71 | LOSS: 1.305353671341436e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 25/71 | LOSS: 1.3319722676477306e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 26/71 | LOSS: 1.3254130964115677e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 27/71 | LOSS: 1.3292483702441262e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 28/71 | LOSS: 1.3292245339784884e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 29/71 | LOSS: 1.3303811617030684e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 30/71 | LOSS: 1.3346898471823923e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 31/71 | LOSS: 1.3403979096437979e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 32/71 | LOSS: 1.3350120449710326e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 33/71 | LOSS: 1.3312570685871725e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 34/71 | LOSS: 1.3418999073370026e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 35/71 | LOSS: 1.3459025037971312e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 36/71 | LOSS: 1.3516689692993611e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 37/71 | LOSS: 1.3523652700984568e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 38/71 | LOSS: 1.3466900572194181e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 39/71 | LOSS: 1.3450303799800168e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 40/71 | LOSS: 1.3490138482621859e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 41/71 | LOSS: 1.3428464691449854e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 42/71 | LOSS: 1.3393621440289196e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 43/71 | LOSS: 1.3308892469219494e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 44/71 | LOSS: 1.3216437279576591e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 45/71 | LOSS: 1.3150874763042616e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 46/71 | LOSS: 1.3095560403753746e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 47/71 | LOSS: 1.3049420601873862e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 48/71 | LOSS: 1.297778756136778e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 49/71 | LOSS: 1.2893523089587689e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 50/71 | LOSS: 1.286547404899965e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 51/71 | LOSS: 1.2791851671723774e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 52/71 | LOSS: 1.2753017111912036e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 53/71 | LOSS: 1.2760417188541464e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 54/71 | LOSS: 1.269320599187102e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 55/71 | LOSS: 1.2609185400184028e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 56/71 | LOSS: 1.2579811035513494e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 57/71 | LOSS: 1.2534290786349849e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 58/71 | LOSS: 1.2544319299380176e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 59/71 | LOSS: 1.2467711621866329e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 60/71 | LOSS: 1.2465411183242037e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 61/71 | LOSS: 1.2406362510509713e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 62/71 | LOSS: 1.2388383982484119e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 63/71 | LOSS: 1.2297519802473289e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 64/71 | LOSS: 1.225593251337369e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 65/71 | LOSS: 1.2202224890202299e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 66/71 | LOSS: 1.2146056341502551e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 67/71 | LOSS: 1.2126938105104595e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 68/71 | LOSS: 1.2108518854222273e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 69/71 | LOSS: 1.2064327503529577e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 70/71 | LOSS: 1.2016344928116256e-05\n",
      "VAL: EPOCH 84/100 | BATCH 0/8 | LOSS: 8.543341209588107e-06\n",
      "VAL: EPOCH 84/100 | BATCH 1/8 | LOSS: 9.323011454398511e-06\n",
      "VAL: EPOCH 84/100 | BATCH 2/8 | LOSS: 9.6765849472528e-06\n",
      "VAL: EPOCH 84/100 | BATCH 3/8 | LOSS: 9.520085995973204e-06\n",
      "VAL: EPOCH 84/100 | BATCH 4/8 | LOSS: 9.614657028578221e-06\n",
      "VAL: EPOCH 84/100 | BATCH 5/8 | LOSS: 9.996719957901709e-06\n",
      "VAL: EPOCH 84/100 | BATCH 6/8 | LOSS: 1.0054605679345383e-05\n",
      "VAL: EPOCH 84/100 | BATCH 7/8 | LOSS: 1.0503952125873184e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 0/71 | LOSS: 9.16947828955017e-06\n",
      "TRAIN: EPOCH 85/100 | BATCH 1/71 | LOSS: 1.0354493952036137e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 2/71 | LOSS: 9.121887766620299e-06\n",
      "TRAIN: EPOCH 85/100 | BATCH 3/71 | LOSS: 9.765140703166253e-06\n",
      "TRAIN: EPOCH 85/100 | BATCH 4/71 | LOSS: 9.82942456175806e-06\n",
      "TRAIN: EPOCH 85/100 | BATCH 5/71 | LOSS: 1.0075209956994513e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 6/71 | LOSS: 1.025433708231763e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 7/71 | LOSS: 1.0203853321399947e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 8/71 | LOSS: 1.0368863513576798e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 9/71 | LOSS: 1.0464951265021227e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 10/71 | LOSS: 1.0385859323750166e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 11/71 | LOSS: 1.0644565615317939e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 12/71 | LOSS: 1.0667235423730184e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 13/71 | LOSS: 1.0588948628407837e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 14/71 | LOSS: 1.046450634021312e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 15/71 | LOSS: 1.0476303941686638e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 16/71 | LOSS: 1.0802501706433867e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 17/71 | LOSS: 1.0838853945137493e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 18/71 | LOSS: 1.0987486396180956e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 19/71 | LOSS: 1.0942113749479177e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 20/71 | LOSS: 1.1074654789159208e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 21/71 | LOSS: 1.103157617101467e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 22/71 | LOSS: 1.0931733136796696e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 23/71 | LOSS: 1.0912467663123001e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 24/71 | LOSS: 1.0789080843096599e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 25/71 | LOSS: 1.0898197685422088e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 26/71 | LOSS: 1.0880137745482864e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 27/71 | LOSS: 1.0807207932625065e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 28/71 | LOSS: 1.0783706690337317e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 29/71 | LOSS: 1.0736717013060115e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 30/71 | LOSS: 1.0774667991368446e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 31/71 | LOSS: 1.0793472455361552e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 32/71 | LOSS: 1.0914572047829896e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 33/71 | LOSS: 1.0868015472035896e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 34/71 | LOSS: 1.0820255920407362e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 35/71 | LOSS: 1.0822871571589429e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 36/71 | LOSS: 1.0821170444369027e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 37/71 | LOSS: 1.0822763263269352e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 38/71 | LOSS: 1.080196055297095e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 39/71 | LOSS: 1.0873399287447682e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 40/71 | LOSS: 1.0890151936952707e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 41/71 | LOSS: 1.0982660591079843e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 42/71 | LOSS: 1.1004020856235848e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 43/71 | LOSS: 1.1079066098318435e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 44/71 | LOSS: 1.119103722481264e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 45/71 | LOSS: 1.1163349545708828e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 46/71 | LOSS: 1.1197357862522737e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 47/71 | LOSS: 1.1256036467936307e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 48/71 | LOSS: 1.1235456923361421e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 49/71 | LOSS: 1.1344909744366305e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 50/71 | LOSS: 1.1351997843531517e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 51/71 | LOSS: 1.1369173692471053e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 52/71 | LOSS: 1.1412303910271115e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 53/71 | LOSS: 1.139863860333679e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 54/71 | LOSS: 1.1368344696000515e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 55/71 | LOSS: 1.1407431251037841e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 56/71 | LOSS: 1.145918818030878e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 57/71 | LOSS: 1.1408644899473011e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 58/71 | LOSS: 1.139804096626102e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 59/71 | LOSS: 1.1409037718597877e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 60/71 | LOSS: 1.1359167490540965e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 61/71 | LOSS: 1.1357626238262334e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 62/71 | LOSS: 1.1340978354772752e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 63/71 | LOSS: 1.1351080885901865e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 64/71 | LOSS: 1.1374059935266045e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 65/71 | LOSS: 1.1360223225258807e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 66/71 | LOSS: 1.1373721653358393e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 67/71 | LOSS: 1.1364677822498111e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 68/71 | LOSS: 1.1358099969135989e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 69/71 | LOSS: 1.1386193779929143e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 70/71 | LOSS: 1.1389932040683433e-05\n",
      "VAL: EPOCH 85/100 | BATCH 0/8 | LOSS: 9.11401912162546e-06\n",
      "VAL: EPOCH 85/100 | BATCH 1/8 | LOSS: 9.876597687252797e-06\n",
      "VAL: EPOCH 85/100 | BATCH 2/8 | LOSS: 1.0461437341291457e-05\n",
      "VAL: EPOCH 85/100 | BATCH 3/8 | LOSS: 1.0419423460916732e-05\n",
      "VAL: EPOCH 85/100 | BATCH 4/8 | LOSS: 1.0610343997541349e-05\n",
      "VAL: EPOCH 85/100 | BATCH 5/8 | LOSS: 1.0695453208124187e-05\n",
      "VAL: EPOCH 85/100 | BATCH 6/8 | LOSS: 1.073836834361178e-05\n",
      "VAL: EPOCH 85/100 | BATCH 7/8 | LOSS: 1.1239928198847338e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 0/71 | LOSS: 1.255491042684298e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 1/71 | LOSS: 1.1275325050519314e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 2/71 | LOSS: 1.0961965926981065e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 3/71 | LOSS: 1.1092848126281751e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 4/71 | LOSS: 1.094382441806374e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 5/71 | LOSS: 1.125754291327515e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 6/71 | LOSS: 1.072007021970681e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 7/71 | LOSS: 1.059404371517303e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 8/71 | LOSS: 1.061408774047676e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 9/71 | LOSS: 1.052277730195783e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 10/71 | LOSS: 1.0348947563960047e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 11/71 | LOSS: 1.0171335664684497e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 12/71 | LOSS: 1.0172632615789413e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 13/71 | LOSS: 1.0086198666873056e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 14/71 | LOSS: 1.0158816742963002e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 15/71 | LOSS: 1.016817913068735e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 16/71 | LOSS: 1.0088962827627684e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 17/71 | LOSS: 1.0082965925701299e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 18/71 | LOSS: 1.004125830437069e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 19/71 | LOSS: 1.0133326031791513e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 20/71 | LOSS: 1.0145214089009511e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 21/71 | LOSS: 1.0063373090154279e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 22/71 | LOSS: 9.995052807979569e-06\n",
      "TRAIN: EPOCH 86/100 | BATCH 23/71 | LOSS: 1.0009314678427472e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 24/71 | LOSS: 1.0003907191276084e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 25/71 | LOSS: 1.0008406941499114e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 26/71 | LOSS: 1.0056748275625674e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 27/71 | LOSS: 1.0131137677100404e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 28/71 | LOSS: 1.0304638039840562e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 29/71 | LOSS: 1.029490100942591e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 30/71 | LOSS: 1.0295657375536393e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 31/71 | LOSS: 1.0237414727498617e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 32/71 | LOSS: 1.0243171125308216e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 33/71 | LOSS: 1.0280861341230133e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 34/71 | LOSS: 1.0274210892800641e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 35/71 | LOSS: 1.0320980158616698e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 36/71 | LOSS: 1.0381330252425255e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 37/71 | LOSS: 1.0376205936106425e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 38/71 | LOSS: 1.0436272998795641e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 39/71 | LOSS: 1.047732982897287e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 40/71 | LOSS: 1.0451404283763046e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 41/71 | LOSS: 1.0506302974785545e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 42/71 | LOSS: 1.062739355865931e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 43/71 | LOSS: 1.0665573592467064e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 44/71 | LOSS: 1.0659488058687808e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 45/71 | LOSS: 1.0660807635323852e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 46/71 | LOSS: 1.0681067501439991e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 47/71 | LOSS: 1.066459437500574e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 48/71 | LOSS: 1.0683159063996662e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 49/71 | LOSS: 1.0682840656954795e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 50/71 | LOSS: 1.0639282931258682e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 51/71 | LOSS: 1.0631790631193257e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 52/71 | LOSS: 1.0617337740665601e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 53/71 | LOSS: 1.0614733835407404e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 54/71 | LOSS: 1.0631608215176544e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 55/71 | LOSS: 1.0602278507576557e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 56/71 | LOSS: 1.0618992044392303e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 57/71 | LOSS: 1.0668884897313546e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 58/71 | LOSS: 1.0767225029343597e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 59/71 | LOSS: 1.0820707166203647e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 60/71 | LOSS: 1.085059548280637e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 61/71 | LOSS: 1.079780156854418e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 62/71 | LOSS: 1.0949891829012711e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 63/71 | LOSS: 1.0958508475766848e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 64/71 | LOSS: 1.1030511751204568e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 65/71 | LOSS: 1.1104117103252175e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 66/71 | LOSS: 1.1180484795944206e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 67/71 | LOSS: 1.1311072010839244e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 68/71 | LOSS: 1.1304408311896596e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 69/71 | LOSS: 1.1485354200755995e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 70/71 | LOSS: 1.1576260367548176e-05\n",
      "VAL: EPOCH 86/100 | BATCH 0/8 | LOSS: 1.0583098628558218e-05\n",
      "VAL: EPOCH 86/100 | BATCH 1/8 | LOSS: 1.1459991128504043e-05\n",
      "VAL: EPOCH 86/100 | BATCH 2/8 | LOSS: 1.1500287352343245e-05\n",
      "VAL: EPOCH 86/100 | BATCH 3/8 | LOSS: 1.124281220654666e-05\n",
      "VAL: EPOCH 86/100 | BATCH 4/8 | LOSS: 1.1156887376273517e-05\n",
      "VAL: EPOCH 86/100 | BATCH 5/8 | LOSS: 1.1737582402323218e-05\n",
      "VAL: EPOCH 86/100 | BATCH 6/8 | LOSS: 1.1694089542808278e-05\n",
      "VAL: EPOCH 86/100 | BATCH 7/8 | LOSS: 1.1613743936322862e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 0/71 | LOSS: 1.3345187653612811e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 1/71 | LOSS: 1.210641357829445e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 2/71 | LOSS: 1.2892138026169656e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 3/71 | LOSS: 1.2720108543362585e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 4/71 | LOSS: 1.282322482438758e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 5/71 | LOSS: 1.2452171783176405e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 6/71 | LOSS: 1.2229650175348589e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 7/71 | LOSS: 1.2226505305079627e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 8/71 | LOSS: 1.2138201176033666e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 9/71 | LOSS: 1.2181885904283262e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 10/71 | LOSS: 1.1847806242506273e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 11/71 | LOSS: 1.164784885077097e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 12/71 | LOSS: 1.1369950838203435e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 13/71 | LOSS: 1.1385180252026267e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 14/71 | LOSS: 1.1281190927547868e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 15/71 | LOSS: 1.1210861941890471e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 16/71 | LOSS: 1.1170214533836662e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 17/71 | LOSS: 1.1213894797240047e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 18/71 | LOSS: 1.1218741938615781e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 19/71 | LOSS: 1.1186264282514458e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 20/71 | LOSS: 1.1137823483815218e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 21/71 | LOSS: 1.1182710006158397e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 22/71 | LOSS: 1.1088471262115965e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 23/71 | LOSS: 1.1044292439995237e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 24/71 | LOSS: 1.0953309720207472e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 25/71 | LOSS: 1.0949129633585331e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 26/71 | LOSS: 1.096635649581138e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 27/71 | LOSS: 1.0920633680403366e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 28/71 | LOSS: 1.0836058083504182e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 29/71 | LOSS: 1.0789478255901486e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 30/71 | LOSS: 1.0813235378243976e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 31/71 | LOSS: 1.075661492677682e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 32/71 | LOSS: 1.0814900881748393e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 33/71 | LOSS: 1.0744145904883386e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 34/71 | LOSS: 1.0759513211918861e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 35/71 | LOSS: 1.0726162498839483e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 36/71 | LOSS: 1.0726559668074589e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 37/71 | LOSS: 1.0688803070623085e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 38/71 | LOSS: 1.0675966829391351e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 39/71 | LOSS: 1.0699921608647855e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 40/71 | LOSS: 1.076979428833555e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 41/71 | LOSS: 1.0722396793772488e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 42/71 | LOSS: 1.0741471754066202e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 43/71 | LOSS: 1.0781873015730525e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 44/71 | LOSS: 1.0767980246681772e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 45/71 | LOSS: 1.0753675565944793e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 46/71 | LOSS: 1.0799337689333813e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 47/71 | LOSS: 1.0783910245966885e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 48/71 | LOSS: 1.0768435978596763e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 49/71 | LOSS: 1.0750645196822007e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 50/71 | LOSS: 1.079101489534985e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 51/71 | LOSS: 1.0777679790408001e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 52/71 | LOSS: 1.074132450292313e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 53/71 | LOSS: 1.0765702713629085e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 54/71 | LOSS: 1.0726201451854484e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 55/71 | LOSS: 1.0774656353922702e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 56/71 | LOSS: 1.0752918075141088e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 57/71 | LOSS: 1.0770586507940857e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 58/71 | LOSS: 1.0811466921241243e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 59/71 | LOSS: 1.0896360648378807e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 60/71 | LOSS: 1.0984679990587374e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 61/71 | LOSS: 1.0980335211511814e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 62/71 | LOSS: 1.102213711477354e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 63/71 | LOSS: 1.1037767180255287e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 64/71 | LOSS: 1.1006653767253739e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 65/71 | LOSS: 1.1015731629551297e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 66/71 | LOSS: 1.100436686861777e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 67/71 | LOSS: 1.1034108718590955e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 68/71 | LOSS: 1.1039582389365524e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 69/71 | LOSS: 1.1064777884582456e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 70/71 | LOSS: 1.1043476187181331e-05\n",
      "VAL: EPOCH 87/100 | BATCH 0/8 | LOSS: 1.3841601685271598e-05\n",
      "VAL: EPOCH 87/100 | BATCH 1/8 | LOSS: 1.497541870776331e-05\n",
      "VAL: EPOCH 87/100 | BATCH 2/8 | LOSS: 1.556166777542482e-05\n",
      "VAL: EPOCH 87/100 | BATCH 3/8 | LOSS: 1.5625255400664173e-05\n",
      "VAL: EPOCH 87/100 | BATCH 4/8 | LOSS: 1.5715930931037293e-05\n",
      "VAL: EPOCH 87/100 | BATCH 5/8 | LOSS: 1.6170797001298826e-05\n",
      "VAL: EPOCH 87/100 | BATCH 6/8 | LOSS: 1.639891524973791e-05\n",
      "VAL: EPOCH 87/100 | BATCH 7/8 | LOSS: 1.7163525399155333e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 0/71 | LOSS: 1.7347509128740057e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 1/71 | LOSS: 1.500964526712778e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 2/71 | LOSS: 1.3855743418389466e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 3/71 | LOSS: 1.3122616792315966e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 4/71 | LOSS: 1.2659624735533725e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 5/71 | LOSS: 1.239957706881493e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 6/71 | LOSS: 1.2483741816790176e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 7/71 | LOSS: 1.306740648487903e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 8/71 | LOSS: 1.274618352908874e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 9/71 | LOSS: 1.3182129350752803e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 10/71 | LOSS: 1.3254557829482524e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 11/71 | LOSS: 1.3595301046128347e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 12/71 | LOSS: 1.351843373133586e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 13/71 | LOSS: 1.333905674982816e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 14/71 | LOSS: 1.3530681947789465e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 15/71 | LOSS: 1.3376181470903248e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 16/71 | LOSS: 1.3487209959267228e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 17/71 | LOSS: 1.3434984844530441e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 18/71 | LOSS: 1.3522793778975029e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 19/71 | LOSS: 1.3636370704261936e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 20/71 | LOSS: 1.3445190580525723e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 21/71 | LOSS: 1.3907710994813401e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 22/71 | LOSS: 1.3926234042509863e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 23/71 | LOSS: 1.4442256694261838e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 24/71 | LOSS: 1.4574956803699024e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 25/71 | LOSS: 1.4902890283197989e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 26/71 | LOSS: 1.486461607985095e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 27/71 | LOSS: 1.5397396834617082e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 28/71 | LOSS: 1.5431746791803342e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 29/71 | LOSS: 1.548175441712374e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 30/71 | LOSS: 1.5746880425497018e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 31/71 | LOSS: 1.556072601260894e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 32/71 | LOSS: 1.5493167170048267e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 33/71 | LOSS: 1.5417079901132692e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 34/71 | LOSS: 1.5274877582019793e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 35/71 | LOSS: 1.523021561903685e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 36/71 | LOSS: 1.5194072737948454e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 37/71 | LOSS: 1.5138081069285123e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 38/71 | LOSS: 1.5002402412788704e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 39/71 | LOSS: 1.4943576115911127e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 40/71 | LOSS: 1.5002062488148534e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 41/71 | LOSS: 1.4844259090923948e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 42/71 | LOSS: 1.4849573300696563e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 43/71 | LOSS: 1.4780387201343812e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 44/71 | LOSS: 1.4697447760328133e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 45/71 | LOSS: 1.4658859586431745e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 46/71 | LOSS: 1.4544057348951143e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 47/71 | LOSS: 1.4397478319475946e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 48/71 | LOSS: 1.4314087514563022e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 49/71 | LOSS: 1.4214389366316028e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 50/71 | LOSS: 1.4169476477999364e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 51/71 | LOSS: 1.4068918333648002e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 52/71 | LOSS: 1.399208567475072e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 53/71 | LOSS: 1.3941141105533461e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 54/71 | LOSS: 1.3858854725351028e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 55/71 | LOSS: 1.3784100758584827e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 56/71 | LOSS: 1.3747824125363095e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 57/71 | LOSS: 1.3673048035092632e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 58/71 | LOSS: 1.3588286916366719e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 59/71 | LOSS: 1.3524675546250365e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 60/71 | LOSS: 1.3472376096734613e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 61/71 | LOSS: 1.3426058726949005e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 62/71 | LOSS: 1.3362254906366746e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 63/71 | LOSS: 1.334265929386902e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 64/71 | LOSS: 1.3343553606458814e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 65/71 | LOSS: 1.332591300111248e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 66/71 | LOSS: 1.3253248049132612e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 67/71 | LOSS: 1.3202463318360542e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 68/71 | LOSS: 1.3188647896996649e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 69/71 | LOSS: 1.3202648064439667e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 70/71 | LOSS: 1.3184938434022396e-05\n",
      "VAL: EPOCH 88/100 | BATCH 0/8 | LOSS: 7.451606052200077e-06\n",
      "VAL: EPOCH 88/100 | BATCH 1/8 | LOSS: 8.332555125889485e-06\n",
      "VAL: EPOCH 88/100 | BATCH 2/8 | LOSS: 8.759648001917716e-06\n",
      "VAL: EPOCH 88/100 | BATCH 3/8 | LOSS: 8.7093482079581e-06\n",
      "VAL: EPOCH 88/100 | BATCH 4/8 | LOSS: 8.824429733067518e-06\n",
      "VAL: EPOCH 88/100 | BATCH 5/8 | LOSS: 9.111437293540803e-06\n",
      "VAL: EPOCH 88/100 | BATCH 6/8 | LOSS: 9.162613098721653e-06\n",
      "VAL: EPOCH 88/100 | BATCH 7/8 | LOSS: 9.613591998913762e-06\n",
      "TRAIN: EPOCH 89/100 | BATCH 0/71 | LOSS: 1.0183647646044847e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 1/71 | LOSS: 1.2229405911057256e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 2/71 | LOSS: 1.2674670870183036e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 3/71 | LOSS: 1.2389868516038405e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 4/71 | LOSS: 1.2552344924188218e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 5/71 | LOSS: 1.2472436689373959e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 6/71 | LOSS: 1.3064664147220486e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 7/71 | LOSS: 1.2790195796696935e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 8/71 | LOSS: 1.2705308310816892e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 9/71 | LOSS: 1.2698031059699134e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 10/71 | LOSS: 1.2740959970291112e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 11/71 | LOSS: 1.2729255407369541e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 12/71 | LOSS: 1.2439833951178185e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 13/71 | LOSS: 1.2652987282178532e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 14/71 | LOSS: 1.2669816229996893e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 15/71 | LOSS: 1.2532858818303794e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 16/71 | LOSS: 1.2544433796874192e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 17/71 | LOSS: 1.2500212505983654e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 18/71 | LOSS: 1.2260273347477013e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 19/71 | LOSS: 1.2136070699853007e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 20/71 | LOSS: 1.2084879060802494e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 21/71 | LOSS: 1.1948608227447204e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 22/71 | LOSS: 1.181724600547054e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 23/71 | LOSS: 1.1699601335142992e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 24/71 | LOSS: 1.1524615802045446e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 25/71 | LOSS: 1.1489468411477552e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 26/71 | LOSS: 1.1455425919848494e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 27/71 | LOSS: 1.1357740634464011e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 28/71 | LOSS: 1.1238836373979275e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 29/71 | LOSS: 1.1211645808847e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 30/71 | LOSS: 1.119729817981997e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 31/71 | LOSS: 1.1183674700987467e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 32/71 | LOSS: 1.1122663360738168e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 33/71 | LOSS: 1.1236739363298779e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 34/71 | LOSS: 1.1181251310128054e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 35/71 | LOSS: 1.1213093279770368e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 36/71 | LOSS: 1.1306326726028331e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 37/71 | LOSS: 1.1264002128691094e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 38/71 | LOSS: 1.123477891386033e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 39/71 | LOSS: 1.1194124181201914e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 40/71 | LOSS: 1.1137538266910536e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 41/71 | LOSS: 1.1118896119504435e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 42/71 | LOSS: 1.1110118616353395e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 43/71 | LOSS: 1.1059808354151425e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 44/71 | LOSS: 1.1079385027793856e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 45/71 | LOSS: 1.1003844440932406e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 46/71 | LOSS: 1.0964448982973564e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 47/71 | LOSS: 1.0944347745104702e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 48/71 | LOSS: 1.0926529229232537e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 49/71 | LOSS: 1.090973421014496e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 50/71 | LOSS: 1.0883257613493375e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 51/71 | LOSS: 1.0853907094944869e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 52/71 | LOSS: 1.0864885141172465e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 53/71 | LOSS: 1.0870125690955859e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 54/71 | LOSS: 1.0880343291897919e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 55/71 | LOSS: 1.0915164011748857e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 56/71 | LOSS: 1.089061520600042e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 57/71 | LOSS: 1.0935437205324857e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 58/71 | LOSS: 1.0926377830925296e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 59/71 | LOSS: 1.096060959753231e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 60/71 | LOSS: 1.0948507328752473e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 61/71 | LOSS: 1.096115286805622e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 62/71 | LOSS: 1.096577458762719e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 63/71 | LOSS: 1.0952504226224846e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 64/71 | LOSS: 1.0968697852849101e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 65/71 | LOSS: 1.0912164925409314e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 66/71 | LOSS: 1.0913820582871593e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 67/71 | LOSS: 1.0879416984554315e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 68/71 | LOSS: 1.0886517087045291e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 69/71 | LOSS: 1.0869788593481643e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 70/71 | LOSS: 1.0848237105190645e-05\n",
      "VAL: EPOCH 89/100 | BATCH 0/8 | LOSS: 8.43162888486404e-06\n",
      "VAL: EPOCH 89/100 | BATCH 1/8 | LOSS: 8.683324722369434e-06\n",
      "VAL: EPOCH 89/100 | BATCH 2/8 | LOSS: 9.120716337444415e-06\n",
      "VAL: EPOCH 89/100 | BATCH 3/8 | LOSS: 9.072083457795088e-06\n",
      "VAL: EPOCH 89/100 | BATCH 4/8 | LOSS: 9.306822721555364e-06\n",
      "VAL: EPOCH 89/100 | BATCH 5/8 | LOSS: 9.490768206887878e-06\n",
      "VAL: EPOCH 89/100 | BATCH 6/8 | LOSS: 9.578235611635527e-06\n",
      "VAL: EPOCH 89/100 | BATCH 7/8 | LOSS: 9.943091981767793e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 0/71 | LOSS: 1.0464938895893283e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 1/71 | LOSS: 1.0356412531109527e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 2/71 | LOSS: 1.051509267805765e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 3/71 | LOSS: 9.968029416995705e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 4/71 | LOSS: 9.817378850129898e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 5/71 | LOSS: 9.770998834331598e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 6/71 | LOSS: 9.93626508716261e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 7/71 | LOSS: 9.766412631506682e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 8/71 | LOSS: 9.831888342483177e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 9/71 | LOSS: 9.869425048236735e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 10/71 | LOSS: 9.999959398886528e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 11/71 | LOSS: 1.0052967657732855e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 12/71 | LOSS: 9.920698735536112e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 13/71 | LOSS: 9.78764709153828e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 14/71 | LOSS: 9.716401594535758e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 15/71 | LOSS: 9.63641241469304e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 16/71 | LOSS: 9.68355914879152e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 17/71 | LOSS: 9.569264446124887e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 18/71 | LOSS: 9.53892669330296e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 19/71 | LOSS: 9.484499173595395e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 20/71 | LOSS: 9.568823965569859e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 21/71 | LOSS: 9.544703082362751e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 22/71 | LOSS: 9.492524460928626e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 23/71 | LOSS: 9.391044632896714e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 24/71 | LOSS: 9.435080446564825e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 25/71 | LOSS: 9.412600163089406e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 26/71 | LOSS: 9.397590862068614e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 27/71 | LOSS: 9.482612459318521e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 28/71 | LOSS: 9.502038806360736e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 29/71 | LOSS: 9.517547384045126e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 30/71 | LOSS: 9.519933398898442e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 31/71 | LOSS: 9.516292138300741e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 32/71 | LOSS: 9.546665572115947e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 33/71 | LOSS: 9.585751031944015e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 34/71 | LOSS: 9.65442134526841e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 35/71 | LOSS: 9.585258617840333e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 36/71 | LOSS: 9.611362951840715e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 37/71 | LOSS: 9.718084895307824e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 38/71 | LOSS: 9.780272706941618e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 39/71 | LOSS: 9.773723104444797e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 40/71 | LOSS: 9.776970528480711e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 41/71 | LOSS: 9.794218255722495e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 42/71 | LOSS: 9.759287718753872e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 43/71 | LOSS: 9.819266221132818e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 44/71 | LOSS: 9.794903396848693e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 45/71 | LOSS: 9.826002141103938e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 46/71 | LOSS: 9.807099346251058e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 47/71 | LOSS: 9.798459492079322e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 48/71 | LOSS: 9.796163219422796e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 49/71 | LOSS: 9.844286105362698e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 50/71 | LOSS: 9.830272378452469e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 51/71 | LOSS: 9.77824645185767e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 52/71 | LOSS: 9.80488219386832e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 53/71 | LOSS: 9.779754287188149e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 54/71 | LOSS: 9.732620839548127e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 55/71 | LOSS: 9.695221219122427e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 56/71 | LOSS: 9.708047065559137e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 57/71 | LOSS: 9.735643727943735e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 58/71 | LOSS: 9.712998520225776e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 59/71 | LOSS: 9.765206292892496e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 60/71 | LOSS: 9.747720200646684e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 61/71 | LOSS: 9.761445829597857e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 62/71 | LOSS: 9.751781666030487e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 63/71 | LOSS: 9.764441529114265e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 64/71 | LOSS: 9.73796608237675e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 65/71 | LOSS: 9.757982407984056e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 66/71 | LOSS: 9.798842012372265e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 67/71 | LOSS: 9.781924531767692e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 68/71 | LOSS: 9.782535781214129e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 69/71 | LOSS: 9.796571729176711e-06\n",
      "TRAIN: EPOCH 90/100 | BATCH 70/71 | LOSS: 9.846122415644497e-06\n",
      "VAL: EPOCH 90/100 | BATCH 0/8 | LOSS: 8.584796887589619e-06\n",
      "VAL: EPOCH 90/100 | BATCH 1/8 | LOSS: 9.465301900490886e-06\n",
      "VAL: EPOCH 90/100 | BATCH 2/8 | LOSS: 9.883531068529313e-06\n",
      "VAL: EPOCH 90/100 | BATCH 3/8 | LOSS: 9.934390845955932e-06\n",
      "VAL: EPOCH 90/100 | BATCH 4/8 | LOSS: 1.0100731560669374e-05\n",
      "VAL: EPOCH 90/100 | BATCH 5/8 | LOSS: 1.0423527025219906e-05\n",
      "VAL: EPOCH 90/100 | BATCH 6/8 | LOSS: 1.049624149475546e-05\n",
      "VAL: EPOCH 90/100 | BATCH 7/8 | LOSS: 1.0999452797477716e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 0/71 | LOSS: 1.3557648344431072e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 1/71 | LOSS: 1.0697235666157212e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 2/71 | LOSS: 9.43973691391875e-06\n",
      "TRAIN: EPOCH 91/100 | BATCH 3/71 | LOSS: 9.380985261486785e-06\n",
      "TRAIN: EPOCH 91/100 | BATCH 4/71 | LOSS: 9.09203217815957e-06\n",
      "TRAIN: EPOCH 91/100 | BATCH 5/71 | LOSS: 9.202145899204576e-06\n",
      "TRAIN: EPOCH 91/100 | BATCH 6/71 | LOSS: 9.026622982284088e-06\n",
      "TRAIN: EPOCH 91/100 | BATCH 7/71 | LOSS: 9.326986685209704e-06\n",
      "TRAIN: EPOCH 91/100 | BATCH 8/71 | LOSS: 9.446030010925218e-06\n",
      "TRAIN: EPOCH 91/100 | BATCH 9/71 | LOSS: 9.503680712441564e-06\n",
      "TRAIN: EPOCH 91/100 | BATCH 10/71 | LOSS: 9.868005814827153e-06\n",
      "TRAIN: EPOCH 91/100 | BATCH 11/71 | LOSS: 9.750877514610087e-06\n",
      "TRAIN: EPOCH 91/100 | BATCH 12/71 | LOSS: 9.855686899894723e-06\n",
      "TRAIN: EPOCH 91/100 | BATCH 13/71 | LOSS: 9.907864978231373e-06\n",
      "TRAIN: EPOCH 91/100 | BATCH 14/71 | LOSS: 1.0064006498093173e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 15/71 | LOSS: 9.972206584052401e-06\n",
      "TRAIN: EPOCH 91/100 | BATCH 16/71 | LOSS: 1.0121248471798882e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 17/71 | LOSS: 1.0033892597978896e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 18/71 | LOSS: 1.0039710799135623e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 19/71 | LOSS: 9.967236951524683e-06\n",
      "TRAIN: EPOCH 91/100 | BATCH 20/71 | LOSS: 1.0027351176436891e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 21/71 | LOSS: 1.0127594557409545e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 22/71 | LOSS: 1.0063661509617384e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 23/71 | LOSS: 1.0329625392084077e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 24/71 | LOSS: 1.0208297444478376e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 25/71 | LOSS: 1.0364534721072861e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 26/71 | LOSS: 1.0380005076310934e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 27/71 | LOSS: 1.0474512350810983e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 28/71 | LOSS: 1.0477852387778292e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 29/71 | LOSS: 1.0529461223995896e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 30/71 | LOSS: 1.0557337939263239e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 31/71 | LOSS: 1.0615060276109034e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 32/71 | LOSS: 1.0618603468945807e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 33/71 | LOSS: 1.0556982528321574e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 34/71 | LOSS: 1.0516466746984016e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 35/71 | LOSS: 1.0476428150872784e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 36/71 | LOSS: 1.047345325344862e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 37/71 | LOSS: 1.0442098911019121e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 38/71 | LOSS: 1.0466021159607828e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 39/71 | LOSS: 1.0513407812595687e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 40/71 | LOSS: 1.051061796782206e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 41/71 | LOSS: 1.0485731709628626e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 42/71 | LOSS: 1.0490398603165927e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 43/71 | LOSS: 1.0467672971323985e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 44/71 | LOSS: 1.0418349170827747e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 45/71 | LOSS: 1.0365925895119132e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 46/71 | LOSS: 1.031417371044146e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 47/71 | LOSS: 1.032405965399145e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 48/71 | LOSS: 1.0314458849075445e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 49/71 | LOSS: 1.0263594967909739e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 50/71 | LOSS: 1.0236976982460142e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 51/71 | LOSS: 1.0203913890249029e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 52/71 | LOSS: 1.0182865060174526e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 53/71 | LOSS: 1.0200947186873506e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 54/71 | LOSS: 1.018567536448245e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 55/71 | LOSS: 1.0176747886297172e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 56/71 | LOSS: 1.029069489321269e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 57/71 | LOSS: 1.0312020086875518e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 58/71 | LOSS: 1.0317767045292432e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 59/71 | LOSS: 1.0288627580242367e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 60/71 | LOSS: 1.0263934834002895e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 61/71 | LOSS: 1.0271714304068272e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 62/71 | LOSS: 1.022124108261316e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 63/71 | LOSS: 1.0240725892174396e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 64/71 | LOSS: 1.0213877312008453e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 65/71 | LOSS: 1.0221520280097277e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 66/71 | LOSS: 1.0187782541535527e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 67/71 | LOSS: 1.0166429700909836e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 68/71 | LOSS: 1.0137256489595503e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 69/71 | LOSS: 1.0169016754194412e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 70/71 | LOSS: 1.0209628726443043e-05\n",
      "VAL: EPOCH 91/100 | BATCH 0/8 | LOSS: 7.328957963181892e-06\n",
      "VAL: EPOCH 91/100 | BATCH 1/8 | LOSS: 7.941226840557647e-06\n",
      "VAL: EPOCH 91/100 | BATCH 2/8 | LOSS: 8.183498872919396e-06\n",
      "VAL: EPOCH 91/100 | BATCH 3/8 | LOSS: 7.76529725499131e-06\n",
      "VAL: EPOCH 91/100 | BATCH 4/8 | LOSS: 7.838506735424745e-06\n",
      "VAL: EPOCH 91/100 | BATCH 5/8 | LOSS: 8.206788682703822e-06\n",
      "VAL: EPOCH 91/100 | BATCH 6/8 | LOSS: 8.266302821929067e-06\n",
      "VAL: EPOCH 91/100 | BATCH 7/8 | LOSS: 8.40035028204511e-06\n",
      "TRAIN: EPOCH 92/100 | BATCH 0/71 | LOSS: 7.176273811637657e-06\n",
      "TRAIN: EPOCH 92/100 | BATCH 1/71 | LOSS: 7.98873156782065e-06\n",
      "TRAIN: EPOCH 92/100 | BATCH 2/71 | LOSS: 8.79383045078915e-06\n",
      "TRAIN: EPOCH 92/100 | BATCH 3/71 | LOSS: 9.024333053275768e-06\n",
      "TRAIN: EPOCH 92/100 | BATCH 4/71 | LOSS: 9.645926456869348e-06\n",
      "TRAIN: EPOCH 92/100 | BATCH 5/71 | LOSS: 1.002463485140955e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 6/71 | LOSS: 1.013577684716438e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 7/71 | LOSS: 1.0015116174599825e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 8/71 | LOSS: 9.851787025480815e-06\n",
      "TRAIN: EPOCH 92/100 | BATCH 9/71 | LOSS: 9.801784335650154e-06\n",
      "TRAIN: EPOCH 92/100 | BATCH 10/71 | LOSS: 9.610900013450935e-06\n",
      "TRAIN: EPOCH 92/100 | BATCH 11/71 | LOSS: 9.469795221169383e-06\n",
      "TRAIN: EPOCH 92/100 | BATCH 12/71 | LOSS: 9.559208067982512e-06\n",
      "TRAIN: EPOCH 92/100 | BATCH 13/71 | LOSS: 9.755477256996009e-06\n",
      "TRAIN: EPOCH 92/100 | BATCH 14/71 | LOSS: 1.0115264346192514e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 15/71 | LOSS: 1.0185136687823615e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 16/71 | LOSS: 1.0297326271564302e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 17/71 | LOSS: 1.0261084879756607e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 18/71 | LOSS: 1.030932839727795e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 19/71 | LOSS: 1.0410039453745413e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 20/71 | LOSS: 1.0373016807750987e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 21/71 | LOSS: 1.0568159082553889e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 22/71 | LOSS: 1.0585330943760471e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 23/71 | LOSS: 1.0524531281438007e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 24/71 | LOSS: 1.0445765128679341e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 25/71 | LOSS: 1.0491114835531334e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 26/71 | LOSS: 1.0438838941169067e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 27/71 | LOSS: 1.034785813252321e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 28/71 | LOSS: 1.0308296889982966e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 29/71 | LOSS: 1.0229819675563098e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 30/71 | LOSS: 1.0181248266998146e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 31/71 | LOSS: 1.0163809506025245e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 32/71 | LOSS: 1.0161192070275623e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 33/71 | LOSS: 1.0204698556288563e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 34/71 | LOSS: 1.0193977102192417e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 35/71 | LOSS: 1.0192723582096934e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 36/71 | LOSS: 1.0129314395689525e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 37/71 | LOSS: 1.0103622822785646e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 38/71 | LOSS: 1.0068339120969027e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 39/71 | LOSS: 1.004834908826524e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 40/71 | LOSS: 1.000815814777303e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 41/71 | LOSS: 9.972272534599011e-06\n",
      "TRAIN: EPOCH 92/100 | BATCH 42/71 | LOSS: 9.9919817372676e-06\n",
      "TRAIN: EPOCH 92/100 | BATCH 43/71 | LOSS: 9.941006706784803e-06\n",
      "TRAIN: EPOCH 92/100 | BATCH 44/71 | LOSS: 9.886344499439776e-06\n",
      "TRAIN: EPOCH 92/100 | BATCH 45/71 | LOSS: 9.863278038487213e-06\n",
      "TRAIN: EPOCH 92/100 | BATCH 46/71 | LOSS: 9.827001992968515e-06\n",
      "TRAIN: EPOCH 92/100 | BATCH 47/71 | LOSS: 9.7837127176111e-06\n",
      "TRAIN: EPOCH 92/100 | BATCH 48/71 | LOSS: 9.750593265807208e-06\n",
      "TRAIN: EPOCH 92/100 | BATCH 49/71 | LOSS: 9.710983213153669e-06\n",
      "TRAIN: EPOCH 92/100 | BATCH 50/71 | LOSS: 9.68297755367958e-06\n",
      "TRAIN: EPOCH 92/100 | BATCH 51/71 | LOSS: 9.652421462865626e-06\n",
      "TRAIN: EPOCH 92/100 | BATCH 52/71 | LOSS: 9.660818130145825e-06\n",
      "TRAIN: EPOCH 92/100 | BATCH 53/71 | LOSS: 9.633196677222494e-06\n",
      "TRAIN: EPOCH 92/100 | BATCH 54/71 | LOSS: 9.619054964621847e-06\n",
      "TRAIN: EPOCH 92/100 | BATCH 55/71 | LOSS: 9.63420097832568e-06\n",
      "TRAIN: EPOCH 92/100 | BATCH 56/71 | LOSS: 9.638881525269857e-06\n",
      "TRAIN: EPOCH 92/100 | BATCH 57/71 | LOSS: 9.655183146543126e-06\n",
      "TRAIN: EPOCH 92/100 | BATCH 58/71 | LOSS: 9.64036339070644e-06\n",
      "TRAIN: EPOCH 92/100 | BATCH 59/71 | LOSS: 9.690898074647218e-06\n",
      "TRAIN: EPOCH 92/100 | BATCH 60/71 | LOSS: 9.667179778053188e-06\n",
      "TRAIN: EPOCH 92/100 | BATCH 61/71 | LOSS: 9.644967825982847e-06\n",
      "TRAIN: EPOCH 92/100 | BATCH 62/71 | LOSS: 9.6152946911937e-06\n",
      "TRAIN: EPOCH 92/100 | BATCH 63/71 | LOSS: 9.608147635731257e-06\n",
      "TRAIN: EPOCH 92/100 | BATCH 64/71 | LOSS: 9.575663856454552e-06\n",
      "TRAIN: EPOCH 92/100 | BATCH 65/71 | LOSS: 9.548696046598101e-06\n",
      "TRAIN: EPOCH 92/100 | BATCH 66/71 | LOSS: 9.530287326327456e-06\n",
      "TRAIN: EPOCH 92/100 | BATCH 67/71 | LOSS: 9.532984944705864e-06\n",
      "TRAIN: EPOCH 92/100 | BATCH 68/71 | LOSS: 9.531373375532406e-06\n",
      "TRAIN: EPOCH 92/100 | BATCH 69/71 | LOSS: 9.523346592946577e-06\n",
      "TRAIN: EPOCH 92/100 | BATCH 70/71 | LOSS: 9.546961951906577e-06\n",
      "VAL: EPOCH 92/100 | BATCH 0/8 | LOSS: 1.0149207810172811e-05\n",
      "VAL: EPOCH 92/100 | BATCH 1/8 | LOSS: 1.0578545243333792e-05\n",
      "VAL: EPOCH 92/100 | BATCH 2/8 | LOSS: 1.0459411896590609e-05\n",
      "VAL: EPOCH 92/100 | BATCH 3/8 | LOSS: 1.0045471526609617e-05\n",
      "VAL: EPOCH 92/100 | BATCH 4/8 | LOSS: 9.865227730188053e-06\n",
      "VAL: EPOCH 92/100 | BATCH 5/8 | LOSS: 1.0345659878415367e-05\n",
      "VAL: EPOCH 92/100 | BATCH 6/8 | LOSS: 1.0213838842381457e-05\n",
      "VAL: EPOCH 92/100 | BATCH 7/8 | LOSS: 9.964114042304573e-06\n",
      "TRAIN: EPOCH 93/100 | BATCH 0/71 | LOSS: 1.0950248906738125e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 1/71 | LOSS: 1.4137390280666295e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 2/71 | LOSS: 1.2178925620294953e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 3/71 | LOSS: 1.2871460285168723e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 4/71 | LOSS: 1.2944310037710239e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 5/71 | LOSS: 1.2556078521204958e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 6/71 | LOSS: 1.2526611109413871e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 7/71 | LOSS: 1.2211595731059788e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 8/71 | LOSS: 1.1967615339219466e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 9/71 | LOSS: 1.1990778330073226e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 10/71 | LOSS: 1.1907979238788936e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 11/71 | LOSS: 1.1570003152883146e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 12/71 | LOSS: 1.181576786061319e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 13/71 | LOSS: 1.1619517278761902e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 14/71 | LOSS: 1.1455803663314631e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 15/71 | LOSS: 1.1391718601316825e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 16/71 | LOSS: 1.132612519808497e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 17/71 | LOSS: 1.1316627124971193e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 18/71 | LOSS: 1.1174308431356239e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 19/71 | LOSS: 1.1132255440315931e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 20/71 | LOSS: 1.0923817929627452e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 21/71 | LOSS: 1.09527232490853e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 22/71 | LOSS: 1.0968016380027858e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 23/71 | LOSS: 1.0913480252838781e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 24/71 | LOSS: 1.095345958674443e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 25/71 | LOSS: 1.0822251467218917e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 26/71 | LOSS: 1.0782768101678272e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 27/71 | LOSS: 1.0731180655706599e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 28/71 | LOSS: 1.0635299782608917e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 29/71 | LOSS: 1.0614306953963629e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 30/71 | LOSS: 1.060370383246693e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 31/71 | LOSS: 1.0509847797379734e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 32/71 | LOSS: 1.0409844821190221e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 33/71 | LOSS: 1.0453506895239971e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 34/71 | LOSS: 1.04899380468331e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 35/71 | LOSS: 1.0455480454159746e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 36/71 | LOSS: 1.0406105502624996e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 37/71 | LOSS: 1.0356632389486645e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 38/71 | LOSS: 1.0337284854755033e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 39/71 | LOSS: 1.0304227976121183e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 40/71 | LOSS: 1.0258052032263439e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 41/71 | LOSS: 1.0255918282165935e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 42/71 | LOSS: 1.0240234989269507e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 43/71 | LOSS: 1.0208789283503641e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 44/71 | LOSS: 1.0297980346270681e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 45/71 | LOSS: 1.0233878544133166e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 46/71 | LOSS: 1.0217148573769434e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 47/71 | LOSS: 1.0188822765636965e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 48/71 | LOSS: 1.0132774642276059e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 49/71 | LOSS: 1.0104737257279339e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 50/71 | LOSS: 1.0080286934752104e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 51/71 | LOSS: 1.0062408880283048e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 52/71 | LOSS: 1.008978055976496e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 53/71 | LOSS: 1.0086394695463241e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 54/71 | LOSS: 1.0119123966102382e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 55/71 | LOSS: 1.0067977192258176e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 56/71 | LOSS: 1.0023856280065701e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 57/71 | LOSS: 9.983804477150412e-06\n",
      "TRAIN: EPOCH 93/100 | BATCH 58/71 | LOSS: 9.951068446845688e-06\n",
      "TRAIN: EPOCH 93/100 | BATCH 59/71 | LOSS: 9.923024867930507e-06\n",
      "TRAIN: EPOCH 93/100 | BATCH 60/71 | LOSS: 9.92473633977781e-06\n",
      "TRAIN: EPOCH 93/100 | BATCH 61/71 | LOSS: 9.881384819950213e-06\n",
      "TRAIN: EPOCH 93/100 | BATCH 62/71 | LOSS: 9.89901658006905e-06\n",
      "TRAIN: EPOCH 93/100 | BATCH 63/71 | LOSS: 9.90646062604128e-06\n",
      "TRAIN: EPOCH 93/100 | BATCH 64/71 | LOSS: 9.912501956513062e-06\n",
      "TRAIN: EPOCH 93/100 | BATCH 65/71 | LOSS: 9.898249965606903e-06\n",
      "TRAIN: EPOCH 93/100 | BATCH 66/71 | LOSS: 9.883770118578278e-06\n",
      "TRAIN: EPOCH 93/100 | BATCH 67/71 | LOSS: 9.897953980405018e-06\n",
      "TRAIN: EPOCH 93/100 | BATCH 68/71 | LOSS: 9.910194309497086e-06\n",
      "TRAIN: EPOCH 93/100 | BATCH 69/71 | LOSS: 9.990367386438135e-06\n",
      "TRAIN: EPOCH 93/100 | BATCH 70/71 | LOSS: 9.966629752686369e-06\n",
      "VAL: EPOCH 93/100 | BATCH 0/8 | LOSS: 1.4938825188437477e-05\n",
      "VAL: EPOCH 93/100 | BATCH 1/8 | LOSS: 1.7631426089792512e-05\n",
      "VAL: EPOCH 93/100 | BATCH 2/8 | LOSS: 1.7568826782129083e-05\n",
      "VAL: EPOCH 93/100 | BATCH 3/8 | LOSS: 1.8140247448172886e-05\n",
      "VAL: EPOCH 93/100 | BATCH 4/8 | LOSS: 1.8244255625177174e-05\n",
      "VAL: EPOCH 93/100 | BATCH 5/8 | LOSS: 1.8964104431991775e-05\n",
      "VAL: EPOCH 93/100 | BATCH 6/8 | LOSS: 1.9037467219667243e-05\n",
      "VAL: EPOCH 93/100 | BATCH 7/8 | LOSS: 1.9376826003281167e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 0/71 | LOSS: 2.0501694962149486e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 1/71 | LOSS: 1.781297760317102e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 2/71 | LOSS: 1.4898457569264186e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 3/71 | LOSS: 1.6960063476290088e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 4/71 | LOSS: 1.6150579540408216e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 5/71 | LOSS: 1.6120188168618672e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 6/71 | LOSS: 1.5848827907965252e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 7/71 | LOSS: 1.5615455367878894e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 8/71 | LOSS: 1.4813844245509245e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 9/71 | LOSS: 1.4744227064511506e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 10/71 | LOSS: 1.4351090282037728e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 11/71 | LOSS: 1.4034156644508281e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 12/71 | LOSS: 1.410764126866035e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 13/71 | LOSS: 1.378139813823509e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 14/71 | LOSS: 1.3770976693194825e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 15/71 | LOSS: 1.3456999965910654e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 16/71 | LOSS: 1.3337459730631535e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 17/71 | LOSS: 1.3094158778888717e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 18/71 | LOSS: 1.3065592898592035e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 19/71 | LOSS: 1.2960604954059817e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 20/71 | LOSS: 1.2726952393547566e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 21/71 | LOSS: 1.2641415072390704e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 22/71 | LOSS: 1.2594413656979272e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 23/71 | LOSS: 1.2478318391610324e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 24/71 | LOSS: 1.2353041929600295e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 25/71 | LOSS: 1.2213392262940313e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 26/71 | LOSS: 1.2079726157362867e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 27/71 | LOSS: 1.1962022881562007e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 28/71 | LOSS: 1.1891380569883541e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 29/71 | LOSS: 1.1863962784749068e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 30/71 | LOSS: 1.1889247029865792e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 31/71 | LOSS: 1.189433709214427e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 32/71 | LOSS: 1.204225053229355e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 33/71 | LOSS: 1.2101814450836495e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 34/71 | LOSS: 1.2025211877439038e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 35/71 | LOSS: 1.2223074817383247e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 36/71 | LOSS: 1.2176598141416656e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 37/71 | LOSS: 1.2293194135724517e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 38/71 | LOSS: 1.2268615999999336e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 39/71 | LOSS: 1.2370611921141972e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 40/71 | LOSS: 1.2227985191601896e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 41/71 | LOSS: 1.231083336149043e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 42/71 | LOSS: 1.2202964023535335e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 43/71 | LOSS: 1.21726415562245e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 44/71 | LOSS: 1.2057831473713223e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 45/71 | LOSS: 1.2017826916037492e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 46/71 | LOSS: 1.195175033725273e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 47/71 | LOSS: 1.188995804568549e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 48/71 | LOSS: 1.1867937710307235e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 49/71 | LOSS: 1.1786294326157076e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 50/71 | LOSS: 1.1855007976598298e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 51/71 | LOSS: 1.1780263950920646e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 52/71 | LOSS: 1.174868054288211e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 53/71 | LOSS: 1.1706966705349731e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 54/71 | LOSS: 1.1683941391476599e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 55/71 | LOSS: 1.1686719192636832e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 56/71 | LOSS: 1.161298575281858e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 57/71 | LOSS: 1.1620860175690754e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 58/71 | LOSS: 1.1553673984808947e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 59/71 | LOSS: 1.154716044311499e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 60/71 | LOSS: 1.154508209601474e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 61/71 | LOSS: 1.1530260310117288e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 62/71 | LOSS: 1.1496140641178326e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 63/71 | LOSS: 1.1440081799207746e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 64/71 | LOSS: 1.1419988095440203e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 65/71 | LOSS: 1.1397066311308828e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 66/71 | LOSS: 1.1342242433146118e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 67/71 | LOSS: 1.1341638215151975e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 68/71 | LOSS: 1.12751098363926e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 69/71 | LOSS: 1.1252568629060989e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 70/71 | LOSS: 1.1193389563116616e-05\n",
      "VAL: EPOCH 94/100 | BATCH 0/8 | LOSS: 8.33266312838532e-06\n",
      "VAL: EPOCH 94/100 | BATCH 1/8 | LOSS: 9.48659317145939e-06\n",
      "VAL: EPOCH 94/100 | BATCH 2/8 | LOSS: 9.886038545422101e-06\n",
      "VAL: EPOCH 94/100 | BATCH 3/8 | LOSS: 9.509439905741601e-06\n",
      "VAL: EPOCH 94/100 | BATCH 4/8 | LOSS: 9.524058441456873e-06\n",
      "VAL: EPOCH 94/100 | BATCH 5/8 | LOSS: 9.73332150048615e-06\n",
      "VAL: EPOCH 94/100 | BATCH 6/8 | LOSS: 9.861727578806625e-06\n",
      "VAL: EPOCH 94/100 | BATCH 7/8 | LOSS: 9.740636528476898e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 0/71 | LOSS: 9.46775071497541e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 1/71 | LOSS: 9.836937351792585e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 2/71 | LOSS: 1.0020242674121013e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 3/71 | LOSS: 9.937914455804275e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 4/71 | LOSS: 9.577358287060634e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 5/71 | LOSS: 9.521022093395004e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 6/71 | LOSS: 9.367244404399702e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 7/71 | LOSS: 9.148475783149479e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 8/71 | LOSS: 9.13812275232178e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 9/71 | LOSS: 9.142159979091957e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 10/71 | LOSS: 9.079926216641484e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 11/71 | LOSS: 9.0076322673364e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 12/71 | LOSS: 9.150735632619426e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 13/71 | LOSS: 9.023807673916703e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 14/71 | LOSS: 9.195639631798257e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 15/71 | LOSS: 9.17739558303765e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 16/71 | LOSS: 9.362760599341367e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 17/71 | LOSS: 9.387371265903414e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 18/71 | LOSS: 9.382679045780892e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 19/71 | LOSS: 9.430793102183089e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 20/71 | LOSS: 9.4877270081876e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 21/71 | LOSS: 9.479647794498966e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 22/71 | LOSS: 9.544484090957154e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 23/71 | LOSS: 9.61700995579425e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 24/71 | LOSS: 9.641089254728286e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 25/71 | LOSS: 9.772106697989959e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 26/71 | LOSS: 9.731807528657589e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 27/71 | LOSS: 9.65825319911216e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 28/71 | LOSS: 9.723592437036006e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 29/71 | LOSS: 9.67979787371102e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 30/71 | LOSS: 9.647960885030766e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 31/71 | LOSS: 9.65732643010142e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 32/71 | LOSS: 9.604747033749431e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 33/71 | LOSS: 9.649654911674374e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 34/71 | LOSS: 9.64863451632222e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 35/71 | LOSS: 9.583021614566354e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 36/71 | LOSS: 9.50361296368961e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 37/71 | LOSS: 9.50075600246093e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 38/71 | LOSS: 9.420624325848155e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 39/71 | LOSS: 9.403949400166312e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 40/71 | LOSS: 9.36899614982128e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 41/71 | LOSS: 9.304904530099953e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 42/71 | LOSS: 9.273353191738986e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 43/71 | LOSS: 9.26413904587455e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 44/71 | LOSS: 9.270362119827445e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 45/71 | LOSS: 9.229057241100236e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 46/71 | LOSS: 9.182743163345298e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 47/71 | LOSS: 9.159678739933952e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 48/71 | LOSS: 9.166169872644656e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 49/71 | LOSS: 9.126378918153933e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 50/71 | LOSS: 9.135911943089655e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 51/71 | LOSS: 9.146805251265714e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 52/71 | LOSS: 9.138630322819875e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 53/71 | LOSS: 9.179982481765802e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 54/71 | LOSS: 9.275596279291097e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 55/71 | LOSS: 9.293676433636782e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 56/71 | LOSS: 9.4846454406874e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 57/71 | LOSS: 9.47477093208802e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 58/71 | LOSS: 9.613648972398072e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 59/71 | LOSS: 9.595447590982075e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 60/71 | LOSS: 9.587597083200158e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 61/71 | LOSS: 9.623089168129692e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 62/71 | LOSS: 9.708308728780443e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 63/71 | LOSS: 9.744378914433582e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 64/71 | LOSS: 9.793393320041315e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 65/71 | LOSS: 9.807370478290867e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 66/71 | LOSS: 9.839784355274637e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 67/71 | LOSS: 9.884147429522059e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 68/71 | LOSS: 9.943619735840121e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 69/71 | LOSS: 9.952969958249013e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 70/71 | LOSS: 1.0026525570269065e-05\n",
      "VAL: EPOCH 95/100 | BATCH 0/8 | LOSS: 7.137773536669556e-06\n",
      "VAL: EPOCH 95/100 | BATCH 1/8 | LOSS: 7.689503490837524e-06\n",
      "VAL: EPOCH 95/100 | BATCH 2/8 | LOSS: 7.913758963695727e-06\n",
      "VAL: EPOCH 95/100 | BATCH 3/8 | LOSS: 7.762649829601287e-06\n",
      "VAL: EPOCH 95/100 | BATCH 4/8 | LOSS: 7.791082498442848e-06\n",
      "VAL: EPOCH 95/100 | BATCH 5/8 | LOSS: 8.017788938256368e-06\n",
      "VAL: EPOCH 95/100 | BATCH 6/8 | LOSS: 8.033919194921119e-06\n",
      "VAL: EPOCH 95/100 | BATCH 7/8 | LOSS: 7.954672810228658e-06\n",
      "TRAIN: EPOCH 96/100 | BATCH 0/71 | LOSS: 9.09344817046076e-06\n",
      "TRAIN: EPOCH 96/100 | BATCH 1/71 | LOSS: 9.257048532163026e-06\n",
      "TRAIN: EPOCH 96/100 | BATCH 2/71 | LOSS: 9.774685774270134e-06\n",
      "TRAIN: EPOCH 96/100 | BATCH 3/71 | LOSS: 9.645225873100571e-06\n",
      "TRAIN: EPOCH 96/100 | BATCH 4/71 | LOSS: 1.0520029354665894e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 5/71 | LOSS: 1.0430332470908374e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 6/71 | LOSS: 1.0143307398331152e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 7/71 | LOSS: 9.913577628140047e-06\n",
      "TRAIN: EPOCH 96/100 | BATCH 8/71 | LOSS: 1.0025665182587949e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 9/71 | LOSS: 9.692504045233364e-06\n",
      "TRAIN: EPOCH 96/100 | BATCH 10/71 | LOSS: 9.719195738646455e-06\n",
      "TRAIN: EPOCH 96/100 | BATCH 11/71 | LOSS: 9.841299364173514e-06\n",
      "TRAIN: EPOCH 96/100 | BATCH 12/71 | LOSS: 9.701667225342066e-06\n",
      "TRAIN: EPOCH 96/100 | BATCH 13/71 | LOSS: 9.590673146573994e-06\n",
      "TRAIN: EPOCH 96/100 | BATCH 14/71 | LOSS: 9.448867088697928e-06\n",
      "TRAIN: EPOCH 96/100 | BATCH 15/71 | LOSS: 9.444166579442026e-06\n",
      "TRAIN: EPOCH 96/100 | BATCH 16/71 | LOSS: 9.343173667944297e-06\n",
      "TRAIN: EPOCH 96/100 | BATCH 17/71 | LOSS: 9.253643712428231e-06\n",
      "TRAIN: EPOCH 96/100 | BATCH 18/71 | LOSS: 9.254036285710754e-06\n",
      "TRAIN: EPOCH 96/100 | BATCH 19/71 | LOSS: 9.192808488478476e-06\n",
      "TRAIN: EPOCH 96/100 | BATCH 20/71 | LOSS: 9.350958017811145e-06\n",
      "TRAIN: EPOCH 96/100 | BATCH 21/71 | LOSS: 9.312182121654835e-06\n",
      "TRAIN: EPOCH 96/100 | BATCH 22/71 | LOSS: 9.335226719565032e-06\n",
      "TRAIN: EPOCH 96/100 | BATCH 23/71 | LOSS: 9.244962427601422e-06\n",
      "TRAIN: EPOCH 96/100 | BATCH 24/71 | LOSS: 9.197485142067307e-06\n",
      "TRAIN: EPOCH 96/100 | BATCH 25/71 | LOSS: 9.159546380033134e-06\n",
      "TRAIN: EPOCH 96/100 | BATCH 26/71 | LOSS: 9.119118271040929e-06\n",
      "TRAIN: EPOCH 96/100 | BATCH 27/71 | LOSS: 9.067521383648064e-06\n",
      "TRAIN: EPOCH 96/100 | BATCH 28/71 | LOSS: 9.077644586670494e-06\n",
      "TRAIN: EPOCH 96/100 | BATCH 29/71 | LOSS: 9.136821775731126e-06\n",
      "TRAIN: EPOCH 96/100 | BATCH 30/71 | LOSS: 9.157978418501912e-06\n",
      "TRAIN: EPOCH 96/100 | BATCH 31/71 | LOSS: 9.111383874937928e-06\n",
      "TRAIN: EPOCH 96/100 | BATCH 32/71 | LOSS: 9.157917909099275e-06\n",
      "TRAIN: EPOCH 96/100 | BATCH 33/71 | LOSS: 9.245917988878586e-06\n",
      "TRAIN: EPOCH 96/100 | BATCH 34/71 | LOSS: 9.264901473216014e-06\n",
      "TRAIN: EPOCH 96/100 | BATCH 35/71 | LOSS: 9.248015898164238e-06\n",
      "TRAIN: EPOCH 96/100 | BATCH 36/71 | LOSS: 9.3229462411703e-06\n",
      "TRAIN: EPOCH 96/100 | BATCH 37/71 | LOSS: 9.335130315728174e-06\n",
      "TRAIN: EPOCH 96/100 | BATCH 38/71 | LOSS: 9.434354197788381e-06\n",
      "TRAIN: EPOCH 96/100 | BATCH 39/71 | LOSS: 9.46182029792908e-06\n",
      "TRAIN: EPOCH 96/100 | BATCH 40/71 | LOSS: 9.556970492600913e-06\n",
      "TRAIN: EPOCH 96/100 | BATCH 41/71 | LOSS: 9.548102850333504e-06\n",
      "TRAIN: EPOCH 96/100 | BATCH 42/71 | LOSS: 9.563272331349232e-06\n",
      "TRAIN: EPOCH 96/100 | BATCH 43/71 | LOSS: 9.507271184619723e-06\n",
      "TRAIN: EPOCH 96/100 | BATCH 44/71 | LOSS: 9.508442346688956e-06\n",
      "TRAIN: EPOCH 96/100 | BATCH 45/71 | LOSS: 9.50283988063545e-06\n",
      "TRAIN: EPOCH 96/100 | BATCH 46/71 | LOSS: 9.51814683232558e-06\n",
      "TRAIN: EPOCH 96/100 | BATCH 47/71 | LOSS: 9.501213232473068e-06\n",
      "TRAIN: EPOCH 96/100 | BATCH 48/71 | LOSS: 9.659230096187865e-06\n",
      "TRAIN: EPOCH 96/100 | BATCH 49/71 | LOSS: 9.637858902351581e-06\n",
      "TRAIN: EPOCH 96/100 | BATCH 50/71 | LOSS: 9.683822661506437e-06\n",
      "TRAIN: EPOCH 96/100 | BATCH 51/71 | LOSS: 9.730754238891957e-06\n",
      "TRAIN: EPOCH 96/100 | BATCH 52/71 | LOSS: 9.848944801150245e-06\n",
      "TRAIN: EPOCH 96/100 | BATCH 53/71 | LOSS: 9.872924482368506e-06\n",
      "TRAIN: EPOCH 96/100 | BATCH 54/71 | LOSS: 9.987492946741192e-06\n",
      "TRAIN: EPOCH 96/100 | BATCH 55/71 | LOSS: 9.995581787539517e-06\n",
      "TRAIN: EPOCH 96/100 | BATCH 56/71 | LOSS: 1.0092356548914705e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 57/71 | LOSS: 1.0135619528102388e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 58/71 | LOSS: 1.0202760715740515e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 59/71 | LOSS: 1.0258028555654165e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 60/71 | LOSS: 1.0297798288789994e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 61/71 | LOSS: 1.0264887695513842e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 62/71 | LOSS: 1.0312419283489737e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 63/71 | LOSS: 1.0297965665984066e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 64/71 | LOSS: 1.033561317784076e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 65/71 | LOSS: 1.0401543856157028e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 66/71 | LOSS: 1.044446959772969e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 67/71 | LOSS: 1.0532990827146824e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 68/71 | LOSS: 1.0646324966913106e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 69/71 | LOSS: 1.065844556770961e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 70/71 | LOSS: 1.0667384685802972e-05\n",
      "VAL: EPOCH 96/100 | BATCH 0/8 | LOSS: 1.5079850527399685e-05\n",
      "VAL: EPOCH 96/100 | BATCH 1/8 | LOSS: 1.4654018286819337e-05\n",
      "VAL: EPOCH 96/100 | BATCH 2/8 | LOSS: 1.4440439978595046e-05\n",
      "VAL: EPOCH 96/100 | BATCH 3/8 | LOSS: 1.357426299364306e-05\n",
      "VAL: EPOCH 96/100 | BATCH 4/8 | LOSS: 1.3368161671678535e-05\n",
      "VAL: EPOCH 96/100 | BATCH 5/8 | LOSS: 1.3587296659049267e-05\n",
      "VAL: EPOCH 96/100 | BATCH 6/8 | LOSS: 1.3655468007657743e-05\n",
      "VAL: EPOCH 96/100 | BATCH 7/8 | LOSS: 1.323192918789573e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 0/71 | LOSS: 1.793476621969603e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 1/71 | LOSS: 1.372425595036475e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 2/71 | LOSS: 1.6728123227949254e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 3/71 | LOSS: 1.4724724223924568e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 4/71 | LOSS: 1.5741566676297226e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 5/71 | LOSS: 1.4604011236466855e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 6/71 | LOSS: 1.4331830893102701e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 7/71 | LOSS: 1.3836577181791654e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 8/71 | LOSS: 1.389168957959757e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 9/71 | LOSS: 1.3332185699255205e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 10/71 | LOSS: 1.3286342354232064e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 11/71 | LOSS: 1.3285158956932719e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 12/71 | LOSS: 1.3182319260242646e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 13/71 | LOSS: 1.3862999821867561e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 14/71 | LOSS: 1.3595187010650988e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 15/71 | LOSS: 1.3796933046705817e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 16/71 | LOSS: 1.3911269567923977e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 17/71 | LOSS: 1.4084296152658579e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 18/71 | LOSS: 1.4197874454686443e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 19/71 | LOSS: 1.4463955039900611e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 20/71 | LOSS: 1.4618779329923979e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 21/71 | LOSS: 1.4824563674086345e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 22/71 | LOSS: 1.4886540908343665e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 23/71 | LOSS: 1.4666957175298497e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 24/71 | LOSS: 1.509769539552508e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 25/71 | LOSS: 1.4974072620624122e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 26/71 | LOSS: 1.5013743736576376e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 27/71 | LOSS: 1.4975194192499788e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 28/71 | LOSS: 1.4889967195782007e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 29/71 | LOSS: 1.4792151281047458e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 30/71 | LOSS: 1.46374973302309e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 31/71 | LOSS: 1.4696447237838584e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 32/71 | LOSS: 1.453365522055802e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 33/71 | LOSS: 1.4734642718394753e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 34/71 | LOSS: 1.457065159879026e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 35/71 | LOSS: 1.456159952795133e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 36/71 | LOSS: 1.4515552622371516e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 37/71 | LOSS: 1.4473965955786347e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 38/71 | LOSS: 1.4333261764803841e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 39/71 | LOSS: 1.4270870701693639e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 40/71 | LOSS: 1.4234721697348726e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 41/71 | LOSS: 1.4077369791093155e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 42/71 | LOSS: 1.408978932674874e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 43/71 | LOSS: 1.3959665331888324e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 44/71 | LOSS: 1.3811851193976408e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 45/71 | LOSS: 1.3713072694767819e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 46/71 | LOSS: 1.366419414904506e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 47/71 | LOSS: 1.3549065035552607e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 48/71 | LOSS: 1.3552265871260956e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 49/71 | LOSS: 1.3465497313518426e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 50/71 | LOSS: 1.3408324017897636e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 51/71 | LOSS: 1.3360462468461685e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 52/71 | LOSS: 1.3238716575252288e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 53/71 | LOSS: 1.3170352674394423e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 54/71 | LOSS: 1.3107200389971364e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 55/71 | LOSS: 1.307922668404769e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 56/71 | LOSS: 1.3033521769868553e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 57/71 | LOSS: 1.2987256201901392e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 58/71 | LOSS: 1.290672976904454e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 59/71 | LOSS: 1.2830019538038565e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 60/71 | LOSS: 1.277271132762582e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 61/71 | LOSS: 1.2717562963042574e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 62/71 | LOSS: 1.2655578959513352e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 63/71 | LOSS: 1.2614103866326332e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 64/71 | LOSS: 1.252145271232271e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 65/71 | LOSS: 1.2474851129092823e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 66/71 | LOSS: 1.2428920657459539e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 67/71 | LOSS: 1.2374514566919756e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 68/71 | LOSS: 1.2325695993438231e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 69/71 | LOSS: 1.2249585071393605e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 70/71 | LOSS: 1.220786525279859e-05\n",
      "VAL: EPOCH 97/100 | BATCH 0/8 | LOSS: 6.448541626014048e-06\n",
      "VAL: EPOCH 97/100 | BATCH 1/8 | LOSS: 7.126064019757905e-06\n",
      "VAL: EPOCH 97/100 | BATCH 2/8 | LOSS: 7.266368508377734e-06\n",
      "VAL: EPOCH 97/100 | BATCH 3/8 | LOSS: 6.927517006261041e-06\n",
      "VAL: EPOCH 97/100 | BATCH 4/8 | LOSS: 6.8800871304119935e-06\n",
      "VAL: EPOCH 97/100 | BATCH 5/8 | LOSS: 7.259111801734737e-06\n",
      "VAL: EPOCH 97/100 | BATCH 6/8 | LOSS: 7.3726534018143345e-06\n",
      "VAL: EPOCH 97/100 | BATCH 7/8 | LOSS: 7.365020621818985e-06\n",
      "TRAIN: EPOCH 98/100 | BATCH 0/71 | LOSS: 8.214360605052207e-06\n",
      "TRAIN: EPOCH 98/100 | BATCH 1/71 | LOSS: 8.677428922965191e-06\n",
      "TRAIN: EPOCH 98/100 | BATCH 2/71 | LOSS: 9.851318282017019e-06\n",
      "TRAIN: EPOCH 98/100 | BATCH 3/71 | LOSS: 9.815256362344371e-06\n",
      "TRAIN: EPOCH 98/100 | BATCH 4/71 | LOSS: 9.734356353874318e-06\n",
      "TRAIN: EPOCH 98/100 | BATCH 5/71 | LOSS: 9.705919183033984e-06\n",
      "TRAIN: EPOCH 98/100 | BATCH 6/71 | LOSS: 9.49130065107186e-06\n",
      "TRAIN: EPOCH 98/100 | BATCH 7/71 | LOSS: 9.790734111447819e-06\n",
      "TRAIN: EPOCH 98/100 | BATCH 8/71 | LOSS: 9.830318352517983e-06\n",
      "TRAIN: EPOCH 98/100 | BATCH 9/71 | LOSS: 9.927016617439221e-06\n",
      "TRAIN: EPOCH 98/100 | BATCH 10/71 | LOSS: 1.0102232822232922e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 11/71 | LOSS: 1.022287293987271e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 12/71 | LOSS: 9.934734897307442e-06\n",
      "TRAIN: EPOCH 98/100 | BATCH 13/71 | LOSS: 9.933541735205966e-06\n",
      "TRAIN: EPOCH 98/100 | BATCH 14/71 | LOSS: 9.907207095238846e-06\n",
      "TRAIN: EPOCH 98/100 | BATCH 15/71 | LOSS: 9.785515487692464e-06\n",
      "TRAIN: EPOCH 98/100 | BATCH 16/71 | LOSS: 1.0010163404170753e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 17/71 | LOSS: 9.924192302908827e-06\n",
      "TRAIN: EPOCH 98/100 | BATCH 18/71 | LOSS: 1.0045390473877839e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 19/71 | LOSS: 1.005098279165395e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 20/71 | LOSS: 1.0215003515310985e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 21/71 | LOSS: 1.0221761757830707e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 22/71 | LOSS: 1.0390442738476533e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 23/71 | LOSS: 1.0497723602990542e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 24/71 | LOSS: 1.0515255744394381e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 25/71 | LOSS: 1.0810550275681844e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 26/71 | LOSS: 1.0686294377524905e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 27/71 | LOSS: 1.0809202616915822e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 28/71 | LOSS: 1.0860235996570455e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 29/71 | LOSS: 1.0796803265596584e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 30/71 | LOSS: 1.0835897169712435e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 31/71 | LOSS: 1.097547058748205e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 32/71 | LOSS: 1.0938061815536596e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 33/71 | LOSS: 1.0895944288837627e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 34/71 | LOSS: 1.0986121820418963e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 35/71 | LOSS: 1.0890731181866107e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 36/71 | LOSS: 1.0974474564038147e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 37/71 | LOSS: 1.091803978612772e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 38/71 | LOSS: 1.094426351044557e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 39/71 | LOSS: 1.1010412106315925e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 40/71 | LOSS: 1.1023036222468363e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 41/71 | LOSS: 1.11718537114246e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 42/71 | LOSS: 1.1148976132379937e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 43/71 | LOSS: 1.1463244525409458e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 44/71 | LOSS: 1.1364171758840611e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 45/71 | LOSS: 1.1550327549823281e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 46/71 | LOSS: 1.163380996312047e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 47/71 | LOSS: 1.1707603912706569e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 48/71 | LOSS: 1.1816139137909905e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 49/71 | LOSS: 1.179675130515534e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 50/71 | LOSS: 1.2053372025619347e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 51/71 | LOSS: 1.1959198025998865e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 52/71 | LOSS: 1.2132944336698065e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 53/71 | LOSS: 1.2098818876472285e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 54/71 | LOSS: 1.2164819814973849e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 55/71 | LOSS: 1.2141232381119543e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 56/71 | LOSS: 1.2133037336369804e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 57/71 | LOSS: 1.2125258245791413e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 58/71 | LOSS: 1.2067093694448408e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 59/71 | LOSS: 1.210029596829069e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 60/71 | LOSS: 1.2038497232901673e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 61/71 | LOSS: 1.2005626191943697e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 62/71 | LOSS: 1.1957478067964235e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 63/71 | LOSS: 1.1938411020651074e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 64/71 | LOSS: 1.190064813272329e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 65/71 | LOSS: 1.1834788383015566e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 66/71 | LOSS: 1.1788797686975484e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 67/71 | LOSS: 1.1731931261452059e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 68/71 | LOSS: 1.168998979678229e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 69/71 | LOSS: 1.1637279879193686e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 70/71 | LOSS: 1.1654198190203907e-05\n",
      "VAL: EPOCH 98/100 | BATCH 0/8 | LOSS: 6.014449354552198e-06\n",
      "VAL: EPOCH 98/100 | BATCH 1/8 | LOSS: 6.882816251163604e-06\n",
      "VAL: EPOCH 98/100 | BATCH 2/8 | LOSS: 7.3101327870972455e-06\n",
      "VAL: EPOCH 98/100 | BATCH 3/8 | LOSS: 7.135458758966706e-06\n",
      "VAL: EPOCH 98/100 | BATCH 4/8 | LOSS: 7.156973879318684e-06\n",
      "VAL: EPOCH 98/100 | BATCH 5/8 | LOSS: 7.410047449714814e-06\n",
      "VAL: EPOCH 98/100 | BATCH 6/8 | LOSS: 7.553350250028805e-06\n",
      "VAL: EPOCH 98/100 | BATCH 7/8 | LOSS: 7.59732108690514e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 0/71 | LOSS: 8.170679393515456e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 1/71 | LOSS: 9.851300546870334e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 2/71 | LOSS: 9.677257973332113e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 3/71 | LOSS: 9.180656206808635e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 4/71 | LOSS: 8.981847895483951e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 5/71 | LOSS: 8.843200248520589e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 6/71 | LOSS: 9.115808617415106e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 7/71 | LOSS: 9.141659234046529e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 8/71 | LOSS: 9.290766785044172e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 9/71 | LOSS: 9.247481466445607e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 10/71 | LOSS: 9.276478935631035e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 11/71 | LOSS: 9.427086600529341e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 12/71 | LOSS: 9.312737137616541e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 13/71 | LOSS: 9.432774667012772e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 14/71 | LOSS: 9.314294523695328e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 15/71 | LOSS: 9.29610047251117e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 16/71 | LOSS: 9.212805172651047e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 17/71 | LOSS: 9.205456990457606e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 18/71 | LOSS: 9.17070208647362e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 19/71 | LOSS: 9.044683838510537e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 20/71 | LOSS: 8.991085624508005e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 21/71 | LOSS: 8.880812195209067e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 22/71 | LOSS: 8.857073146867586e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 23/71 | LOSS: 8.946344678406604e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 24/71 | LOSS: 8.92481464688899e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 25/71 | LOSS: 8.928538731057555e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 26/71 | LOSS: 8.93907545779021e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 27/71 | LOSS: 8.961876314320502e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 28/71 | LOSS: 8.915644763152974e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 29/71 | LOSS: 8.944525340363423e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 30/71 | LOSS: 8.962481945585047e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 31/71 | LOSS: 8.950157237563872e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 32/71 | LOSS: 8.95229003149187e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 33/71 | LOSS: 8.928950926608249e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 34/71 | LOSS: 8.865830776423016e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 35/71 | LOSS: 8.853823601384647e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 36/71 | LOSS: 8.863016896734857e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 37/71 | LOSS: 8.887265793371052e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 38/71 | LOSS: 8.818667316891384e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 39/71 | LOSS: 8.773625415869901e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 40/71 | LOSS: 8.807561787362807e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 41/71 | LOSS: 8.827031370812966e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 42/71 | LOSS: 8.811046120891385e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 43/71 | LOSS: 8.769473786494267e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 44/71 | LOSS: 8.816763925602169e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 45/71 | LOSS: 8.828573942557103e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 46/71 | LOSS: 8.928778436498722e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 47/71 | LOSS: 9.03370995312495e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 48/71 | LOSS: 9.007672101038874e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 49/71 | LOSS: 9.09002775188128e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 50/71 | LOSS: 9.066637131564122e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 51/71 | LOSS: 9.053900953404082e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 52/71 | LOSS: 9.098619720357677e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 53/71 | LOSS: 9.128747689357152e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 54/71 | LOSS: 9.130987059920285e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 55/71 | LOSS: 9.12351480662567e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 56/71 | LOSS: 9.234739169634148e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 57/71 | LOSS: 9.20494110310944e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 58/71 | LOSS: 9.249187885069335e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 59/71 | LOSS: 9.268480835089576e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 60/71 | LOSS: 9.24121128472969e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 61/71 | LOSS: 9.238650305163784e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 62/71 | LOSS: 9.238766353288185e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 63/71 | LOSS: 9.217847761533449e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 64/71 | LOSS: 9.189366741898434e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 65/71 | LOSS: 9.1796858104124e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 66/71 | LOSS: 9.156270295276424e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 67/71 | LOSS: 9.162541640334358e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 68/71 | LOSS: 9.202730035357872e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 69/71 | LOSS: 9.174184145064959e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 70/71 | LOSS: 9.147956884397657e-06\n",
      "VAL: EPOCH 99/100 | BATCH 0/8 | LOSS: 7.036775514279725e-06\n",
      "VAL: EPOCH 99/100 | BATCH 1/8 | LOSS: 7.625655371157336e-06\n",
      "VAL: EPOCH 99/100 | BATCH 2/8 | LOSS: 7.794348221068503e-06\n",
      "VAL: EPOCH 99/100 | BATCH 3/8 | LOSS: 7.4606376756491954e-06\n",
      "VAL: EPOCH 99/100 | BATCH 4/8 | LOSS: 7.427858417941025e-06\n",
      "VAL: EPOCH 99/100 | BATCH 5/8 | LOSS: 7.614229237636512e-06\n",
      "VAL: EPOCH 99/100 | BATCH 6/8 | LOSS: 7.717266726103844e-06\n",
      "VAL: EPOCH 99/100 | BATCH 7/8 | LOSS: 7.596615489546821e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 0/71 | LOSS: 5.70744623473729e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 1/71 | LOSS: 8.18348758002685e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 2/71 | LOSS: 8.521454068007491e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 3/71 | LOSS: 8.387554657929286e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 4/71 | LOSS: 8.432351660303538e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 5/71 | LOSS: 8.573605631075528e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 6/71 | LOSS: 8.62624210640206e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 7/71 | LOSS: 8.534969595075381e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 8/71 | LOSS: 8.751500445214333e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 9/71 | LOSS: 8.53081796776678e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 10/71 | LOSS: 8.563812488401625e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 11/71 | LOSS: 8.656386057737109e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 12/71 | LOSS: 8.526120487099084e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 13/71 | LOSS: 8.79046092450153e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 14/71 | LOSS: 8.792232074483764e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 15/71 | LOSS: 8.777817356531159e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 16/71 | LOSS: 8.94113696524394e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 17/71 | LOSS: 8.907353579464447e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 18/71 | LOSS: 8.915558139147164e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 19/71 | LOSS: 9.101498471864034e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 20/71 | LOSS: 9.266812932245167e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 21/71 | LOSS: 9.247679289811375e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 22/71 | LOSS: 9.240926809307775e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 23/71 | LOSS: 9.214028485378853e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 24/71 | LOSS: 9.18816789635457e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 25/71 | LOSS: 9.304740930159684e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 26/71 | LOSS: 9.335002148334212e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 27/71 | LOSS: 9.404517900942924e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 28/71 | LOSS: 9.478746286386641e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 29/71 | LOSS: 9.431526693030417e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 30/71 | LOSS: 9.376740367330175e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 31/71 | LOSS: 9.377510963304303e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 32/71 | LOSS: 9.387526915816125e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 33/71 | LOSS: 9.388575803955865e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 34/71 | LOSS: 9.41485489290374e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 35/71 | LOSS: 9.43915564574935e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 36/71 | LOSS: 9.546781527476284e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 37/71 | LOSS: 9.461762769397433e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 38/71 | LOSS: 9.442865983799446e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 39/71 | LOSS: 9.445188902645896e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 40/71 | LOSS: 9.402169802697465e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 41/71 | LOSS: 9.364170658781881e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 42/71 | LOSS: 9.399205060448325e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 43/71 | LOSS: 9.362843436313348e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 44/71 | LOSS: 9.353060866285685e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 45/71 | LOSS: 9.413606893877327e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 46/71 | LOSS: 9.417425412430795e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 47/71 | LOSS: 9.389346492601666e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 48/71 | LOSS: 9.368616998954724e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 49/71 | LOSS: 9.379442290082806e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 50/71 | LOSS: 9.43281900296878e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 51/71 | LOSS: 9.417988027192992e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 52/71 | LOSS: 9.392078284707958e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 53/71 | LOSS: 9.393622180882462e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 54/71 | LOSS: 9.396227158669551e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 55/71 | LOSS: 9.362550388557014e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 56/71 | LOSS: 9.332111212164531e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 57/71 | LOSS: 9.31623019917832e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 58/71 | LOSS: 9.318247343876234e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 59/71 | LOSS: 9.297399553058009e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 60/71 | LOSS: 9.284591274603163e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 61/71 | LOSS: 9.24548413099875e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 62/71 | LOSS: 9.203702170958019e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 63/71 | LOSS: 9.222238190886856e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 64/71 | LOSS: 9.201622294942634e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 65/71 | LOSS: 9.172503165576597e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 66/71 | LOSS: 9.138720366509141e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 67/71 | LOSS: 9.162995752376446e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 68/71 | LOSS: 9.164421887850251e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 69/71 | LOSS: 9.165621077045216e-06\n",
      "TRAIN: EPOCH 100/100 | BATCH 70/71 | LOSS: 9.118699714415183e-06\n",
      "VAL: EPOCH 100/100 | BATCH 0/8 | LOSS: 1.1295553122181445e-05\n",
      "VAL: EPOCH 100/100 | BATCH 1/8 | LOSS: 1.0950609976134729e-05\n",
      "VAL: EPOCH 100/100 | BATCH 2/8 | LOSS: 1.0565114886655161e-05\n",
      "VAL: EPOCH 100/100 | BATCH 3/8 | LOSS: 9.877038337435806e-06\n",
      "VAL: EPOCH 100/100 | BATCH 4/8 | LOSS: 9.63093298196327e-06\n",
      "VAL: EPOCH 100/100 | BATCH 5/8 | LOSS: 9.879055293519437e-06\n",
      "VAL: EPOCH 100/100 | BATCH 6/8 | LOSS: 9.937075057158446e-06\n",
      "VAL: EPOCH 100/100 | BATCH 7/8 | LOSS: 9.698107646727294e-06\n"
     ]
    }
   ],
   "source": [
    "trained_state = train_model(state, train_loader, val_loader, 100, hyperparams=hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
